ORCID,Author,Institution,Title,PubDate,Abstract,DOI,IDlist,flag,found_words
0009-0002-6642-6904,Florian Adamsky,Hochschule Hof,Smartphones in a Microwave: Formal and Experimental Feasibility Study on   Fingerprinting the Corona-Warn-App,2023,"  Contact Tracing Apps (CTAs) have been developed to contain the coronavirus disease 19 (COVID-19) spread. By design, such apps invade their users' privacy by recording data about their health, contacts, and partially location. Many CTAs frequently broadcast pseudorandom numbers via Bluetooth to detect encounters. These numbers are changed regularly to prevent individual smartphones from being trivially trackable. However, the effectiveness of this procedure has been little studied. We measured real smartphones and observed that the German Corona-Warn-App (CWA) exhibits a device-specific latency between two subsequent broadcasts. These timing differences provide a potential attack vector for fingerprinting smartphones by passively recording Bluetooth messages. This could conceivably lead to the tracking of users' trajectories and, ultimately, the re-identification of users. ",https://doi.org/10.1145/3600160.3605011,2307.02931v1,Yes,potent(1)
0000-0002-7740-1967,Marcus Wagner,OTH Regensburg,Optimal control of the bidomain system (IV): Corrected proofs of the   stability and regularity theorems,2014,"  In a series of papers on optimal control problems for the monodomain as well as for the bidomain equations of cardiac electrophysiology, the authors studied existence of minimizers and derived first-order necessary optimality conditions. The analysis of these control problems is based on a regularity discussion for weak solutions, resulting in a stability estimate and a uniqueness theorem for the monodomain and bidomain system, respectively. Unfortunately, the authors recognized a serious error within the proof of these theorems. However, the present investigation shows that the assertions from [Kunisch/Wagner 12] and [Kunisch/Wagner 13a] can be maintained (with minor changes only) while the proofs must be subjected to considerable alterations. As a consequence, the optimization theorems from [Kunisch/Wagner 13b] allow for substantial improvements. Therefore, in the present paper we provide a refined regularity discussion of the bidomain system together with corrected proofs. ",Kein DOI-Link verfügbar,1409.6904v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Sound Absorption and Dispersion in Dilute Polyatomic Gases: A   Generalized Kinetic Approach,2013,  A generalized kinetic model equation which takes into account the frequency depence of the thermal conductivity is used to analyze the problem of sound propagation in dilute polyatomic gases. By comparing the theoretical results with some available experimental data we infer that our model equation provides a precise transition between low and high-frequency limits. ,https://doi.org/10.1088/1742-5468/2013/08/P08004,1304.2480v3,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Pay More Attention - Neural Architectures for Question-Answering,2018,"  Machine comprehension is a representative task of natural language understanding. Typically, we are given context paragraph and the objective is to answer a question that depends on the context. Such a problem requires to model the complex interactions between the context paragraph and the question. Lately, attention mechanisms have been found to be quite successful at these tasks and in particular, attention mechanisms with attention flow from both context-to-question and question-to-context have been proven to be quite useful. In this paper, we study two state-of-the-art attention mechanisms called Bi-Directional Attention Flow (BiDAF) and Dynamic Co-Attention Network (DCN) and propose a hybrid scheme combining these two architectures that gives better overall performance. Moreover, we also suggest a new simpler attention mechanism that we call Double Cross Attention (DCA) that provides better results compared to both BiDAF and Co-Attention mechanisms while providing similar performance as the hybrid scheme. The objective of our paper is to focus particularly on the attention layer and to suggest improvements on that. Our experimental evaluations show that both our proposed models achieve superior results on the Stanford Question Answering Dataset (SQuAD) compared to BiDAF and DCN attention mechanisms. ",Kein DOI-Link verfügbar,1803.09230v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,On the shape of barchan dunes,2005,"  Barchans are crescent-shaped sand dunes forming in aride regions with unidirectional wind and limited sand supply. We report analytical and numerical results for dune shapes under different environmental conditions as obtained from the so-called `minimal model' of aeolian sand dunes. The profiles of longitudinal vertical slices (i.e. along the wind direction) are analyzed as a function of wind speed and sand supply. Shape transitions can be induced by changes of mass, wind speed and sand supply. Within a minimal extension of the model to the transverse direction the scale-invariant profile of transverse vertical cuts can be derived analytically. ",https://doi.org/10.1088/0953-8984/17/14/012,cond-mat/0501152v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Salt-Induced Counterion-Mobility Anomaly in Polyelectrolyte   Electrophoresis,2009,"  We study the electrokinetics of a single polyelectrolyte chain in salt solution using hydrodynamic simulations. The salt-dependent chain mobility compares well with experimental DNA data. The mobility of condensed counterions exhibits a salt-dependent change of sign, an anomaly that is also reflected in the counterion excess conductivity. Using Green's function techniques this anomaly is explained by electrostatic screening of the hydrodynamic interactions between chain and counterions. ",https://doi.org/10.1103/PhysRevLett.101.176103,0905.3853v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,The central kpc of edge-on AGN,2013,"  The NIR is less influenced by dust extinction than optical light. This enables us to look to an extent through dusty regions. In addition, it is sensitive to the mass-dominating stellar population. The combination of NIR imaging and spectroscopy of the VLT integral field spectrograph SINFONI gives us the opportunity to analyze several emission and absorption lines and to investigate the stellar population and ionization mechanisms over a field of view (FOV) of 4x4 square arcsec. We detect several emission lines ([Si VI], Pa\alpha, Br\gamma, H_2, [Fe II]) and analyze these to disentangle the ionization mechanisms and the kinematics of the gas. We use the absorption features of Si I, CO(6-3) and CO(2-0) and the large wavelength range of H+K-band for a thorough continuum synthesis. We fit a stellar, dust and power-law contribution together with an extinction component to the H+K-band emission. The result presents regions dominated by the different contributors and extinction information over the full FOV. ",Kein DOI-Link verfügbar,1305.4938v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,GPTopic: Dynamic and Interactive Topic Representations,2024,"  Topic modeling seems to be almost synonymous with generating lists of top words to represent topics within large text corpora. However, deducing a topic from such list of individual terms can require substantial expertise and experience, making topic modelling less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A topic representation limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a topic might have. To address these challenges, we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations. GPTopic provides an intuitive chat interface for users to explore, analyze, and refine topics interactively, making topic modeling more accessible and comprehensive. The corresponding code is available here: https://github.com/ArikReuter/TopicGPT. ",Kein DOI-Link verfügbar,2403.03628v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Monitoring of tritium purity during long-term circulation in the KATRIN   test experiment LOOPINO using laser Raman spectroscopy,2012,"  The gas circulation loop LOOPINO has been set up and commissioned at Tritium Laboratory Karlsruhe (TLK) to perform Raman measurements of circulating tritium mixtures under conditions similar to the inner loop system of the neutrino-mass experiment KATRIN, which is currently under construction. A custom-made interface is used to connect the tritium containing measurement cell, located inside a glove box, with the Raman setup standing on the outside. A tritium sample (purity > 95%, 20 kPa total pressure) was circulated in LOOPINO for more than three weeks with a total throughput of 770 g of tritium. Compositional changes in the sample and the formation of tritiated and deuterated methanes CT_(4-n)X_n (X=H,D; n=0,1) were observed. Both effects are caused by hydrogen isotope exchange reactions and gas-wall interactions, due to tritium {\beta} decay. A precision of 0.1% was achieved for the monitoring of the T_2 Q_1-branch, which fulfills the requirements for the KATRIN experiment and demonstrates the feasibility of high-precision Raman measurements with tritium inside a glove box. ",Kein DOI-Link verfügbar,1208.1605v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,The nuclear radio structure of X-ray bright AGN,2008,"  The physical nature of the X-ray/radio correlation of AGN is still an unsolved question. High angular resolution observations are necessary to disentangle the associated energy dynamics into nuclear and stellar components. We present MERLIN/EVN 18cm observations of 13 X-raying AGN. The sample consists of Seyfert 1, Narrow Line Seyfert 1, and LINER-like galaxies. We find that for all objects the radio emission is unresolved and that the radio luminosities and brightness temperatures are too high for star formation to play an important role. This indicates that the radio emission in these sources is closely connected to processes that occur in the vicinity of the central massive black hole, also where the X-ray emission is believed to originate in. ",https://doi.org/10.1088/1742-6596/131/1/012042,0806.3947v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Compact radio emission from z~0.2 X-ray bright AGN,2012,"  Radio and X-ray emission of AGN appears to be correlated. The details of the underlying physical processes, however, are still not fully understood, i.e., to what extent is the X-ray and radio emission originating from the same relativistic particles or from the accretion-disk or corona or both. We study the cm radio emission of an SDSS/ROSAT/FIRST matched sample of 13 X-raying AGN in the redshift range 0.11< z < 0.37 at high angular resolution with the goal of searching for jet structures or diffuse, extended emission on sub-kpc scales. We use MERLIN at 18 cm for all objects and Western EVN at 18 cm for four objects to study the radio emission on scales of ~500 pc and ~40 pc for the MERLIN and EVN observations, respectively. The detected emission is dominated by compact nuclear radio structures. We find no kpc collimated jet structures. The EVN data indicate for compact nuclei on 40 pc scales, with brightness temperatures typical for accretion-disk scenarios. Comparison with FIRST shows that the 18 cm emission is resolved out up to 50% by MERLIN. Star-formation rates based on large aperture SDSS spectra are generally too small to produce considerable contamination of the nuclear radio emission. We can, therefore, assume the 18 cm flux densities to be produced in the nuclei of the AGN. Together with the ROSAT soft X-ray luminosities and black hole mass estimates from the literature, our sample objects follow closely the Merloni et al. (2003) fundamental plane relation, which appears to trace the accretion processes. Detailed X-ray spectral modeling from deeper hard X-ray observations and higher angular resolution at radio wavelengths are required to further proceed in the disentangling of jet and accretion related processes. ",https://doi.org/10.1051/0004-6361/201118200,1204.3162v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,A low-redshift low luminosity QSO sample: Comparison with NUGA galaxies   and PG QSOs and first interferometric images of three sample members,2013,"  The low luminosity QSO (LLQSO) sample consists of type 1 active galactic nuclei (AGN) up to a redshift of z=0.06 in the Hamburg/ESO QSO survey. Its purpose is to study how the brightest AGN in the nearby universe evolve with respect to AGN activity and host properties as a function of redshift. We show that our sample lies well between the NUclei of GAlaxies (NUGA) sample and the Palomar Green (PG) QSO sample in terms of redshift, gas masses and luminosities and seems to connect them. The continuous growth in mass, luminosity and, linked to this, the AGN activity over the samples has either a statistical reason or is indicative of an evolutionary link between the different populations and might be related to cosmic downsizing. In addition, we present first results of our observations of three galaxies from our sample with the Submillimeter Array (SMA). ",Kein DOI-Link verfügbar,1309.6921v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,BENN: Bias Estimation Using Deep Neural Network,2020,"  The need to detect bias in machine learning (ML) models has led to the development of multiple bias detection methods, yet utilizing them is challenging since each method: i) explores a different ethical aspect of bias, which may result in contradictory output among the different methods, ii) provides an output of a different range/scale and therefore, can't be compared with other methods, and iii) requires different input, and therefore a human expert needs to be involved to adjust each method according to the examined model. In this paper, we present BENN -- a novel bias estimation method that uses a pretrained unsupervised deep neural network. Given a ML model and data samples, BENN provides a bias estimation for every feature based on the model's predictions. We evaluated BENN using three benchmark datasets and one proprietary churn prediction model used by a European Telco and compared it with an ensemble of 21 existing bias estimation methods. Evaluation results highlight the significant advantages of BENN over the ensemble, as it is generic (i.e., can be applied to any ML model) and there is no need for a domain expert, yet it provides bias estimations that are aligned with those of the ensemble. ",Kein DOI-Link verfügbar,2012.12537v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,What produces the extended LINER-type emission in the NUGA galaxy NGC   5850?,2013,"  (Abridged) The role of low ionization nuclear emission region (LINER) galaxies within the picture of active galactic nuclei (AGN) has been controversial. It is still not clear whether they host an AGN in a low accretion mode, or whether they are not active at all but dominated by alternative ionization mechanisms, namely shocks, winds/outflows, or photoionization by a post asymptotic giant branch (p-AGB) stellar population. The detection of extended LINER-like emission was often taken as evidence of ionization by stellar components but this has not been undisputed. We performed optical integral field spectroscopic observations on the central approx. 4 kpc of NGC 5850 using VIMOS at the VLT, which provides spatially-resolved spectra for the gas emission and the stellar continuum. We derive and analyse emission line and kinematic maps. We find the central few kpc of NGC 5850 to be dominated by extended LINER-like emission. The emission-line ratios that are sensitive to the ionization parameter increase with radial distance to the nucleus. Therefore, the extended LINER-like emission in NGC 5850 is dominated by ionization from distributed ionization sources, probably by stars on the p-AGB. The LINER-like region is surrounded by emission that is classed as 'composite', likely due to a mixture of a LINER-like ionization pattern and photoionization by low-level star formation. Two star-forming regions are present in the 21""x19"" field of view. One of them is located approximately in the ring surrounding the kinematically decoupled core. The second one is close to the nucleus and is the origin of a region of decreased emission line ratios oriented radially outwards. We find the interstellar gas to have areas of steep velocity gradients and a complex kinematic morphology, probably caused by the lopsided (m=1) distribution of the gas. The inflow of gas toward the center appears possible. ",https://doi.org/10.1051/0004-6361/201322009,1308.3492v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Molecular gas in the immediate vicinity of Sgr A* seen with ALMA,2017,"  We report serendipitous detections of line emission with ALMA in band 3, 6, and 7 in the central parsec of the Galactic center at an up to now highest resolution (<0.7''). Among the highlights are the very first and highly resolved images of sub-mm molecular emission of CS, H13CO+, HC3N, SiO, SO, C2H, and CH3OH in the immediate vicinity (~1'' in projection) of Sgr A* and in the circumnuclear disk (CND). The central association (CA) of molecular clouds shows three times higher CS/X (X: any other observed molecule) luminosity ratios than the CND suggesting a combination of higher excitation - by a temperature gradient and/or IR-pumping - and abundance enhancement due to UV- and/or X-ray emission. We conclude that the CA is closer to the center than the CND is and could be an infalling clump consisting of denser cloud cores embedded in diffuse gas. Moreover, we identified further regions in and outside the CND that are ideally suited for future studies in the scope of hot/cold core and extreme PDR/XDR chemistry and consequent star formation in the central few parsecs. ",https://doi.org/10.1017/S1743921316012126,1707.02951v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Ab Initio No Core Shell Model - Recent Results and Further Prospects,2015,  There has been significant recent progress in solving the long-standing problems of how nuclear shell structure and collective motion emerge from underlying microscopic inter-nucleon interactions. We review a selection of recent significant results within the ab initio No Core Shell Model (NCSM) closely tied to three major factors enabling this progress: (1) improved nuclear interactions that accurately describe the experimental two-nucleon and three-nucleon interaction data; (2) advances in algorithms to simulate the quantum many-body problem with strong interactions; and (3) continued rapid development of high-performance computers now capable of performing $20 \times 10^{15}$ floating point operations per second. We also comment on prospects for further developments. ,Kein DOI-Link verfügbar,1507.04693v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Probing the physics of narrow-line regions of Seyfert galaxies I: The   case of NGC 5427,2014,"  We have used the Wide Field Spectrograph (WiFeS) on the ANU 2.3m telescope at Siding Spring to observe the nearby, nearly face-on, Seyfert 2 galaxy, NGC 5427. We have obtained integral field spectroscopy of both the nuclear regions and the HII regions in the spiral arms. We have constrained the chemical abundance in the interstellar medium of the extended narrow line region (ENLR) by measuring the abundance gradient in the circum-nuclear \ion{H}{ii} regions to determine the nuclear chemical abundances, and to use these to in turn determine the EUV spectral energy distribution for comparison with theoretical models. We find a very high nuclear abundance, $\sim 3.0$ times solar, with clear evidence of a nuclear enhancement of N and He, possibly caused by massive star formation in the extended ($\sim 100$pc) central disk structure. The circum-nuclear narrow-line region spectrum is fit by a radiation pressure dominated photoionisation model model with an input EUV spectrum from a Black Hole with mass $5\times10^7 M_{\odot}$ radiating at $\sim 0.1$ of its Eddington luminosity. The bolometric luminosity is closely constrained to be $\log L_{\mathrm bol.} = 44.3\pm 0.1$ erg s$^{-1}$. The EUV spectrum characterised by a soft accretion disk and a harder component extending to above 15keV. The ENLR region is extended in the NW-SE direction. The line ratio variation in circum-nuclear spaxels can be understood as the result of mixing \ion{H}{ii} regions with an ENLR having a radius-invariant spectrum. ",https://doi.org/10.1051/0004-6361/201423467,1404.3369v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,ALMA backed NIR high resolution integral field spectroscopy of the NUGA   galaxy NGC 1433,2014,"  We present the results of near-infrared (NIR) H- and K-band European Southern Observatory SINFONI integral field spectroscopy (IFS) of the Seyfert 2 galaxy NGC 1433. We present emission and absorption line measurements in the central kpc of NGC 1433. We detect a narrow Balmer line and several H2 lines. We find that the stellar continuum peaks in the optical and NIR in the same position, indicating that there is no covering of the center by a nuclear dust lane. A strong velocity gradient is detected in all emission lines at that position. The position angle of this gradient is at 155\deg whereas the galactic rotation is at a position angle of 201\deg. Our measures of the molecular hydrogen lines, hydrogen recombination lines, and [Feii] indicate that the excitation at the nucleus is caused by thermal excitation, i.e. shocks which can be associated with active galactic nuclei emission, supernovae or outflows. The line ratios [Feii]/Pa{\beta} and H2/Br{\gamma} show a Seyfert to LINER identification of the nucleus. The stellar continuum is dominated by spectral signatures of red-giant M stars. The stellar line-of-sight velocity follows the galactic field whereas the light continuum follows the nuclear bar. The dynamical center of NGC 1433 coincides with the optical and NIR center of the galaxy and the black hole position. Within the central arcsecond, the molecular hydrogen and the 12CO(3-2) emissions - observed in the NIR and in the sub-millimeter with SINFONI and ALMA, respectively - are indicative for a nuclear outflow originating from the galaxy's SMBH. A small circum nuclear disk cannot be fully excluded. Derived gravitational torques show that the nuclear bar is able to drive gas inwards to scales where viscosity torques and dynamical friction become important. The black hole mass derived using stellar velocity dispersion is 10^7 M_sun. ",https://doi.org/10.1051/0004-6361/201423642,1404.6562v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,Approaching hell's kitchen: Molecular daredevil clouds in the vicinity   of Sgr A*,2016,"  We report serendipitous detections of line emission with the Atacama Large Millimeter/submillimeter Array (ALMA) in band 3, 6, and 7 in the central parsec down to within 1"" around Sgr A* at an up to now highest resolution (<0.5"") view of the Galactic Center (GC) in the sub-millimeter (sub-mm) domain. From the 100 GHz continuum and the H39\alpha emission we obtain a uniform electron temperature around 6000 K for the minispiral. The spectral index of Sgr A* is ~ 0.5 at 100 - 250 GHz and ~ 0.0 at 230 - 340 GHz. The bright sources in the center show spectral indices around -0.1 implying Bremsstrahlung emission, while dust emission is emerging in the minispiral exterior. Apart from CS, which is most widespread in the center, H13CO+, HC3N, SiO, SO, C2H, CH3OH, 13CS and N2H+ are also detected. The bulk of the clumpy emission regions is at positive velocities and in a region confined by the minispiral northern arm, bar and the sources IRS 3 and 7. Although partly spatially overlapping with the radio recombination line (RRL) emission at same negative velocities, the relation to the minispiral remains unclear. A likely explanation is an infalling clump consisting of denser cloud cores embedded in diffuse gas. The central association of clouds (CA) shows three times higher CS/X (X: any other observed molecule) ratios than the circumnuclear disk (CND) suggesting a combination of higher excitation, by a temperature gradient and/or IR-pumping, and abundance enhancement due to UV- and/or X-ray emission. Hence, we conclude that this CA is closer to the center than the CND is to the center. Moreover, we find molecular emission at velocities up to 200 km s-1. ... ",https://doi.org/10.1051/0004-6361/201628385,1603.00801v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,A low-luminosity type-1 QSO sample: IV. Molecular gas contents and   conditions of star formation in three nearby Seyfert galaxies,2017,"  We present a pilot study of ~ 3"" resolution observations of low CO transitions with the Submillimeter Array in three nearby Seyfert galaxies, which are part of the low-luminosity quasi-stellar object (LLQSOs) sample consisting of 99 nearby (z = 0.06) type-1 active galactic nuclei (AGN) taken from the Hamburg/ESO quasi-stellar object (QSO) survey. Two sources were observed in 12CO(2-1) and 13CO(2-1) and the third in 12CO(3-2) and HCO+(4-3). None of the sources is detected in continuum emission. More than 80% of the 12CO detected molecular gas is concentrated within a diameter (FWHM) < 1.8 kpc. 13CO is tentatively detected, while HCO+ emission could not be detected. All three objects show indications of a kinematically decoupled central unresolved molecular gas component. The molecular gas masses of the three galaxies are in the range M_mol = (0.7 - 8.7) x 10^9 M_sun. We give lower limits for the dynamical masses of M_dyn > 1.5 x 10^9 M_sun and for the dust masses of M_dust > 1.6 x 10^6 M_sun. The R21 =12CO/13CO(2-1) line luminosity ratios show Galactic values of R21 ~ 5 - 7 in the outskirts and R21 > 20 in the central region, similar to starbursts and (ultra)luminous infrared galaxies ((U)LIRGs; i.e. LIRGs and ULIRGs), implying higher temperatures and stronger turbulence. All three sources show indications of 12CO(2-1)/12CO(1-0) ratios of ~ 0.5, suggesting a cold or diffuse gas phase. Strikingly, the 12CO(3-2)/(1-0) ratio of ~ 1 also indicates a higher excited phase. Since these galaxies have high infrared luminosities of L_IR > 10^11 L_sun and seem to contain a circumnuclear starburst with minimum surface densities of gas and star formation rate (SFR) around {\Sigma}_mol = 50 - 550 M_sun pc^-2 and {\Sigma}_SFR = 1.1 - 3.1 M_sun kpc^-2 yr^-1, we conclude that the interstellar medium in the centers of these LIRG Seyferts is strongly affected by violent star formation and better ... ",https://doi.org/10.1051/0004-6361/201526358,1707.01081v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,A low-luminosity type-1 QSO sample: I. Overluminous host spheroidals or   undermassive black holes?,2013,"  Recognizing the properties of the host galaxies of quasi-stellar objects (QSOs) is essential to understand the suspected coevolution of central supermassive black holes (BHs) and their host galaxies. We selected a subsample of the Hamburg/ESO survey for bright UV-excess QSOs, containing only the 99 nearest QSOs with redshift z<=0.06, that are close enough to allow detailed structural analysis. From this ""low-luminosity type-1 QSO sample"", we observed 20 galaxies and performed aperture photometry and bulge-disk-bar-AGN-decomposition with BUDDA on near-infrared J, H, K band images. From the photometric decomposition of these 20 objects and visual inspection of images of another 26, we find that ~50% of the hosts are disk galaxies and most of them (86%) are barred. Stellar masses, calculated from parametric models based on inactive galaxy colors, range from 2x10^9 M_sun to 2x10^11 M_sun. Black hole masses measured from single epoch spectroscopy range from 1x10^6 M_sun to 5x10^8 M_sun. In comparison to higher luminosity QSO samples, LLQSOs tend to have lower stellar and BH masses. Also, in the effective radius vs. mean surface-brightness projection of the fundamental plane, they lie in the transition area between luminous QSOs and ""normal"" galaxies. This can be seen as further evidence that they can be pictured as a ""bridge"" between the local Seyfert population and luminous QSOs at higher redshift. Eleven low-luminosity QSOs for which we have reliable morphological decompositions and BH mass estimations lie below the published BH mass vs. bulge luminosity relations for inactive galaxies. This could be partially explained by bulges of active galaxies containing much younger stellar populations than bulges of inactive galaxies. Also, one could suspect that their BHs are undermassive. This might hint at the growth of the host spheroid to precede that of the BH. ",https://doi.org/10.1051/0004-6361/201322486,1310.0272v2,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,A low-luminosity type-1 QSO sample. V. Overluminous host spheroids and   their excitation mechanisms,2015,"  We present near-infrared (NIR) $H+K$-band longslit spectra of eleven galaxies which are obtained with SOFI at the NTT (ESO). The galaxies are chosen from the low-luminosity type-1 quasi-stellar object (LLQSO) sample which comprises the 99 closest ($z\leq 0.06$) QSOs from the Hamburg/ESO survey for bright UV-excess QSOs. These objects are ideal targets to study the gap between local Seyfert galaxies and high-redshift quasars, since they show much stronger AGN activity compared to local objects but are still close enough for a detailed structural analysis.   We fit hydrogen recombination, molecular hydrogen, and [FeII] lines after carefully subtracting the continuum emission. From the broad Pa$\alpha$ components, we estimate black hole masses and enlarge the sample of LLQSOs that show a deviation from the $M_\mathrm{BH}-L_\mathrm{bulge}$ relations of inactive galaxies from 12 to 16 objects.   All objects show emission from hot dust ($T\sim 1200\,\mathrm{K}$) as well as stellar contribution. However, the particular fractions vary a lot between the objects. More than half of the objects show H$_2$ emission lines that are indicating a large reservoir of molecular gas which is needed to feed the AGN and star formation.   In the NIR diagnostic diagram all objects lie in the location of AGN dominated objects. However, most of the objects show indications of star formation activity, suggesting that their offset location with respect to $M_\mathrm{BH}-L_\mathrm{bulge}$ relations of inactive galaxies may be a consequence of overluminous bulges. ",https://doi.org/10.1051/0004-6361/201526753,1511.00904v1,No,
0000-0003-3147-9125,Sebastian Fischer,Regensburg Universität der Angewandte Wissenschaften,"Go-Smart: Open-Ended, Web-Based Modelling of Minimally Invasive Cancer   Treatments via a Clinical Domain Approach",2018,"  Clinicians benefit from online treatment planning systems, through off-site accessibility, data sharing and professional interaction. As well as enhancing clinical value, incorporation of simulation tools affords innovative avenues for open-ended, multi-disciplinary research collaboration. An extensible system for clinicians, technicians, manufacturers and researchers to build on a simulation framework is presented. This is achieved using a domain model that relates entities from theoretical, engineering and clinical domains, allowing algorithmic generation of simulation configuration for several open source solvers.   The platform is applied to Minimally Invasive Cancer Treatments (MICTs), allowing interventional radiologists to upload patient data, segment patient images and validate simulated treatments of radiofrequency ablation, cryoablation, microwave ablation and irreversible electroporation. A traditional radiology software layout is provided in-browser for clinical use, with simple, guided simulation, primarily for training and research. Developers and manufacturers access a web-based system to manage their own simulation components (equipment, numerical models and clinical protocols) and related parameters.   This system is tested by interventional radiologists at four centres, using pseudonymized patient data, as part of the Go-Smart Project (http://gosmart-project.eu). The simulation technology is released as a set of open source components http://github.com/go-smart. ",Kein DOI-Link verfügbar,1803.09166v1,Yes,innovative(1)
0000-0002-1440-1652,Michael Sterner,Regensburg Universität der Angewandte Wissenschaften,The impact of temporal hydrogen regulation on hydrogen exporters and   their domestic energy transition,2024,"  As global demand for green hydrogen rises, potential hydrogen exporters move into the spotlight. However, the large-scale installation of on-grid hydrogen electrolysis for export can have profound impacts on domestic energy prices and energy-related emissions. Our investigation explores the interplay of hydrogen exports, domestic energy transition and temporal hydrogen regulation, employing a sector-coupled energy model in Morocco. We find substantial co-benets of domestic climate change mitigation and hydrogen exports, whereby exports can reduce domestic electricity prices while mitigation reduces hydrogen export prices. However, increasing hydrogen exports quickly in a system that is still dominated by fossil fuels can substantially raise domestic electricity prices, if green hydrogen production is not regulated. Surprisingly, temporal matching of hydrogen production lowers domestic electricity cost by up to 31% while the effect on exporters is minimal. This policy instrument can steer the welfare (re-)distribution between hydrogen exporting firms, hydrogen importers, and domestic electricity consumers and hereby increases acceptance among actors. ",Kein DOI-Link verfügbar,2405.14717v1,Yes,potent(1)
0000-0002-7870-0504,Philipp Rumschinski,Hochschule Furtwangen Fakultät Mechanical and Medical Engineering,Estimation of consistent parameter sets for continuous-time nonlinear   systems using occupation measures and LMI relaxations,2013,"  Obtaining initial conditions and parameterizations leading to a model consistent with available measurements or safety specifications is important for many applications. Examples include model (in-)validation, prediction, fault diagnosis, and controller design. We present an approach to determine inner- and outer-approximations of the set containing all consistent initial conditions/parameterizations for nonlinear continuous-time systems. These approximations are found by occupation measures that encode the system dynamics and measurements, and give rise to an infinite-dimensional linear program. We exploit the flexibility and linearity of the decision problem to incorporate uncertain-but-bounded and pointwise-in-time state and output constraints, a feature which was not addressed in previous works. The infinite-dimensional linear program is relaxed by a hierarchy of LMI problems that provide certificates in case no consistent initial condition/parameterization exists. Furthermore, the applied LMI relaxation guarantees that the approximations converge (almost uniformly) to the true consistent set. We illustrate the approach with a biochemical reaction network involving unknown initial conditions and parameters. ",Kein DOI-Link verfügbar,1303.4615v1,No,
0000-0002-1492-1121,Herag Arabian,"Hochschule Furtwangen, Institute der Technical Medicine, Furtwangen Universität",Surgical tool classification and localization: results and methods from   the MICCAI 2022 SurgToolLoc challenge,2023,"  The ability to automatically detect and track surgical instruments in endoscopic videos can enable transformational interventions. Assessing surgical performance and efficiency, identifying skilled tool use and choreography, and planning operational and logistical aspects of OR resources are just a few of the applications that could benefit. Unfortunately, obtaining the annotations needed to train machine learning models to identify and localize surgical tools is a difficult task. Annotating bounding boxes frame-by-frame is tedious and time-consuming, yet large amounts of data with a wide variety of surgical tools and surgeries must be captured for robust training. Moreover, ongoing annotator training is needed to stay up to date with surgical instrument innovation. In robotic-assisted surgery, however, potentially informative data like timestamps of instrument installation and removal can be programmatically harvested. The ability to rely on tool installation data alone would significantly reduce the workload to train robust tool-tracking models. With this motivation in mind we invited the surgical data science community to participate in the challenge, SurgToolLoc 2022. The goal was to leverage tool presence data as weak labels for machine learning models trained to detect tools and localize them in video frames with bounding boxes. We present the results of this challenge along with many of the team's efforts. We conclude by discussing these results in the broader context of machine learning and surgical data science. The training data used for this challenge consisting of 24,695 video clips with tool presence labels is also being released publicly and can be accessed at https://console.cloud.google.com/storage/browser/isi-surgtoolloc-2022. ",Kein DOI-Link verfügbar,2305.07152v2,Yes,potent(1)
0009-0001-8588-7151,Maja Temerinac-Ott,Furtwangen Universität,Deciding when to stop: Efficient stopping of active learning guided   drug-target prediction,2015,"  Active learning has shown to reduce the number of experiments needed to obtain high-confidence drug-target predictions. However, in order to actually save experiments using active learning, it is crucial to have a method to evaluate the quality of the current prediction and decide when to stop the experimentation process. Only by applying reliable stoping criteria to active learning, time and costs in the experimental process can be actually saved. We compute active learning traces on simulated drug-target matrices in order to learn a regression model for the accuracy of the active learner. By analyzing the performance of the regression model on simulated data, we design stopping criteria for previously unseen experimental matrices. We demonstrate on four previously characterized drug effect data sets that applying the stopping criteria can result in upto 40% savings of the total experiments for highly accurate predictions. ",Kein DOI-Link verfügbar,1504.02406v1,No,
0000-0003-1234-2211,Felix Blendinger,Furtwangen Universität,Tunable strong coupling of two adjacent optical λ/2 Fabry-Pérot   microresonators,2019,"  Optical half-wave microresonators enable to control the optical mode density around a quantum system and thus to modify the temporal emission properties. If the coupling rate exceeds the damping rate, strong coupling between a microresonator and a quantum system can be achieved, leading to a coherent energy exchange and the creation of new hybrid modes. Here, we investigate strong coupling between two adjacent lambda/2 Fabry-P\'erot microresonators, where the resonance of one microresonator can be actively tuned across the resonance of the other microresonator. The transmission spectra of the coupled microresonators show a clear anticrossing behavior, which proves that the two cavity modes are strongly coupled. Additionally, we can vary the coupling rate by changing the resonator geometry and thereby investigate the basic principles of strong coupling with a well-defined model system. Finally, we will show that such a coupled system can theoretically be modelled by coupled damped harmonic oscillators. ",https://doi.org/10.1364/OE.380068,1908.01566v1,No,
0000-0002-4339-0438,Bernhard Bachmann,Hochschule Bielefeld,On the choice of initial guesses for the Newton-Raphson algorithm,2019,"  The initialization of equation-based differential-algebraic system models, and more in general the solution of many engineering and scientific problems, require the solution of systems of nonlinear equations. Newton-Raphson's method is widely used for this purpose; it is very efficient in the computation of the solution if the initial guess is close enough to it, but it can fail otherwise. In this paper, several criteria are introduced to analyze the influence of the initial guess on the evolution of Newton-Raphson's algorithm and to identify which initial guesses need to be improved in case of convergence failure. In particular, indicators based on first and second derivatives of the residual function are introduced, whose values allow to assess how much the initial guess of each variable can be responsible for the convergence failure. The use of such criteria, which are based on rigorously proven results, is successfully demonstrated in three exemplary test cases. ",https://doi.org/10.1016/j.amc.2021.125991,1911.12433v3,No,
0000-0002-4339-0438,Bernhard Bachmann,Hochschule Bielefeld,Index reduction of differential algebraic equations by differential   algebraic elimination,2015,"  High index differential algebraic equations (DAEs) are ordinary differential equations (ODEs) with constraints and arise frequently from many mathematical models of physical phenomenons and engineering fields. In this paper, we generalize the idea of differential elimination with Dixon resultant to polynomially nonlinear DAEs. We propose a new algorithm for index reduction of DAEs and establish the notion of differential algebraic elimination, which can provide the differential algebraic resultant of the enlarged system of original equations. To make use of structure of DAEs, variable pencil technique is given to determine the termination of differentiation. Moreover, we also provide a heuristics method for removing the extraneous factors from differential algebraic resultant. The experimentation shows that the proposed algorithm outperforms existing ones for many examples taken from the literature. ",Kein DOI-Link verfügbar,1504.04977v1,No,
0000-0002-4339-0438,Bernhard Bachmann,Hochschule Bielefeld,Efficient algorithm for computing large scale systems of differential   algebraic equations,2015,"  In many mathematical models of physical phenomenons and engineering fields, such as electrical circuits or mechanical multibody systems, which generate the differential algebraic equations (DAEs) systems naturally. In general, the feature of DAEs is a sparse large scale system of fully nonlinear and high index. To make use of its sparsity, this paper provides a simple and efficient algorithm for computing the large scale DAEs system. We exploit the shortest augmenting path algorithm for finding maximum value transversal (MVT) as well as block triangular forms (BTF). We also present the extended signature matrix method with the block fixed point iteration and its complexity results. Furthermore, a range of nontrivial problems are demonstrated by our algorithm. ",Kein DOI-Link verfügbar,1506.03963v1,No,
0000-0002-4339-0438,Bernhard Bachmann,Hochschule Bielefeld,Multi-rate Runge-Kutta methods: stability analysis and applications,2024,"  We present an approach for the efficient implementation of self-adjusting multi-rate Runge-Kutta methods and we extend the previously available stability analyses of these methods to the case of an arbitrary number of sub-steps for the active components. We propose a physically motivated model problem that can be used to assess the stability of different multi-rate versions of standard Runge-Kutta methods and the impact of different interpolation methods for the latent variables. Finally, we present the results of several numerical experiments, performed with implementations of the proposed methods in the framework of the \textit{OpenModelica} open-source modelling and simulation software, which demonstrate the efficiency gains deriving from the use of the proposed multi-rate approach for physical modelling problems with multiple time scales. ",Kein DOI-Link verfügbar,2405.02139v2,No,
0009-0005-6858-9813,Sergej Schultenkämper,Hochschule Bielefeld,"WisPerMed at ""Discharge Me!"": Advancing Text Generation in Healthcare   with Large Language Models, Dynamic Expert Selection, and Priming Techniques   on MIMIC-IV",2024,"  This study aims to leverage state of the art language models to automate generating the ""Brief Hospital Course"" and ""Discharge Instructions"" sections of Discharge Summaries from the MIMIC-IV dataset, reducing clinicians' administrative workload. We investigate how automation can improve documentation accuracy, alleviate clinician burnout, and enhance operational efficacy in healthcare facilities. This research was conducted within our participation in the Shared Task Discharge Me! at BioNLP @ ACL 2024. Various strategies were employed, including few-shot learning, instruction tuning, and Dynamic Expert Selection (DES), to develop models capable of generating the required text sections. Notably, utilizing an additional clinical domain-specific dataset demonstrated substantial potential to enhance clinical language processing. The DES method, which optimizes the selection of text outputs from multiple predictions, proved to be especially effective. It achieved the highest overall score of 0.332 in the competition, surpassing single-model outputs. This finding suggests that advanced deep learning methods in combination with DES can effectively automate parts of electronic health record documentation. These advancements could enhance patient care by freeing clinician time for patient interactions. The integration of text selection strategies represents a promising avenue for further research. ",Kein DOI-Link verfügbar,2405.11255v1,Yes,potent(1)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld Universität der Angewandte Wissenschaften,Nonclassical Attack on a Quantum KeyDistribution System,2021,"  The article is focused on research of an attack on the quantum key distribution system and proposes a countermeasure method. Particularly noteworthy is that this is not a classic attack on a quantum protocol. We describe an attack on the process of calibration. Results of the research show that quantum key distribution systems have vulnerabilities not only in the protocols, but also in other vital system components. The described type of attack does not affect the cryptographic strength of the received keys and does not point to the vulnerability of the quantum key distribution protocol. We also propose a method for autocompensating optical communication system development, which protects synchronization from unauthorized access. The proposed method is based on the use of sync pulses attenuated to a photon level in the process of detecting a time interval with a signal. The paper presents the results of experimental studies that show the discrepancies between the theoretical and real parameters of the system. The obtained data allow the length of the quantum channel to be calculated with high accuracy. ",https://doi.org/10.3390/e23050509,2104.13720v1,Yes,noteworthy(1)
0000-0003-1771-407X,Anant Patel,Bielefeld Universität der Angewandte Wissenschaften and Arts,BlockChain and Decentralized Apps,2023,"  Blockchain, the backbone of Bitcoin, has recently gained a lot of attention. Blockchain functions as an immutable record that enables decentralized transactions. Blockchain-based applications are sprouting up in a variety of industries, including financial services, reputation systems, and the Internet of Things (IoT), among others. However, many hurdles of blockchain technology, including scalability and security issues, have to be overcome. Many industries, including finance, medicine, manufacturing, and education, use blockchain applications to capitalize on this technology's unique set of properties. Blockchain technology (BT) has the potential to improve trustworthiness, collaboration, organization, identity, credibility, and transparency. We provide an overview of blockchain architecture, various different kinds of blockchain as well as information about the Decentralized apps which are also known as Dapps. This paper provides an in-depth look at blockchain technology ",Kein DOI-Link verfügbar,2303.12536v1,Yes,potent(1)
0000-0001-9143-2121,Maximilian Wolf,Hochschule Albstadt-Sigmaringen,Systematic Evaluation of Synthetic Data Augmentation for Multi-class   NetFlow Traffic,2024,"  The detection of cyber-attacks in computer networks is a crucial and ongoing research challenge. Machine learning-based attack classification offers a promising solution, as these models can be continuously updated with new data, enhancing the effectiveness of network intrusion detection systems (NIDS). Unlike binary classification models that simply indicate the presence of an attack, multi-class models can identify specific types of attacks, allowing for more targeted and effective incident responses. However, a significant drawback of these classification models is their sensitivity to imbalanced training data. Recent advances suggest that generative models can assist in data augmentation, claiming to offer superior solutions for imbalanced datasets. Classical balancing methods, although less novel, also provide potential remedies for this issue. Despite these claims, a comprehensive comparison of these methods within the NIDS domain is lacking. Most existing studies focus narrowly on individual methods, making it difficult to compare results due to varying experimental setups. To close this gap, we designed a systematic framework to compare classical and generative resampling methods for class balancing across multiple popular classification models in the NIDS domain, evaluated on several NIDS benchmark datasets. Our experiments indicate that resampling methods for balancing training data do not reliably improve classification performance. Although some instances show performance improvements, the majority of results indicate decreased performance, with no consistent trend in favor of a specific resampling technique enhancing a particular classifier. ",Kein DOI-Link verfügbar,2408.16034v1,Yes,potent(1)
0000-0003-4143-6452,Karsten Köhler,Hochschule Albstadt-Sigmaringen,Dynamics of Natural Killer cell receptor revealed by quantitative   analysis of photoswitchable protein,2013,"  Natural Killer (NK) cell activation is dynamically regulated by numerous activating and inhibitory surface receptors that accumulate at the immune synapse. Quantitative analysis of receptor dynamics has been limited by methodologies which rely on indirect measurements such as fluorescence recovery after photobleaching. Here, we report a novel approach to study how proteins traffic to and from the immune synapse using NK cell receptors tagged with the photoswitchable fluorescent protein tdEosFP, which can be irreversibly photoswitched from a green to red fluorescent state by ultraviolet light. Thus, following a localized switching event, the movement of the photoswitched molecules can be temporally and spatially resolved by monitoring fluorescence in two regions of interest. By comparing images with mathematical models, we evaluated the diffusion coefficient of the receptor KIR2DL1 (0.23 +- 0.06 micron^2/s) and assessed how synapse formation affects receptor dynamics. Our data conclude that the inhibitory NK cell receptor KIR2DL1 is continually trafficked into the synapse and remains surprisingly stable there. Unexpectedly however, in NK cells forming synapses with multiple target cells simultaneously, KIR2DL1 at one synapse can relocate to another synapse. Thus, our results reveal a previously undetected inter-synaptic exchange of protein. ",https://doi.org/10.1016/j.bpj.2013.09.025,1310.2415v1,No,
0000-0003-0419-8192,Andreas Schmid,Albstadt-Sigmaringen Universität,Computing Tutte Paths,2017,"  Tutte paths are one of the most successful tools for attacking Hamiltonicity problems in planar graphs. Unfortunately, results based on them are non-constructive, as their proofs inherently use an induction on overlapping subgraphs and these overlaps hinder to bound the running time to a polynomial. For special cases however, computational results of Tutte paths are known: For 4-connected planar graphs, Tutte paths are in fact Hamiltonian paths and Chiba and Nishizeki showed how to compute such paths in linear time. For 3-connected planar graphs, Tutte paths have a more complicated structure, and it has only recently been shown that they can be computed in polynomial time. However, Tutte paths are defined for general 2-connected planar graphs and this is what most applications need. Unfortunately, no computational results are known. We give the first efficient algorithm that computes a Tutte path (for the general case of 2-connected planar graphs). One of the strongest existence results about such Tutte paths is due to Sanders, which allows to prescribe the end vertices and an intermediate edge of the desired path. Encompassing and strengthening all previous computational results on Tutte paths, we show how to compute this special Tutte path efficiently. Our method refines both, the results of Thomassen and Sanders, and avoids overlapping subgraphs by using a novel iterative decomposition along 2-separators. Finally, we show that our algorithm runs in quadratic time. ",Kein DOI-Link verfügbar,1707.05994v1,No,
0000-0003-0419-8192,Andreas Schmid,Albstadt-Sigmaringen Universität,A Tight Extremal Bound on the Lovász Cactus Number in Planar Graphs,2018,"  A cactus graph is a graph in which any two cycles are edge-disjoint. We present a constructive proof of the fact that any plane graph $G$ contains a cactus subgraph $C$ where $C$ contains at least a $\frac{1}{6}$ fraction of the triangular faces of $G$. We also show that this ratio cannot be improved by showing a tight lower bound. Together with an algorithm for linear matroid parity, our bound implies two approximation algorithms for computing ""dense planar structures"" inside any graph: (i) A $\frac{1}{6}$ approximation algorithm for, given any graph $G$, finding a planar subgraph with a maximum number of triangular faces; this improves upon the previous $\frac{1}{11}$-approximation; (ii) An alternate (and arguably more illustrative) proof of the $\frac{4}{9}$ approximation algorithm for finding a planar subgraph with a maximum number of edges.   Our bound is obtained by analyzing a natural local search strategy and heavily exploiting the exchange arguments. Therefore, this suggests the power of local search in handling problems of this kind. ",https://doi.org/10.4230/LIPIcs.STACS.2019.19,1804.03485v3,No,
0000-0003-0419-8192,Andreas Schmid,Albstadt-Sigmaringen Universität,"Interference detection in radio astronomy applying Shapiro-Wilks   normality test, spectral entropy, and spectral relative entropy",2024,"  Radio-frequency interference (RFI) is becoming an increasingly significant problem for most radio telescopes. Working with Green Bank Telescope data from PSR J1730+0747 in the form of complex-valued channelized voltages and their respective high-resolution power spectral densities, we evaluate a variety of statistical measures to characterize RFI. As a baseline for performance comparison, we use median absolute deviation (MAD) in complex channelized voltage data and spectral kurtosis (SK) in power spectral density data to characterize and filter out RFI. From a new perspective, we implement the Shapiro-Wilks (SW) test for normality and two information theoretical measures, spectral entropy (SE) and spectral relative entropy (SRE), and apply them to mitigate RFI. The baseline RFI mitigation algorithms are compared against our novel RFI detection algorithms to determine how effective and robust the performance is. Except for MAD, we find significant improvements in signal-to-noise ratio through the application of SE, symmetrical SRE, asymmetrical SRE, SK, and SW. These algorithms also do a good job of characterizing broadband RFI. Time- and frequency-variable RFI signals are best detected by SK and SW tests. ",Kein DOI-Link verfügbar,2408.06488v1,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,Generative Proxemics: A Prior for 3D Social Interaction from Images,2023,"  Social interaction is a fundamental aspect of human behavior and communication. The way individuals position themselves in relation to others, also known as proxemics, conveys social cues and affects the dynamics of social interaction. Reconstructing such interaction from images presents challenges because of mutual occlusion and the limited availability of large training datasets. To address this, we present a novel approach that learns a prior over the 3D proxemics two people in close social interaction and demonstrate its use for single-view 3D reconstruction. We start by creating 3D training data of interacting people using image datasets with contact annotations. We then model the proxemics using a novel denoising diffusion model called BUDDI that learns the joint distribution over the poses of two people in close social interaction. Sampling from our generative proxemics model produces realistic 3D human interactions, which we validate through a perceptual study. We use BUDDI in reconstructing two people in close proximity from a single image without any contact annotation via an optimization approach that uses the diffusion model as a prior. Our approach recovers accurate and plausible 3D social interactions from noisy initial estimates, outperforming state-of-the-art methods. Our code, data, and model are availableat our project website at: muelea.github.io/buddi. ",Kein DOI-Link verfügbar,2306.09337v2,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,Pose Priors from Language Models,2024,"  We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.   We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact. ",Kein DOI-Link verfügbar,2405.03689v1,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,SPEC: Seeing People in the Wild with an Estimated Camera,2021,"  Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. Both qualitative and quantitative analysis confirm that knowing camera parameters during inference regresses better human bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de. ",Kein DOI-Link verfügbar,2110.00620v2,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,On Self-Contact and Human Pose,2021,"  People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc. While many images of people contain some form of self-contact, current 3D human pose and shape (HPS) regression methods typically fail to estimate this contact. To address this, we develop new datasets and methods that significantly improve human pose estimation with self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to ensure good contact. Second, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during fitting to create near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild images with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that exploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH (Towards Understanding Contact in Humans). We show that the new self-contact training data significantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research purposes at https://tuch.is.tue.mpg.de. ",Kein DOI-Link verfügbar,2104.03176v2,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,3D Human Pose Estimation via Intuitive Physics,2023,"  Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a ""stable"" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression methods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with synchronized multi-view images, ground-truth 3D bodies with complex poses, body-floor contact, CoM and pressure. IPMAN produces more plausible results than the state of the art, improving accuracy for static poses, while not hurting dynamic ones. Code and data are available for research at https://ipman.is.tue.mpg.de. ",Kein DOI-Link verfügbar,2303.18246v3,No,
0009-0009-6538-8046,Lea Müller,Albstadt-Sigmaringen Universität,Causal Inference in Nonverbal Dyadic Communication with Relevant   Interval Selection and Granger Causality,2018,"  Human nonverbal emotional communication in dyadic dialogs is a process of mutual influence and adaptation. Identifying the direction of influence, or cause-effect relation between participants is a challenging task, due to two main obstacles. First, distinct emotions might not be clearly visible. Second, participants cause-effect relation is transient and variant over time. In this paper, we address these difficulties by using facial expressions that can be present even when strong distinct facial emotions are not visible. We also propose to apply a relevant interval selection approach prior to causal inference to identify those transient intervals where adaptation process occurs. To identify the direction of influence, we apply the concept of Granger causality to the time series of facial expressions on the set of relevant intervals. We tested our approach on synthetic data and then applied it to newly, experimentally obtained data. Here, we were able to show that a more sensitive facial expression detection algorithm and a relevant interval detection approach is most promising to reveal the cause-effect pattern for dyadic communication in various instructed interaction conditions. ",Kein DOI-Link verfügbar,1810.12171v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Effective mass in quasi two-dimensional systems,2002,  The effective mass of the quasiparticle excitations in quasi two-dimensional systems is calculated analytically. It is shown that the effective mass increases sharply when the density approaches the critical one of metal-insulator transition. This suggests a Mott type of transition rather than an Anderson like transition. ,https://doi.org/10.1209/epl/i2003-10280-2,cond-mat/0210168v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Critical Tsallis exponent in heavy ion reaction,2001,  The numerical solution of the nonlocal kinetic equation allows to simulate heavy ion reactions around Fermi energy. The expansion velocity and density profile show specific radial dependence which can be described with a Tsallis exponent of $q=5/3$. This might be considered as an indication of a phase transition. ,https://doi.org/10.1016/S0378-4371(01)00667-7,nucl-th/0111074v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Charged hanging chain - catenary exposed to force,2012,  A new solution of a charged catenary is presented which allows to determine the static stability conditions where charged liquid bridges or charged hanging robes are possible. ,Kein DOI-Link verfügbar,1204.4296v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Reversed Currents in Charged Liquid Bridges,2017,"  The velocity profile in a water bridge is reanalyzed. Assuming hypothetically that the bulk charge has a radial distribution, a surface potential is formed that is analogous to the Zeta potential. The Navier Stokes equation is solved, neglecting the convective term; then, analytically and for special field and potential ranges, a sign change of the total mass flow is reported caused by the radial charge distribution. ",https://doi.org/10.3390/w9050353,1705.06228v1,Yes,potent(3)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Chiral anomaly in Weyl systems: no violation of classical conservation   laws,2018,  The anomalous term $\sim\mathbf{EB}$ in the balance of the chiral density can be rewritten as quantum current in the classical balance of density. Therefore it does not violate classical conservation laws as it is claimed to be caused by quantum fluctuations. ,https://doi.org/10.1016/j.physleta.2019.01.042,1809.01547v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Quantum currents and pair correlation of electrons in a chain of   localized dots,2017,"  The quantum transport of electrons in a wire of localized dots by hopping, interaction and dissipation is calculated and a representation by an equivalent RCL circuit is found. The exact solution for the electric-field induced currents allows to discuss the role of virtual currents to decay initial correlations and Bloch oscillations. The dynamical response function in random phase approximation (RPA) is calculated analytically with the help of which the static structure function and pair correlation function are determined. The pair correlation function contains a form factor from the Brillouin zone and a structure factor caused by the localized dots in the wire. ",https://doi.org/10.1140/epjb/e2017-70642-6,1703.04306v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,"Dynamical local field, compressibility and frequency sum rules for   quasiparticles",2001,"  The finite temperature dynamical response function including the dynamical local field is derived within a quasiparticle picture for interacting one-, two- and three dimensional Fermi systems. The correlations are assumed to be given by a density dependent effective mass, quasiparticle energy shift and relaxation time. The latter one describes disorder or collisional effects. This parameterization of correlations includes local density functionals as a special case and is therefore applicable for density functional theories. With a single static local field, the third order frequency sum rule can be fulfilled simultaneously with the compressibility sum rule by relating the effective mass and quasiparticle energy shift to the structure function or pair correlation function. Consequently, solely local density functionals without taking into account effective masses cannot fulfill both sum rules simultaneously with a static local field. The comparison to the Monte-Carlo data seems to support such quasiparticle picture. ",https://doi.org/10.1103/PhysRevB.66.075125,cond-mat/0104229v5,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Electric field dependence of pairing temperature and tunneling,2001,"  Using the Bethe-Salpeter equation including high electric fields, the dependence of the critical temperature of onsetting superconductivity on the applied field is calculated analytically. The critical temperature of pairing is shown to increase with the applied field strength. This is a new field effect and could contribute to the explanation of recent experiments on field induced superconductivity. From the field dependence of the Bethe-Salpeter equation, the two--particle bound state solution is obtained as a resonance with a tunneling probability analogous to the WKB solution of a single particle confined in a potential and coupled to the electrical field. ",https://doi.org/10.1103/PhysRevB.66.172508,cond-mat/0111450v3,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Conductivity in quasi two-dimensional systems,2002,"  The conductivity in quasi two-dimensional systems is calculated using the quantum kinetic equation. Linearizing the Lenard-Balescu collision integral with the extension to include external field dependences allows one to calculate the conductivity with diagrams beyond the GW approximation including maximally crossed lines. Consequently the weak localization correction as an interference effect appears here from the field dependence of the collision integral (the latter dependence sometimes called intra-collisional field effect). It is shown that this weak localization correction has the same origin as the Debye-Onsager relaxation effect in plasma physics. The approximation is applied to a system of quasi two-dimensional electrons in hetero-junctions which interact with charged and neutral impurities and the low temperature correction to the conductivity is calculated analytically. It turns out that the dynamical screening due to charged impurities leads to a linear temperature dependence, while the scattering from neutral impurities leads to the usual Fermi-liquid behavior. By considering an appropriate mass action law to determine the ratio of charged to neutral impurities we can describe the experimental metal-insulator transition at low temperatures as a Mott-Hubbard transition. ",https://doi.org/10.1103/PhysRevB.67.115125,cond-mat/0210268v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Transport with three-particle interaction,2000,  Starting from a point - like two - and three - particle interaction the kinetic equation is derived. While the drift term of the kinetic equation turns out to be determined by the known Skyrme mean field the collision integral appears in two - and three - particle parts. The cross section results from the same microscopic footing and is naturally density dependent due to the three - particle force. By this way no hybrid model for drift and cross section is needed for nuclear transport. Besides the mean field correlation energy the resulting equation of state has also a two - and three - particle correlation energy which are both calculated analytically for the ground state. These energies contribute to the equation of state and lead to an occurrence of a maximum at 3 times nuclear density in the total energy. ,https://doi.org/10.1103/PhysRevC.63.014609,nucl-th/0003068v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Dynamical constraints on phase transitions,2000,"  The numerical solutions of nonlocal and local Boltzmann kinetic equations for the simulation of central heavy ion reactions are parameterized in terms of time dependent thermodynamical variables in the Fermi liquid sense. This allows one to discuss dynamical trajectories in phase space. The nonequilibrium state is characterized by non-isobaric, non-isochoric etc. conditions, shortly called iso-nothing conditions. Therefore a combination of thermodynamical observables is constructed which allows one to locate instabilities and points of possible phase transition in a dynamical sense. We find two different mechanisms of instability, a short time surface - dominated instability and later a spinodal - dominated volume instability. The latter one occurs only if the incident energies do not exceed significantly the Fermi energy and might be attributed to spinodal decomposition. In contrast the fast surface explosion occurs far outside the spinodal region and pertains also in the cases where the system develops too fast to suffer a spinodal decomposition and where the system approaches equilibrium outside the spinodal region. ",https://doi.org/10.1103/PhysRevC.62.044606,nucl-th/0004024v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Bifurcation in kinetic equation for interacting Fermi systems,2000,  The finite duration of collisions appear as time-nonlocality in the kinetic equation. Analyzing the corresponding quantum kinetic equation for dense interacting Fermi systems a delay differential equation is obtained which combines time derivatives with finite time stepping known from the logistic mapping. The responsible delay time is explicitly calculated and discussed. As a novel feature oscillations in the time evolution of the distribution function itself appear and bifurcations up to chaotic behavior can occur. The temperature and density conditions are presented where such oscillations and bifurcations arise indicating an onset of phase transition. ,https://doi.org/10.1063/1.1576209,nucl-th/0012028v5,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Non-local kinetic simulation of heavy ion collisions,2001,"  The numerical solutions of nonlocal and local Boltzmann kinetic equations for the simulation of peripheral and central heavy ion reactions are compared. The experimental finding of enhancement of mid-rapidity matter shows the necessity to include nonlocal corrections. While the in-medium cross section changes the number of collisions and not the transferred energy, the nonlocal scenario changes the energy transferred during collisions. The renormalisation of quasiparticle energies by using the Pauli-rejected collisions leads to a further enhancement of mid-rapidity matter distribution and is accompanied by a dynamical softening of the equation of state.   The simulation results are parameterised in terms of time dependent thermodynamical variables in the Fermi liquid sense. This allows one to discuss dynamical trajectories in phase space. A combination of thermodynamical observables is constructed which locates instabilities and points of possible phase transition under iso-nothing conditions. Two different mechanisms of instability, a short time surface-dominated instability and later a spinodal-dominated volume instability is found. The latter one occurs only if the incident energies do not exceed significantly the Fermi energy and might be attributed to spinodal decomposition. ",Kein DOI-Link verfügbar,nucl-th/0103025v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Quantum response of finite Fermi systems and the relation of Lyapunov   exponent to transport coefficients,1998,"  Within the frame of kinetic theory a response function is derived for finite Fermi systems which includes dissipation in relaxation time approximation and a contribution from additional chaotic processes characterized by the largest Lyapunov exponent. A generalized local density approximation is presented including the effect of many particle relaxation and the additional chaotic scattering. For small Lyapunov exponents relative to the product of wave vector and Fermi velocity in the system, the largest Lyapunov exponent modifies the response in the same way as the relaxation time. Therefore the transport coefficients can be connected with the largest positive Lyapunov exponent in the same way as known from the transport theory in relaxation time approximation. ",Kein DOI-Link verfügbar,physics/9810046v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Nonlinear relaxation field in charged systems under high electric fields,2000,  The influence of an external electric field on the current in charged systems is investigated. The results from the classical hierarchy of density matrices are compared with the results from the quantum kinetic theory. The kinetic theory yields a systematic treatment of the nonlinear current beyond linear response. To this end the dynamically screened and field-dependent Lenard-Balescu equation is integrated analytically and the nonlinear relaxation field is calculated. The classical linear response result known as Debye - Onsager relaxation effect is only obtained if asymmetric screening is assumed. Considering the kinetic equation of one specie the other species have to be screened dynamically while the screening with the same specie itself has to be performed statically. Different other approximations are discussed and compared. ,https://doi.org/10.1103/PhysRevE.62.6135,physics/0005050v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,On a correspondence between classical and quantum particle systems,2000,  An exact correspondence is established between a $N$-body classical interacting system and a $N-1$-body quantum system with respect to the partition function. The resulting quantum-potential is a $N-1$-body one. Inversely the Kelbg potential is reproduced which describes quantum systems at a quasi-classical level. The found correspondence between classical and quantum systems allows also to approximate dense classical many body systems by lower order quantum perturbation theory replacing Planck's constant properly by temperature and density dependent expressions. As an example the dynamical behaviour of an one - component plasma is well reproduced concerning the formation of correlation energy after a disturbance utilising solely the analytical quantum - Born result for dense degenerated Fermi systems. As a practical guide the quantum - Bruckner parameter $r_s$ has been replaced by the classical plasma parameter $\Gamma$ as $r_s\approx0.3 \Gamma^{3/2}$ ,https://doi.org/10.1103/PhysRevE.66.022103,physics/0011059v3,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Asymmetric Bethe-Salpeter equation for pairing and condensation,2010,"  The Martin-Schwinger hierarchy of correlations are reexamined and the three-particle correlations are investigated under various partial summations. Besides the known approximations of screened, ladder and maximally crossed diagrams the pair-pair correlations are considered. It is shown that the recently proposed asymmetric Bethe-Salpeter equation to avoid unphysical repeated collisions is derived as a result of the hierarchical dependencies of correlations. Exceeding the parquet approximation we show that an asymmetry appears in the selfconsistent propagators. This form is superior over the symmetric selfconsistent one since it provides the Nambu-Gorkov equations and gap equation for fermions and the Beliaev equations for bosons while from the symmetric form no gap equation results. The selfenergy diagrams which account for the subtraction of unphysical repeated collisions are derived from the pair-pair correlation in the three-particle Greenfunction. It is suggested to distinguish between two types of selfconsistency, the channel-dressed propagators and the completely dressed propagators, with the help of which the asymmetric expansion completes the Ward identity and is $\Phi$-derivable. ",https://doi.org/10.1007/s10955-011-0186-y,1006.4695v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Theory of water and charged liquid bridges,2011,"  The phenomena of liquid bridge formation due to an applied electric field is investigated. A new solution for the charged catenary is presented which allows to determine the static and dynamical stability conditions where charged liquid bridges are possible. The creeping height, the bridge radius and length as well as the shape of the bridge is calculated showing an asymmetric profile in agreement with observations. The flow profile is calculated from the Navier Stokes equation leading to a mean velocity which combines charge transport with neutral mass flow and which describes recent experiments on water bridges. ",https://doi.org/10.1103/PhysRevE.86.026302,1107.0459v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,"Quasiparticle parameterization of meanfields, Galilei invariance and   universal conserving response functions",2013,"  The general possible form of meanfield parameterization in a running frame in terms of current, energy and density functionals are examined under the restrictions of Galilean invariance. It is found that only two density-dependent parameters remain which are usually condensed in a position-dependent effective mass and the selfenergy formed by current and mass. The position-dependent mass induces a position-dependent local current which is identified for different nonlinear frames. In a second step the response to an external perturbation and relaxation towards a local equilibrium is investigated. The response function is found to be universal in the sense that the actual parameterization of the local equilibrium does not matter and is eliminated from the theory due to the conservation laws. The explicit form of the response with respect to density, momentum and energy is derived. The compressibility sum rule as well as the sum rule by first and third-order frequency moments are proved analytically to be fulfilled simultaneously. The results are presented for Bose- or Fermi systems in one- two and three dimensions. ",https://doi.org/10.1103/PhysRevE.88.022148,1308.2479v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Nonequilibrium thermodynamics with binary quantum correlations,2017,"  The balance equations for thermodynamic quantities are derived from the nonlocal quantum kinetic equation. The nonlocal collisions lead to molecular contributions to the observables and currents. The corresponding correlated part of the observables is found to be given by the rate to form a molecule multiplied with its lifetime which can be considered as collision duration. Explicit expressions of these molecular contributions are given in terms of the scattering phase shifts. The two-particle form of the entropy is derived. This extends the Landau quasiparticle picture by two-particle molecular contributions. There is a continuous exchange of correlations into kinetic parts condensing into the rate of correlated variables for energy and momentum. For the entropy, an explicit gain remains and Boltzmann's H-theorem is proved including the molecular parts of the entropy. ",https://doi.org/10.1103/PhysRevE.96.032106,1708.05602v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Exploring anomalies by many-body correlations,2020,"  The quantum anomaly can be written alternatively into a form violating conservation laws or as non-gauge invariant currents seen explicitly on the example of chiral anomaly. By reinterpreting the many-body averaging, the connection to Pauli-Villars regularization is established which gives the anomalous term a new interpretation as arising from quantum fluctuations by many-body correlations at short distances. This is exemplified by using an effective many-body quantum potential which realizes quantum Slater sums by classical calculations. It is shown that these quantum potentials avoid the quantum anomaly but approaches the same anomalous result by many-body correlations. A measure for the quality of quantum potentials is suggested to describe these quantum fluctuations in the mean energy. Consequently quantum anomalies might be a short-cut way of single-particle field theory to account for many-body effects. This conjecture is also supported since the chiral anomaly can be derived by a completely conserving quantum kinetic theory. ",Kein DOI-Link verfügbar,2004.01507v1,Yes,potent(3)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Consistent solution of Einstein-Cartan equations with torsion outside   matter,2020,  The Einstein-Cartan equations in first-order action of torsion are considered. From Belinfante-Rosenfeld equation special consistence conditions are derived for the torsion parameters relating them to the metric. Inside matter the torsion is given by the spin which leads to an extended Oppenhaimer-Volkov equation. Outside matter a second solution is found besides the torsion-free Schwarzschild one with the torsion completely determined by the metric and vice-versa. This solution is shown to be of non-spherical origin and its uniqueness with respect to the consistence is demonstrated. Unusual properties are discussed in different coordinate systems where the cosmological constant assumes the role of the Friedman parameter in Friedman-Lama\^itre-Robertson-Walker cosmoses. Parameters are specified where wormholes are possible. Transformations are presented to explore and map regions of expanding and contracting universes to the form of static metrics. The autoparallel equations are solved exactly and compared with geodesic motion. The Weyl tensor reveals that the here found solution is of Petrov-D type. ,https://doi.org/10.1088/1361-6382/ac2417,2010.01393v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Giant Octupole Resonance Simulation,1998,  Using a pseudo-particle technique we simulate large-amplitude isoscalar giant octupole excitations in a finite nuclear system. Dependent on the initial conditions we observe either clear octupole modes or over-damped octupole modes which decay immediately into quadrupole ones. This shows clearly a behavior beyond linear response. We propose that octupole modes might be observed in central collisions of heavy ions. ,https://doi.org/10.1103/PhysRevC.60.017301,nucl-th/9810005v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,ISGDR- bulk or surface mode ?,1999,  A polarization and response function for finite systems and temperatures is derived from linearizing the Vlasov equation. Besides the Lindhard response function in local density approximation we obtain an additional contribution due to the surface. This formula is applied to isoscalar giant resonances which we consider as next harmonics to the basic dipole mode. The spurious center of mass motion is subtracted within the polarization. The results based on simple Fermi liquid considerations are in reasonable agreement with experimental data. ,Kein DOI-Link verfügbar,nucl-th/9907013v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Momentum conservation and local field corrections for the response of   interacting Fermi gases,2000,"  We reanalyze the recently derived response function for interacting systems in relaxation time approximation respecting density, momentum and energy conservation. We find that momentum conservation leads exactly to the local field corrections for both cases respecting only density conservation and respecting density and energy conservation. This rewriting simplifies the former formulae dramatically. We discuss the small wave vector expansion and find that the response function shows a high frequency dependence of $\omega^{-5}$ which allows to fulfill higher order sum rules. The momentum conservation also resolves a puzzle about the conductivity which should only be finite in multicomponent systems. ",https://doi.org/10.1103/PhysRevE.62.4382,nucl-th/0003044v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Quasiparticle transport equation with collision delay. I.   Phenomenological approach,1998,"  For a system of non-interacting electrons scattered by resonant levels of neutral impurities, we show that virial and quasiparticle corrections have nearly equal magnitudes. We propose a modification of the Boltzmann equation that includes quasiparticle and virial corrections and discuss their interplay on a dielectric function. ",Kein DOI-Link verfügbar,cond-mat/9803075v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,In-medium relativistic kinetic theory and nucleon-meson systems,1993,"  Within the $\sigma-\omega$ model of coupled nucleon-meson systems, a generalized relativistic Lenard--Balescu--equation is presented resulting from a relativistic random phase approximation (RRPA). This provides a systematic derivation of relativistic transport equations in the frame of nonequilibrium Green's function technique including medium effects as well as flucuation effects. It contains all possible processes due to one meson exchange and special attention is kept to the off--shell character of the particles. As a new feature of many particle effects, processes are possible which can be interpreted as particle creation and annihilation due to in-medium one meson exchange. In-medium cross sections are obtained from the generalized derivation of collision integrals, which possess complete crossing symmetries. ",Kein DOI-Link verfügbar,nucl-th/9310006v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,General response function for interacting quantum liquids,1999,"  Linearizing the appropriate kinetic equation we derive general response functions including selfconsistent mean fields or density functionals and collisional dissipative contributions. The latter ones are considered in relaxation time approximation conserving successively different balance equations. The effect of collisions is represented by correlation functions which are possible to calculate with the help of the finite temperature Lindhard RPA expression. The presented results are applicable to finite temperature response of interacting quantum systems if the quasiparticle or mean field energy is parameterized within Skyrme - type of functionals including density, current and energy dependencies which can be considered alternatively as density functionals. By this way we allow to share correlations between density functional and collisional dissipative contributions appropriate for the special treatment. We present results for collective modes like the plasmon in plasma systems and the giant resonance in nuclei. The collisions lead in general to an enhanced damping of collective modes. If the collision frequency is close to the frequency of the collective mode, resonance occurs and the collective mode is enhanced showing a collisional narrowing. ",https://doi.org/10.1103/PhysRevE.61.2272,nucl-th/9907011v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Damping of IVGDR - Fermi-liquid or Fermi-gas ?,1999,"  Collisional relaxation rates of collective modes in nuclei are calculated using the Levinson equation for the reduced density matrix with a memory dependent collision term. Linearizing the collision integral two contribution have to be distinguished, the one from the quasiparticle energy and the one from occupation factors. The first one yields the known Landau formula of zero sound damping and the second one leads to the Fermi gas model of Ref.1 with the additional factor 3 in front of the frequencies. Adding both contribution we obtain a final relaxation rate for the Fermi liquid model. Calculations of the temperature dependence of the damping rates and of the shape evolution of IVGDR are in good agreement with the experiment and show only minor differences between both models. ",Kein DOI-Link verfügbar,nucl-th/9907012v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Squeezing mode in nuclear collisions,2000,  The time dependent Schroedinger equation is solved analytically for a simplified model of moving infinite walls. A new knock-out mode is described which might occur during heavy ion collisions. The outer shell-nucleons are ionised due to the increase of level energy when two nuclei are approaching fast enough. This is analogous to the Mott effect but in contrast occurs only if the reaction time is short enough that no common ionisation threshold in the compound system is established. To demonstrate this pure nonequilibrium effect a simulation of realistic heavy ion collision by a nonlocal Boltzmann equation is performed. ,https://doi.org/10.1103/PhysRevC.63.061602,nucl-th/0011009v4,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Quasiparticle transport equation with collision delay. II. Microscopic   Theory,1998,"  For a system of non-interacting electrons scattered by neutral impurities, we derive a modified Boltzmann equation that includes quasiparticle and virial corrections. We start from quasiclassical transport equation for non-equilibrium Green's functions and apply limit of small scattering rates. Resulting transport equation for quasiparticles has gradient corrections to scattering integrals. These gradient corrections are rearranged into a form characteristic for virial corrections. ",https://doi.org/10.1103/PhysRevB.55.5095,cond-mat/9803076v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Kinetic equation consistent with the equation of state of nuclear matter,1998,"  A kinetic equation which combines the quasiparticle drift of Landau's equation with a dissipation governed by a nonlocal and noninstant scattering integral in the spirit of Snider's equation for gases is derived. Consequent balance equations for the density, momentum and energy include quasiparticle contributions and the second order quantum virial corrections. The medium effects on binary collisions are shown to mediate the latent heat, i.e., an energy conversion between correlation and thermal energy. An implementation to heavy ion collisions is discussed. ",https://doi.org/10.1016/S0375-9601(98)00061-9,nucl-th/9803019v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Non-instant collisions and two concepts of quasiparticle,1998,"  The kinetic theory recently implemented in heavy ion reactions combines a non-local and non-instant picture of binary collisions with quasiparticle features. We show that the non-instant description is compatible with the spectral concept of quasiparticles while the commonly used variational concept is consistent only with instant collisions. The rearrangement energy, by which the variational concept surpasses the spectral one, is shown to be covered by a medium effect on non-instant collisions. ",https://doi.org/10.1103/PhysRevE.59.R1291,nucl-th/9812026v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Nonlocal kinetic theory,1999,  The short time behavior of a disturbed system is influenced by off-shell motion and best characterized by the reduced density matrix possessing high energetic tails. We present analytically the formation of correlations due to collisions in an interacting Fermionic system with and without initial correlation. After this short time regime the time evolution is controlled by small gradients. This leads to a nonlocal Boltzmann equation for the quasiparticle distribution and a functional relating the latter one to the reduced density matrix. The nonlocalities are presented as time and space shifts arising from gradient expansion and are leading to virial corrections in the thermodynamical limit. ,https://doi.org/10.1142/9789812792754_0054,quant-ph/9911066v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Consequences of coarse grained Vlasov equations,1998,"  The Vlasov equation is analyzed for coarse grained distributions resembling a finite width of test-particles as used in numerical implementations. It is shown that this coarse grained distribution obeys a kinetic equation similar to the Vlasov equation, but with additional terms. These terms give rise to entropy production indicating dissipative features due to a nonlinear mode coupling The interchange of coarse graining and dynamical evolution is discussed with the help of an exactly solvable model for the selfconsistent Vlasov equation and practical consequences are worked out. By calculating analytically the stationary solution of a general Vlasov equation we can show that a sum of modified Boltzmann-like distributions is approached dependent on the initial distribution. This behavior is independent of degeneracy and only controlled by the width of test-particles. The condition for approaching a stationary solution is derived and it is found that the coarse graining energy given by the momentum width of test particles should be smaller than a quarter of the kinetic energy. Observable consequences of this coarse graining are: (i) spatial correlations in observables, (ii) too large radii of clusters or nuclei in self-consistent Thomas-Fermi treatments, (iii) a structure term in the response function resembling vertex correction correlations or internal structure effects and (iv) a modified centroid energy and higher damping width of collective modes. ",Kein DOI-Link verfügbar,nucl-th/9807045v4,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Instability types at ion-assisted alloy deposition: from two-dimensional   to three-dimensional nanopattern growth,2011,"  Ion irradiation during film growth has a strong impact on structural properties. Linear stability analysis is employed to study surface instabilities during ion-assisted growth of binary alloys. An interplay between curvature-dependent ion-driven and deposition-driven instabilities is investigated. We demonstrate that ion irradiation of growing binary alloys leads to the formation of composition-modulated surface patterns. It is shown that the ion-to-atom arrival ratio R is the pattern control parameter. Close to the instability threshold we identify different regimes of instabilities driven by ion- or deposition-induced surface roughness processes, or roughness-composition feedback interactions. In particular, the synergistic effects of the curvature-dependent displacement and deposition coupling to the preferential sputtering or to the preferential diffusivity are found to induce instabilities and pattern formation. Depending on the film growth and ion-irradiation conditions, the instabilities show stationary or oscillating behavior. The latter one is exclusively connected with ion irradiation. The corresponding phase diagrams are presented in terms of experimentally accessible parameters. This shows an alternative way to control surface patterning and to grow three-dimensional laterally or vertically ordered nanostructures. ",https://doi.org/10.1103/PhysRevB.86.085452,1109.5461v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Surface deformation caused by the Abrikosov vortex lattice,2008,  In superconductors penetrated by Abrikosov vortices the magnetic pressure and the inhomogeneous condensate density induce a deformation of the ionic lattice. We calculate how this deformation corrugates the surface of a semi-infinite sample. The effect of the surface dipole is included. ,https://doi.org/10.1103/PhysRevB.77.184509,0802.0831v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Shifts of the nuclear resonance in the vortex lattice in   YBa$_2$Cu$_3$O$_7$,2002,"  The NMR and NQR spectra of $^{63}$Cu in the CuO$_2$ plane of YBa$_2$Cu$_3$O$_7$ in the superconducting state are discussed in terms of the phenomenological theory of Ginzburg-Landau type extended to lower temperatures. We show that the observed spectra, Kumagai {\em et al.}, PRB {\bf 63}, 144502 (2001), can be explained by a standard theory of the Bernoulli potential with the charge transfer between CuO$_2$ planes and CuO chains assumed. ",https://doi.org/10.1103/PhysRevB.66.134525,cond-mat/0205595v2,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Off-shell selfenergy for 1-D Fermi liquids,2023,"  The selfenergy in Born approximation including exchange of interacting one-dimensional systems is expressed in terms of a single integral about the potential which allows a fast and precise calculation for any potential analytically. The imaginary part of the self energy as damping of single-particle excitations shows a rich structure of different areas limited by single-particle and collective excitation lines. The corresponding spectral function reveals a pseudogap, a splitting of excitation into holons and antiholons as well as bound states. ",Kein DOI-Link verfügbar,2309.11428v2,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Formation of binary correlations in plasma,1998,"  The formation of binary correlations in plasma is studied from the quantum kinetic equation. It is shown that this formation is much faster than dissipation due to collisions, in hot (dense) plasma the correlations form on the timescale of inverse plasma frequency (Fermi energy). This hierarchy of characteristic times is used to derive analytical formulae for time dependency of the potential energy of binary interactions which measures the extent of correlations. We discuss the dynamical formation of screening and compare with the statical screened result.Comparisons are made with molecular dynamic simulations. In the low temperature limit we find an analytical expression for the formation of correlation which is general for any binary interaction. It can be applied in nuclear situations as well as dense metals. ",Kein DOI-Link verfügbar,cond-mat/9802303v2,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Retarded versus time-nonlocal quantum kinetic equations,2000,"  The finite duration of the collisions in Fermionic systems as expressed by the retardation time in non-Markovian Levinson-type kinetic equations is discussed in the quasiclassical limit. We separate individual contributions included in the memory effect resulting in (i) off-shell tails of the Wigner distribution, (ii) renormalization of scattering rates and (iii) of the single-particle energy, (iv) collision delay and (v) related non-local corrections to the scattering integral. In this way we transform the Levinson equation into the Landau-Silin equation extended by the non-local corrections known from the theory of dense gases. The derived nonlocal kinetic equation unifies the Landau theory of quasiparticle transport with the classical kinetic theory of dense gases. The space-time symmetry is discussed versus particle-hole symmetry and a solution is proposed which transforms these two exclusive pictures into each other. ",https://doi.org/10.1006/aphy.2001.6197,cond-mat/0005287v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Nonlocal Kinetic Equation and Simulations of Heavy Ion Reactions,1998,"  A kinetic equation which combines the quasiparticle drift of Landau's equation with a dissipation governed by a nonlocal and noninstantaneous scattering integral in the spirit of Enskog corrections is discussed. Numerical values of the off-shell contribution to the Wigner distribution, of the collision duration and of the collision nonlocality are presented for different realistic potentials. On preliminary results we show that simulations of quantum molecular dynamics extended by the nonlocal treatment of collisions leads to a broader proton distribution bringing the theoretical spectra closer towards the experimental values than the local approach. ",https://doi.org/10.1016/S0146-6410(99)00068-X,nucl-th/9811091v1,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Space-time versus particle-hole symmetry of collisions in a non-local   kinetic equation for dense Fermi systems,1999,"  Binary collisions in Fermi systems obey two fundamental symmetries corresponding to the space and time inversion and to the interchange of particles and holes. We show that beyond the local and instant approximation of scattering-in and -out integrals of a kinetic equation, only one of symmetries can be explicit while the other has to be covered by a constraint. This constraint, derived from the optical theorem, allows one to convert at need an explicit particle-hole symmetric form to the space-time symmetric form and vice versa. We implement this constraint to heavy ion reactions, where simulation algorithms require the space-time symmetry while former theories offer kinetic equations with the explicit particle-hole symmetry. ",Kein DOI-Link verfügbar,nucl-th/9902045v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Space-time versus particle-hole symmetry in quantum Enskog equations,2000,  The non-local scattering-in and -out integrals of the Enskog equation have reversed displacements of colliding particles reflecting that the -in and -out processes are conjugated by the space and time inversions. Generalisations of the Enskog equation to Fermi liquid systems are hindered by a request of the particle-hole symmetry which contradicts the reversed displacements. We resolve this problem with the help of the optical theorem. It is found that space-time and particle-hole symmetry can only be fulfilled simultaneously for the Bruckner-type of internal Pauli-blocking while the Feynman-Galitskii form allows only for particle-hole symmetry but not for space-time symmetry due to a stimulated emission of Bosons. ,https://doi.org/10.1103/PhysRevE.64.046107,nucl-th/0012040v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Effect of an electric field on the surface superconductivity,2009,"  We discuss an effect of the electrostatic field on superconductivity near the surface. First, we use the microscopic theory of de Gennes to show that the electric field changes the boundary condition for the Ginzburg-Landau function. Second, the effect of the electric field is evaluated in the vicinity of $H_{\rm c3}$, where the boundary condition plays a crucial role. We predict that the field effect on the surface superconductivity leads to a discontinuity of the magnetocapacitance. We estimate that the predicted discontinuity is accessible for nowadays experimental tools and materials. It is shown that the magnitude of this discontinuity can be used to predict the dependence of the critical temperature on the charge carrier density which can be tailored by doping. ",Kein DOI-Link verfügbar,0912.1265v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Budd-Vannimenus theorem for superconductors,2002,  The Budd-Vannimenus theorem is modified to the surface of superconductors in the Meissner state. This identity links the surface value of the electrostatic potential to the free energy in the bulk which allows one to evaluate the observed potential without the explicit solution of the charge profile at the surface. ,Kein DOI-Link verfügbar,cond-mat/0212152v2,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Vortex induced deformation of the superconductor crystal lattice,2006,  Deformation of the superconductor crystal lattice caused by Abrikosov vortices is formulated as a response of the elastic crystal lattice to electrostatic forces. It is shown that the lattice compression is linearly proportional to the electrostatic potential known as the Bernoulli potential. Eventual consequences of the crystal lattice deformation on the effective vortex mass are discussed. ,https://doi.org/10.1103/PhysRevB.76.052502,cond-mat/0609669v1,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Interaction between ionic lattices and superconducting condensates,2007,  The interaction of the ionic lattice with the superconducting condensate is treated in terms of the electrostatic force in superconductors. It is shown that this force is similar but not identical to the force suggested by the volume difference of the normal and superconducting states. The BCS theory shows larger deviations than the two-fluid model. ,https://doi.org/10.1103/PhysRevB.77.014506,0708.3760v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Collective modes in asymmetric nuclei,2000,  The collective motion of a finite nuclear system is investigated by numerical simulation and by linear response theory. Using a pseudo-particle simulation technique we analyze the giant resonances with a multipole decomposition scheme. We examine the energy and the damping of different giant collective modes and obtain the dependence of these quantities on the proton-neutron ratio. ...(abbreviated) Alternatively the giant collective modes in asymmetric nuclear matter are investigated within linear response theory including the collisional correlations via a dynamic relaxation time approximation. For a multicomponent system we derive a coupled dispersion relation and show that two sources of coupling appear: (i) a coupling of isoscalar and isovector modes due to the action of different mean-fields and (ii) an explicit new coupling in asymmetric matter due to collisional interaction. We show that the latter is responsible for a new mode arising besides isovector and isoscalar modes...(abbreviated) Collective motion beyond the linear regime is demonstrated for the example of large-amplitude isoscalar giant octupole excitations in finite nuclear systems. Depending on the initial conditions we observe either clear octupole modes or over-damped octupole modes which decay immediately into quadrupole ones. This dependence on initial correlations represents a behaviour beyond linear response. ,Kein DOI-Link verfügbar,nucl-th/0001032v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Bernoulli potential at a superconductor surface,2001,  The electrostatic Bernoulli potential measured at the surface of a superconductor via Kelvin capacitive coupling is shown to be independent of the pairing mechanism. This contrasts with the Bernoulli potential in the bulk where contributions due to pairing dominate close to $T_c$. ,https://doi.org/10.1103/PhysRevB.65.012507,cond-mat/0104192v2,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Chaotic scattering on surfaces and collisional damping of collective   modes,1998,"  The damping of hot giant dipole resonances is investigated. The contribution of surface scattering is compared with the contribution from interparticle collisions. A unified response function is presented which includes surface damping as well as collisional damping. The surface damping enters the response via the Lyapunov exponent and the collisional damping via the relaxation time. The former is calculated for different shape deformations of quadrupole and octupole type. The surface as well as the collisional contribution each reproduce almost the experimental value, therefore we propose a proper weighting between both contributions related to their relative occurrence due to collision frequencies between particles and of particles with the surface. We find that for low and high temperatures the collisional contribution dominates whereas the surface damping is dominant around the temperatures $\sqrt{3}/2\pi$ of the centroid energy. ",https://doi.org/10.1103/PhysRevC.60.054601,astro-ph/9807185v4,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Duration and non-locality of a nucleon-nucleon collision,1998,"  For a set of realistic nucleon-nucleon potentials we evaluate microscopic parameters of binary collisions: a time duration of the scattering state, a mean distance and a rotation of nucleons during a collision. These parameters enter the kinetic equation as non-instantaneous and non-local corrections of the scattering integral, i.e., they can be experimentally tested. Being proportional to off-shell derivatives of the scattering T-matrix, non-instantaneous and non-local corrections make it possible to compare the off-shell behavior of different potentials in a vicinity of the energy shell. The Bonn one-Boson-exchange (A-C) and Paris potentials are found to yield very close results, while the separable Paris potential differs. ",https://doi.org/10.1103/PhysRevC.59.3052,nucl-th/9811089v2,Yes,potent(4)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Conditions where RPA becomes exact in the high-density limit,2018,"  It is shown that in $d$-dimensional systems, the vertex corrections beyond the random phase approximation (RPA) or GW approximation scales with the power $d-\beta-\alpha$ of the Fermi momentum if the relation between Fermi energy and Fermi momentum is $\epsilon_{\rm f}\sim p_{\rm f}^\beta$ and the interacting potential possesses a momentum-power-law of $\sim p^{-\alpha}$. The condition $d-\beta-\alpha<0$ specifies systems where RPA is exact in the high-density limit. The one-dimensional structure factor is found to be the interaction-free one in the high-density limit for contact interaction. A cancellation of RPA and vertex corrections render this result valid up to second-order in contact interaction. For finite-range potentials of cylindrical wires a large-scale cancellation appears and found to be independent of the width parameter of the wire. The proposed high-density expansion agrees with the Quantum Monte Carlo simulations. ",Kein DOI-Link verfügbar,1802.10312v1,Yes,potent(2)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Electrostatic potential in a superconductor,2001,"  The electrostatic potential in a superconductor is studied. To this end Bardeen's extension of the Ginzburg-Landau theory to low temperatures is used to derive three Ginzburg-Landau equations - the Maxwell equation for the vector potential, the Schroedinger equation for the wave function and the Poisson equation for the electrostatic potential. The electrostatic and the thermodynamic potential compensate each other to a great extent resulting into an effective potential acting on the superconducting condensate. For the Abrikosov vortex lattice in Niobium, numerical solutions are presented and the different contributions to the electrostatic potential and the related charge distribution are discussed. ",https://doi.org/10.1103/PhysRevB.65.144511,cond-mat/0111214v1,Yes,potent(6)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Non-linear theory of deformable superconductors,2008,  Interaction of the superconducting condensate with deformations of the crystal lattice is formulated assuming the electrostatic potential of Bernoulli type and the effect of strain on material parameters. In the isotropic approximation it is shown that within the Ginzburg-Landau theory both contributions can be recast into the local but non-linear interaction term of the free energy. ,https://doi.org/10.1103/PhysRevB.78.174516,0806.3660v2,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Boundary condition for Ginzburg-Landau theory of superconducting layers,2009,"  Electrostatic charging changes the critical temperature of superconducting thin layers. To understand the basic mechanism, it is possible to use the Ginzburg-Landau theory with the boundary condition derived by de Gennes from the BCS theory. Here we show that a similar boundary condition can be obtained from the principle of minimum free energy. We compare the two boundary conditions and use the Budd-Vannimenus theorem as a test of approximations. ",https://doi.org/10.1103/PhysRevB.79.174510,0901.4244v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Stability of condensate in superconductors,2010,  According to the BCS theory the superconducting condensate develops in a single quantum mode and no Cooper pairs out of the condensate are assumed. Here we discuss a mechanism by which the successful mode inhibits condensation in neighboring modes and suppresses a creation of noncondensed Cooper pairs. It is shown that condensed and noncondensed Cooper pairs are separated by an energy gap which is smaller than the superconducting gap but large enough to prevent nucleation in all other modes and to eliminate effects of noncondensed Cooper pairs on properties of superconductors. Our result thus justifies basic assumptions of the BCS theory and confirms that the BCS condensate is stable with respect to two-particle excitations. ,https://doi.org/10.1140/epjb/e2013-40932-2,1008.5293v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,The concept of correlated density and its application,2006,"  The correlated density appears in many physical systems ranging from dense interacting gases up to Fermi liquids which develop a coherent state at low temperatures, the superconductivity. One consequence of the correlated density is the Bernoulli potential in superconductors which compensates forces from dielectric currents. This Bernoulli potential allows to access material parameters. Though within the surface potential these contributions are largely canceled, the bulk measurements with NMR can access this potential. Recent experiments are explained and new ones suggested. The underlying quantum statistical theory in nonequilibrium is the nonlocal kinetic theory developed earlier. ",https://doi.org/10.1142/S0217979207043713,cond-mat/0612080v1,Yes,potent(4)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Virial corrections to simulations of heavy ion reactions,1998,"  Within QMD simulations we demonstrate the effect of virial corrections on heavy ion reactions. Unlike in standard codes, the binary collisions are treated as non-local so that the contribution of the collision flux to the reaction dynamics is covered. A comparison with standard QMD simulations shows that the virial corrections lead to a broader proton distribution bringing theoretical spectra closer towards experimental values. Complementary BUU simulations reveal that the non-locality enhances the collision rate in the early stage of the reaction. It suggests that the broader distribution appears due to an enhanced pre-equilibrium emission of particles. ",https://doi.org/10.1103/PhysRevLett.82.3767,nucl-th/9810043v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Phase-field theory of brine entrapment in sea ice: Short-time frozen   microstructures,2014,"  We analyze the early phase of brine entrapment in sea ice, using a phase field model. This model for a first-order phase transition couples non-conserved order parameter kinetics to salt diffusion. The evolution equations are derived from a Landau-Ginzburg order parameter gradient dynamics together with salinity conservation. The numerical solution of model equations by an exponential time differencing scheme describes the time evolution of phase separation between liquid water with high salinity and the ice phase with low salinity. The numerical solution in one and two dimensions indicates the formation of one dominant wavelength which sets the length scale of short-time frozen structures. A stability analysis provides the phase diagram in terms of two Landau parameters. It is distinguished an uniform ice phase, a homogeneous liquid saline water solution and a phase where solidification structures can be formed. The Landau parameters are extracted from the supercooling and superheating as well as the freezing point temperature of water. With the help of realistic parameters the distribution of brine inclusions is calculated and found in agreement with the measured samples. The size of the ice domains separating regions of concentrated seawater depends on salinity and temperature and corresponds to the size of sea ice platelets obtained from a morphological stability analysis for the solidification of salt water. ",Kein DOI-Link verfügbar,1405.0304v1,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Conserving T-matrix theory of superconductivity,2009,"  We remove a self-interaction from the Galitskii-Feynman T-matrix approximation. This correction has no effect in the normal state but makes the theory applicable to the superconducting state. It is shown that identical theory is obtained by removing nonphysical repeated collisions in the spirit of the Fadeev-Watson-Lovelace multiple scattering expansion. Our correction does not violate the two-particle symmetry of the T-matrix, therefore the present theory is conserving in the Baym-Kadanoff sense. The theory is developed for retarded interactions leading to the Eliashberg theory in the approximation of a single pairing channel. ",https://doi.org/10.1103/PhysRevB.84.094529,0906.3677v4,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Dependence of structure factor and correlation energy on the width of   electron wires,2017,"  The structure factor and correlation energy of a quantum wire of thickness $b\ll a_B$ are studied in random phase approximation and for the less investigated region $r_s<1$. Using the single-loop approximation, analytical expressions of the structure factor have been obtained. The exact expressions for the exchange energy are also derived for a cylindrical and harmonic wire. The correlation energy $\epsilon_c$ is found to be represented by $\epsilon_c (b,r_s)= \frac{\alpha(r_s)}{b} + \beta(r_s)\; ln(b) + \eta(r_s)$, for small $b$ and high densities. For a pragmatic width of the wire, the correlation energy is in agreement with the quantum Monte Carlo simulation data. ",https://doi.org/10.1140/epjb/e2017-80530-8,1708.06835v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Exact ground-state properties of one-dimensional electron gas at high   density,2019,  The dynamical response theory is used to obtain an analytical expression for the exchange energy of a quantum wire for arbitrary polarization and width. It reproduces the known form of exchange energy for 1D electron gas in the limit of infinitely thin cylindrical and harmonic wires. The structure factor for these wires are also obtained analytically in the high-density or small $r_s$ limit. This structure factor enables us to get the {\it exact} correlation energy for both the wires and demonstrates that there are at least two methods to get the ideal Coulomb limit in one dimension. The structure factor and the correlation energy are found to be independent of the way the one-dimensional Coulomb potential is regularized. The analytical expression for the pair correlation function is also presented for small distances and provides a justification for the small $r_s$ expansion as long as $r_s< \frac{3}{2} \left(\frac{\pi^2}{\pi^2+3} \right)=1.15$. ,https://doi.org/10.1103/PhysRevB.101.075130,1909.09331v1,Yes,potent(1)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Mid-rapidity charge distribution in peripheral heavy ion collisions,2000,"  The charge density distribution with respect to the velocity of matter produced in peripheral heavy ion reactions around Fermi energy is investigated. The experimental finding of enhancement of mid-rapidity matter shows the necessity to include correlations beyond BUU which was performed in the framework of nonlocal kinetic theory. Different theoretical improvements are discussed. While the in-medium cross section changes the number of collisions, it leaves the transferred energy almost unchanged. In contrast the nonlocal scenario changes the energy transferred during collisions and leads to an enhancement of mid-rapidity matter. The renormalisation of quasiparticle energies can be included in nonlocal scenarios and leads to a further enhancement of mid-rapidity matter distribution. This renormalisation is accompanied by a dynamical softening of the equation of state seen in longer oscillation periods of the excited compressional collective mode. We propose to include quasiparticle renormalisation by using the Pauli-rejected collisions which circumvent the problem of backflows in Landau theory. Using the maximum relative velocity of projectile and target like fragments we associate experimental events with impact parameters of the simulations. For peripheral collisions we find a reasonable agreement between experiment and theory. For more central collisions, the velocity damping is higher in one-body simulations than observed experimentally, because of missing cluster formations in the kinetic theory used. ",https://doi.org/10.1103/PhysRevC.63.034619,nucl-th/0011045v3,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Electronic quantum wires in extended quasiparticle picture,2023,"  A one-dimensional quantum wire of Fermions is considered and ground state properties are calculated in the high density regime within the extended quasiparticle picture and Born approximation. Expanding the two-particle Green functions determines the selfenergy and the polarization as well as the response function on the same footing. While the on-shell selfenergies are strictly zero due to Pauli-blocking of elastic scattering, the off-shell behaviour shows a rich structure of a gap in the damping of excitation which is closed when the momentum approaches the Fermi one. The consistent spectral function is presented completing the first two energy-weighted sum rules. The excitation spectrum shows a splitting due to holons and antiholons as non-Fermi liquid behaviour. A renormalization procedure is proposed by subtracting an energy constant to render the Fock exchange energy finite. The effective mass derived from meanfield shows a dip as onset of Peierls instability. The correlation energy is calculated with the help of the extended quasiparticle picture which accounts for off-shell effects. The corresponding response function leads to the same correlation energy as the selfenergy in agreement with perturbation theory. The reduced density matrix or momentum distribution is calculated with the help of a Pad\'e regularization repairing deficiencies of the perturbation theory. A seemingly finite step at the Fermi energy indicating Fermi-liquid behaviour is repaired in this way. ",Kein DOI-Link verfügbar,2311.02414v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Bernoulli potential in type-I and weak type-II superconductors: III.   Electrostatic potential above the vortex lattice,2004,"  The electrostatic potential above the Abrikosov vortex lattice, discussed earlier by Blatter {\em et al.} {[}PRL {\bf 77}, 566 (1996){]}, is evaluated within the Ginzburg-Landau theory. Unlike previous studies we include the surface dipole. Close to the critical temperature, the surface dipole reduces the electrostatic potential to values below a sensitivity of recent sensors. At low temperatures the surface dipole is less effective and the electrostatic potential remains observable as predicted earlier. ",https://doi.org/10.1103/PhysRevB.71.024526,cond-mat/0409397v2,Yes,potent(3)
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Electron correlation and confinement effects in quasi-one-dimensional   quantum wires at high density,2021,"  We study the ground-state properties of ferromagnetic quasi-one-dimensional quantum wires using the quantum Monte Carlo (QMC) method for various wire widths $b$ and density parameters $r_\text{s}$. The correlation energy, pair-correlation function, static structure factor, and momentum density are calculated at high density, $r_\text{s}=0.5$. It is observed that the peak in the static structure factor at $k=2k_\text{F}$ grows as the wire width decreases. We obtain the Tomonaga-Luttinger liquid parameter $K_\rho$ from the momentum density. It is found that $K_\rho$ increases by about $10$\% between wire widths $b=0.01$ and $b=0.5$. We also obtain ground-state properties of finite thickness wires theoretically using the first-order random phase approximation (RPA) with exchange and self-energy contributions, which is exact in the high-density limit. Analytical expressions for the static structure factor and correlation energy are derived for $b \ll r_\text{s}<1$. It is found that the correlation energy varies as $b^2$ for $b \ll r_\text{s}$ from its value for an infinitely thin wire. It is observed that the correlation energy depends significantly on the wire model used (harmonic versus cylindrical confinement). The first-order RPA expressions for the structure factor, pair-correlation function, and correlation energy are numerically evaluated for several values of $b$ and $r_\text{s} \leq 1$. These are compared with the QMC results in the range of applicability of the theory. ",https://doi.org/10.1103/PhysRevB.105.115140,2112.12064v2,No,
0000-0003-2119-9461,Klaus Morawetz,FH Münster,Ground-state properties of electron-electron biwire systems,2021,"  The correlation between electrons in different quantum wires is expected to affect the electronic properties of quantum electron-electron biwire systems. Here, we use the variational Monte Carlo method to study the ground-state properties of parallel, infinitely thin electron-electron biwires for several electron densities ($r_\text{s}$) and interwire separations ($d$). Specifically, the ground-state energy, the correlation energy, the interaction energy, the pair-correlation function (PCF), the static structure factor (SSF), and the momentum distribution (MD) function are calculated. We find that the interaction energy increases as $\ln(d)$ for $d\to 0$ and it decreases as $d^{-2}$ when $d\to \infty$. The PCF shows oscillatory behavior at all densities considered here. As two parallel wires approach each other, interwire correlations increase while intrawire correlations decrease as evidenced by the behavior of the PCF, SSF, and MD. The system evolves from two monowires of density parameter $r_\text{s}$ to a single monowire of density parameter $r_\text{s}/2$ as $d$ is reduced from infinity to zero. The MD reveals Tomonaga-Luttinger (TL) liquid behavior with a power-law nature near $k_\text{F}$ even in the presence of an extra interwire interaction between the electrons in biwire systems. It is observed that when $d$ is reduced the MD decreases for $k<k_\text{F}$ and increases for $k>k_\text{F}$, similar to its behavior with increasing $r_\text{s}$. The TL liquid exponent is extracted by fitting the MD data near $k_\text{F}$, from which the TL liquid interaction parameter $K_{\rho}$ is calculated. The value of the TL parameter is found to be in agreement with that of a single wire for large separation between the two wires. ",https://doi.org/10.1103/PhysRevB.104.035149,2104.00494v2,No,
0000-0002-1584-2337,Jonas Homrighausen,FH Münster Universität der Angewandte Wissenschaften,Microscale Fiber-Integrated Vector Magnetometer with On-Tip Field   Biasing using NV Ensembles in Diamond Microcystals,2024,"  In quantum sensing of magnetic fields, ensembles of nitrogen-vacancy centers in diamond offer high sensitivity, high bandwidth and outstanding spatial resolution while operating in harsh environments. Moreover, the orientation of defect centers along four crystal axes forms an intrinsic coordinate system, enabling vector magnetometry within a single diamond crystal. While most vector magnetometers rely on a known bias magnetic field for full recovery of three-dimensional field information, employing external 3D Helmholtz coils or permanent magnets results in bulky, laboratory-bound setups, impeding miniaturization of the device. Here, a novel approach is presented that utilizes a fiber-integrated microscale coil at the fiber tip to generate a localized uniaxial magnetic field. The same fiber-tip coil is used in parallel for spin control by combining DC and microwave signals in a bias tee. To implement vector magnetometry using a uniaxial bias field, we preselect the orientation of the diamond crystal and then fully characterize it by rotating a static magnetic field in three planes of rotation. We demonstrate the measurement of vector magnetic fields in the full solid angle with a shot-noise limited sensitivity of $19.4\:\textrm{nT/Hz}^{1/2}$ and microscale spatial resolution while achieving a cross section of the fiber sensor head below $1\:\textrm{mm}^2.$ ",Kein DOI-Link verfügbar,2404.14089v2,No,
0000-0002-1584-2337,Jonas Homrighausen,FH Münster Universität der Angewandte Wissenschaften,Using low-cost Blu-Ray Optical Pickup Units for Measurement of Single   Photon Emission from NV-Centers,2024,"  This work presents a cost-effective method for collecting single photons emitted from single nitrogen-vacancy centers in nanodiamonds. Conventional components of a confocal laser-scanning microscope, such as microscope objectives and the piezo translation stages, are replaced by two affordable Blu-ray optical pickup units. A Hanbury Brown and Twiss setup is used to identify single photon emission. The proposed approach is inexpensive and simple and lowers the entry-level to single photon research for quantum technologies. This enables student lab experiments or demonstration experiments at schools and shows that efficient sources of quantum light can be made from standard components compatible with established industry processes. ",Kein DOI-Link verfügbar,2408.00541v1,No,
0000-0002-0192-6919,Joachim Breternitz,Münster Universität der Angewandte Wissenschaften,Iodide-methylammonium interaction is responsible for ferroelectricity in   CH3NH3PbI3,2019,"  Excellent conversion efficiencies of over 20 % and facile cell production have placed hybrid perovskites at the forefront of novel solar cell materials with CH3NH3PbI3 being its archetypal compound. The question why CH3NH3PbI3 has such extraordinary characteristics, particularly a hugely efficient light absorption, is hotly debated with ferroelectricity being a promising candidate. This does, however, afford the crystal structure to be non-centrosymmetric and we herein present crystallographic evidence as to how the symmetry breaking occurs on a crystallographic, and therefore long-scale, level. While the molecular cation CH3NH3+ is intrinsically polar, it is heavily disordered and cannot be the sole reason for ferroelectricity. We show that it, nonetheless, plays an important role as it distorts the neighboring iodide positions from their centrosymmetric positions. ",Kein DOI-Link verfügbar,1907.06510v1,No,
0000-0002-0192-6919,Joachim Breternitz,Münster Universität der Angewandte Wissenschaften,Mechanochemical Synthesis of the Lead-Free Double Perovskite   Cs2[AgIn]Br6 and its Optical Properties,2018,"  Hitting hard on the binary halides yields in the formation of Cs2[AgIn]Br6. The lead-free double perovskite marks, although not usable itself, a further step forward in finding sustainable and durable perovskite materials for photovoltaic applications. Cs2[AgIn]Br6 is one of the prominent examples of double perovskites materials that have been suggested to circumvent the use of lead compounds in perovskite solar cells. We herein report the successful synthesis of the material using a mechanochemical approach. It crystallizes in an elpasolite-type structure, an ordered perovskite superstructure, with a cell parameter of a = 11.00 {\AA}. However, the compound exhibits a relatively large optical bandgap of 2.36 eV and is unstable under illumination, which impedes its use as solar absorber material at this early stage. Still, substitution of lead and the potential of this synthesis method are promising as well as the fruitful combination of theoretical considerations with experimental materials discovery. ",Kein DOI-Link verfügbar,1810.11330v1,Yes,potent(1)
0000-0002-0192-6919,Joachim Breternitz,Münster Universität der Angewandte Wissenschaften,The human factor: results of a small-angle scattering data analysis   Round Robin,2023,"  A Round Robin study has been carried out to estimate the impact of the human element in small-angle scattering data analysis. Four corrected datasets were provided to participants ready for analysis. All datasets were measured on samples containing spherical scatterers, with two datasets in dilute dispersions, and two from powders. Most of the 46 participants correctly identified the number of populations in the dilute dispersions, with half of the population mean entries within 1.5% and half of the population width entries within 40%, respectively. Due to the added complexity of the structure factor, much fewer people submitted answers on the powder datasets. For those that did, half of the entries for the means and widths were within 44% and 86% respectively. This Round Robin experiment highlights several causes for the discrepancies, for which solutions are proposed. ",Kein DOI-Link verfügbar,2303.03772v1,No,
0000-0002-0921-2574,Marco Bonini,Hochschule Reutlingen,The structural architecture of the Los Humeros volcanic complex and   geothermal field,2019,"  The Los Humeros Volcanic Complex (LHVC) is a large silicic caldera complex in the Trans-Mexican Volcanic Belt (TMVB), hosting a geothermal field currently in exploitation by the Comision Federal de Electricidad (CFE) of Mexico, with an installed capacity of ca. 95 MW of electric power. Understanding the structural architecture of LHVC is important to get insights into the interplay between the volcano-tectonic setting and the characteristics of the geothermal resources in the area. The analysis of volcanotectonic interplay in LHVC benefits from the availability of subsurface data obtained during the exploration of the geothermal reservoir that allows the achievement of a 3D structural view of the volcano system. The LHVC thus represents an important natural laboratory for the development of general models of volcano-tectonic interaction in calderas. ",Kein DOI-Link verfügbar,1907.08799v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Two and three photon excited luminescence of single gold nanoparticles:   Switching between plasmon- and electron-hole-pair emission by ultrashort   laser pulses,2018,"  In this work we use femtosecond laser pulses of 800 nm wavelength to excite and characterize the multiphoton luminescence emission of single gold nanoparticles. For excitation with 100 fs laser pulses we observe a two and three photon emission dominated by radiative electron hole pair recombination, while the emission is caused by radiative plasmon decay for excitation with 500 fs pulses. For single gold nanorods with different aspect ratios, we study the interplay between the particle plasmon and electron hole pairs, which enables us to develop a quantitative model to fully describe the two and three photon luminescence emission of single gold nanoparticles. ",Kein DOI-Link verfügbar,1812.01409v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Tunable strong coupling of two adjacent optical λ/2 Fabry-Pérot   microresonators,2019,"  Optical half-wave microresonators enable to control the optical mode density around a quantum system and thus to modify the temporal emission properties. If the coupling rate exceeds the damping rate, strong coupling between a microresonator and a quantum system can be achieved, leading to a coherent energy exchange and the creation of new hybrid modes. Here, we investigate strong coupling between two adjacent lambda/2 Fabry-P\'erot microresonators, where the resonance of one microresonator can be actively tuned across the resonance of the other microresonator. The transmission spectra of the coupled microresonators show a clear anticrossing behavior, which proves that the two cavity modes are strongly coupled. Additionally, we can vary the coupling rate by changing the resonator geometry and thereby investigate the basic principles of strong coupling with a well-defined model system. Finally, we will show that such a coupled system can theoretically be modelled by coupled damped harmonic oscillators. ",https://doi.org/10.1364/OE.380068,1908.01566v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Multimode Vibrational Strong Coupling of Methyl Salicylate to a   Fabry-Perot Microcavity,2020,"  The strong coupling of an IR-active molecular transition with an optical mode of the cavity results in vibrational polaritons, which opens a new way to control chemical reactivity via confined electromagnetic fields of the cavity. In this study, we design a voltage-tunable open microcavity and we show the formation of multiple vibrational polaritons in methyl salicylate. A Rabi splitting and polariton anticrossing behaviour is observed when the cavity mode hybridizes with the C=O stretching vibration of methyl salicylate. As this vibration contributes to the reaction coordinate of the photoinduced proton transfer process in methyl salicylate, we suggest the coupling might be used to modulate the photophysical properties of the molecule. Furthermore, the proposed theoretical model based on coupled harmonic oscillator reveals that the absorption of uncoupled molecules must also be considered to model the experimental spectra properly and that simultaneous coupling of multiple molecular vibrations to the same cavity mode has a significant influence on the Rabi splitting. ",Kein DOI-Link verfügbar,2001.10812v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Direct phase mapping of the light scattered by single plasmonic   nanoparticles,2019,"  In this work, we present a novel technique to directly measure the phase shift of the optical signal scattered by single plasmonic nanoparticles in a diffraction-limited laser focus. We accomplish this by equipping an inverted confocal microscope with a Michelson interferometer and scanning single nanoparticles through the focal volume while recording interferograms of the scattered and a reference wave for each pixel. For the experiments, lithographically prepared gold nanorods where used, since their plasmon resonances can be controlled via their aspect ratio. We have developed a theoretical model for image formation in confocal scattering microscopy for nanoparticles considerably smaller than the diffraction limited focus We show that the phase shift observed for particles with different longitudinal particle plasmon resonances can be well explained by the harmonic oscillator model. The direct measurement of the phase shift can further improve the understanding of the elastic scattering of individual gold nanoparticles with respect to their plasmonic properties. ",https://doi.org/10.1039/C9NR10358A,1909.12040v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Hypericin: Single molecule spectroscopy of an active natural ingredient,2019,"  Hypericin can be found in nature in Hypericum perforatum (St. John's Wort) and has become subject of intense biochemical research. Studies report of antidepressive, antineoplastic, antitumor and antiviral activity of hypericin. Among the variety of potential applications hypericin can be used as photosensitizer in photodynamic therapy (PDT), where it is brought into cancer cells and produces singlet oxygen upon irradiation with a suitable light source. Therefore, the photophysical properties of hypericin are crucial for a successful application in a medical treatment. Here, we present the first single molecule optical spectroscopy study of hypericin. Its photostability is large enough to obtain single molecule fluorescence, surface enhanced Raman spectra (SERS), fluorescence lifetime, antibunching and blinking dynamics. Embedding hypericin in a PVA matrix changes the blinking dynamics, reduces the fluorescence lifetime and increases the photostability. Single molecule SERS spectra show both the neutral and deprotonated form of hypericin and exhibit sudden spectral changes, which can be associated with a reorientation of the single molecule with respect to the surface. ",https://doi.org/10.1021/acs.jpca.9b11532,1910.03921v1,Yes,potent(1)
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Direct observation of structural heterogeneity and tautomerization of   single hypericin molecules,2020,"  Tautomerization is a fast chemical reaction where structures of the reactants differ only in the position of a proton and a double bond. Tautomerization often occurs in natural substances and is a fundamental process in organic- and biochemistry. However, studying the optical properties of tautomeric species is challenging due to ensemble averaging. Many molecules, such as porphines, porphycenes or phenanthroperylene quinones, exhibit a reorientation of the transition dipole moment (TDM) during tautomerization, which can be directly observed in a single molecule experiment. A prominent phenanthroperylene quinone is hypericin showing antiviral, antidepressive, and photodynamical properties. Here, we study single hypericin molecules by using confocal microscopy combined with higher order laser modes. Observing abrupt flipping of the image pattern allows to draw conclusions about the coexistence of different tautomers and their conversion path. Time-dependent density functional theory calculations show that hypericin is cycling between the four most stable tautomers. This approach allows to unambiguously assign a TDM orientation to a specific tautomer and enables to determine the chemical structure in situ. Additionally, tautomerization can not only be observed by the image pattern orientation, but also as intermittency in the fluorescence emission of a single molecule. Time correlated single photon counting enables to determine the excited state lifetimes of the hypericin tautomers. Our approach is not only limited to hypericin, but can be applied to other molecules showing a TDM reorientation during tautomerization, helping to get a deeper understanding of this important process. ",Kein DOI-Link verfügbar,2011.01645v1,No,
0000-0001-6554-6600,Frank Wackenhut,Hochschule Reutlingen,Periodic Fluorescence Variations of CdSe Quantum Dots Coupled to   Aryleneethynylenes with Aggregation Induced Emission,2020,"  CdSe nanocrystals and aggregates of an aryleneethynylene derivative are assembled into a hybrid thin film with dual fluorescence from both fluorophores. Under continuous excitation, the nanocrystals and the molecules exhibit anti-correlated fluorescence intensity variations, which become periodic at low temperature. We attribute this to a structure-dependent aggregation induced emission of the aryleneethynylene derivative, which impacts the rate of excitation energy transfer between the molecules and nanocrystals. Energy transfer also affects the electric transport properties of the hybrid material under optical excitation. This work highlights that combining semiconductor nanocrystals with molecular aggregates, which exhibit aggregation induced emission, can result in unprecedented emerging optical properties. ",Kein DOI-Link verfügbar,2006.04108v1,No,
0000-0002-8575-7525,Antonia Weber,Reutlingen Universität,The Bose-Marletto-Vedral proposal in different frames of reference and   the quantum nature of gravity,2024,"  Observing spatial entanglement in the Bose-Marletto-Vedral (BMV) experiment would demonstrate the existence of non-classical properties of the gravitational field. We show that the special relativistic invariance of the linear regime of general relativity implies that all the components of the gravitational potential must be non-classical. This is simply necessary in order to describe the BMV entanglement consistently across different inertial frames of reference. On the other hand, we show that the entanglement in accelerated frames could differ from that in stationary frames. ",Kein DOI-Link verfügbar,2406.14334v1,Yes,potent(1)
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Quantum Monte Carlo simulations of infinitely strongly correlated   fermions,1997,"  Numerical simulations of the two-dimensional t-J model in the limit $J/t \ll 1$ are performed for rather large systems (up to $N = 12 \times 12$) using a world-line loop-algorithm. It is shown that in the one-hole case with J=0, where no minus signs appear, very low temperatures ($\beta t \sim 3000$) are necessary in order to reach Nagaoka's state. $J/t \ltsim 0.05$ leads to the formation of partially polarized systems, whereas $J/t \gtsim 0.05 $ corresponds to minimal spin. The two-hole case shows enhanced total spin up to the lowest attainable temperatures ($\beta t = 150$). ",https://doi.org/10.1103/PhysRevB.58.R10100,cond-mat/9707108v1,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Risk Management Practices in Information Security: Exploring the Status   Quo in the DACH Region,2020,"  Information security management aims at ensuring proper protection of information values and information processing systems (i.e. assets). Information security risk management techniques are incorporated to deal with threats and vulnerabilities that impose risks to information security properties of these assets. This paper investigates the current state of risk management practices being used in information security management in the DACH region (Germany, Austria, Switzerland). We used an anonymous online survey targeting strategic and operative information security and risk managers and collected data from 26 organizations. We analyzed general practices, documentation artifacts, patterns of stakeholder collaboration as well as tool types and data sources used by enterprises to conduct information security management activities. Our findings show that the state of practice of information security risk management is in need of improvement. Current industrial practice heavily relies on manual data collection and complex potentially subjective decision processes with multiple stakeholders involved. Dedicated risk management tools and methods are used selectively and neglected in favor of general-purpose documentation tools and direct communication between stakeholders. In light of our results we propose guidelines for the development of risk management practices that are better aligned with the current operational situation in information security management. ",https://doi.org/10.1016/j.cose.2020.101776,2003.07674v1,Yes,potent(1)
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Single hole dynamics in the one dimensional t-J model,1999,"  We present a new finite-temperature quantum Monte Carlo algorithm to compute imaginary-time Green functions for a single hole in the t-J model on non-frustrated lattices. Spectral functions are then obtained with the Maximum Entropy method. Simulations of the one-dimensional case show that a simple charge-spin separation Ansatz is able to describe the overall features of the spectral function over the whole energy range for values of J/t from 1/3 to 4. This includes the bandwidth W \sim 4t + J and the compact support of the spectral function. The quasiparticle weight Z_k is computed on lattices up to L=96 sites, and scales as Z_k\propto L^{-1/2}. ",https://doi.org/10.1007/s100510070220,cond-mat/9904150v1,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Single hole dynamics in the t-J model on a square lattice,2000,"  We present quantum Monte Carlo (QMC) simulations for a single hole in a t-J model from J=0.4t to J=4t on square lattices with up to 24 x 24 sites. The lower edge of the spectrum is directly extracted from the imaginary time Green's function. In agreement with earlier calculations, we find flat bands around $(0,\pm\pi)$, $(\pm\pi,0)$ and the minimum of the dispersion at $(\pm\pi/2,\pm\pi/2)$. For small J both self-consistent Born approximation and series expansions give a bandwidth for the lower edge of the spectrum in agreement with the simulations, whereas for J/t > 1, only series expansions agree quantitatively with our QMC results. This band corresponds to a coherent quasiparticle. This is shown by a finite size scaling of the quasiparticle weight $Z(\vec k)$ that leads to a finite result in the thermodynamic limit for the considered values of $J/t$. The spectral function $A(\vec k, \omega)$ is obtained from the imaginary time Green's function via the maximum entropy method. Resonances above the lowest edge of the spectrum are identified, whose J-dependence is quantitatively described by string excitations up to J/t=2. ",https://doi.org/10.1103/PhysRevB.62.15480,cond-mat/0002321v1,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Single hole dynamics in the t-J model on two- and three-leg ladders,2001,"  The dynamics of a single hole in the t-J model on two- (2LL) and three- (3LL) leg ladders is studied using a recently developed quantum Monte Carlo algorithm. For the 2LL it is shown that in addition to the most pronounced features of the spectral function, well described by the limit of strong coupling along the rungs, a clear shadow band appears in the antibonding channel. Moreover, both the bonding band and its shadow have a finite quasiparticle (QP) weight in the thermodynamic limit. For strong coupling along the rungs of the 3LL, the low-energy spectrum in the antisymmetric channel is similar to a one-dimensional chain, whereas in the two symmetric channels it resembles the 2LL. The QP weight vanishes in the antisymmetric channel, but is finite in the symmetric one. ",https://doi.org/10.1103/PhysRevB.63.180511,cond-mat/0101462v1,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Static quark potential and string tension for compact U(1) in (2+1)   dimensions,2002,"  Compact U(1) lattice gauge theory in (2+1) dimensions is studied on anisotropic lattices using Standard Path Integral Monte Carlo techniques. We extract the static quark potential and the string tension from 1.0 <= Dtau <= 0.333 simulations at 1.0 <= beta <= 3.0. Estimating the actual value of the renormalization constant, (c = 44), we observe the evidence of scaling in the string tension for 1.4142 <= beta <= 2.5; with the asymptotic behaviour in the large-beta limit given by K sqrt(beta) = e^(-2.494 beta +2.29). Extrapolations are made to the extreme anisotropic or ""Hamiltonian"" limit, and comparisons are made with previous estimates obtained by various other methods in the Hamiltonian formulation. ",Kein DOI-Link verfügbar,hep-lat/0208047v1,Yes,potent(1)
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Mass gap in compact U(1) Model in (2+1) dimensions,2002,"  A numerical study of low-lying glueball masses of compact U(1) lattice gauge theory in (2+1) dimensions is performed using Standard Path integral Monte Carlo techniques. The masses are extracted, at fixed (low) temperature, from simulations on anisotropic lattices, with temporal lattice spacing much smaller than the spatial ones. Convincing evidence of the scaling behaviour in the antisymmetric mass gap is observed over the range $1.4<\beta <2.25$. The observed behaviour is very consistent with asymptotic form predicted by G{\"" o}pfert and Mack. Extrapolations are made to the ""Hamiltonian"" limit, and the results are compared with previous estimates obtained by many other Hamiltonian studies. ",Kein DOI-Link verfügbar,hep-lat/0209161v1,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Quantum fluctuation theorems in the strong damping limit,2009,"  We consider a driven quantum particle in the strong friction regime described by the quantum Smoluchowski equation. We derive Crooks and Jarzynski type relations for the reduced quantum system by properly generalizing the entropy production to take into account the non-Gibbsian character of the equilibrium distribution. In the case of a nonequilibrium steady state, we obtain a quantum version of the Hatano-Sasa relation. We, further, propose an experiment with driven Josephson junctions that would allow to investigate nonequilibrium entropy fluctuations in overdamped quantum systems. ",https://doi.org/10.1209/0295-5075/94/30001,0902.1858v2,No,
0000-0002-5126-8002,Michael Brunner,Reutlingen Universität,Path Integral Monte Carlo Approach to the U(1) Lattice Gauge Theory in   (2+1) Dimensions,2002,"  Path Integral Monte Carlo simulations have been performed for U(1) lattice gauge theory in (2+1) dimensions on anisotropic lattices. We extractthe static quark potential, the string tension and the low-lying ""glueball"" spectrum.The Euclidean string tension and mass gap decrease exponentially at weakcoupling in excellent agreement with the predictions of Polyakov and G{\"" o}pfert and Mack, but their magnitudes are five times bigger than predicted. Extrapolations are made to the extreme anisotropic or Hamiltonian limit, and comparisons are made with previous estimates obtained in the Hamiltonian formulation. ",https://doi.org/10.1103/PhysRevD.68.034504,hep-lat/0209159v5,Yes,potent(1)
0000-0002-8816-5541,Uwe Breitenbücher,Reutlingen Universität,Transactional Properties of Permissioned Blockchains,2019,"  Traditional distributed transaction processing (TP) systems, such as replicated databases, faced difficulties in getting wide adoption for scenarios of enterprise integration due to the level of mutual trust required. Ironically, public blockchains, which promised to solve the problem of mutual trust in collaborative processes, suffer from issues like scalability, probabilistic transaction finality, and lack of data confidentiality. To tackle these issues, permissioned blockchains were introduced as an alternative approach combining the positives of the two worlds and avoiding their drawbacks. However, no sufficient analysis has been done to emphasize their actual capabilities regarding TP. In this paper, we identify a suitable collection of TP criteria to analyze permissioned blockchains and apply them to a prominent set of these systems. Finally, we compare the derived properties and provide general conclusions. ",https://doi.org/10.1007/s00450-019-00411-y,1907.13218v2,No,
0009-0009-4056-4883,Arthur Bernhardt,Reutlingen Universität,bloomRF: On Performing Range-Queries with Bloom-Filters based on   Piecewise-Monotone Hash Functions and Dyadic Trace-Trees,2020,"  We introduce bloomRF as a unified method for approximate membership testing that supports both point- and range-queries on a single data structure. bloomRF extends Bloom-Filters with range query support and may replace them. The core idea is to employ a dyadic interval scheme to determine the set of dyadic intervals covering a data point, which are then encoded and inserted. bloomRF introduces Dyadic Trace-Trees as novel data structure that represents those covering intervals implicitly. A Trace-Tree encoding scheme represents the set of covering intervals efficiently, in a compact bit representation. Furthermore, bloomRF introduces novel piecewise-monotone hash functions that are locally order-preserving and thus support range querying. We present an efficient membership computation method for range-queries. Although, bloomRF is designed for integers it also supports string and floating-point data types. It can also handle multiple attributes and serve as multi-attribute filter.   We evaluate bloomRF in RocksDB and in a standalone library. bloomRF is more efficient and outperforms existing point-range-filters by up to 4x across a range of settings. ",Kein DOI-Link verfügbar,2012.15596v1,No,
0009-0009-4056-4883,Arthur Bernhardt,Reutlingen Universität,bloomRF: On Performing Range-Queries in Bloom-Filters with   Piecewise-Monotone Hash Functions and Prefix Hashing,2022,"  We introduce bloomRF as a unified method for approximate membership testing that supports both point- and range-queries. As a first core idea, bloomRF introduces novel prefix hashing to efficiently encode range information in the hash-code of the key itself. As a second key concept, bloomRF proposes novel piecewise-monotone hash-functions that preserve local order and support fast range-lookups with fewer memory accesses. bloomRF has near-optimal space complexity and constant query complexity. Although, bloomRF is designed for integer domains, it supports floating-points, and can serve as a multi-attribute filter. The evaluation in RocksDB and in a standalone library shows that it is more efficient and outperforms existing point-range-filters by up to 4x across a range of settings and distributions, while keeping the false-positive rate low. ",Kein DOI-Link verfügbar,2207.04789v2,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,A new Definition and Classification of Physical Unclonable Functions,2015,"  A new definition of ""Physical Unclonable Functions"" (PUFs), the first one that fully captures its intuitive idea among experts, is presented. A PUF is an information-storage system with a security mechanism that is   1. meant to impede the duplication of a precisely described storage-functionality in another, separate system and   2. remains effective against an attacker with temporary access to the whole original system.   A novel classification scheme of the security objectives and mechanisms of PUFs is proposed and its usefulness to aid future research and security evaluation is demonstrated. One class of PUF security mechanisms that prevents an attacker to apply all addresses at which secrets are stored in the information-storage system, is shown to be closely analogous to cryptographic encryption. Its development marks the dawn of a new fundamental primitive of hardware-security engineering: cryptostorage. These results firmly establish PUFs as a fundamental concept of hardware security. ",https://doi.org/10.1145/2694805.2694807,1501.06363v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,CoRT: A Communication Robustness Testbed for Industrial Control System   Components,2019,"  The number of interconnected devices is growing constantly due to rapid digitalization, thus providing attackers with a larger attack surface. Particularly in critical infrastructures and manufacturing, where processes can be observed and controlled remotely, successful attacks could lead to high costs and damage. Therefore, it is necessary to investigate Industrial Control System (ICS) devices like Programmable Logic Controllers (PLCs) to make these sectors more secure. One possible attack vector is the exploitation of the network communication of devices. Thus, a robust communication system is essential to ensure security. Unfortunately, the high demand for real-world ICSs makes it difficult to assess component security during its runtime. However, this is possible in a research testbed where tests could be done and analyzed in a safe environment. In this paper, we introduce our testbed and measurement methods for communication robustness test research of ICS components. ",Kein DOI-Link verfügbar,1904.04286v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,A Secure Dual-MCU Architecture for Robust Communication of IIoT Devices,2019,"  The Industrial Internet of Things (IIoT) has already become a part of our everyday life be it water supply, smart grid, or production, IIoT is everywhere. For example, factory operators want to know the current state of the production line. These new demands for data acquisition in modern plants require industrial components to be able to communicate. Nowadays, network communication in Industrial Control Systems (ICSs) is often implemented via an IP-based protocol. This intercommunication also brings a larger attack surface for hackers. If an IIoT device is influenced by attackers, the physical process could be affected. For example, a high network load could cause a high Central Processing Unit (CPU) load and influence the reaction time on the physical control side. In this paper, we introduce a dual Microcontroller Unit (MCU) setup to ensure a resilient controlling for IIoT devices like Programmable Logic Controllers (PLCs). We introduce a possible solution for the demand of secure architectures in the IIoT. Moreover, we provide a Proof of Concept (PoC) implementation with a benchmark and a comparison with a standard PLC. ",https://doi.org/10.1109/MECO.2019.8760188,1908.04133v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,LICSTER -- A Low-cost ICS Security Testbed for Education and Research,2019,"  Unnoticed by most people, Industrial Control Systems (ICSs) control entire productions and critical infrastructures such as water distribution, smart grid and automotive manufacturing. Due to the ongoing digitalization, these systems are becoming more and more connected in order to enable remote control and monitoring. However, this shift bears significant risks, namely a larger attack surface, which can be exploited by attackers. In order to make these systems more secure, it takes research, which is, however, difficult to conduct on productive systems, since these often have to operate twenty-four-seven. Testbeds are mostly very expensive or based on simulation with no real-world physical process. In this paper, we introduce LICSTER, an open-source low-cost ICS testbed, which enables researchers and students to get hands-on experience with industrial security for about 500 Euro. We provide all necessary material to quickly start ICS hacking, with the focus on low-cost and open-source for education and research. ",https://doi.org/10.14236/ewic/icscsr19.1,1910.00303v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,Network Scanning and Mapping for IIoT Edge Node Device Security,2019,"  The amount of connected devices in the industrial environment is growing continuously, due to the ongoing demands of new features like predictive maintenance. New business models require more data, collected by IIoT edge node sensors based on inexpensive and low performance Microcontroller Units (MCUs). A negative side effect of this rise of interconnections is the increased attack surface, enabled by a larger network with more network services. Attaching badly documented and cheap devices to industrial networks often without permission of the administrator even further increases the security risk. A decent method to monitor the network and detect ""unwanted"" devices is network scanning. Typically, this scanning procedure is executed by a computer or server in each sub-network. In this paper, we introduce network scanning and mapping as a building block to scan directly from the Industrial Internet of Things (IIoT) edge node devices. This module scans the network in a pseudo-random periodic manner to discover devices and detect changes in the network structure. Furthermore, we validate our approach in an industrial testbed to show the feasibility of this approach. ",https://doi.org/10.23919/AE.2019.8867032,1910.07622v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,On the Security of IO-Link Wireless Communication in the Safety Domain,2022,"  Security is an essential requirement of Industrial Control System (ICS) environments and its underlying communication infrastructure. Especially the lowest communication level within Supervisory Control and Data Acquisition (SCADA) systems - the field level - commonly lacks security measures. Since emerging wireless technologies within field level expose the lowest communication infrastructure towards potential attackers, additional security measures above the prevalent concept of air-gapped communication must be considered. Therefore, this work analyzes security aspects for the wireless communication protocol IO-LinkWireless (IOLW), which is commonly used for sensor and actuator field level communication. A possible architecture for an IOLW safety layer has already been presented recently [1]. In this paper, the overall attack surface of IOLW within its typical environment is analyzed and attack preconditions are investigated to assess the effectiveness of different security measures. Additionally, enhanced security measures are evaluated for the communication systems and the results are summarized. Also, interference of security measures and functional safety principles within the communication are investigated, which do not necessarily complement one another but may also have contradictory requirements. This work is intended to discuss and propose enhancements of the IOLW standard with additional security considerations in future implementations. ",Kein DOI-Link verfügbar,2207.12938v3,Yes,potent(1)
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,Efficient Intrusion Detection on Low-Performance Industrial IoT Edge   Node Devices,2019,"  Communication between sensors, actors and Programmable Logic Controllers (PLCs) in industrial systems moves from two-wire field buses to IP-based protocols such as Modbus/TCP. This increases the attack surface because the IP-based network is often reachable from everywhere within the company. Thus, centralized defenses, e.g. at the perimeter of the network do not offer sufficient protection. Rather, decentralized defenses, where each part of the network protects itself, are needed. Network Intrusion Detection Systems (IDSs) monitor the network and report suspicious activity. They usually run on a single host and are not able to capture all events in the network and they are associated with a great integration effort. To bridge this gap, we introduce a method for intrusion detection that combines distributed agents on Industrial Internet of Things (IIoT) edge devices with a centralized logging. In contrast to existing IDSs, the distributed approach is suitable for industrial low performance microcontrollers. We demonstrate a Proof of Concept (PoC) implementation on a MCU running FreeRTOS with LwIP and show the feasibility of our approach in an IIoT application. ",Kein DOI-Link verfügbar,1908.03964v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,Testbed for Functional Safety-Relevant Wireless Communication Based on   IO-Link Wireless and 5G,2022,"  In the field of industrial production automation, wireless networks support highly flexible manufacturing processes and enable technologies to set-up new production chains and future software businesses. The IO-Link Wireless (IOLW) protocol is an already established energy-efficient and cost-effective communication standard for smart sensor devices on the industrial shop floor, whereas the mobile communication standard 5G will be mainly applied for medium and long-range wireless communication applications promising low latency times and high reliability. Therefore, 5G with the coming enhancement of deterministic ultra-Reliable Low-Latency Communication (uRLLC) is combined with the robustness and low-latency performance characteristics of IO-Link Wireless. Features of both technologies are highly beneficial to realize even highly demanding safety-related applications. The presented testbed shall qualify wireless functional safety communication with respect to its Residual Error Probability (REP) and quantify the Probability of Failure per Hour (PFH). ",https://doi.org/10.24405/14544,2212.14364v1,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,Efficient Passive ICS Device Discovery and Identification by MAC Address   Correlation,2019,"  Owing to a growing number of attacks, the assessment of Industrial Control Systems (ICSs) has gained in importance. An integral part of an assessment is the creation of a detailed inventory of all connected devices, enabling vulnerability evaluations. For this purpose, scans of networks are crucial. Active scanning, which generates irregular traffic, is a method to get an overview of connected and active devices. Since such additional traffic may lead to an unexpected behavior of devices, active scanning methods should be avoided in critical infrastructure networks. In such cases, passive network monitoring offers an alternative, which is often used in conjunction with complex deep-packet inspection techniques. There are very few publications on lightweight passive scanning methodologies for industrial networks. In this paper, we propose a lightweight passive network monitoring technique using an efficient Media Access Control (MAC) address-based identification of industrial devices. Based on an incomplete set of known MAC address to device associations, the presented method can guess correct device and vendor information. Proving the feasibility of the method, an implementation is also introduced and evaluated regarding its efficiency. The feasibility of predicting a specific device/vendor combination is demonstrated by having similar devices in the database. In our ICS testbed, we reached a host discovery rate of 100% at an identification rate of more than 66%, outperforming the results of existing tools. ",https://doi.org/10.14236/ewic/ICS2018.3,1904.04271v2,No,
0000-0003-2310-5895,Dominik Merli,Augsburg Universität der Angewandte Wissenschaften,Analysis of Industrial Device Architectures for Real-Time Operations   under Denial of Service Attacks,2020,"  More and more industrial devices are connected to IP-based networks, as this is essential for the success of Industry 4.0. However, this interconnection also results in an increased attack surface for various network-based attacks. One of the easiest attacks to carry out are DoS attacks, in which the attacked target is overloaded due to high network traffic and corresponding CPU load. Therefore, the attacked device can no longer provide its regular services. This is especially critical for devices, which perform real-time operations in industrial processes. To protect against DoS attacks, there is the possibility of throttling network traffic at the perimeter, e.g. by a firewall, to develop robust device architectures. In this paper, we analyze various concepts for secure device architectures and compare them with regard to their robustness against DoS attacks. Here, special attention is paid to how the control process of an industrial controller behaves during the attack. For this purpose, we compare different schedulers on single-core and dual-core Linux-based systems, as well as a heterogeneous multi-core architecture under various network loads and additional system stress. ",Kein DOI-Link verfügbar,2007.08885v1,No,
0000-0003-1320-0338,Kay Hofmann,Hochschule Osnabrück,Lane formation in gravitationally driven colloid mixtures consisting of   up to three different particle sizes,2024,"  Brownian dynamics simulations are utilized to study segregation phenomena far from thermodynamic equilibrium. In the present study, we expand upon the analysis of binary colloid mixtures and additionally introduce a third particle species to further our understanding of colloidal systems. Gravitationally driven, spherical colloids immersed in an implicit solvent are confined in two-dimensional linear microchannels. The interaction between the colloids is modeled by the Weeks-Chandler-Andersen potential, and the confinement of the colloids is realized by hard walls based on the solution of the Smoluchowski equation in half space. In binary and ternary colloidal systems, a difference in the driving force is achieved by differing colloid sizes, but fixed mass density. We observe for both the binary and ternary systems that a driving force difference induces a nonequilibrium phase transition to lanes. For ternary systems, we study the tendency of lane formation in dependence of the diameter of the medium-sized colloids. Here, we find a sweetspot for lane formation in ternary systems. Furthermore, we study the interaction of two differently sized colloids at the channel walls. Recently, we observed that driven large colloids push smaller colloids to the walls. This results in small particle lanes at the walls at early simulation times. In this work, we additionally find that thin lanes are unstable and dissolve over very long time frames. Furthermore, we observe a connection between lane formation and the nonuniform distribution of particles along the channel length. This nonuniform distribution occurs either alongside lane formation or in shared lanes (i.e. lanes consisting of two colloid types). ",https://doi.org/10.1103/PhysRevE.109.064601,2401.05067v1,Yes,potent(1)
0000-0002-0298-976X,Thomas Schulz,Ernst-Abbe-Hochschule Jena,"Hexavalent (Me-W/Mo)-modified (Ba,Ca)TiO$_3$-Bi(Mg,Me)O$_3$ perovskites   for high-temperature dielectrics",2021,"  We report on the synthesis of complex lead-free perovskite-type (1-x)(Ba$_{0.8}$Ca$_{0.2}$)TiO$_3$-xBi(Mg$_{0.75}$W$_{0.25}$)O$_3$ (BCT-xBMW) and (1-x)(Ba$_{0.8}$Ca$_{0.2}$)TiO$_3$-xBi(Mg$_{0.75}$Mo$_{0.25}$)O$_3$ (BCT-xBMM) solid solutions via conventional solid-state reaction route. The sintering temperature was adjusted as a function of composition x to obtain dense samples (relative densities over 95%) at the same time minimizing bismuth evaporation. X-ray diffraction analysis shows formation of single-phase perovskites for $0 \le x \le 0.10$ in the BCT-xBMW series and increasing concentrations of impurity phases for $x \ge 0.15$ and for $x \ge 0.05$ in BCT-xBMM. A transition from a tetragonal to pseudo-cubic perovskite structure is observed in BCT-xBMW and BCT-xBMM at $x = 0.05$. The dielectric response has been characterized between -60 $^\circ$C and 300 $^\circ$C for BCT-xBMW, and between 30 $^\circ$C and 300 $^\circ$C for BCT-xBMM using impedance spectroscopy, showing a transition from ferroelectric to relaxor-like behavior at $x \ge 0.05$. Additional polarization and Raman spectroscopy measurements reveal the occurrence of highly disordered systems. Analysis of the Raman spectra indicates structural phase changes and lattice modifications caused by chemical substitution. For the composition 0.8Ba$_{0.8}$Ca$_{0.2}$TiO$_3$-0.2Bi(Mg$_{0.75}$W$_{0.25}$)O$_3$ a temperature-stable permittivity of about 600 ($\pm 15$% between 60 $^\circ$C and 300 $^\circ$C) and small losses of $\tan\delta < 0.02$ for $T \le 230$ $^\circ$C at 1 kHz are observed, making it a suitable dielectric material for high temperature capacitors. ",https://doi.org/10.1111/jace.17403,2111.00940v1,No,
0000-0002-1466-3206,Alexander Riedel,Ernst-Abbe-Hochschule Jena,Adapting Brain-Like Neural Networks for Modeling Cortical Visual   Prostheses,2022,"  Cortical prostheses are devices implanted in the visual cortex that attempt to restore lost vision by electrically stimulating neurons. Currently, the vision provided by these devices is limited, and accurately predicting the visual percepts resulting from stimulation is an open challenge. We propose to address this challenge by utilizing 'brain-like' convolutional neural networks (CNNs), which have emerged as promising models of the visual system. To investigate the feasibility of adapting brain-like CNNs for modeling visual prostheses, we developed a proof-of-concept model to predict the perceptions resulting from electrical stimulation. We show that a neurologically-inspired decoding of CNN activations produces qualitatively accurate phosphenes, comparable to phosphenes reported by real patients. Overall, this is an essential first step towards building brain-like models of electrical stimulation, which may not just improve the quality of vision provided by cortical prostheses but could also further our understanding of the neural code of vision. ",Kein DOI-Link verfügbar,2209.13561v1,No,
0000-0002-1466-3206,Alexander Riedel,Ernst-Abbe-Hochschule Jena,PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural   Machine Learning Models,2021,"  We present PyTorch Geometric Temporal a deep learning framework combining state-of-the-art machine learning algorithms for neural spatiotemporal signal processing. The main goal of the library is to make temporal geometric deep learning available for researchers and machine learning practitioners in a unified easy-to-use framework. PyTorch Geometric Temporal was created with foundations on existing libraries in the PyTorch eco-system, streamlined neural network layer definitions, temporal snapshot generators for batching, and integrated benchmark datasets. These features are illustrated with a tutorial-like case study. Experiments demonstrate the predictive performance of the models implemented in the library on real world problems such as epidemiological forecasting, ridehail demand prediction and web-traffic management. Our sensitivity analysis of runtime shows that the framework can potentially operate on web-scale datasets with rich temporal features and spatial structure. ",Kein DOI-Link verfügbar,2104.07788v3,Yes,potent(1)
0009-0003-9736-8775,Christopher Schneider,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Optimized FTO seeding enables the growth of high efficient Ta-doped   TiO$_2$ nanorod photoanodes,2020,"  Tantalum doped rutile nanorods were hydrothermally grown on FTO substrates using a new seeding approach. This approach allows the incorporation of high concentrations of up to 4.8 at% tantalum as active doping and results in a significant enhancement of photoelectrochemical water splitting rate (1.8 mA/cm2 at a potential of +1.5 V vs RHE) which corresponds to ca. 1% photocurrent conversion efficiency under AM 1.5, 100 mW/cm2 simulated sunlight irradiation. ",https://doi.org/10.1039/C7CC05168A,2005.02266v1,Yes,potent(1)
0009-0003-9736-8775,Christopher Schneider,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Benson-Type Algorithm for Bounded Convex Vector Optimization Problems   with Vertex Selection,2020,"  We present an algorithm for approximately solving bounded convex vector optimization problems. The algorithm provides both an outer and an inner polyhedral approximation of the upper image. It is a modification of the primal algorithm presented by L\""ohne, Rudloff, and Ulus in 2014. There, vertices of an already known outer approximation are successively cut off to improve the approximation error. We propose a new and efficient selection rule for deciding which vertex to cut off. Numerical examples are provided which illustrate that this method may solve fewer scalar problems overall and therefore may be faster while achieving the same approximation quality. ",https://doi.org/10.1080/10556788.2021.1880579,2006.15600v1,No,
0009-0003-9736-8775,Christopher Schneider,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Black Magic in Gray Titania: Noble-Metal-Free Photocatalytic H2   Evolution from Hydrogenated Anatase,2020,"  ""Black"" TiO2 has gained increasing interest because of its outstanding properties and promising applications in a wide range of fields. Among the outstanding features of the material is that certain synthesis processes lead to the formation of an intrinsic co-catalytic center and thus enable noble-metal free photocatalytic H2 generation. In this work, we report ""grey TiO2"" by an appropriate hydrogenation treatment exhibits excellent photocatalytic hydrogen. In this case, by the employment of thermally stable and high-surface-area TiO2 nanoparticles as well as mesoporous particles as the hydrogenation precursor, the appropriate extent of reduction of TiO2 (coloration) and the formation of Ti3+ is the key for the efficient noble-metal-free photocatalytic H2 generation. The EPR results reveal that ""grey TiO2"" shows stronger Ti3+ feature at g ca. 1.93 than ""black TiO2"" contributing to the intrinsic catalytic center for H2 evolution. ",https://doi.org/10.1002/cssc.201601264,2004.11800v1,No,
0009-0003-9736-8775,Christopher Schneider,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Hydrogenated anatase: Strong photocatalytic H2 evolution without the use   of a co-catalyst,2016,"  In the present work we show, how a high pressure hydrogenation of commercial anatase or anatase/rutile powder can create a photocatalyst for hydrogen evolution that is highly effective and stable without the need of any additional co-catalyst. This activation effect can not be observed for rutile. For anatase/rutile mixtures, however, a strong synergistic effect is found (similar to findings commonly observed for noble metal decorated TiO2). ESR measurements indicate the intrinsic co-catalytic activation of anatase TiO2 to be due to specific defect centers formed during hydrogenation. ",https://doi.org/10.1002/anie.201408493,1610.06564v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Compositional Query Algebra for Second-Order Logic and Uncertain   Databases,2008,"  World-set algebra is a variable-free query language for uncertain databases. It constitutes the core of the query language implemented in MayBMS, an uncertain database system. This paper shows that world-set algebra captures exactly second-order logic over finite structures, or equivalently, the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition, a previously open problem. ",Kein DOI-Link verfügbar,0807.4620v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Optimizing Queries Using a Meta-level Database,2002,"  Graph simulation (using graph schemata or data guides) has been successfully proposed as a technique for adding structure to semistructured data. Design patterns for description (such as meta-classes and homomorphisms between schema layers), which are prominent in the object-oriented programming community, constitute a generalization of this graph simulation approach.   In this paper, we show description applicable to a wide range of data models that have some notion of object (-identity), and propose to turn it into a data model primitive much like, say, inheritance. We argue that such an extension fills a practical need in contemporary data management. Then, we present algebraic techniques for query optimization (using the notions of described and description queries). Finally, in the semistructured setting, we discuss the pruning of regular path queries (with nested conditions) using description meta-data. In this context, our notion of meta-data extends graph schemata and data guides by meta-level values, allowing to boost query performance and to reduce the redundancy of data. ",Kein DOI-Link verfügbar,cs/0205060v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,On the Complexity of Nonrecursive XQuery and Functional Query Languages   on Complex Values,2005,"  This paper studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery.   We show that monad algebra with equality restricted to atomic values is complete for the class TA[2^{O(n)}, O(n)] of problems solvable in linear exponential time with a linear number of alternations. The monotone fragment of monad algebra with atomic value equality but without negation is complete for nondeterministic exponential time. For monad algebra with deep equality, we establish TA[2^{O(n)}, O(n)] lower and exponential-space upper bounds.   Then we study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with ``child'' as the only axis) is exhibited, and it is shown that these languages are expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra w.r.t. combined complexity, and that it is in TC0 if the query is assumed fixed. ",Kein DOI-Link verfügbar,cs/0503062v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Visual Query Language for Complex-Value Databases,2006,"  In this paper, a visual language, VCP, for queries on complex-value databases is proposed. The main strength of the new language is that it is purely visual: (i) It has no notion of variable, quantification, partiality, join, pattern matching, regular expression, recursion, or any other construct proper to logical, functional, or other database query languages and (ii) has a very natural, strong, and intuitive design metaphor. The main operation is that of copying and pasting in a schema tree.   We show that despite its simplicity, VCP precisely captures complex-value algebra without powerset, or equivalently, monad algebra with union and difference. Thus, its expressive power is precisely that of the language that is usually considered to play the role of relational algebra for complex-value databases. ",Kein DOI-Link verfügbar,cs/0602006v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Formal Comparison of Visual Web Wrapper Generators,2003,  We study the core fragment of the Elog wrapping language used in the Lixto system (a visual wrapper generator) and formally compare Elog to other wrapping languages proposed in the literature. ,Kein DOI-Link verfügbar,cs/0310012v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Monadic Datalog and the Expressive Power of Languages for Web   Information Extraction,2002,"  Research on information extraction from Web pages (wrapping) has seen much activity recently (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping. In this paper, we first study monadic datalog over trees as a wrapping language. We show that this simple language is equivalent to monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and propose MSO as a yardstick for evaluating and comparing wrappers. Along the way, several other results on the complexity of query evaluation and query containment for monadic datalog over trees are established, and a simple normal form for this language is presented. Using the above results, we subsequently study the kernel fragment Elog$^-$ of the Elog wrapping language used in the Lixto system (a visual wrapper generator). Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. ",Kein DOI-Link verfügbar,cs/0211020v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Bootstrap percolation on geometric inhomogeneous random graphs,2016,"  Geometric inhomogeneous random graphs (GIRGs) are a model for scale-free networks with underlying geometry. We study bootstrap percolation on these graphs, which is a process modelling the spread of an infection of vertices starting within a (small) local region. We show that the process exhibits a phase transition in terms of the initial infection rate in this region. We determine the speed of the process in the supercritical case, up to lower order terms, and show that its evolution is fundamentally influenced by the underlying geometry. For vertices with given position and expected degree, we determine the infection time up to lower order terms. Finally, we show how this knowledge can be used to contain the infection locally by removing relatively few edges from the graph. This is the first time that the role of geometry on bootstrap percolation is analysed mathematically for geometric scale-free networks. ",Kein DOI-Link verfügbar,1603.02057v3,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Tight Lower Bounds for Query Processing on Streaming and External Memory   Data,2005,"  We study a clean machine model for external memory and stream processing. We show that the number of scans of the external data induces a strict hierarchy (as long as work space is sufficiently small, e.g., polylogarithmic in the size of the input). We also show that neither joins nor sorting are feasible if the product of the number $r(n)$ of scans of the external memory and the size $s(n)$ of the internal memory buffers is sufficiently small, e.g., of size $o(\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath evaluation and filtering. ",Kein DOI-Link verfügbar,cs/0505002v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,The size of the giant component in random hypergraphs,2015,"  The phase transition in the size of the giant component in random graphs is one of the most well-studied phenomena in random graph theory. For hypergraphs, there are many possible generalisations of the notion of a component, and for all but the simplest example, the phase transition phenomenon was first proved by Cooley, Kang and Person. In this paper we build on this and determine the asymptotic size of the unique giant component. ",Kein DOI-Link verfügbar,1501.07835v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Conditioning Probabilistic Databases,2008,"  Past research on probabilistic databases has studied the problem of answering queries on a static database. Application scenarios of probabilistic databases however often involve the conditioning of a database using additional information in the form of new evidence. The conditioning problem is thus to transform a probabilistic database of priors into a posterior probabilistic database which is materialized for subsequent query processing or further refinement. It turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values.   It is known that exact confidence computation is an NP-hard problem. This has led researchers to consider approximation techniques for confidence computation. However, neither conditioning nor exact confidence computation can be solved using such techniques.   In this paper we present efficient techniques for both problems. We study several problem decomposition methods and heuristics that are based on the most successful search techniques from constraint satisfaction, such as the Davis-Putnam algorithm. We complement this with a thorough experimental evaluation of the algorithms proposed. Our experiments show that our exact algorithms scale well to realistic database sizes and can in some scenarios compete with the most efficient previous approximation algorithms. ",Kein DOI-Link verfügbar,0803.2212v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Cooperative Update Exchange in the Youtopia System,2009,"  Youtopia is a platform for collaborative management and integration of relational data. At the heart of Youtopia is an update exchange abstraction: changes to the data propagate through the system to satisfy user-specified mappings. We present a novel change propagation model that combines a deterministic chase with human intervention. The process is fundamentally cooperative and gives users significant control over how mappings are repaired. An additional advantage of our model is that mapping cycles can be permitted without compromising correctness.   We investigate potential harmful interference between updates in our model; we introduce two appropriate notions of serializability that avoid such interference if enforced. The first is very general and related to classical final-state serializability; the second is more restrictive but highly practical and related to conflict-serializability. We present an algorithm to enforce the latter notion. Our algorithm is an optimistic one, and as such may sometimes require updates to be aborted. We develop techniques for reducing the number of aborts and we test these experimentally. ",Kein DOI-Link verfügbar,0903.5346v1,Yes,potent(1)
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Approximation Schemes for Many-Objective Query Optimization,2014,  The goal of multi-objective query optimization (MOQO) is to find query plans that realize a good compromise between conflicting objectives such as minimizing execution time and minimizing monetary fees in a Cloud scenario. A previously proposed exhaustive MOQO algorithm needs hours to optimize even simple TPC-H queries. This is why we propose several approximation schemes for MOQO that generate guaranteed near-optimal plans in seconds where exhaustive optimization takes hours.   We integrated all MOQO algorithms into the Postgres optimizer and present experimental results for TPC-H queries; we extended the Postgres cost model and optimize for up to nine conflicting objectives in our experiments. The proposed algorithms are based on a formal analysis of typical cost functions that occur in the context of MOQO. We identify properties that hold for a broad range of objectives and can be exploited for the design of future MOQO algorithms. ,Kein DOI-Link verfügbar,1404.0046v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Multiple Query Optimization on the D-Wave 2X Adiabatic Quantum Computer,2015,"  The D-Wave adiabatic quantum annealer solves hard combinatorial optimization problems leveraging quantum physics. The newest version features over 1000 qubits and was released in August 2015. We were given access to such a machine, currently hosted at NASA Ames Research Center in California, to explore the potential for hard optimization problems that arise in the context of databases.   In this paper, we tackle the problem of multiple query optimization (MQO). We show how an MQO problem instance can be transformed into a mathematical formula that complies with the restrictive input format accepted by the quantum annealer. This formula is translated into weights on and between qubits such that the configuration minimizing the input formula can be found via a process called adiabatic quantum annealing. We analyze the asymptotic growth rate of the number of required qubits in the MQO problem dimensions as the number of qubits is currently the main factor restricting applicability. We experimentally compare the performance of the quantum annealer against other MQO algorithms executed on a traditional computer. While the problem sizes that can be treated are currently limited, we already find a class of problem instances where the quantum annealer is three orders of magnitude faster than other approaches. ",Kein DOI-Link verfügbar,1510.06437v1,Yes,potent(1)
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Probably Approximately Optimal Query Optimization,2015,"  Evaluating query predicates on data samples is the only way to estimate their selectivity in certain scenarios. Finding a guaranteed optimal query plan is not a reasonable optimization goal in those cases as it might require an infinite number of samples. We therefore introduce probably approximately optimal query optimization (PAO) where the goal is to find a query plan whose cost is near-optimal with a certain probability. We will justify why PAO is a suitable formalism to model scenarios in which predicate sampling and optimization need to be interleaved.   We present the first algorithm for PAO. Our algorithm is non-intrusive and uses standard query optimizers and sampling components as sub-functions. It is generic and can be applied to a wide range of scenarios. Our algorithm is iterative and calculates in each iteration a query plan together with a region in the selectivity space where the plan has near-optimal cost. It determines the confidence that the true selectivity values fall within the aforementioned region and chooses the next samples to take based on the current state if the confidence does not reach the threshold specified as problem input. We devise different algorithm variants and analyze their complexity. We experimentally compare them in terms of the number of optimizer invocations, samples, and iterations over many different query classes. ",Kein DOI-Link verfügbar,1511.01782v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Solving the Join Ordering Problem via Mixed Integer Linear Programming,2015,"  We transform join ordering into a mixed integer linear program (MILP). This allows to address query optimization by mature MILP solver implementations that have evolved over decades and steadily improved their performance. They offer features such as anytime optimization and parallel search that are highly relevant for query optimization.   We present a MILP formulation for searching left-deep query plans. We use sets of binary variables to represent join operands and intermediate results, operator implementation choices or the presence of interesting orders. Linear constraints restrict value assignments to the ones representing valid query plans. We approximate the cost of scan and join operations via linear functions, allowing to increase approximation precision up to arbitrary degrees. Our experimental results are encouraging: we are able to find optimal plans for joins between 60 tables; a query size that is beyond the capabilities of prior exhaustive query optimization methods. ",Kein DOI-Link verfügbar,1511.02071v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Fast Randomized Algorithm for Multi-Objective Query Optimization,2016,"  Query plans are compared according to multiple cost metrics in multi-objective query optimization. The goal is to find the set of Pareto plans realizing optimal cost tradeoffs for a given query. So far, only algorithms with exponential complexity in the number of query tables have been proposed for multi-objective query optimization. In this work, we present the first algorithm with polynomial complexity in the query size.   Our algorithm is randomized and iterative. It improves query plans via a multi-objective version of hill climbing that applies multiple transformations in each climbing step for maximal efficiency. Based on a locally optimal plan, we approximate the Pareto plan set within the restricted space of plans with similar join orders. We maintain a cache of Pareto-optimal plans for each potentially useful intermediate result to share partial plans that were discovered in different iterations. We show that each iteration of our algorithm performs in expected polynomial time based on an analysis of the expected path length between a random plan and local optima reached by hill climbing. We experimentally show that our algorithm can optimize queries with hundreds of tables and outperforms other randomized algorithms such as the NSGA-II genetic algorithm over a wide range of scenarios. ",Kein DOI-Link verfügbar,1603.00400v1,Yes,potent(1)
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Parallelizing Query Optimization on Shared-Nothing Architectures,2015,"  Data processing systems offer an ever increasing degree of parallelism on the levels of cores, CPUs, and processing nodes. Query optimization must exploit high degrees of parallelism in order not to gradually become the bottleneck of query evaluation. We show how to parallelize query optimization at a massive scale.   We present algorithms for parallel query optimization in left-deep and bushy plan spaces. At optimization start, we divide the plan space for a given query into partitions of equal size that are explored in parallel by worker nodes. At the end of optimization, each worker returns the optimal plan in its partition to the master which determines the globally optimal plan from the partition-optimal plans. No synchronization or data exchange is required during the actual optimization phase. The amount of data sent over the network, at the start and at the end of optimization, as well as the complexity of serial steps within our algorithms increase only linearly in the number of workers and in the query size. The time and space complexity of optimization within one partition decreases uniformly in the number of workers. We parallelize single- and multi-objective query optimization over a cluster with 100 nodes in our experiments, using more than 250 concurrent worker threads (Spark executors). Despite high network latency and task assignment overheads, parallelization yields speedups of up to one order of magnitude for large queries whose optimization takes minutes on a single node. ",Kein DOI-Link verfügbar,1511.01768v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A Compiler-Compiler for DSL Embedding,2018,"  In this paper, we present a framework to generate compilers for embedded domain-specific languages (EDSLs). This framework provides facilities to automatically generate the boilerplate code required for building DSL compilers on top of extensible optimizing compilers. We evaluate the practicality of our framework by demonstrating several use-cases successfully built with it. ",Kein DOI-Link verfügbar,1808.01344v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Schema-based Scheduling of Event Processors and Buffer Minimization for   Queries on Structured Data Streams,2004,"  We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach. ",Kein DOI-Link verfügbar,cs/0406016v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Conjunctive Queries over Trees,2006,"  We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as ``child'', ``descendant'', and ``following'' as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, i.e., we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size. ",Kein DOI-Link verfügbar,cs/0602004v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,10^(10^6) Worlds and Beyond: Efficient Representation and Processing of   Incomplete Information,2006,"  Current systems and formalisms for representing incomplete information generally suffer from at least one of two weaknesses. Either they are not strong enough for representing results of simple queries, or the handling and processing of the data, e.g. for query evaluation, is intractable.   In this paper, we present a decomposition-based approach to addressing this problem. We introduce world-set decompositions (WSDs), a space-efficient formalism for representing any finite set of possible worlds over relational databases. WSDs are therefore a strong representation system for any relational query language. We study the problem of efficiently evaluating relational algebra queries on sets of worlds represented by WSDs. We also evaluate our technique experimentally in a large census data scenario and show that it is both scalable and efficient. ",Kein DOI-Link verfügbar,cs/0606075v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,"DBToaster: Higher-order Delta Processing for Dynamic, Frequently Fresh   Views",2012,"  Applications ranging from algorithmic trading to scientific data analysis require realtime analytics based on views over databases that change at very high rates. Such views have to be kept fresh at low maintenance cost and latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this paper, we present viewlet transforms, a recursive finite differencing technique applied to queries. The viewlet transform materializes a query and a set of its higher-order deltas as views. These views support each other's incremental maintenance, leading to a reduced overall view maintenance cost. The viewlet transform of a query admits efficient evaluation, the elimination of certain expensive query operations, and aggressive parallelization. We develop viewlet transforms into a workable query execution technique, present a heuristic and cost-based optimization framework, and report on experiments with a prototype dynamic data management system that combines viewlet transforms with an optimizing compilation technique. The system supports tens of thousands of complete view refreshes a second for a wide range of queries. ",Kein DOI-Link verfügbar,1207.0137v1,Yes,fresh(2)
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,"The phase transition in the multi-type binomial random graph   $G(\mathbf{n},P)$",2014,"  We determine the asymptotic size of the largest component in the $2$-type binomial random graph $G(\mathbf{n},P)$ near criticality using a refined branching process approach. In $G(\mathbf{n},P)$ every vertex has one of two types, the vector $\mathbf{n}$ describes the number of vertices of each type, and any edge $\{u,v\}$ is present independently with a probability that is given by an entry of the probability matrix $P$ according to the types of $u$ and $v.$ We prove that in the weakly supercritical regime, i.e. if the distance to the critical point of the phase transition is given by an $\varepsilon=\varepsilon(\mathbf{n})\to0,$ with probability $1-o(1),$ the largest component in $G(\mathbf{n},P)$ contains asymptotically $2\varepsilon \|\mathbf{n}\|_1$ vertices and all other components are of size $o(\varepsilon \|\mathbf{n}\|_1).$ ",Kein DOI-Link verfügbar,1407.6248v3,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Evolution of high-order connected components in random hypergraphs,2017,"  We consider high-order connectivity in $k$-uniform hypergraphs defined as follows: Two $j$-sets are $j$-connected if there is a walk of edges between them such that two consecutive edges intersect in at least $j$ vertices. We describe the evolution of $j$-connected components in the $k$-uniform binomial random hypergraph $\mathcal{H}^k(n,p)$. In particular, we determine the asymptotic size of the giant component shortly after its emergence and establish the threshold at which the $\mathcal{H}^k(n,p)$ becomes $j$-connected with high probability. We also obtain a hitting time result for the related random hypergraph process $\{\mathcal{H}^k(n,M)\}_M$ -- the hypergraph becomes $j$-connected exactly at the moment when the last isolated $j$-set disappears. This generalises well-known results for graphs and vertex-connectivity in hypergraphs. ",https://doi.org/10.1016/j.endm.2015.06.077,1704.05732v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Bivariate fluctuations for the number of arithmetic progressions in   random sets,2019,"  We study arithmetic progressions $\{a,a+b,a+2b,\dots,a+(\ell-1) b\}$, with $\ell\ge 3$, in random subsets of the initial segment of natural numbers $[n]:=\{1,2,\dots, n\}$. Given $p\in[0,1]$ we denote by $[n]_p$ the random subset of $[n]$ which includes every number with probability $p$, independently of one another. The focus lies on sparse random subsets, i.e.\ when $p=p(n)=o(1)$ as $n\to+\infty$.   Let $X_\ell$ denote the number of distinct arithmetic progressions of length $\ell$ which are contained in $[n]_p$. We determine the limiting distribution for $X_\ell$ not only for fixed $\ell\ge 3$ but also when $\ell=\ell(n)\to+\infty$. The main result concerns the joint distribution of the pair $(X_{\ell},X_{\ell'})$, $\ell>\ell'$, for which we prove a bivariate central limit theorem for a wide range of $p$. Interestingly, the question of whether the limiting distribution is trivial, degenerate, or non-trivial is characterised by the asymptotic behaviour (as $n\to+\infty$) of the threshold function $\psi_\ell=\psi_\ell(n):=np^{\ell-1}\ell$. The proofs are based on the method of moments and combinatorial arguments, such as an algorithmic enumeration of collections of arithmetic progressions. ",Kein DOI-Link verfügbar,1902.04176v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,The size of the giant component in random hypergraphs: a short proof,2018,"  We consider connected components in $k$-uniform hypergraphs for the following notion of connectedness: given integers $k\ge 2$ and $1\le j \le k-1$, two $j$-sets (of vertices) lie in the same $j$-component if there is a sequence of edges from one to the other such that consecutive edges intersect in at least $j$ vertices.   We prove that certain collections of $j$-sets constructed during a breadth-first search process on $j$-components in a random $k$-uniform hypergraph are reasonably regularly distributed with high probability. We use this property to provide a short proof of the asymptotic size of the giant $j$-component shortly after it appears. ",Kein DOI-Link verfügbar,1803.02809v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Jigsaw percolation on random hypergraphs,2016,"  The jigsaw percolation process on graphs was introduced by Brummitt, Chatterjee, Dey, and Sivakoff as a model of collaborative solutions of puzzles in social networks. Percolation in this process may be viewed as the joint connectedness of two graphs on a common vertex set. Our aim is to extend a result of Bollob\'as, Riordan, Slivken, and Smith concerning this process to hypergraphs for a variety of possible definitions of connectedness. In particular, we determine the asymptotic order of the critical threshold probability for percolation when both hypergraphs are chosen binomially at random. ",Kein DOI-Link verfügbar,1603.07883v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Dynamic Approaches to In-Network Aggregation,2008,"  Collaboration between small-scale wireless devices hinges on their ability to infer properties shared across multiple nearby nodes. Wireless-enabled mobile devices in particular create a highly dynamic environment not conducive to distributed reasoning about such global properties. This paper addresses a specific instance of this problem: distributed aggregation. We present extensions to existing unstructured aggregation protocols that enable estimation of count, sum, and average aggregates in highly dynamic environments. With the modified protocols, devices with only limited connectivity can maintain estimates of the aggregate, despite \textit{unexpected} peer departures and arrivals. Our analysis of these aggregate maintenance extensions demonstrates their effectiveness in unstructured environments despite high levels of node mobility. ",Kein DOI-Link verfügbar,0810.3227v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Threshold and hitting time for high-order connectivity in random   hypergraphs,2015,  We consider the following definition of connectivity in $k$-uniform hypergraphs: Two $j$-sets are $j$-connected if there is a walk of edges between them such that two consecutive edges intersect in at least $j$ vertices. We determine the threshold at which the random $k$-uniform hypergraph with edge probability $p$ becomes $j$-connected with high probability. We also deduce a hitting time result for the random hypergraph process -- the hypergraph becomes $j$-connected at exactly the moment when the last isolated $j$-set disappears. This generalises well-known results for graphs. ,Kein DOI-Link verfügbar,1502.07289v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,World-set Decompositions: Expressiveness and Efficient Algorithms,2007,"  Uncertain information is commonplace in real-world data management scenarios. The ability to represent large sets of possible instances (worlds) while supporting efficient storage and processing is an important challenge in this context. The recent formalism of world-set decompositions (WSDs) provides a space-efficient representation for uncertain data that also supports scalable processing. WSDs are complete for finite world-sets in that they can represent any finite set of possible worlds. For possibly infinite world-sets, we show that a natural generalization of WSDs precisely captures the expressive power of c-tables. We then show that several important decision problems are efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient algorithm for minimizing such representations. ",Kein DOI-Link verfügbar,0705.4442v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Fast and Simple Relational Processing of Uncertain Data,2007,"  This paper introduces U-relations, a succinct and purely relational representation system for uncertain databases. U-relations support attribute-level uncertainty using vertical partitioning. If we consider positive relational algebra extended by an operation for computing possible answers, a query on the logical level can be translated into, and evaluated as, a single relational algebra query on the U-relation representation. The translation scheme essentially preserves the size of the query in terms of number of operations and, in particular, number of joins. Standard techniques employed in off-the-shelf relational database management systems are effective for optimizing and processing queries on U-relations. In our experiments we show that query evaluation on U-relations scales to large amounts of data with high degrees of uncertainty. ",Kein DOI-Link verfügbar,0707.1644v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,LINVIEW: Incremental View Maintenance for Complex Analytical Queries,2014,"  Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude. ",https://doi.org/10.1145/2588555.2610519,1403.6968v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Properties of stochastic Kronecker graphs,2014,"  The stochastic Kronecker graph model introduced by Leskovec et al. is a random graph with vertex set $\mathbb Z_2^n$, where two vertices $u$ and $v$ are connected with probability $\alpha^{{u}\cdot{v}}\gamma^{(1-{u})\cdot(1-{v})}\beta^{n-{u}\cdot{v}-(1-{u})\cdot(1-{v})}$ independently of the presence or absence of any other edge, for fixed parameters $0<\alpha,\beta,\gamma<1$. They have shown empirically that the degree sequence resembles a power law degree distribution. In this paper we show that the stochastic Kronecker graph a.a.s. does not feature a power law degree distribution for any parameters $0<\alpha,\beta,\gamma<1$. In addition, we analyze the number of subgraphs present in the stochastic Kronecker graph and study the typical neighborhood of any given vertex. ",Kein DOI-Link verfügbar,1410.6328v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Incremental View Maintenance For Collection Programming,2014,"  In the context of incremental view maintenance (IVM), delta query derivation is an essential technique for speeding up the processing of large, dynamic datasets. The goal is to generate delta queries that, given a small change in the input, can update the materialized view more efficiently than via recomputation. In this work we propose the first solution for the efficient incrementalization of positive nested relational calculus (NRC+) on bags (with integer multiplicities). More precisely, we model the cost of NRC+ operators and classify queries as efficiently incrementalizable if their delta has a strictly lower cost than full re-evaluation. Then, we identify IncNRC+; a large fragment of NRC+ that is efficiently incrementalizable and we provide a semantics-preserving translation that takes any NRC+ query to a collection of IncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is within the complexity class NC0 and we showcase how recursive IVM, a technique that has provided significant speedups over traditional IVM in the case of flat queries [25], can also be applied to IncNRC+. ",Kein DOI-Link verfügbar,1412.4320v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Push vs. Pull-Based Loop Fusion in Query Engines,2016,"  Database query engines use pull-based or push-based approaches to avoid the materialization of data across query operators. In this paper, we study these two types of query engines in depth and present the limitations and advantages of each engine. Similarly, the programming languages community has developed loop fusion techniques to remove intermediate collections in the context of collection programming. We draw parallels between the DB and PL communities by demonstrating the connection between pipelined query engines and loop fusion techniques. Based on this connection, we propose a new type of pull-based engine, inspired by a loop fusion technique, which combines the benefits of both approaches. Then we experimentally evaluate the various engines, in the context of query compilation, for the first time in a fair environment, eliminating the biasing impact of ancillary optimizations that have traditionally only been used with one of the approaches. We show that for realistic analytical workloads, there is no considerable advantage for either form of pipelined query engine, as opposed to what recent research suggests. Also, by using microbenchmarks we show that our proposed engine dominates the existing engines by combining the benefits of both. ",Kein DOI-Link verfügbar,1610.09166v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Bootstrap percolation in random $k$-uniform hypergraphs,2017,"  We investigate bootstrap percolation with infection threshold $r> 1$ on the binomial $k$-uniform random hypergraph $H_k(n,p)$ in the regime $n^{-1}\ll n^{k-2}p \ll n^{-1/r}$, when the initial set of infected vertices is chosen uniformly at random from all sets of given size. We establish a threshold such that if there are less vertices in the initial set of infected vertices, then whp only a few additional vertices become infected, while if the initial set of infected vertices exceeds the threshold then whp almost every vertex becomes infected. In addition, for $k=2$, we show that the probability of failure decreases exponentially. ",https://doi.org/10.1016/j.endm.2015.06.081,1704.07144v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Finding tight Hamilton cycles in random hypergraphs faster,2017,"  In an $r$-uniform hypergraph on $n$ vertices a tight Hamilton cycle consists of $n$ edges such that there exists a cyclic ordering of the vertices where the edges correspond to consecutive segments of $r$ vertices. We provide a first deterministic polynomial time algorithm, which finds a.a.s. tight Hamilton cycles in random $r$-uniform hypergraphs with edge probability at least $C \log^3n/n$. Our result partially answers a question of Dudek and Frieze [Random Structures & Algorithms 42 (2013), 374-385] who proved that tight Hamilton cycles exists already for $p=\omega(1/n)$ for $r=3$ and $p=(e + o(1))/n$ for $r\ge 4$ using a second moment argument. Moreover our algorithm is superior to previous results of Allen, B\""ottcher, Kohayakawa and Person [Random Structures & Algorithms 46 (2015), 446-465] and Nenadov and \v{S}kori\'c [arXiv:1601.04034] in various ways: the algorithm of Allen et al. is a randomised polynomial time algorithm working for edge probabilities $p\ge n^{-1+\varepsilon}$, while the algorithm of Nenadov and \v{S}kori\'c is a randomised quasipolynomial time algorithm working for edge probabilities $p\ge C\log^8n/n$. ",https://doi.org/10.1017/S0963548320000450,1710.08988v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Deviation probabilities for arithmetic progressions and irregular   discrete structures,2020,"  Let the random variable $X\, :=\, e(\mathcal{H}[B])$ count the number of edges of a hypergraph $\mathcal{H}$ induced by a random $m$-element subset $B$ of its vertex set. Focussing on the case that the degrees of vertices in $\mathcal{H}$ vary significantly we prove bounds on the probability that $X$ is far from its mean. It is possible to apply these results to discrete structures such as the set of $k$-term arithmetic progressions in the $\{1,\dots, N\}$. Furthermore, our main theorem allows us to deduce results for the case $B\sim B_p$ is generated by including each vertex independently with probability $p$. In this setting our result on arithmetic progressions extends a result of Bhattacharya, Ganguly, Shao and Zhao \cite{BGSZ}. We also mention connections to related central limit theorems. ",Kein DOI-Link verfügbar,2012.09280v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Robustness against Read Committed for Transaction Templates,2021,"  The isolation level Multiversion Read Committed (RC), offered by many database systems, is known to trade consistency for increased transaction throughput. Sometimes, transaction workloads can be safely executed under RC obtaining the perfect isolation of serializability at the lower cost of RC. To identify such cases, we introduce an expressive model of transaction programs to better reason about the serializability of transactional workloads. We develop tractable algorithms to decide whether any possible schedule of a workload executed under RC is serializable (referred to as the robustness problem). Our approach yields robust subsets that are larger than those identified by previous methods. We provide experimental evidence that workloads that are robust against RC can be evaluated faster under RC compared to stronger isolation levels. We discuss techniques for making workloads robust against RC by promoting selective read operations to updates. Depending on the scenario, the performance improvements can be considerable. Robustness testing and safely executing transactions under the lower isolation level RC can therefore provide a direct way to increase transaction throughput without changing DBMS internals. ",Kein DOI-Link verfügbar,2107.12239v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Detecting Robustness against MVRC for Transaction Programs with   Predicate Reads,2023,"  The transactional robustness problem revolves around deciding whether, for a given workload, a lower isolation level than Serializable is sufficient to guarantee serializability. The paper presents a new characterization for robustness against isolation level (multi-version) Read Committed. It supports transaction programs with control structures (loops and conditionals) and inserts, deletes, and predicate reads -- scenarios that trigger the phantom problem, which is known to be hard to analyze in this context. The characterization is graph-theoretic and not unlike previous decision mechanisms known from the concurrency control literature that database researchers and practicians are comfortable with. We show experimentally that our characterization pushes the frontier in allowing to recognize more and more complex workloads as robust than before. ",Kein DOI-Link verfügbar,2302.08789v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Bootstrap Percolation on the Binomial Random $k$-uniform Hypergraph,2024,"  We investigate the behaviour of $r$-neighbourhood bootstrap percolation on the binomial $k$-uniform random hypergraph $H_k(n,p)$ for given integers $k\geq 2$ and $r\geq 2$. In $r$-neighbourhood bootstrap percolation, infection spreads through the hypergraph, starting from a set of initially infected vertices, and in each subsequent step of the process every vertex with at least $r$ infected neighbours becomes infected. For our analysis the set of initially infected vertices is chosen uniformly at random from all sets of given size. In the regime $n^{-1}\ll n^{k-2}p \ll n^{-1/r}$ we establish a threshold such that if the number of initially infected vertices remains below the threshold, then with high probability only a few additional vertices become infected, while if the number of initially infected vertices exceeds the threshold then with high probability almost every vertex becomes infected. In fact we show that the probability of failure decreases exponentially. ",Kein DOI-Link verfügbar,2403.12775v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Building Efficient Query Engines in a High-Level Language,2016,"  Abstraction without regret refers to the vision of using high-level programming languages for systems development without experiencing a negative impact on performance. A database system designed according to this vision offers both increased productivity and high performance, instead of sacrificing the former for the latter as is the case with existing, monolithic implementations that are hard to maintain and extend. In this article, we realize this vision in the domain of analytical query processing. We present LegoBase, a query engine written in the high-level language Scala. The key technique to regain efficiency is to apply generative programming: LegoBase performs source-to-source compilation and optimizes the entire query engine by converting the high-level Scala code to specialized, low-level C code. We show how generative programming allows to easily implement a wide spectrum of optimizations, such as introducing data partitioning or switching from a row to a column data layout, which are difficult to achieve with existing low-level query compilers that handle only queries. We demonstrate that sufficiently powerful abstractions are essential for dealing with the complexity of the optimization effort, shielding developers from compiler internals and decoupling individual optimizations from each other. We evaluate our approach with the TPC-H benchmark and show that: (a) With all optimizations enabled, LegoBase significantly outperforms a commercial database and an existing query compiler. (b) Programmers need to provide just a few hundred lines of high-level code for implementing the optimizations, instead of complicated low-level code that is required by existing query compilation approaches. (c) The compilation overhead is low compared to the overall execution time, thus making our approach usable in practice for compiling query engines. ",Kein DOI-Link verfügbar,1612.05566v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Robustness against Read Committed for Transaction Templates with   Functional Constraints,2022,"  The popular isolation level Multiversion Read Committed (RC) trades some of the strong guarantees of serializability for increased transaction throughput. Sometimes, transaction workloads can be safely executed under RC obtaining serializability at the lower cost of RC. Such workloads are said to be robust against RC. Previous work has yielded a tractable procedure for deciding robustness against RC for workloads generated by transaction programs modeled as transaction templates. An important insight of that work is that, by more accurately modeling transaction programs, we are able to recognize larger sets of workloads as robust. In this work, we increase the modeling power of transaction templates by extending them with functional constraints, which are useful for capturing data dependencies like foreign keys. We show that the incorporation of functional constraints can identify more workloads as robust that otherwise would not be. Even though we establish that the robustness problem becomes undecidable in its most general form, we show that various restrictions on functional constraints lead to decidable and even tractable fragments that can be used to model and test for robustness against RC for realistic scenarios. ",https://doi.org/10.46298/lmcs-19(4:39)2023,2201.05021v5,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Sparse projections onto the simplex,2012,"  Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the $\ell_1$-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints. ",Kein DOI-Link verfügbar,1206.1529v5,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Repairing Conflicts among MVCC Transactions,2016,"  The optimistic variants of MVCC (Multi-Version Concurrency Control) avoid blocking concurrent transactions at the cost of having a validation phase. Upon failure in the validation phase, the transaction is usually aborted and restarted from scratch. The ""abort and restart"" approach becomes a performance bottleneck for the use cases with high contention objects or long running transactions. In addition, restarting from scratch creates a negative feedback loop in the system, because the system incurs additional overhead that may create even further conflicts.   In this paper, we propose a novel approach for conflict resolution in MVCC for in-memory databases. This low overhead approach summarizes the transaction programs in the form of a dependency graph. The dependency graph also contains the constructs used in the validation phase of the MVCC algorithm. Then, in the case of encountering conflicts among transactions, the conflict locations in the program are quickly detected, and the conflicting transactions are partially re-executed. This approach maximizes the reuse of the computations done in the initial execution round, and increases the transaction processing throughput. ",Kein DOI-Link verfügbar,1603.00542v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,DLV - A System for Declarative Problem Solving,2000,"  DLV is an efficient logic programming and non-monotonic reasoning (LPNMR) system with advanced knowledge representation mechanisms and interfaces to classic relational database systems.   Its core language is disjunctive datalog (function-free disjunctive logic programming) under the Answer Set Semantics with integrity constraints, both default and strong (or explicit) negation, and queries. Integer arithmetics and various built-in predicates are also supported.   In addition DLV has several frontends, namely brave and cautious reasoning, abductive diagnosis, consistency-based diagnosis, a subset of SQL3, planning with action languages, and logic programming with inheritance. ",Kein DOI-Link verfügbar,cs/0003036v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Efficient Differentiable Programming in a Functional Array-Processing   Language,2018,"  We present a system for the automatic differentiation of a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source automatic differentiation and global optimizations such as loop transformations. Thanks to this feature, we demonstrate how for some real-world machine learning and computer vision benchmarks, the system outperforms the state-of-the-art automatic differentiation tools. ",Kein DOI-Link verfügbar,1806.02136v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A phase transition regarding the evolution of bootstrap processes in   inhomogeneous random graphs,2016,"  A bootstrap percolation process on a graph with infection threshold $r\ge 1$ is a dissemination process that evolves in time steps. The process begins with a subset of infected vertices and in each subsequent step every uninfected vertex that has at least $r$ infected neighbours becomes infected and remains so forever.   Critical phenomena in bootstrap percolation processes were originally observed by Aizenman and Lebowitz in the late 1980s as finite-volume phase transitions in $\mathbb{Z}^d$ that are caused by the accumulation of small local islands of infected vertices. They were also observed in the case of dense (homogeneous) random graphs by Janson, \L uczak, Turova and Valier (2012). In this paper, we consider the class of inhomogeneous random graphs known as the Chung-Lu model: each vertex is equipped with a positive weight and each pair of vertices appears as an edge with probability proportional to the product of the weights. In particular, we focus on the sparse regime, where the number of edges is proportional to the number of vertices.   The main results of this paper determine those weight sequences for which a critical phenomenon occurs: there is a critical density of vertices that are infected at the beginning of the process, above which a small (sublinear) set of infected vertices creates an avalanche of infections that in turn leads to an outbreak. We show that this occurs essentially only when the tail of the weight distribution dominates a power law with exponent 3 and we determine the critical density in this case. ",Kein DOI-Link verfügbar,1609.08892v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Pulsed thermal deposition of binary and ternary transition metal   dichalcogenide monolayers and heterostructures,2018,"  Application of transition metal dichalcogenides (TMDC) in photonic, optoelectronic or valleytronic devices requires the growth of continuous monolayers, heterostructures and alloys of different materials in a single process. We present a facile pulsed thermal deposition method which provides precise control over layer thickness and stoichiometry of two-dimensional systems. The versatility of the method is demonstrated on ternary monolayers of Mo$_{1-x}$W$_{x}$S$_{2}$ and on heterostructures combining metallic TaS$_{2}$ and semiconducting MoS$_{2}$ layers. The fabricated ternary monolayers cover the entire composition range of $x$ = 0...1 without phase separation. Band gap engineering and control over the spin-orbit coupling strength is demonstrated by absorption and photoluminescence spectroscopy. Vertical heterostructures are grown without intermixing. The formation of clean and atomically abrupt interfaces is evidenced by high-resolution transmission electron microscopy. Since both the metal components as well as the chalcogenides are thermally evaporated complex alloys and heterostructures can thus be prepared. ",https://doi.org/10.1063/1.5088758,1811.07663v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,The Homeostasis Protocol: Avoiding Transaction Coordination Through   Program Analysis,2014,"  Datastores today rely on distribution and replication to achieve improved performance and fault-tolerance. But correctness of many applications depends on strong consistency properties - something that can impose substantial overheads, since it requires coordinating the behavior of multiple nodes. This paper describes a new approach to achieving strong consistency in distributed systems while minimizing communication between nodes. The key insight is to allow the state of the system to be inconsistent during execution, as long as this inconsistency is bounded and does not affect transaction correctness. In contrast to previous work, our approach uses program analysis to extract semantic information about permissible levels of inconsistency and is fully automated. We then employ a novel homeostasis protocol to allow sites to operate independently, without communicating, as long as any inconsistency is governed by appropriate treaties between the nodes. We discuss mechanisms for optimizing treaties based on workload characteristics to minimize communication, as well as a prototype implementation and experiments that demonstrate the benefits of our approach on common transactional benchmarks. ",Kein DOI-Link verfügbar,1403.2307v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Atomic signatures of local environment from core-level spectroscopy in   $β$-Ga$_2$O$_3$,2016,"  We present a joint theoretical and experimental study on core-level excitations from the oxygen $K$ edge of $\beta$-Ga$_2$O$_3$. A detailed analysis of the electronic structure reveals the importance of O-Ga hybridization effects in the conduction region. The spectrum from O 1$s$ core electrons is dominated by excitonic effects, which overall redshift the absorption onset by 0.5 eV, and significantly redistribute the intensity to lower energies. Analysis of the spectra obtained within many-body perturbation theory reveals atomic fingerprints of the inequivalent O atoms. From the comparison of energy-loss near-edge fine-structure (ELNES) spectra computed with respect to different crystal planes, with measurements recorded under the corresponding diffraction conditions, we show how the spectral contributions of specific O atoms can be enhanced while quenching others. These results suggest ELNES, combined with ab initio many-body theory, as a very powerful technique to characterize complex systems, with sensitivity to individual atomic species and to their local environment. ",https://doi.org/10.1103/PhysRevB.94.075147,1605.05839v2,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Compiling Database Application Programs,2018,"  There is a trend towards increased specialization of data management software for performance reasons. In this paper, we study the automatic specialization and optimization of database application programs -- sequences of queries and updates, augmented with control flow constructs as they appear in database scripts, UDFs, transactional workloads and triggers in languages such as PL/SQL. We show how to build an optimizing compiler for database application programs using generative programming and state-of-the-art compiler technology.   We evaluate a hand-optimized low-level implementation of TPC-C, and identify the key optimization techniques that account for its good performance. Our compiler fully automates these optimizations and, applied to this benchmark, outperforms the manually optimized baseline by a factor of two. By selectively disabling some of the optimizations in the compiler, we derive a clinical and precise way of obtaining insight into their individual performance contributions. ",Kein DOI-Link verfügbar,1807.09887v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Multi-scale Convolutional Neural Networks for Inverse Problems,2018,"  Inverse problems exist in many domains such as phase imaging, image processing, and computer vision. These problems are often solved with application-specific algorithms, even though their nature remains the same: mapping input image(s) to output image(s). Deep convolutional neural networks have shown great potential for highly variable tasks across many image-based domains, but are usually difficult to train due to their inner high non-linearities. We propose a novel neural network architecture highlighting fast convergence as a generic solution addressing image(s)-to-image(s) inverse problems of different domains. Here we show that this approach is effective at predicting phases from direct intensity measurements, imaging objects from diffused reflections and denoising scanning transmission electron microscopy images, with just different training datasets. This opens a way to solve problems statistically through big data, in contrast to implementing explicit inversion algorithms from their mathematical formulas. Previous works have targeted much more on \textit{how} can we reconstruct rather than \textit{what} can be reconstructed. Our strategy offers a paradigm shift. ",https://doi.org/10.1038/s41598-020-62484-z,1810.12183v3,Yes,potent(1)
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,A consistent picture of excitations in cubic BaSnO$_{3}$ revealed by   combining theory and experiment,2021,"  Among the transparent conducting oxides, the perovskite barium stannate is most promising for various electronic applications due to its outstanding carrier mobility achieved at room temperature. However, most of its important characteristics, such as band gaps, effective masses, and absorption edge, remain controversial. Here, we provide a fully consistent picture by combining state-of-the-art {\it ab initio} methodology with forefront electron energy-loss spectroscopy and optical absorption measurements. Valence electron energy-loss spectra, featuring signals originating from band gap transitions, are acquired on defect-free sample regions of a BaSnO$_{3}$ single crystal. These high-energy-resolution measurements are able to capture also very weak excitations below the optical gap, attributed to indirect transitions. By temperature-dependent optical absorption measurements, we assess band-gap renormalization effects induced by electron-phonon coupling. Overall, we find for the effective electronic mass, the direct and the indirect gap, the optical gap, as well as the absorption onsets and spectra, excellent agreement between both experimental techniques and the theoretical many-body results, supporting also the picture of a phonon-mediated mechanism where indirect transitions are activated by phonon-induced symmetry lowering. This work demonstrates a fruitful connection between different high-level theoretical and experimental methods for exploring the characteristics of advanced materials. ",https://doi.org/10.1038/s43246-022-00234-6,2105.07817v3,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Silver nanowires with optimized silica coating as versatile plasmonic   resonators,2018,"  Metal nanoparticles are the most frequently used nanostructures in plasmonics. However, besides nanoparticles, metal nanowires feature several advantages for applications. Their elongation offers a larger interaction volume, their resonances can reach higher quality factors, and their mode structure provides better coupling into integrated hybrid dielectric-plasmonic circuits. It is crucial though, to control the distance of the wire to a supporting substrate, to another metal layer or to active materials with sub-nanometer precision. A dielectric coating can be utilized for distance control, but it must not degrade the plasmonic properties. In this paper, we introduce a controlled synthesis and coating approach for silver nanowires to fulfill these demands. We synthesize and characterize silver nanowires of around 70 nm in diameter. These nanowires are coated with nm-sized silica shells using a modified St\""ober method to achieve a homogeneous and smooth surface quality. We use transmission electron microscopy, dark-field microscopy and electron-energy loss spectroscopy to study morphology and plasmonic resonances of individual nanowires and quantify the influence of the silica coating. Thorough numerical simulations support the experimental findings showing that the coating does not deteriorate the plasmonic properties and thus introduce silver nanowires as usable building blocks for integrated hybrid plasmonic systems. ",Kein DOI-Link verfügbar,1811.07671v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Ultrafast phonon-mediated dephasing of color centers in hexagonal boron   nitride probed by electron beams,2024,"  Defect centers in hexagonal boron nitride have been extensively studied as room temperature single photon sources. The electronic structure of these defects exhibits strong coupling to phonons, as evidenced by the observation of phonon sidebands in both photoluminescence and cathodoluminescence spectra. However, the dynamics of the electron phonon coupling as well as phonon mediated dephasing of the color centers in hexagonal boron nitride remain unexplored. Here, we apply a novel time resolved CL spectroscopy technique to explore the population decay to phonon states and the dephasing time T2 with sub femtosecond time resolution. We demonstrate an ultrafast dephasing time of only 200 fs and a radiative decay of about 585 fs at room temperature, in contrast with all optical time resolved photoluminescence techniques that report a decay of a few nanoseconds. This behavior is attributed to efficient electron-beam excitation of coherent phonon polaritons in hexagonal boron nitride, resulting in faster dephasing of electronic transitions. Our results demonstrate the capability of our sequential cathodoluminescence spectroscopy technique to probe the ultrafast dephasing time of single emitters in quantum materials with sub femtosecond time resolution, heralding access to quantum path interferences in single emitters coupled to their complex environment. ",Kein DOI-Link verfügbar,2404.09879v1,No,
0000-0002-7882-0784,Christoph Koch,Ernst Abbe Universität der Angewandte Wissenschaften Jena,Unidirectional Kondo scattering in layered NbS2,2021,"  Crystalline defects can modify quantum interactions in solids, causing unintuitive, even favourable, properties such as quantum Hall effect or superconducting vortex pinning. Here we present another example of this notion - an unexpected unidirectional Kondo scattering in single crystals of 2H-NbS2. This manifests as a pronounced low-temperature enhancement in the out-of-plane resistivity and thermopower below 40 K, hidden for the in-plane charge transport. The anomaly can be suppressed by the c-axis-oriented magnetic field, but is unaffected by field applied along the planes. The magnetic moments originate from layers of 1T-NbS2, which inevitably form during the growth, undergoing a charge-density-wave reconstruction with each superlattice cell (David-star-shaped cluster of Nb atoms) hosting a localised spin. Our results demonstrate the unique and highly anisotropic response of a spontaneously formed Kondo lattice heterostructure, intercalated in a layered conductor. ",https://doi.org/10.1038/s41699-021-00265-6,2104.09147v2,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Teaching Research Design in Software Engineering,2024,"  In the dynamic field of Software Engineering (SE), where practice is constantly evolving and adapting to new technologies, conducting research is a daunting quest. This poses a challenge for researchers: how to stay relevant and effective in their studies? Empirical Software Engineering (ESE) has emerged as a contending force aiming to critically evaluate and provide knowledge that informs practice in adopting new technologies. Empirical research requires a rigorous process of collecting and analyzing data to obtain evidence-based findings. Challenges to this process are numerous, and many researchers, novice and experienced, found difficulties due to many complexities involved in designing their research.   The core of this chapter is to teach foundational skills in research design, essential for educating software engineers and researchers in ESE. It focuses on developing a well-structured research design, which includes defining a clear area of investigation, formulating relevant research questions, and choosing appropriate methodologies. While the primary focus is on research design, this chapter also covers aspects of research scoping and selecting research methods. This approach prepares students to handle the complexities of the ever-changing technological landscape in SE, making it a critical component of their educational curriculum. ",Kein DOI-Link verfügbar,2407.05184v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Improving Students With Rubric-Based Self-Assessment and Oral Feedback,2023,"  Rubrics and oral feedback are approaches to help students improve performance and meet learning outcomes. However, their effect on the actual improvement achieved is inconclusive. This paper evaluates the effect of rubrics and oral feedback on student learning outcomes. An experiment was conducted in a software engineering course on requirements engineering, using the two approaches in course assignments. Both approaches led to statistically significant improvements, though no material improvement (i.e., a change by more than one grade) was achieved. The rubrics led to a significant decrease in the number of complaints and questions regarding grades. ",https://doi.org/10.1109/TE.2011.2172981,2307.12849v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,An Empirically Evaluated Checklist for Surveys in Software Engineering,2019,"  Context: Over the past decade Software Engineering research has seen a steady increase in survey-based studies, and there are several guidelines providing support for those willing to carry out surveys. The need for auditing survey research has been raised in the literature. Checklists have been used to assess different types of empirical studies, such as experiments and case studies. Objective: This paper proposes a checklist to support the design and assessment of survey-based research in software engineering grounded in existing guidelines for survey research. We further evaluated the checklist in the research practice context. Method: To construct the checklist, we systematically aggregated knowledge from 14 methodological papers supporting survey-based research in software engineering. We identified the key stages of the survey process and its recommended practices through thematic analysis and vote counting. To improve our initially designed checklist we evaluated it using a mixed evaluation approach involving experienced researchers. Results: The evaluation provided insights regarding limitations of the checklist in relation to its understanding and objectivity. In particular, 19 of the 38 checklist items were improved according to the feedback received from its evaluation. Finally, a discussion on how to use the checklist and what its implications are for research practice is also provided. Conclusion: The proposed checklist is an instrument suitable for auditing survey reports as well as a support tool to guide ongoing research with regard to the survey design process. ",Kein DOI-Link verfügbar,1901.09850v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Exploratory Testing: One Size Doesn't Fit All,2017,"  Exploratory testing (ET) is a powerful and efficient way of testing software by integrating design, execution, and analysis of tests during a testing session. ET is often contrasted with scripted testing, and seen as a choice between black and white. We pose that there are different levels of exploratory testing from fully exploratory to fully scripted and propose a scale for the degree of exploration for ET. The degree is defined through levels of ET, which correspond to the way test charters are formulated. We have evaluated the classification through focus groups at four companies and identified factors that influence the level of exploratory testing. The results show that the proposed ET levels have distinguishing characteristics and that the levels can be used as a guide to structure test charters. Our study also indicates that applying a combination of ET levels can be beneficial in achieving effective testing. ",Kein DOI-Link verfügbar,1704.00537v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Checklists to Support Test Charter Design in Exploratory Testing,2017,"  During exploratory testing sessions the tester simultaneously learns, designs and executes tests. The activity is iterative and utilizes the skills of the tester and provides flexibility and creativity.Test charters are used as a vehicle to support the testers during the testing. The aim of this study is to support practitioners in the design of test charters through checklists. We aimed to identify factors allowing practitioners to critically reflect on their designs and contents of test charters to support practitioners in making informed decisions of what to include in test charters. The factors and contents have been elicited through interviews. Overall, 30 factors and 35 content elements have been elicited. ",Kein DOI-Link verfügbar,1704.00988v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,A Decision Support Method for Recommending Degrees of Exploration in   Exploratory Testing,2017,"  Exploratory testing is neither black nor white, but rather a continuum of exploration exists. In this research we propose an approach for decision support helping practitioners to distribute time between different degrees of exploratory testing on that continuum. To make the continuum manageable, five levels have been defined: freestyle testing, high, medium and low degrees of exploration, and scripted testing. The decision support approach is based on the repertory grid technique. The approach has been used in one company. The method for data collection was focus groups. The results showed that the proposed approach aids practitioners in the reflection of what exploratory testing levels to use, and aligns their understanding for priorities of decision criteria and the performance of exploratory testing levels in their contexts. The findings also showed that the participating company, which is currently conducting mostly scripted testing, should spend more time on testing using higher degrees of exploration in comparison to scripted testing. ",Kein DOI-Link verfügbar,1704.00994v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,A Value-driven Approach for Software Process Improvement -- A Solution   Proposal,2021,"  Software process improvement (SPI) is a means to an end, not an end in itself (e.g., a goal is to achieve shorter time to market and not just compliance to a process standard). Therefore, SPI initiatives ought to be streamlined to meet the desired values for an organization. Through a literature review, seven secondary studies aggregating maturity models and assessment frameworks were identified. Furthermore, we identified six proposals for building a new maturity model. We analyzed the existing maturity models for (a) their purpose, structure, guidelines, and (b) the degree to which they explicitly consider values and benefits. Based on this analysis and utilizing the guidelines from the proposals to build maturity models, we have introduced an approach for developing a value-driven approach for SPI. The proposal leveraged the benefits-dependency networks. We argue that our approach enables the following key benefits: (a) as a value-driven approach, it streamlines value-delivery and helps to avoid unnecessary process interventions, (b) as a knowledge-repository, it helps to codify lessons learned i.e. whether adopted practices lead to value realization, and (c) as an internal process maturity assessment tool, it tracks the progress of process realization, which is necessary to monitor progress towards the intended values. ",Kein DOI-Link verfügbar,2105.04767v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Lessons learned from replicating a study on information-retrieval based   test case prioritization,2022,"  Objective: In this study, we aim to replicate an artefact-based study on software testing to address the gap. We focus on (a) providing a step by step guide of the replication, reflecting on challenges when replicating artefact-based testing research, (b) Evaluating the replicated study concerning its validity and robustness of the findings.   Method: We replicate a test case prioritization technique by Kwon et al. We replicated the original study using four programs, two from the original study and two new programs. The replication study was implemented using Python to support future replications. Results: Various general factors facilitating replications are identified, such as: (1) the importance of documentation; (2) the need of assistance from the original authors; (3) issues in the maintenance of open source repositories (e.g., concerning needed software dependencies); (4) availability of scripts. We also raised several observations specific to the study and its context, such as insights from using different mutation tools and strategies for mutant generation. Conclusion: We conclude that the study by Kwon et al. is replicable for small and medium programs and could be automated to facilitate software practitioners, given the availability of required information. ",Kein DOI-Link verfügbar,2204.06325v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Tester Interactivity makes a Difference in Search-Based Software   Testing: A Controlled Experiment,2015,"  Context: Search-based software testing promises to provide users with the ability to generate high-quality test cases, and hence increase product quality, with a minimal increase in the time and effort required. One result that emerged out of a previous study to investigate the application of search-based software testing (SBST) in an industrial setting was the development of the Interactive Search-Based Software Testing (ISBST) system. ISBST allows users to interact with the underlying SBST system, guiding the search and assessing the results. An industrial evaluation indicated that the ISBST system could find test cases that are not created by testers employing manual techniques. The validity of the evaluation was threatened, however, by the low number of participants.   Objective: This paper presents a follow-up study, to provide a more rigorous evaluation of the ISBST system.   Method: To assess the ISBST system a two-way crossover controlled experiment was conducted with 58 students taking a Verification and Validation course. The NASA Task Load Index (NASA-TLX) is used to assess the workload experienced by the participants in the experiment.   Results: The experimental results validated the hypothesis that the ISBST system generates test cases that are not found by the same participants employing manual testing techniques. A follow-up laboratory experiment also investigates the importance of interaction in obtaining the results. In addition to this main result, the subjective workload was assessed for each participant by means of the NASA-TLX tool. The evaluation showed that, while the ISBST system required more effort from the participants, they achieved the same performance.   Conclusions: The paper provides evidence that the ISBST system develops test cases that are not found by manual techniques, and that interaction plays an important role in achieving that result. ",Kein DOI-Link verfügbar,1512.04812v1,No,
0000-0002-1532-8223,Kai Petersen,Hochschule Flensburg,Survey Research in Software Engineering: Problems and Strategies,2017,"  Background: The need for empirical investigations in software engineering is growing. Many researchers nowadays, conduct and validate their solutions using empirical research. Survey is one empirical method which enables researchers to collect data from a large population. Main aim of the survey is to generalize the findings. Aims: In this study we aim to identify the problems researchers face during survey design, and mitigation strategies. Method: A literature review as well as semi-structured interviews with nine software engineering researchers were conducted to elicit their views on problems and mitigation strategies. The researchers are all focused on empirical software engineering. Results: We identified 24 problems and 65 strategies, structured according to the survey research process. The most commonly discussed problem was sampling, in particular the ability to obtain a sufficiently large sample. To improve survey instrument design, evaluation and execution recommendations for question formulation and survey pre-testing were given. The importance of involving multiple researchers in the analysis of survey results was stressed. Conclusions: The elicited problems and strategies may serve researchers during the design of their studies. However, it was observed that some strategies were conflicting. This shows that it is important to conduct a trade-off analysis between strategies. ",https://doi.org/10.1109/ACCESS.2018.2881041,1704.01090v1,No,
0000-0002-6670-2684,Holger Hesse,Hochschule Kempten,Depreciation Cost is a Poor Proxy for Revenue Lost to Aging in Grid   Storage Optimization,2024,"  Dispatch of a grid energy storage system for arbitrage is typically formulated into a rolling-horizon optimization problem that includes a battery aging model within the cost function. Quantifying degradation as a depreciation cost in the objective can increase overall profits by extending lifetime. However, depreciation is just a proxy metric for battery aging; it is used because simulating the entire system life is challenging due to computational complexity and the absence of decades of future data. In cases where the depreciation cost does not match the loss of possible future revenue, different optimal usage profiles result and this reduces overall profit significantly compared to the best case (e.g., by 30-50%). Representing battery degradation perfectly within the rolling-horizon optimization does not resolve this - in addition, the economic cost of degradation throughout life should be carefully considered. For energy arbitrage, optimal economic dispatch requires a trade-off between overuse, leading to high return rate but short lifetime, vs. underuse, leading to a long but not profitable life. We reveal the intuition behind selecting representative costs for the objective function, and propose a simple moving average filter method to estimate degradation cost. Results show that this better captures peak revenue, assuming reliable price forecasts are available. ",Kein DOI-Link verfügbar,2403.10617v1,No,
0000-0002-1089-4007,Stefan Sauer,Kempten Universität der Angewandte Wissenschaften,Self-Adaptive Digital Assistance Systems for Work 4.0,2022,"  In the era of digital transformation, new technological foundations and possibilities for collaboration, production as well as organization open up many opportunities to work differently in the future. The digitization of workflows results in new forms of working which is denoted by the term Work 4.0. In the context of Work 4.0, digital assistance systems play an important role as they give users additional situation-specific information about a workflow or a product via displays, mobile devices such as tablets and smartphones, or data glasses. Furthermore, such digital assistance systems can be used to provide instructions and technical support in the working process as well as for training purposes. However, existing digital assistance systems are mostly created focusing on the ""design for all"" paradigm neglecting the situation-specific tasks, skills, preferences, or environments of an individual human worker. To overcome this issue, we present a monitoring and adaptation framework for supporting self-adaptive digital assistance systems for Work 4.0. Our framework supports context monitoring as well as UI adaptation for augmented (AR) and virtual reality (VR)-based digital assistance systems. The benefit of our framework is shown based on exemplary case studies from different domains, e.g. context-aware maintenance application in AR or warehouse management training in VR. ",Kein DOI-Link verfügbar,2211.16895v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Prospects for neutrino oscillation parameters,2016,  In this contribution we discuss the future of the global long-baseline neutrino oscillation program. The case is made that our current lack of understanding of neutrino-nucleus interactions is a serious challenge which will need to be met with new experimental initiatives in neutrino scattering. ,Kein DOI-Link verfügbar,1612.04843v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"CP, T and CPT violation in future long baseline experiments",2002,  I give a short overview about the possibilities and problems related to the measurement of CP violation in long baseline experiments. Special attention is paid to the issue of degeneracies and a method for their resolution is quantitatively discussed. The CP violation reach for different experiments is compared in dependence of $\sin^22\theta_{13}$ and $\dm{21}$. Furthermore a short comment about the possible effects of matter induced T violation is made. Finally the limits on CPT violation obtainable at a neutrino factory are shown. ,https://doi.org/10.1088/0954-3899/29/8/359,hep-ph/0210140v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Reactor antineutrino fluxes - status and challenges,2016,"  In this contribution we describe the current understanding of reactor antineutrino fluxes and point out some recent developments. This is not intended to be a complete review of this vast topic but merely a selection of observations and remarks, which despite their incompleteness, will highlight the status and the challenges of this field. ",https://doi.org/10.1016/j.nuclphysb.2016.04.012,1602.01499v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Submarine neutrino communication,2009,"  We discuss the possibility to use a high energy neutrino beam from a muon storage ring to provide one way communication with a submerged submarine. Neutrino interactions produce muons which can be detected either, directly when they pass through the submarine or by their emission of Cerenkov light in sea water, which, in turn, can be exploited with sensitive photo detectors. Due to the very high neutrino flux from a muon storage ring, it is sufficient to mount either detection system directly onto the hull of the submersible. The achievable data transfer rates compare favorable with existing technologies and do allow for a communication at the usual speed and depth of submarines. ",https://doi.org/10.1016/j.physletb.2010.08.003,0909.4554v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,The 5 MeV bump - a nuclear whodunit mystery,2016,"  We perform a combined analysis of recent NEOS and Daya Bay data on the reactor antineutrino spectrum. This analysis includes approximately 1.5 million antineutrino events, which is the largest neutrino event sample analyzed to date. We use a double ratio which cancels flux model dependence and related uncertainties as well as the effects of the detector response model. We find at 3-4 standard deviation significance level, that plutonium-239 and plutonium-241 are disfavored as the single source for the the so-called 5 MeV bump. This analysis method has general applicability and in particular with higher statistics data sets will be able to shed significant light on the issue of the bump. With some caveat this also should allow to improve the sensitivity for sterile neutrino searches in NEOS. ",https://doi.org/10.1103/PhysRevLett.118.042502,1609.03910v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Testing the Gallium Anomaly,2022,"  We study the online detection by gallium capture of mono-energetic neutrinos produced by a $^{51}$Cr radioactive source in a scintillation experiment. We find that cerium-doped gadolinium aluminum gallium garnet (GAGG) is a suitable scintillator which contains about 21% of gallium per weight and has a high mass density and light yield. Combined with a highly efficient light detection system this allows tagging of the subsequent germanium decay and thus a clean distinction of gallium capture and elastic neutrino electron scattering events. With 1.5 tons of scintillator and 10 source runs of 3.4MCi, each, we obtain about 760 gallium capture events with a purity of 85% and 680,000 neutrino electron scattering events, where the latter provide a precise normalization independent of any nuclear physics. This configuration would allow to test the gallium anomaly at more than $5\sigma$ in an independent way. ",https://doi.org/10.1103/PhysRevD.107.096011,2209.02885v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,On the determination of anti-neutrino spectra from nuclear reactors,2011,"  In this paper we study the effect of, well-known, higher order corrections to the allowed beta decay spectrum on the determination of anti-neutrino spectra resulting from the decays of fission fragments. In particular, we try to estimate the associated theory errors and find that induced currents like weak magnetism may ultimately limit our ability to improve the current accuracy and under certain circumstance could even largely increase the theoretical errors. We also perform a critical evaluation of the errors associated with our method to extract the anti-neutrino spectrum using synthetic beta spectra. It turns out, that a fit using only virtual beta branches with a judicious choice of the effective nuclear charge provides results with a minimal bias. We apply this method to actual data for U235, Pu239 and Pu241 and confirm, within errors, recent results, which indicate a net 3% upward shift in energy averaged anti-neutrino fluxes. However, we also find significant shape differences which can in principle be tested by high statistics anti-neutrino data samples. ",https://doi.org/10.1103/PhysRevC.84.024617,1106.0687v4,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Soft matter in hard confinement: phase transition thermodynamics,   structure, texture, diffusion and flow in nanoporous media - topical review",2015,"  Spatial confinement in nanoporous media affects the structure, thermodynamics and mobility of molecular soft matter often markedly. This article reviews thermodynamic equilibrium phenomena, such as physisorption, capillary condensation, crystallisation, self-diffusion, and structural phase transitions as well as selected aspects of the emerging field of spatially confined, non-equilibrium physics, i.e. the rheology of liquids, capillarity-driven flow phenomena, and imbibition front broadening in nanoporous materials. The observations in the nanoscale systems are related to the corresponding bulk phenomenologies. The complexity of the confined molecular species is varied from simple building blocks, like noble gas atoms, normal alkanes and alcohols to liquid crystals, polymers, ionic liquids, proteins and water. Mostly, experiments with mesoporous solids of alumina, carbon, gold, silica, and silicon having pore diameters ranging from a few up to 50 nanometers are presented. The observed peculiarities of nanopore-confined condensed matter are also discussed with regard to applications. A particular emphasis is put on texture formation upon crystallisation in nanoporous media, a topic both of high fundamental interest and of increasing nanotechnological importance, e.g., for the synthesis of organic/inorganic hybrid materials by melt infiltration, the usage of nanoporous solids in crystal nucleation or in template-assisted electrochemical deposition of nano structures. ",https://doi.org/10.1088/0953-8984/27/10/103102,1502.04659v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutron capture and the antineutrino yield from nuclear reactors,2015,"  We identify a new, flux-dependent correction to the antineutrino spectrum as produced in nuclear reactors. The abundance of certain nuclides, whose decay chains produce antineutrinos above the threshold for inverse beta decay, has a nonlinear dependence on the neutron flux, unlike the vast majority of antineutrino producing nuclides, whose decay rate is directly related to the fission rate. We have identified four of these so-called nonlinear nuclides and determined that they result in an antineutrino excess at low-energies below 3.2MeV, dependent on the reactor thermal neutron flux. We develop an analytic model for the size of the correction and compare it to the results of detailed reactor simulations for various real existing reactors, spanning 3 orders of magnitude in neutron flux. In a typical pressurized water reactor the resulting correction can reach 0.9% of the low energy flux which is comparable in size to other, known low-energy corrections from spent nuclear fuel and the non-equilibrium correction. For naval reactors the nonlinear correction may reach the 10% level. ",https://doi.org/10.1103/PhysRevLett.116.122503,1510.08948v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Determining reactor fuel type from continuous antineutrino monitoring,2016,"  We investigate the ability of an antineutrino detector to determine the fuel type of a reactor. A hypothetical 5t antineutrino detector is placed 25m from the core and measures the spectral shape and rate of antineutrinos emitted by fission fragments in the core for a number of 90 day periods. Our results indicate that four major fuel types can be differentiated from the variation of fission fractions over the irradiation time with a true positive probability of detection at 95%. In addition, we demonstrate that antineutrinos can identify the burn-up at which weapons-grade mixed-oxide (MOX) fuel would be reduced to reactor-grade MOX on average, providing assurance that plutonium disposition goals are met. In addition, we investigate removal scenarios where plutonium is purposefully diverted from a mixture of MOX and low-enriched uranium (LEU) fuel. Finally, we discuss how our analysis is impacted by a spectral distortion around 6MeV observed in the antineutrino spectrum measured from commercial power reactors. ",https://doi.org/10.1103/PhysRevApplied.8.034005,1612.06494v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Antineutrino monitoring for the Iranian heavy water reactor,2014,"  In this note we discuss the potential application of antineutrino monitoring to the Iranian heavy water reactor at Arak, the IR-40, as a non-proliferation measure. We demonstrate that an above ground detector positioned right outside the IR-40 reactor building could meet and in some cases significantly exceed the verification goals identified by IAEA for plutonium production or diversion from declared inventories. In addition to monitoring the reactor during operation, observing antineutrino emissions from long-lived fission products could also allow monitoring the reactor when it is shutdown. Antineutrino monitoring could also be used to distinguish different levels of fuel enrichment. Most importantly, these capabilities would not require a complete reactor operational history and could provide a means to re-establish continuity of knowledge in safeguards conclusions should this become necessary. ",https://doi.org/10.1103/PhysRevLett.113.042503,1403.7065v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Probing the Origins of Neutrino Mass with Supernova Data,2005,"  We study type II supernova signatures of neutrino mass generation via symmetry breaking at a scale in the range from keV to MeV. The scalar responsible for symmetry breaking can be thermalized in the supernova core and restore the symmetry. The neutrinos from scalar decays have about half the average energy of thermal neutrinos and are Bose-Einstein distributed. We find that, even without a detailed knowledge of the supernova parameters, a discovery is well within reach at Super-Kamiokande. ",https://doi.org/10.1103/PhysRevLett.95.191302,hep-ph/0504265v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Thermal Geo-axions,2009,"  We estimate the production rate of axion-type particles in the core of the Earth, at a temperature T~5000K. We constrain thermal geo-axion emission by demanding a core-cooling rate less than 100K/Gyr, as suggested by geophysics. This yields a ""quasi-vacuum"" (unaffected by extreme stellar conditions) bound on the axion-electron fine structure constant \alpha_a^{QV} < 10^{-18}, stronger than the existing accelerator (vacuum) bound by 4 orders of magnitude. We consider the prospects for measuring the geo-axion flux through conversion into photons in a geoscope; such measurements can further constrain \alpha_a^{QV}. ",https://doi.org/10.1103/PhysRevD.79.095024,0903.0618v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Reevaluating Reactor Antineutrino Anomalies with Updated Flux   Predictions,2019,"  Hints for the existence of a sterile neutrino at nuclear reactors are reexamined using two updated predictions for the fluxes of antineutrinos produced in fissions. These new predictions diverge in their preference for the rate deficit anomaly, relative to previous analyses, but the anomaly in the ratios of measured antineutrino spectra persists. We comment on upcoming experiments and their ability to probe the preferred region of the sterile-neutrino parameter space in the electron neutrino disappearance channel. ",https://doi.org/10.1103/PhysRevD.101.015008,1909.09267v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Sterile neutrino searches at tagged kaon beams,2020,"  Tagged kaon beams are attractive neutrino sources, which would provide flavor pure $\nu_e$-beams with exactly measured normalization. We point out that this also leads to an anti-tagged flavor pure $\nu_\mu$-beam, with equally well known normalization. Exposing a 1 kt liquid argon detector at a baseline of 1 km to this combination of unique beams allows to decisively test recent indications by IceCube and Neutrino-4 of sterile neutrino oscillations in the multi-eV range. ",https://doi.org/10.1103/PhysRevD.103.035018,2010.10268v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Towards Domain-Independent Supervised Discourse Parsing Through Gradient   Boosting,2022,"  Discourse analysis and discourse parsing have shown great impact on many important problems in the field of Natural Language Processing (NLP). Given the direct impact of discourse annotations on model performance and interpretability, robustly extracting discourse structures from arbitrary documents is a key task to further improve computational models in NLP. To this end, we present a new, supervised paradigm directly tackling the domain adaptation issue in discourse parsing. Specifically, we introduce the first fully supervised discourse parser designed to alleviate the domain dependency through a staged model of weak classifiers by introducing the gradient boosting framework. ",Kein DOI-Link verfügbar,2210.09565v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Large Discourse Treebanks from Scalable Distant Supervision,2022,"  Discourse parsing is an essential upstream task in Natural Language Processing with strong implications for many real-world applications. Despite its widely recognized role, most recent discourse parsers (and consequently downstream tasks) still rely on small-scale human-annotated discourse treebanks, trying to infer general-purpose discourse structures from very limited data in a few narrow domains. To overcome this dire situation and allow discourse parsers to be trained on larger, more diverse and domain-independent datasets, we propose a framework to generate ""silver-standard"" discourse trees from distant supervision on the auxiliary task of sentiment analysis. ",Kein DOI-Link verfügbar,2212.06038v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Antineutrino reactor safeguards - a case study,2013,"  Antineutrinos have been proposed as a means of reactor safeguards for more than 30 years and there has been impressive experimental progress in neutrino detection. In this paper we conduct, for the first time, a case study of the application of antineutrino safeguards to a real-world scenario - the North Korean nuclear crisis in 1994. We derive detection limits to a partial or full core discharge in 1989 based on actual IAEA safeguards access and find that two independent methods would have yielded positive evidence for a second core with very high confidence. To generalize our results, we provide detailed estimates for the sensitivity to the plutonium content of various types of reactors, including most types of plutonium production reactors, based on detailed reactor simulations. A key finding of this study is that a wide class of reactors with a thermal power of less than 0.1-1 GWth can be safeguarded achieving IAEA goals for quantitative sensitivity and timeliness with detectors right outside the reactor building. This type of safeguards does not rely on the continuity of knowledge and provides information about core inventory and power status in real-time. ",Kein DOI-Link verfügbar,1312.1959v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Solidified Fillings of Nanopores,2005,"  We present a selection of x-ray and neutron diffraction patterns of spherical (He, Ar), dumbbell- (N2, CO), and chain-like molecules (n-C9H20, n-C19H40) solidified in nanopores of silica glass (mean pore diameter 7nm). These patterns allow us to demonstrate how key principles governing crystallization have to be adapted in order to accomplish solidification in restricted geometries. He, Ar, and the spherical close packed phases of CO and N2 adjust to the pore geometry by introducing a sizeable amount of stacking faults. For the pore solidified, medium-length chain-like n-C19H40 we observe a close packed structure without lamellar ordering, whereas for the short-chain C9H20 the layering principle survives, albeit in a modified fashion compared to the bulk phase. ",Kein DOI-Link verfügbar,cond-mat/0508683v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Detecting solar axions using Earth's magnetic field,2005,"  We show that solar axion conversion to photons in the Earth's magnetosphere can produce an x-ray flux, with average energy \sim 4 keV, which is measurable on the dark side of the Earth. The smallness of the Earth's magnetic field is compensated by a large magnetized volume. For axion masses < 10^{-4} eV, a low-Earth-orbit x-ray detector with an effective area of 10^4 cm^2, pointed at the solar core, can probe the photon-axion coupling down to 10^{-11} GeV^{-1}, in one year. Thus, the sensitivity of this new approach will be an order of magnitude beyond current laboratory limits. ",https://doi.org/10.1103/PhysRevLett.97.141302,hep-ph/0509293v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutrino Factory Superbeam,2007,"  We discuss the optimization of a neutrino factory for large \sin^2 2 \theta_{13}, where we assume minimum effort on the accelerator side. This implies that we use low muon energies for the price of an optimized detection system. We demonstrate that such a neutrino factory performs excellent if combined with the electron neutrino appearance channel. Instead of the platinum channel operated with the muon neutrinos from the muon decays, we propose to use the initial superbeam from the decaying pions and kaons, which might be utilized at little extra effort. Since we assume out-of-phase bunches arriving at the same detector, we do not require electron charge identification. In addition, we can choose the proton energy such that we obtain a synergistic spectrum peaking at lower energies. We find that both the superbeam and the neutrino factory beam should used at the identical baseline to reduce matter density uncertainties, possibly with the same detector. This effectively makes the configuration a single experiment, which we call ``neutrino factory superbeam''. We demonstrate that this experiment outperforms a low-energy neutrino factory or a wide band beam alone beyond a simple addition of statistics. ",https://doi.org/10.1016/j.physletb.2007.09.018,0706.2862v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Knudsen Diffusion in Silicon Nanochannels,2008,"  Measurements on helium and argon gas flow through an array of parallel, linear channels of 12 nm diameter and 200 micrometer length in a single crystalline silicon membrane reveal a Knudsen diffusion type transport from 10^2 to 10^7 in Knudsen number Kn. The classic scaling prediction for the transport diffusion coefficient on temperature and mass of diffusing species,D_He ~ sqrt(T), is confirmed over a T range from 40 K to 300 K for He and for the ratio of D_He/D_Ar ~ sqrt(m_Ar/m_He). Deviations of the channels from a cylindrical form, resolved with transmission electron microscopy down to subnanometer scales, quantitatively account for a reduced diffusivity as compared to Knudsen diffusion in ideal tubular channels. The membrane permeation experiments are described over 10 orders of magnitude in Kn, encompassing the transition flow regime, by the unified flow model of Beskok and Karniadakis. ",https://doi.org/10.1103/PhysRevLett.100.064502,0802.1852v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Feasibility Study for Measuring Geomagnetic Conversion of Solar Axions   to X-rays in Low Earth Orbits,2008,"  We present a detailed computation of the expected rate for Geomagnetic Conversion of Solar Axions to X-rays (GECOSAX) along the orbit of an x-ray satellite. We use realistic satellite orbits and propagation in time. A realistic model for the Earth's magnetic field, which properly accounts for its spatial non-uniformity, is used. We also account for the effect of the Earth's atmosphere on the propagation of x-rays in our calculation of axion-photon conversion probability. To estimate possible sensitivities to the axion-photon coupling g_{a\gamma}, we use an actual measurement of the expected backgrounds by the SUZAKU satellite. Assuming a detector area of 10^3 cm^2 and about 10^6 s of data, we show that a 2 \sigma limit of g_{a\gamma} < (4.7-6.6) times 10^{-11} GeV^{-1} from GECOSAX is achievable, for axion masses m_a<10^{-4} eV. This significantly exceeds current laboratory sensitivities to g_{a\gamma}. ",https://doi.org/10.1088/1475-7516/2008/08/026,0804.3543v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Spontaneous Imbibition Dynamics of an n-Alkane in Nanopores: Evidence of   Meniscus Freezing and Monolayer Sticking,2009,"  Capillary filling dynamics of liquid n-tetracosane (n-C24H50) in a network of cylindrical pores with 7 and 10 nm mean diameter in monolithic silica glass (Vycor) exhibit an abrupt temperature-slope change at Ts=54 deg C, ~4 deg C above bulk and ~16 deg C, 8 deg C, respectively, above pore freezing. It can be traced to a sudden inversion of the surface tension's T slope, and thus to a decrease in surface entropy at the advancing pore menisci, characteristic of the formation of a single solid monolayer of rectified molecules, known as surface freezing from macroscopic, quiescent tetracosane melts. The imbibition speeds, that are the squared prefactors of the observed square-root-of-time Lucas-Washburn invasion kinetics, indicate a conserved bulk fluidity and capillarity of the nanopore-confined liquid, if we assume a flat lying, sticky hydrocarbon backbone monolayer at the silica walls. ",https://doi.org/10.1103/PhysRevLett.103.174501,0910.4324v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable   Distant Sentiment Supervision,2020,"  The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment-annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora. ",Kein DOI-Link verfügbar,2011.03017v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,From Sentiment Annotations to Sentiment Prediction through Discourse   Augmentation,2020,"  Sentiment analysis, especially for long documents, plausibly requires methods capturing complex linguistics structures. To accommodate this, we propose a novel framework to exploit task-related discourse for the task of sentiment analysis. More specifically, we are combining the large-scale, sentiment-dependent MEGA-DT treebank with a novel neural architecture for sentiment prediction, based on a hybrid TreeLSTM hierarchical attention model. Experiments show that our framework using sentiment-related discourse augmentations for sentiment prediction enhances the overall performance for long documents, even beyond previous approaches using well-established discourse parsers trained on human annotated data. We show that a simple ensemble approach can further enhance performance by selectively using discourse, depending on the document length. ",Kein DOI-Link verfügbar,2011.03021v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Predicting Discourse Structure using Distant Supervision from Sentiment,2019,"  Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RST-style discourse structure prediction. Our approach combines a neural variant of multiple-instance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. ",Kein DOI-Link verfügbar,1910.14176v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Reactor neutrino applications and coherent elastic neutrino nucleus   scattering,2020,"  Potential applications of neutrino detection to nuclear security have been discussed since the 1970s. Recent years have seen great progress in detector technologies based on inverse beta decay, with the demonstration of ton-scale surface-level detectors capable of high quality neutrino spectrum measurements. At the same time coherent elastic neutrino nucleus scattering has been experimentally confirmed in 2017 with neutrinos from stopped pion decay and there is a number of experiments aimed at seeing this reaction with reactor neutrinos. The large cross section and threshold-less nature of this reaction make it plausible to consider it for applications to nuclear security and here, we present a first direct comparison of the two reaction modes. ",https://doi.org/10.1103/PhysRevD.102.053008,2005.10907v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Unsupervised Learning of Discourse Structures using a Tree Autoencoder,2020,"  Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress. In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks. ",Kein DOI-Link verfügbar,2012.09446v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Towards Understanding Large-Scale Discourse Structures in Pre-Trained   and Fine-Tuned Language Models,2022,"  With a growing number of BERTology work analyzing different components of pre-trained language models, we extend this line of research through an in-depth analysis of discourse information in pre-trained and fine-tuned language models. We move beyond prior work along three dimensions: First, we describe a novel approach to infer discourse structures from arbitrarily long documents. Second, we propose a new type of analysis to explore where and how accurately intrinsic discourse is captured in the BERT and BART models. Finally, we assess how similar the generated structures are to a variety of baselines as well as their distribution within and between models. ",Kein DOI-Link verfügbar,2204.04289v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Unsupervised Inference of Data-Driven Discourse Structures using a Tree   Auto-Encoder,2022,"  With a growing need for robust and general discourse structures in many downstream tasks and real-world applications, the current lack of high-quality, high-quantity discourse trees poses a severe shortcoming. In order the alleviate this limitation, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop such method to complement task-specific models in generating much larger and more diverse discourse treebanks. ",Kein DOI-Link verfügbar,2210.09559v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Wafer-Scale Electroactive Nanoporous Silicon: Large and Fully Reversible   Electrochemo-Mechanical Actuation in Aqueous Electrolytes,2021,"  Nanoporosity in silicon results in an interface-dominated mechanics, fluidics and photonics that are often superior to the ones of the bulk material. However, their active control, e.g. as a response to electronic stimuli, is challenging due to the absence of intrinsic piezoelectricity in the base material. Here, for large-scale nanoporous silicon cantilevers wetted by aqueous electrolytes, we show electrosorption-induced mechanical stress generation of up to 600 kPa that is reversible and adjustable at will by electrical potential variations of approximately 1 V. Laser cantilever bending experiments in combination with in-operando cyclic voltammetry and step-coulombmetry allow us to quantitatively trace this large electro-actuation to the concerted action of 100 billions of parallel nanopores per square centimeter cross section and to determine the capacitive charge-stress coupling parameter upon ion ad- and desorption as well as the intimately related stress actuation dynamics for perchloric and isotonic saline solutions. A comparison with planar silicon surfaces reveals mechanistic insights on the observed electrocapillarity (electrostatic Hellmann-Feynman interactions) with respect to the importance of oxide formation and pore-wall roughness on the single-nanopore scale. The observation of robust electrochemo-mechanical actuation in a mainstream semiconductor with wafer-scale, self-organized nanoporosity opens up entirely novel opportunities for on-chip integrated stress generation and actuorics at exceptionally low operation voltages. ",https://doi.org/10.1002/adma.202105923,2110.14326v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,A Mott-Schottky Analysis of Mesoporous Silicon in Aqueous Electrolyte by   Electrochemical Impedance Spectroscopy,2023,"  Nanoporosity in silicon leads to completely new functionalities of this mainstream semiconductor. In recent years, it has been shown that filling the pores with aqueous electrolytes in addition opens a particularly wide field for modifying and achieving active control of these functionalities, e.g., for electrochemo-mechanical actuation and tunable photonics, or for the design of on-chip supercapacitors. However, a mechanistic understanding of these new features has been hampered by the lack of a detailed characterization of the electrochemical behavior of mesoporous silicon in aqueous electrolytes. Here, the capacitive, potential-controlled charging of the electrical double layer in a mesoporous silicon electrode (pore diameter $7\,\mathrm{nm}$) imbibed with perchloric acid solution is studied by electrochemical impedance spectroscopy. Thorough measurements with detailed explanations of the observed phenomena lead to a comprehensive understanding of the capacitive properties of porous silicon. An analysis based on the Mott-Schottky equation allows general conclusions to be drawn about the state of the band structure within the pore walls. Essential parameters such as the flat band potential, the doping density and the width of the space charge region can be determined. A comparison with bulk silicon shows that the flat band potential in particular is significantly altered by the introduction of nanopores, as it shifts from $1.4\pm0.1\,\mathrm{V}$ to $1.9\pm0.2\,\mathrm{V}$. Overall, this study provides a unique insight into the electrochemical processes, especially the electrical double layer charging, of nanoporous semiconductor electrodes. ",Kein DOI-Link verfügbar,2312.04252v1,Yes,potent(3)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,The benefits of a near detector for JUNO,2017,  It has been proposed to determine the mass hierarchy of neutrinos by exploiting the beat between the oscillation frequencies corresponding to the two neutrino mass squared differences. JUNO is based on this concept and uses a large liquid scintillator detector at a distance of 53km from a powerful nuclear reactor complex. We argue that the micro-structure present in antineutrino fluxes from nuclear reactors makes it essential to experimentally determine a reference spectrum with an energy resolution very similar to the one of JUNO. ,Kein DOI-Link verfügbar,1710.07378v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Telling Solar Neutrinos from Solar Axions When You Can't Shut Off the   Sun,2020,"  The XENON1T experiment recently reported an excess of events at low electron recoil energies, which may be due to interactions of solar neutrinos inside the detector via a large neutrino magnetic moment. We point out that a $^{51}$Cr neutrino source placed close to the detector can directly test this hypothesis. ",Kein DOI-Link verfügbar,2006.15767v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,W-RST: Towards a Weighted RST-style Discourse Framework,2021,"  Aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether RST Nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a Weighted-RST framework. In particular, we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications, compared to nuclearity-centered approaches. We further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators. ",Kein DOI-Link verfügbar,2106.02658v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Capillary Rise and Imbibition of Liquids in Nanoporous Matrices:   Rheological Concepts and Experiments,2010,"  Liquid flow propelled by capillary forces is one of the most important transport mechanisms in porous environments. It is governed by a fascinating interplay of interfacial, viscous drag as well as gravitational forces which liquids encounter upon invasion into geometries with often complex topologies, such as capillary networks of trees or interconnected fractures in soils and ice. Here, we present fundamentals, concepts and an experimental, gravimetric study on the capillarity-driven invasion dynamics of liquids in networks of pores a few nanometers across in monolithic, nanoporous silica glass (porous Vycor). A variation of the complexity of the building blocks of the liquids investigated along with a variation of the humidity and the temperature upon spontaneous imbibition allows us to gain information regarding the fluidity and capillarity of liquids in such nanoporous environments. We observe square-root of time imbibition dynamics for all liquids applied, which we can quantitatively describe by both a conserved bulk fluidity in the pore center and bulk capillarity at the advancing menisci, if we assume a sticky boundary layer (negative velocity slip length). Moreover, pecularities of nanopore-confined liquids, such as transport via the vapor phase leading to preadsorbed liquid layers, have to be properly accounted for. Upon increasing the chain-length in the case of the n-alkanes, we found hints towards a transition from stick- to slip-flow at the pore walls with increasing chain-length and thus polymeric behavior. Meniscus freezing is reported for n-tetracosane confined in porous Vycor. For the rheology of a rod-like liquid nematogen (8OCB) we found no hints of the viscosity drop upon entering into the nematic phase, typical of the bulk rheology of this liquid crystal. ",Kein DOI-Link verfügbar,1005.0730v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Capillarity-Driven Oil Flow in Nanopores: Darcy Scale Analysis of   Lucas-Washburn Imbibition Dynamics,2018,"  We present gravimetrical and optical imaging experiments on the capillarity-driven imbibition of silicone oils in monolithic silica glasses traversed by 3D networks of pores (mesoporous Vycor glass with 6.5 nm or 10 nm pore diameters). As evidenced by a robust square-root-of-time Lucas-Washburn (L-W) filling kinetics, the capillary rise is governed by a balance of capillarity and viscous drag forces in the absence of inertia and gravitational effects over the entire experimental times studied, ranging from a few seconds up to 10 days. A video on the infiltration process corroborates a collective pore filling as well as pronounced imbibition front broadening resulting from the capillarity and permeability disorder, typical of Vycor glasses. The transport process is analyzed within a Darcy scale description, considering a generalized pre-factor of the L-W law, termed Lucas-Washburn-Darcy imbibition ability. It assumes a Hagen-Poiseuille velocity profile in the pores and depends on the porosity, the mean pore diameter, the tortuosity and the velocity slip length and thus on the effective hydraulic pore diameter. For both matrices a reduced imbibition speed and thus reduced imbibition ability, compared to the one assuming the nominal pore diameter, bulk fluidity and bulk capillarity, can be quantitatively traced to an immobile, pore-wall adsorbed boundary layer of 1.4 nm thickness. Presumably, it consists of a monolayer of water molecules adsorbed on the hydrophilic pore walls covered by a monolayer of flat-laying silicone oil molecules. Our study highlights the importance of immobile nanoscopic boundary layers on the flow in tight oil reservoirs as well as the validity of the Darcy scale description for transport in mesoporous media. ",https://doi.org/10.1007/s11242-018-1133-z,1808.01776v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Neutrino Factories and the ""Magic"" Baseline",2003,"  We show that for a neutrino factory baseline of $L \sim 7300 km - 7 600 km$ a ``clean'' measurement of $\sin^2 2 \theta_{13}$ becomes possible, which is almost unaffected by parameter degeneracies. We call this baseline ""magic"" baseline, because its length only depends on the matter density profile. For a complete analysis, we demonstrate that the combination of the magic baseline with a baseline of 3000 km is the ideal solution to perform equally well for the $\sin^2 2 \theta_{13}$, sign of $\Delta m_{31}^2$, and CP violation sensitivities. Especially, this combination can very successfully resolve parameter degeneracies even below $\sin^2 2 \theta_{13} < 10^{-4}$. ",https://doi.org/10.1103/PhysRevD.68.037301,hep-ph/0301257v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Precision spectroscopy with reactor anti-neutrinos,2004,"  In this work we present an accurate parameterization of the anti-neutrino flux produced by the isotopes 235U, 239Pu and 241Pu in nuclear reactors. We determine the coefficients of this parameterization, as well as their covariance matrix, by performing a fit to spectra inferred from experimentally measured beta spectra. Subsequently we show that flux shape uncertainties play only a minor role in the KamLAND experiment, however, we find that future reactor neutrino experiments to measure the mixing angle $\theta_{13}$ are sensitive to the fine details of the reactor neutrino spectra. Finally, we investigate the possibility to determine the isotopic composition in nuclear reactors through an anti-neutrino measurement. We find that with a 3 month exposure of a one ton detector the isotope fractions and the thermal reactor power can be determined at a few percent accuracy, which may open the possibility of an application for safeguard or non-proliferation objectives. ",https://doi.org/10.1103/PhysRevD.70.053011,hep-ph/0407026v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Imbibition in mesoporous silica: rheological concepts and experiments on   water and a liquid crystal,2011,"  We present, along with some fundamental concepts regarding imbibition of liquids in porous hosts, an experimental, gravimetric study on the capillarity-driven invasion dynamics of water and of the rod-like liquid crystal octyloxycyanobiphenyl (8OCB) in networks of pores a few nanometers across in monolithic silica glass (Vycor). We observe, in agreement with theoretical predictions, square root of time invasion dynamics and a sticky velocity boundary condition for both liquids investigated. Temperature-dependent spontaneous imbibition experiments on 8OCB reveal the existence of a paranematic phase due to the molecular alignment induced by the pore walls even at temperatures well beyond the clearing point. The ever present velocity gradient in the pores is likely to further enhance this ordering phenomenon and prevent any layering in molecular stacks, eventually resulting in a suppression of the smectic phase in favor of the nematic phase. ",https://doi.org/10.1088/0953-8984/23/18/184109,1105.2408v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Anapole moment of neutrinos and radioactive sources near liquid xenon   detectors,2024,"  We show that placing a radioactive source such as ${}^{51}$Cr near a liquid xenon detector may allow to detect the contribution induced by the anapole moment to neutrino-electron scattering in the Standard Model. Although the anapole moment of neutrinos induces a scattering rate with the same spectral shape as the neutral and charged current contributions, exposures of $\sim$ 10 tonne $\times$ year at XENONnT or XLZD may be enough to accumulate sufficient statistics. We also discuss a simple model where the anapole moment of neutrinos is shifted with respect to the SM expectation, further demonstrating how a potential measurement of the anapole moment of neutrinos would allow to constrain new physics. ",Kein DOI-Link verfügbar,2408.11904v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,A low energy neutrino factory with non-magnetic detectors,2008,"  We show that a very precise neutrino/anti-neutrino event separation is not mandatory to cover the physics program of a low energy neutrino factory and thus non-magnetized detectors like water Cerenkov or liquid Argon detectors can be used. We point out, that oscillation itself strongly enhances the signal to noise ratio of a wrong sign muon search, provided there is sufficiently accurate neutrino energy reconstruction. Further, we argue that apart from a magnetic field, other means to distinguish neutrino from anti-neutrino events (at least statistically) can be explored. Combined with the fact that non-magnetic detectors potentially can be made very big, we show that modest neutrino/anti-neutrino separations at the level of 50% to 90% are sufficient to obtain good sensitivity to CP violation and the neutrino mass hierarchy for $\sin^22\theta_{13}>10^{-3}$. These non-magnetized detectors have a rich physics program outside the context of a neutrino factory, including topics like supernova neutrinos and proton decay. Hence, our observation opens the possibility to use a multi-purpose detector also in a neutrino factory beam. ",https://doi.org/10.1016/j.physletb.2008.10.009,0805.2019v2,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Quenching of lamellar ordering in an n-alkane embedded in nanopores,2004,"  We present an X-ray diffraction study of the normale alkane nonadecane C_{19}H_{40} embedded in nanoporous Vycor glass. The confined molecular crystal accomplishes a close-packed structure by alignment of the rod-like molecules parallel to the pore axis while sacrificing one basic principle known from the bulk state, i.e. the lamellar ordering of the molecules. Despite this disorder, the phase transitions observed in the confined solid mimic the phase behavior of the 3D unconfined crystal, though enriched by the appearance of a true rotator phase known only from longer alkane chains. ",https://doi.org/10.1209/epl/i2003-10088-0,cond-mat/0402156v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Structural transformations of even-numbered n-alkanes confined in   mesopores,2006,"  The n-alkanes C12H26, C14H30, and C16H34 have been imbibed and solidified in mesoporous Vycor glass with a mean pore diameter of 10nm. The samples have been investigated by x-ray diffractometry and calorimetric measurements. The structures and phase sequences have been determined. Apart from a reduction and the hysteresis of the melting/freezing transition, pore confined C12 reproduces the liquid-triclinic phase sequence of the bulk material, but for C16 an orthorhombic rotator mesophase appears that in the bulk state is absent for C16 but well known from odd numbered alkanes of similar length. In pore confined C14 this phase shows up on cooling but not on heating. ",https://doi.org/10.1103/PhysRevE.74.031610,cond-mat/0610030v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Capillary Rise of Liquids in Nanopores,2007,"  We present measurements on the spontaneous imbibition (capillary rise) of water, a linear hydrocarbon (n-C16H34) and a liquid crystal (8OCB) into the pore space of monolithic, nanoporous Vycor glass (mean pore radius 5 nm). Measurements on the mass uptake of the porous hosts as a function of time, m(t), are in good agreement with the Lucas-Washburn square root of time prediction, typical of imbibition of liquids into porous hosts. The relative capillary rise velocities scale as expected from the bulk fluid parameters. ",Kein DOI-Link verfügbar,0705.1997v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Liquid n-hexane condensed in silica nanochannels: A combined optical   birefringence and vapor sorption isotherm study,2009,"  The optical birefringence of liquid n-hexane condensed in an array of parallel silica channels of 7nm diameter and 400 micrometer length is studied as a function of filling of the channels via the vapor phase. By an analysis with the generalized Bruggeman effective medium equation we demonstrate that such measurements are insensitive to the detailed geometrical (positional) arrangement of the adsorbed liquid inside the channels. However, this technique is particularly suitable to search for any optical anisotropies and thus collective orientational order as a function of channel filling. Nevertheless, no hints for such anisotropies are found in liquid n-hexane. The n-hexane molecules in the silica nanochannels are totally orientationally disordered in all condensation regimes, in particular in the film growth as well as in the the capillary condensed regime. Thus, the peculiar molecular arrangement found upon freezing of liquid n-hexane in nanochannel-confinement, where the molecules are collectively aligned perpendicularly to the channels' long axes, does not originate in any pre-alignment effects in the nanoconfined liquid due to capillary nematization. ",https://doi.org/10.1103/PhysRevB.80.035421,0907.3484v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Polymorphism of the glass former ethanol confined in mesoporous silicon,2010,"  X-ray diffraction patterns of ethanol confined in parallel-aligned channels of approx. 10 nm diameter and 50 micrometer length in mesoporous silicon have been recorded as a function of filling fraction, temperature and for varying cooling and heating rates. A sorption isotherm, recorded in the liquid state, indicates a three monolayer thick, strongly adsorbed wall layer and a capillary condensed fraction of molecules in the pore center. Though the strongly adsorbed film remains in an amorphous state for the entire temperature range investigated, the capillary condensed molecules reproduce the polymorphism of bulk solid ethanol, that is the formation of either crystalline or glass-like states as a function of cooling rate. The critical rate necessary to achieve a vitrification in the mesopores is, however, at least two orders of magnitude smaller than in the bulk state. This finding can be traced both to pure geometrical constraints and quenched disorder effects, characteristic of confinement in mesoporous silicon. ",https://doi.org/10.1080/09500831003766999,1005.2168v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Thermotropic Nematic and Smectic Order in Silica Glass Nanochannels,2010,"  Optical birefringence measurements on a rod-like liquid crystal (8OCB), imbibed in silica channels (7 nm diameter), are presented and compared to the thermotropic bulk behavior. The orientational and positional order of the confined liquid evolves continuously at the paranematic-to-nematic and sizeably broadened at the nematic-to-smectic order transition, resp., in contrast to the discontinuous and well-defined second-order character of the bulk transitions. A Landau-de-Gennes analysis reveals identical strengths of the nematic and smectic ordering fields (imposed by the walls) and indicates that the smectic order is more affected by quenched disorder (originating in channel tortuosity and roughness) than the nematic transition. ",https://doi.org/10.1063/1.3502595,1010.4856v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,The Case for Muon-based Neutrino Beams,2014,"  For the foreseeable future, high energy physics accelerator capabilities in the US will be deployed to study the physics of the neutrino sector. In this context, it is useful to explore the sensitivities and limiting systematic effects of the planned neutrino oscillation program, so that we can evaluate the issues that must be addressed in order to ensure the success of these efforts. It is only in this way that we will ultimately be able to elucidate the fundamental physics processes involved. We conclude that success can only be guaranteed by, at some point in the future, being able to deploy muon accelerator capabilities. Such capabilities provide the only route to precision neutrino beams with which to study and mitigate, at the sub-percent level, the limiting systematic issues of future oscillation measurements. Thus this analysis argues strongly for maintaining a viable accelerator research program towards future muon accelerator capabilities. ",Kein DOI-Link verfügbar,1411.0629v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Protein Adsorption into Mesopores: A Combination of Electrostatic   Interaction, Counterion Release and van der Waals Forces",2015,"  Bovine heart cytochrome c has been immobilized into the mesoporous silica host material SBA-15 in both its native folded and urea-unfolded state. The comparison of the two folding states' behavior casts doubt on the commonly used explanation of cytochrome c adsorption, i.e. the electrostatic interaction model. A detailed investigation of the protein binding as a function of pH and ionic strength of the buffer solution reveals the complex nature of the protein-silica interaction. Electrostatic interaction, van der Waals forces and entropic contributions by counterion release each contribute to adsorption on the silica pore walls. ",https://doi.org/10.1021/la404947j,1503.02704v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,A Hierarchical Approach to Neural Context-Aware Modeling,2018,"  We present a new recurrent neural network topology to enhance state-of-the-art machine learning systems by incorporating a broader context. Our approach overcomes recent limitations with extended narratives through a multi-layered computational approach to generate an abstract context representation. Therefore, the developed system captures the narrative on word-level, sentence-level, and context-level. Through the hierarchical set-up, our proposed model summarizes the most salient information on each level and creates an abstract representation of the extended context. We subsequently use this representation to enhance neural language processing systems on the task of semantic error detection. To show the potential of the newly introduced topology, we compare the approach against a context-agnostic set-up including a standard neural language model and a supervised binary classification network. The performance measures on the error detection task show the advantage of the hierarchical context-aware topologies, improving the baseline by 12.75% relative for unsupervised models and 20.37% relative for supervised models. ",Kein DOI-Link verfügbar,1807.11582v2,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Predicting Above-Sentence Discourse Structure using Distant Supervision   from Topic Segmentation,2021,"  RST-style discourse parsing plays a vital role in many NLP tasks, revealing the underlying semantic/pragmatic structure of potentially complex and diverse documents. Despite its importance, one of the most prevailing limitations in modern day discourse parsing is the lack of large-scale datasets. To overcome the data sparsity issue, distantly supervised approaches from tasks like sentiment analysis and summarization have been recently proposed. Here, we extend this line of research by exploiting distant supervision from topic segmentation, which can arguably provide a strong and oftentimes complementary signal for high-level discourse structures. Experiments on two human-annotated discourse treebanks confirm that our proposal generates accurate tree structures on sentence and paragraph level, consistently outperforming previous distantly supervised models on the sentence-to-document task and occasionally reaching even higher scores on the sentence-to-paragraph level. ",Kein DOI-Link verfügbar,2112.06196v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Interference between the Atmospheric and Solar Oscillation Amplitudes,2019,"  We propose to detect the interference effect between the atmospheric-scale and solar-scale waves of neutrino oscillation, one of the key consequences of the three-generation structure of leptons. In vacuum, we show that there is a natural and general way of decomposing the oscillation amplitude into these two oscillation modes. The nature of the interference is cleanest in the $\bar{\nu}_e$ disappearance channel since it is free from the CP-phase $\delta$. We find that the upcoming JUNO experiment offers an ideal setting to observe this interference with more than $4\,\sigma$ significance, even under conservative assumptions about the systematic uncertainties. ",https://doi.org/10.1103/PhysRevD.101.093002,1912.02426v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Cerium ruthenium low-energy antineutrino measurements for safeguarding   military naval reactors,2021,"  The recent agreement to transfer nuclear submarine reactors and technology from two nuclear-weapon states to a non-nuclear-weapon state (AUKUS deal) highlights an unsolved problem in international safeguards: how to safeguard naval reactor fuel while it is on-board an operational nuclear submarine. Proposals to extend existing safeguards technologies and practices are complicated by the need for civilian international inspectors to gain access to the interior of the submarine and the reactor compartment, which raises national security concerns. In this paper we show that implementing safeguards on submarine propulsion reactors using a low-energy antineutrino reactor-off method, between submarine patrols, can by-pass the need for on-board access all together. We find that, using inverse beta decay (IBD), detectors can achieve a timely and high level of assurance that a submarine's nuclear core has not been diverted (mass of around 100 kg) nor its enrichment level changed (mass of around 10 tons). ",https://doi.org/10.1103/PhysRevLett.128.241803,2111.04510v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,The use of CEvNS to monitor spent nuclear fuel,2021,"  Increasing amounts of spent nuclear fuel are stored in dry storage casks for prolonged periods of time. To date no effective technology exists to re-verify cask contents should this become necessary. We explore the applicability of Coherent-Elastic Neutrino-Nucleus Scattering (CEvNS) to monitor the content of spent nuclear fuel (SNF) from dry storage casks. SNF produces neutrinos chiefly from $^{90}$Sr decays. We compare these results with what can be achieved via Inverse Beta Decay (IBD). We demonstrate that at low nuclear recoil energies CEvNS events rates exceed the IBD event rates by 2--3 orders of magnitude for a given detector mass. We find that a 10\,kg argon or germanium detector 3 meters from a fuel cask can detect over 100 events per year if a nuclear recoil threshold under 100 eV can be achieved. ",https://doi.org/10.1103/PhysRevD.105.056002,2111.15398v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Hints for leptonic CP violation or New Physics?,2016,"  One of the major open questions in the neutrino sector is the issue of leptonic CP violation. Current global oscillation data shows a mild preference for a large, potentially maximal value for the Dirac CP phase in the neutrino mixing matrix. In this letter, we point out that New Physics in the form of neutral-current like non-standard interactions with real couplings would likely yield a similar conclusion even if CP in the neutrino sector were conserved. Therefore, the claim for a discovery of leptonic CP violation will require a robust ability to test New Physics scenarios. ",https://doi.org/10.1103/PhysRevLett.117.031801,1601.03736v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Unleashing the Power of Neural Discourse Parsers -- A Context and   Structure Aware Approach Using Large Scale Pretraining,2020,"  RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art (SOTA) performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale ""silver-standard"" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis. ",Kein DOI-Link verfügbar,2011.03203v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Sterile Neutrinos and the Global Reactor Antineutrino Dataset,2020,"  We present results from global fits to the available reactor antineutrino dataset, as of Fall 2019, to determine the global preference for a fourth, sterile neutrino. We have separately considered experiments that measure the integrated inverse-beta decay (IBD) rate from those that measure the energy spectrum of IBD events at one or more locations. The software used is the newly developed GLoBESfit tool set which is based on the publicly available GLoBES framework and will be released as open-source software. ",https://doi.org/10.1007/JHEP01(2021)167,2005.01756v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Predicting Discourse Trees from Transformer-based Neural Summarizers,2021,"  Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain. ",Kein DOI-Link verfügbar,2104.07058v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Crystallization of medium length 1-alcohols in mesoporous silicon: An   X-ray diffraction study,2008,"  The linear 1-alcohols n-C16H33OH, n-C17H35OH, n-C19H37OH have been imbibed and solidified in lined up, tubular mesopores of silicon with 10 nm and 15 nm mean diameters, respectively. X-ray diffraction measurements reveal a set of six discrete orientation states (''domains'') characterized by a perpendicular alignment of the molecules with respect to the long axis of the pores and by a four-fold symmetry about this direction, which coincides with the crystalline symmetry of the Si host. A Bragg peak series characteristic of the formation of bilayers indicates a lamellar structure of the spatially confined alcohol crystals in 15 nm pores. By contrast, no layering reflections could be detected for 10 nm pores. The growth mechanism responsible for the peculiar orientation states is attributed to a nano-scale version of the Bridgman technique of single-crystal growth, where the dominant growth direction is aligned parallelly to the long pore axes. Our observations are analogous to the growth phenomenology encountered for medium length n-alkanes confined in mesoporous silicon (Phys. Rev. E 75, 021607 (2007)) and may further elucidate why porous silicon matrices act as an effective nucleation-inducing material for protein solution crystallization. ",https://doi.org/10.1103/PhysRevE.77.042602,0803.3901v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Capillary Condensation, Freezing, and Melting in Silica Nanopores: A   Sorption Isotherm and Scanning Calorimetry Study on Nitrogen in Mesoporous   SBA-15",2012,"  Condensation, melting and freezing of nitrogen in a powder of mesoporous silica grains (SBA-15) has been studied by combined volumetric sorption isotherm and scanning calorimetry measurements. Within the mean field model of Saam and Cole for vapor condensation in cylindrical pores a liquid nitrogen sorption isotherm is well described by a bimodal pore radius distribution. It encompasses a narrow peak centered at 3.3 nm, typical of tubular mesopores, and a significantly broader peak characteristic of micropores, located at 1 nm. The material condensed in the micropores as well as the first two adsorbed monolayers in the mesopores do not exhibit any caloric anomaly. The solidification and melting transformation affects only the pore condensate beyond approx. the second monolayer of the mesopores. Here, interfacial melting leads to a single peak in the specific heat measurements. Homogeneous and heterogeneous freezing along with a delayering transition for partial fillings of the mesopores result in a caloric freezing anomaly similarly complex and dependent on the thermal history as has been observed for argon in SBA-15. The axial propagation of the crystallization in pore space is more effective in the case of nitrogen than previously observed for argon, which we attribute to differences in the crystalline textures of the pore solids. ",https://doi.org/10.1103/PhysRevB.85.075403,1202.1835v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,pH-Dependent Selective Protein Adsorption into Mesoporous Silica,2015,"  The adsorption of lysozyme, cytochrome c and myoglobin, similar-sized globular proteins of approximately 1.5 nm radius, into the mesoporous silica material Santa Barbara Amorphous-15 (SBA-15) with 3.3 nm mean pore radius has been studied photometrically for aqueous solutions containing a single protein type and for binary protein mixtures. Distinct variations in the absolute and relative adsorption behavior are observed as a function of the solution's pH-value, and thus pore wall and protein charge. The proteins exhibit the strongest binding below their isoelectric points pI, which indicates the dominance of electrostatic interactions between charged amino acid residues and the -OH groups of the silica surface in the mesopore adsorption process. Moreover, we find for competitive adsorption in the restricted, tubular pore geometry that the protein type which shows the favoured binding to the pore wall can entirely suppress the adsorption of the species with lower binding affinity, even though the latter would adsorb quite well from a single component mixture devoid of the strongly binding protein. We suggest that this different physicochemical behavior along with the large specific surface and thus adsorption capability of mesoporous glasses can be exploited for separation of binary mixtures of proteins with distinct pI by adjusting the aqueous solution's pH. ",https://doi.org/10.1021/acs.jpcc.5b09606,1511.05311v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Automated Evaluation of Out-of-Context Errors,2018,"  We present a new approach to evaluate computational models for the task of text understanding by the means of out-of-context error detection. Through the novel design of our automated modification process, existing large-scale data sources can be adopted for a vast number of text understanding tasks. The data is thereby altered on a semantic level, allowing models to be tested against a challenging set of modified text passages that require to comprise a broader narrative discourse. Our newly introduced task targets actual real-world problems of transcription and translation systems by inserting authentic out-of-context errors. The automated modification process is applied to the 2016 TEDTalk corpus. Entirely automating the process allows the adoption of complete datasets at low cost, facilitating supervised learning procedures and deeper networks to be trained and tested. To evaluate the quality of the modification algorithm a language model and a supervised binary classification model are trained and tested on the altered dataset. A human baseline evaluation is examined to compare the results with human performance. The outcome of the evaluation task indicates the difficulty to detect semantic errors for machine-learning algorithms and humans, showing that the errors cannot be identified when limited to a single sentence. ",Kein DOI-Link verfügbar,1803.08983v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Passive low-energy nuclear recoil detection with color centers,2021,"  Crystal damage events such as tracks and point defects have been used to record and detect radiation for a long time and recently they have been proposed as a means for dark matter detection. Color centers can be read out optically and we propose a scheme based on selective plane illumination microscopy for sub-micron imaging of large volumes corresponding to kilogram mass detectors. This class of detectors would be passive and would operate at room temperature. We apply these concepts to the detection of reactor neutrinos using coherent elastic neutrino nucleus scattering (CEvNS). Crystal damage formation energies are intrinsically on the order of 25eV, resulting in similarly low nuclear recoil thresholds. This would enable the first observation of reactor neutrino CEvNS with detectors as small as 10g. Additionally, a competitive search for spin-dependent dark matter scattering down to a dark matter mass of 0.3GeV could be possible. Passive crystal detectors might also be attractive for nuclear non-proliferation safeguards if used to monitor reactor power and to put limits on plutonium production. The passive nature and small footprint of the proposed detectors implies that these might fit well within accepted reactor safeguards operations. ",Kein DOI-Link verfügbar,2104.13926v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Can OPERA help in constraining neutrino non-standard interactions?,2008,"  We study how much the unique ability of the OPERA experiment to directly detect \nu_\tau can help in probing new, non-standard contact interactions of the third family of neutrinos. We perform a combined analysis of future, high-statistics MINOS and OPERA data. For the case of non-standard interactions in \nu_\mu to \nu_e transitions we also include the impact of possible DoubleCHOOZ data. In all cases we find that the \nu_\tau sample of OPERA is too small to be statistically significant, even if one doubles the nominal exposure of OPERA to 4.5E20 pot. OPERA's real benefit for this measurement lies in its very high neutrino energy and hence very different L/E compared to MINOS. ",https://doi.org/10.1016/j.physletb.2008.07.092,0803.1790v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Constraining sterile neutrinos with a low energy beta-beam,2009,"  We show that a low energy beta-beam facility can be used to search for sterile neutrinos by measuring the disappearance of electron anti-neutrinos. This channel is particularly sensitive since it allows to use inverse beta decay as detection reaction; thus it is free from hadronic uncertainties, provided the neutrino energy is below the pion production threshold. This corresponds to a choice of the Lorentz gamma=30 for the 6He parent ion. Moreover, a disappearance measurement allows the constraint of sterile neutrino properties independently of any CP violating effects. A moderate detector size of a few 100 tons and ion production rates of 2E13 per second are sufficient to constrain mixing angles as small as \sin^22\theta=0.01 at 99% confidence level. ",https://doi.org/10.1007/JHEP01(2010)071,0907.3145v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Thermotropic nematic order upon nano-capillary filling,2013,"  Optical birefringence and light absorption measurements reveal four regimes for the thermotropic behavior of a nematogen liquid (7CB) upon sequential filling of parallel-aligned capillaries of 12 nm diameter in a monolithic, mesoporous silica membrane. No molecular reorientation is observed for the first adsorbed monolayer. In the film-condensed state (up to 1 nm thickness) a weak, continuous paranematic-to-nematic (P-N) transition is found, which is shifted by 10 K below the discontinuous bulk transition at T_IN=305K. The capillary-condensed state exhibits a more pronounced, albeit still continuous P-N reordering, located 4 K below T_IN. This shift vanishes abruptly on complete filling of the capillaries, which we tentatively trace to a 10 MPa tensile pressure release associated with the disappearance of concave menisci in the confined liquid. ",https://doi.org/10.1103/PhysRevE.87.042502,1304.1793v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Switchable Imbibition in Nanoporous Gold,2014,"  Spontaneous imbibition enables the elegant propelling of nano-flows because of the dominance of capillarity at small length scales. The imbibition kinetics are, however, solely determined by the static geometry of the porous host, the capillarity, and the fluidity of the imbibed liquid. This makes active control particularly challenging. Here, we show for aqueous electrolyte imbibition in nanoporous gold that the fluid flow can be reversibly switched on and off through electric potential control of the solid-liquid interfacial tension, i.e. we can accelerate the imbibition front, stop it, and have it proceed at will. Simultaneous measurements of the mass flux and the electrical current allow us to document simple scaling laws for the imbibition kinetics, and to explore the charge flow dynamics in the metallic nanopores. Our findings demonstrate that the high electric conductivity along with the pathways for ionic and/or fluid transport render nanoporous elemental gold a versatile, accurately controllable electro-capillary pump and flow sensor for minute amounts of liquids with exceptionally low operating voltages. ",https://doi.org/10.1038/ncomms5237,1407.1038v1,Yes,"versatile(1), potent(1)"
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Laser-Excited Elastic Guided Waves Reveal the Complex Mechanics of   Nanoporous Silicon,2020,"  Nanoporosity in silicon leads to completely new functionalities of this mainstream semiconductor. A difficult to assess mechanics has however significantly limited its application in fields ranging from nanofluidics and biosensorics to drug delivery, energy storage and photonics. Here, we present a study on laser-excited elastic guided waves detected contactless and non-destructively in dry and liquid-infused single-crystalline porous silicon. These experiments reveal that the self-organised formation of 100 billions of parallel nanopores per square centimetre cross section results in a nearly isotropic elasticity perpendicular to the pore axes and an 80% effective stiffness reduction, altogether leading to significant deviations from the cubic anisotropy observed in bulk silicon. Our thorough assessment of the wafer-scale mechanics of nanoporous silicon provides the base for predictive applications in robust on-chip devices and evidences that recent breakthroughs in laser ultrasonics open up entirely new frontiers for in-situ, non-destructive mechanical characterisation of dry and liquid-functionalised porous materials. ",https://doi.org/10.1038/s41467-021-23398-0,2010.14947v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Supernova neutrinos can tell us the neutrino mass hierarchy   independently of flux models,2005,"  We demonstrate that the detection of shock modulations of the neutrino spectra from a galactic core-collapse supernova is sufficient to obtain a high significance determination of the neutrino mass hierarchy if the supernova event is observed in both a Mton-class water Cherenkov detector and a 100 kton-class liquid argon detector. Neither detailed supernova neutrino flux modelling nor observation of Earth matter effects is needed for this determination. As a corollary, a nonzero value of \theta_x will be established. ",https://doi.org/10.1016/j.physletb.2005.05.017,hep-ph/0501184v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Solar mass-varying neutrino oscillations,2005,  We propose that the solar neutrino deficit may be due to oscillations of mass-varying neutrinos (MaVaNs). This scenario elucidates solar neutrino data beautifully while remaining comfortably compatible with atmospheric neutrino and K2K data and with reactor antineutrino data at short and long baselines (from CHOOZ and KamLAND). We find that the survival probability of solar MaVaNs is independent of how the suppression of neutrino mass caused by the acceleron-matter couplings varies with density. Measurements of MeV and lower energy solar neutrinos will provide a rigorous test of the idea. ,https://doi.org/10.1103/PhysRevLett.95.211802,hep-ph/0502196v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Ultra high energy neutrino-nucleon cross section from cosmic ray   experiments and neutrino telescopes,2006,"  We deduce the cosmogenic neutrino flux by jointly analysing ultra high energy cosmic ray data from HiRes-I and II, AGASA and the Pierre Auger Observatory. We make two determinations of the neutrino flux by using a model-dependent method and a model-independent method. The former is well-known, and involves the use of a power-law injection spectrum. The latter is a regularized unfolding procedure. We then use neutrino flux bounds obtained by the RICE experiment to constrain the neutrino-nucleon inelastic cross section at energies inaccessible at colliders. The cross section bounds obtained using the cosmogenic fluxes derived by unfolding are the most model-independent bounds to date. ",https://doi.org/10.1016/j.physletb.2006.09.067,hep-ph/0606311v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Upgraded experiments with super neutrino beams: Reach versus Exposure,2006,"  We introduce exposure as a means to making balanced comparisons of the sensitivities of long-baseline neutrino experiments to a nonzero \theta_{13}, to CP violation and to the neutrino mass hierarchy. We illustrate its use by comparing the sensitivities of possible upgrades of superbeam experiments, namely NOvA*, T2KK and experiments with wide band beams. For the proposed exposures, we find the best nominal CP violation performance for T2KK. For equal exposures, a wide band beam experiment has the best mass hierarchy performance. The physics concept on which NOvA* is based has the best potential for discovering CP violation only for exposures above a threshold value. ",https://doi.org/10.1103/PhysRevD.76.031301,hep-ph/0610301v3,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,On the impact of systematical uncertainties for the CP violation   measurement in superbeam experiments,2007,"  Superbeam experiments can, in principle, achieve impressive sensitivities for CP violation in neutrino oscillations for large $\theta_{13}$. We study how those sensitivities depend on assumptions about systematical uncertainties. We focus on the second phase of T2K, the so-called T2HK experiment, and we explicitly include a near detector in the analysis. Our main result is that even an idealised near detector cannot remove the dependence on systematical uncertainties completely. Thus additional information is required. We identify certain combinations of uncertainties, which are the key to improve the sensitivity to CP violation, for example the ratio of electron to muon neutrino cross sections and efficiencies. For uncertainties on this ratio larger than 2%, T2HK is systematics dominated. We briefly discuss how our results apply to a possible two far detector configuration, called T2KK. We do not find a significant advantage with respect to the reduction of systematical errors for the measurement of CP violation for this setup. ",https://doi.org/10.1088/1126-6708/2008/03/021,0711.2950v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,First hint for CP violation in neutrino oscillations from upcoming   superbeam and reactor experiments,2009,"  We compare the physics potential of the upcoming neutrino oscillation experiments Daya Bay, Double Chooz, NOvA, RENO, and T2K based on their anticipated nominal luminosities and schedules. After discussing the sensitivity to theta_{13} and the leading atmospheric parameters, we demonstrate that leptonic CP violation will hardly be measurable without upgrades of the T2K and NOvA proton drivers, even if theta_{13} is large. In the presence of the proton drivers, the fast track to hints for CP violation requires communication between the T2K and NOvA collaborations in terms of a mutual synchronization of their neutrino-antineutrino run plans. Even in that case, upgrades will only discover CP violation in a relatively small part of the parameter space at the 3 sigma confidence level, while 90% confidence level hints will most likely be obtained. Therefore, we conclude that a new facility will be required if the goal is to obtain a significant result with high probability. ",https://doi.org/10.1088/1126-6708/2009/11/044,0907.1896v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Potential measurement of the weak mixing angle with neutrino-electron   scattering at low energy,2010,"  We study the possibility to measure sin^2 theta_W by neutrino-electron scattering at a value of the momentum transfer Q ~ 30 MeV with a precision of 0.24% which is only a factor three below the one obtained by LEP-I at the Z-pole. The neutrino source is a proton beam dump providing a clean beam from muon decay at rest and the detector is a 100 kt scale water Cerenkov detector, which results in about 20 million signal events. ",https://doi.org/10.1007/JHEP08(2011)059,1005.1254v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,LSND reloaded,2010,"  In view of the recent result from the anti-neutrino run of MiniBooNE, we suggest to repeat the original Liquid Scintillator Neutrino Detector (LSND) experiment using Super-Kamiokande, doped with Gadolinium, as detector. Due to the more than 100 times larger detector mass offered by Super-Kamiokande, the neutrino source requires a proton beam power of less than 300kW at a proton energy around a 1 GeV. A one year run of this setup can corroborate or refute both the LSND and MiniBooNE claims at more than 5 sigma confidence level. If a signal is observed, the large size of Super-Kamiokande combined with its good ability to determine the position of an anti-neutrino event allows to establish the characteristic L/E-dependence of oscillation. ",https://doi.org/10.1016/j.physletb.2010.12.038,1007.3228v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Physics Performance of a Low-Luminosity Low Energy Neutrino Factory,2013,"  We investigate the minimal performance, in terms of beam luminosity and detector size, of a neutrino factory to achieve a competitive physics reach for the determination of the mass hierarchy and the discovery of leptonic CP violation. We find that a low luminosity of $2\times 10^{20}$ useful muon decays per year and 5 GeV muon energy aimed at a 10 kton magnetized liquid argon detector placed at 1300 km from the source provides a good starting point. This result relies on $\theta_{13}$ being large and assumes that the so-called platinum channel can be used effectively. We find that such a minimal facility would perform significantly better than phase I of the LBNE project and thus could constitute a reasonable step towards a full neutrino factory. ",https://doi.org/10.1103/PhysRevLett.111.061803,1301.7727v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Combining dark matter detectors and electron-capture sources to hunt for   new physics in the neutrino sector,2014,"  In this letter we point out the possibility to study new physics in the neutrino sector using dark matter detectors based on liquid xenon. These are characterized by very good spatial resolution and extremely low thresholds for electron recoil energies. When combined with a radioactive $\nu_e$ source, both features in combination allow for a very competitive sensitivity to neutrino magnetic moments and sterile neutrino oscillations. We find that, for realistic values of detector size and source strength, the bound on the neutrino magnetic moment can be improved by an order of magnitude with respect to the present value. Regarding sterile neutrino searches, we find that most of the gallium anomaly could be explored at the 95% confidence level just using shape information. ",https://doi.org/10.1007/JHEP11(2014)042,1406.4914v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutrino-nucleus interactions and the determination of oscillation   parameters,2015,"  We review the status and prospects of theoretical studies of neutrino-nucleus interactions, and discuss the influence of the treatment of nuclear effects on the determination of oscillation parameters. The models developed to describe the variety of reaction mechanisms contributing to the nuclear cross sections are analysed, with emphasis placed on their capability to reproduce the available electron scattering data.The impact of the uncertainties associated with the description of nuclear dynamics on the the oscillation parameters is illustrated through examples, and possible avenues towards a better understanding of the signals detected by long baseline experiments are outlined. ",https://doi.org/10.1016/j.physrep.2017.07.004,1501.06448v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Antineutrino monitoring of spent nuclear fuel,2016,"  Military and civilian applications of nuclear energy have left a significant amount of spent nuclear fuel over the past 70 years. Currently, in many countries world wide, the use of nuclear energy is on the rise. Therefore, the management of highly radioactive nuclear waste is a pressing issue. In this letter, we explore antineutrino detectors as a tool for monitoring and safeguarding nuclear waste material. We compute the flux and spectrum of antineutrinos emitted by spent nuclear fuel elements as a function of time, and we illustrate the usefulness of antineutrino detectors in several benchmark scenarios. In particular, we demonstrate how a measurement of the antineutrino flux can help to re-verify the contents of a dry storage cask in case the monitoring chain by conventional means gets disrupted. We then comment on the usefulness of antineutrino detectors at long-term storage facilities such as Yucca mountain. Finally, we put forward antineutrino detection as a tool in locating underground ""hot spots"" in contaminated areas such as the Hanford site in Washington state. ",https://doi.org/10.1103/PhysRevApplied.8.054050,1606.06309v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Particle physics origin of the 5 MeV bump in the reactor antineutrino   spectrum?,2018,"  One of the most puzzling questions in neutrino physics is the origin of the excess at 5 MeV in the reactor antineutrino spectrum. In this paper, we explore the excess via the reaction $^{13}$C$(\overline{\nu}, \overline{\nu}^\prime n)^{12}$C$^*$ in organic scintillator detectors. The de-excitation of $^{12}$C$^*$ yields a prompt $4.4$ MeV photon, while the thermalization of the product neutron causes proton recoils, which in turn yield an additional prompt energy contribution with finite width. Together, these effects can mimic an inverse beta decay event with around 5 MeV energy. We consider several non-standard neutrino interactions to produce such a process and find that the parameter space preferred by Daya Bay is disfavored by measurements of neutrino-induced deuteron disintegration and coherent elastic neutrino-nucleus scattering. While non-minimal particle physics scenarios may be viable, a nuclear physics solution to this anomaly appears more appealing. ",https://doi.org/10.1103/PhysRevD.99.055045,1803.08506v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Do We Really Need That Many Parameters In Transformer For Extractive   Summarization? Discourse Can Help !,2020,"  The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed ""Synthesizer"" framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters. ",Kein DOI-Link verfügbar,2012.02144v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Improving Topic Segmentation by Injecting Discourse Dependencies,2022,"  Recent neural supervised topic segmentation models achieve distinguished superior effectiveness over unsupervised methods, with the availability of large-scale training corpora sampled from Wikipedia. These models may, however, suffer from limited robustness and transferability caused by exploiting simple linguistic cues for prediction, but overlooking more important inter-sentential topical consistency. To address this issue, we present a discourse-aware neural topic segmentation model with the injection of above-sentence discourse dependency structures to encourage the model make topic boundary prediction based more on the topical consistency between sentences. Our empirical study on English evaluation datasets shows that injecting above-sentence discourse structures to a neural topic segmenter with our proposed strategy can substantially improve its performances on intra-domain and out-of-domain data, with little increase of model's complexity. ",Kein DOI-Link verfügbar,2209.08626v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Is there maximal mixing in the lepton sector?,2004,  We discuss the potential of long-baseline neutrino oscillation experiments to determine deviations from maximal \nu_\mu-\nu_\tau mixing. We compare the obtainable sensitivities to predictions from neutrino mass models and to the size of quantum corrections. We find that the theoretical expectations for deviations are typically well within experimental reach. ,https://doi.org/10.1103/PhysRevD.70.097302,hep-ph/0404268v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Physics and Performance Evaluation Group,2007,"  We summarize the objectives and results of the ``international scoping study of a future neutrino factory and superbeam facility'' (ISS) physics working group. Furthermore, we discuss how the ISS study should develop into a neutrino factory design study (IDS-NF) from the point of view of physics and performance evaluation. ",https://doi.org/10.1063/1.2898998,0712.0909v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Orientational Order in Liquids upon Condensation in Nanochannels: An   Optical Birefringence Study on Rodlike and Disclike Molecules in Monolithic   Mesoporous Silica,2010,"  We present high-resolution optical birefringence measurements upon sequential filling of an array of parallel-aligned nanochannels (14~nm mean diameter) with rod-like (acetonitrile) and disc-like (hexafluorobenzene) molecules. We will demonstrate that such birefringence isotherms, when performed simultaneously with optically isotropic and index-matched counterparts (neopentane and hexafluoromethane), allow one to characterize the orientational state of the confined liquids with a high accuracy as a function of pore filling. The pore condensates are almost bulk-like, optically isotropic liquids. For both anisotropic species we find, however, a weak orientational order (of a few percent at maximum) upon film-condensation in the monolithic mesoporous membrane. It occurs upon formation of the second and third adsorbed layer, only, and vanishes gradually upon onset of capillary condensation. Presumably, it originates in the breaking of the full rotational symmetry of the interaction potential at the cylindrical, free liquid-vapor interface in the film-condensed state rather than at the silica-liquid interface. This conclusion is corroborated by comparisons of our experimental results with molecular dynamics simulations reported in the literature. ",https://doi.org/10.1103/PhysRevB.82.235404,1012.0997v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Influence of nano confinement on nematic liquid crystals,2012,"  We explore the nematic ordering of the rod-like liquid crystals 5CB and 6CB, embedded into parallel-aligned nanochannels in mesoporous silicon and silica membranes as a function of mean channel radius (4.7<=R <=8.3 nm), and thus geometrical confinement strength, by optical birefringence measurements in the infrared region. The orientational order inside the nanochannels results in an excess birefringence, which is proportional to the nematic order parameter. It evolves continuously upon cooling with a precursor behavior, typical of a paranematic state at high temperatures. These observations are compared with the bulk behavior and analyzed within a phenomenological model. Such an approach indicates that the strength of the nematic ordering fields sigma is beyond a critical threshold sigma_c =1/2, that separates discontinuous from continuous paranematic-to-nematic behavior. In agreement with the predictions of the phenomenological approach a linear dependency of sigma on the inverse channel radius is found and we can infer therefrom the critical channel radii, R_c, separating continuous from discontinuous paranematic-to-isotropic behavior, for 5CB (12.1 nm) and 6CB (14.0 nm). Our analysis suggests that the tangential anchoring at the channel walls is of similar strength in mesoporous silicon and mesoporous silica membranes. A comparison with the bulk phase behavior reveals that the nematic order in nanoconfinement is significantly affected by channel wall roughness leading to a reduction of the effective nematic ordering. ",https://doi.org/10.1103/PhysRevE.86.021701,1208.2820v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Inhomogeneous Relaxation Dynamics and Phase Behaviour of a Liquid   Crystal Confined in a Nanoporous Solid,2015,"  We report filling-fraction dependent dielectric spectroscopy measurements on the relaxation dynamics of the rod-like nematogen 7CB condensed in 13 nm silica nanochannels. In the film-condensed regime, a slow interface relaxation dominates the dielectric spectra, whereas from the capillary-condensed state up to complete filling an additional, fast relaxation in the core of the channels is found. The temperature-dependence of the static capacitance, representative of the averaged, collective molecular orientational ordering, indicates a continuous, paranematic-to-nematic (P-N) transition, in contrast to the discontinuous bulk behaviour. It is well described by a Landau-de-Gennes free energy model for a phase transition in cylindrical confinement. The large tensile pressure of 10 MPa in the capillary-condensed state, resulting from the Young-Laplace pressure at highly curved liquid menisci, quantitatively accounts for a downward-shift of the P-N transition and an increased molecular mobility in comparison to the unstretched liquid state of the complete filling. The strengths of the slow and fast relaxations provide local information on the orientational order: The thermotropic behaviour in the core region is bulk-like, i.e. it is characterized by an abrupt onset of the nematic order at the P-N transition. By contrast, the interface ordering exhibits a continuous evolution at the P-N transition. Thus, the phase behaviour of the entirely filled liquid crystal-silica nanocomposite can be quantitatively described by a linear superposition of these distinct nematic order contributions. ",https://doi.org/10.1039/C5SM00108K,1504.04808v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Adsorption from Binary Liquid Solutions into Mesoporous Silica: A   Capacitance Isotherm on 5CB Nematogen/Methanol Mixtures,2021,"  We present a capacitance method to measure the adsorption of rod-like nematogens (4-cyano-4'-pentylbiphenyl, 5CB) from a binary liquid 5CB/methanol solution into a monolithic mesoporous silica membrane traversed by tubular pores with radii of 5.4 nm at room temperature. The resulting adsorption isotherm is reminiscent of classical type II isotherms of gas adsorption in mesoporous media. Its analysis by a model for adsorption from binary solutions, as inspired by the Brunauer-Emmett-Teller (BET) approach for gas adsorption on solid surfaces, indicates that the first adsorbed monolayer consists of flat-lying (homogeneously anchored) 5CB molecules at the pore walls. An underestimation of the adsorbed 5CB amount by the adsorption model compared to the measured isotherm for high 5CB concentrations hints towards a capillary filling transition in the mesopores similar to capillary condensation, i.e. film-growth at the pore walls is replaced by filling of the pore centers by the liquid crystal. The experimental method and thermodynamic analysis presented here can easily be adapted to other binary liquid solutions and thus allows a controlled filling of mesoporous materials with non-volatile molecular systems. ",https://doi.org/10.1080/00268976.2021.1909160,2102.06908v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Multiple glassy dynamics of a homologous series of triphenylene-based   columnar liquid crystals -- A study by broadband dielectric spectroscopy and   advanced calorimetry,2022,"  Hexakis(n-alkyloxy)triphenylene) (HATn) consisting of an aromatic triphenylene core and alkyl side chains are model discotic liquid crystal (DLC) systems forming a columnar mesophase. In the mesophase, the molecules of HATn self-assemble in columns, which has one-dimensional high charge carrier mobility along the columns. Here, a homologous series of HATn with different length of the alkyl chain (n=5,6,8,10,12) is investigated using differential scanning calorimetry (DSC), broadband dielectric spectroscopy (BDS) and advanced calorimetric techniques including fast scanning calorimetry (FSC) and specific heat spectroscopy (SHS). The investigation of the phase behavior was done utilizing DSC experiments and the influence of the alkyl chain length on the phase behavior was revealed. By the dielectric investigations probing the molecular mobility, a $\gamma$-relaxation due to localized fluctuations as well as two glassy dynamics the $\alpha$ core and $\alpha$ alkyl relaxation were observed in the temperature range of the plastic crystalline phase. Moreover, the observed glassy dynamics were further studied employing advanced calorimetry. All observed relaxation processes are attributed to the possible specific molecular fluctuations and discussed in detail. From the results a transition at around n=8 from a rigid constrained (n=5,6) to a softer system (n=10,12) was revealed with increasing alkyl chain length. A counterbalance of two competing effects of a polyethylene like behavior of the alkyl chains in the intercolumnar domains and self-organized confinement is discussed in the context of a hindered glass transition. ",https://doi.org/10.1016/j.molliq.2022.119212,2205.02299v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Superbeams versus Neutrino Factories,2002,"  We compare the physics potential of planned superbeams with the one of neutrino factories. Therefore, the experimental setups as well as the most relevant uncertainties and errors are considered on the same footing as much as possible. We use an improved analysis including the full parameter correlations, as well as statistical, systematical, and degeneracy errors. Especially, degeneracies have so far not been taken into account in a numerical analysis. We furthermore include external input, such as improved knowledge of the solar oscillation parameters from the KamLAND experiment. This allows us to determine the limiting uncertainties in all cases. For a specific comparison, we choose two representatives of each class: For the superbeam, we take the first conceivable setup, namely the JHF to SuperKamiokande experiment, as well as, on a longer time scale, the JHF to HyperKamiokande experiment. For the neutrino factory, we choose an initially conceivable setup and an advanced machine. We determine the potential to measure the small mixing angle sin^2 2 theta_{13}, the sign of Delta m^2_{31}, and the leptonic CP phase $\deltacp$, which also implies that we compare the limitations of the different setups. We find interesting results, such as the complete loss of the sensitivity to the sign of Delta m^2_{31} due to degeneracies in many cases. ",https://doi.org/10.1016/S0550-3213(02)00825-8,hep-ph/0204352v2,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,From parameter space constraints to the precision determination of the   leptonic Dirac CP phase,2004,"  We discuss the precision determination of the leptonic Dirac CP phase $\delta_{CP}$ in neutrino oscillation experiments, where we apply the concept of ``CP coverage''. We demonstrate that this approach carries more information than a conventional CP violation measurement, since it also describes the exclusion of parameter regions. This will be very useful for next-generation long baseline experiments where for sizable $\sin^2 2 \theta_{13}$ first constraints on $\delta_{CP}$ can be obtained. As the most sophisticated experimental setup, we analyze neutrino factories, where we illustrate the major difficulties in their analysis. In addition, we compare their potential to the one of superbeam upgrades and next-generation experiments, which also includes a discussion of synergy effects. We find a strong dependence on the yet unknown true values of $\sin^2 2 \theta_{13}$ and $\delta_{CP}$, as well as a strong, non-Gaussian dependence on the confidence level. A systematic understanding of the complicated parameter dependence will be given. In addition, it is shown that comparisons of experiments and synergy discussions do in general not allow for an unbiased judgment if they are only performed at selected points in parameter space. Therefore, we present our results in dependence of the yet unknown true values of $\sin^2 2 \theta_{13}$ and $\delta_{CP}$. Finally we show that for $\delta_{CP}$ precision measurements there exist simple strategies including superbeams, reactor experiments, superbeam upgrades, and neutrino factories, where the crucial discriminator is $\sin^2 2 \theta_{13} \sim 10^{-2}$. ",https://doi.org/10.1088/1126-6708/2005/05/020,hep-ph/0412199v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Exploring neutrino parameters with a beta-beam experiment from FNAL to   DUSEL,2009,"  We discuss in detail the physics reach of an experimental set-up where electron neutrinos (anti-neutrinos) produced in a beta-beam facility at Fermi National Accelerator Laboratory (FNAL) are sent, over a distance of L~1300km, to the Deep Underground Science and Engineering Laboratory (DUSEL). A 300kt Water Cherenkov (WC) detector and a 50kt Liquid Argon Time Projection Chamber (LArTPC) are considered as possible detector choices. We propose to use 18Ne and 6He as source ions for electron neutrino and electron anti-neutrino beams, respectively. The maximum Lorentz boost factor, gamma, available for these ions using the Tevatron are gamma_Ne=585 and gamma_He=350. This particular set-up provides the opportunity to probe the first oscillation maximum using the neutrino beam and the second oscillation maximum using the anti-neutrino beam which helps to evade some parameter degeneracies. The resulting physics sensitivities for theta_13, CP violation and the mass hierarchy are compared to those of a conventional superbeam from FNAL to DUSEL. ",https://doi.org/10.1016/j.physletb.2010.08.009,0909.2257v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Two experiments for the price of one? -- The role of the second   oscillation maximum in long baseline neutrino experiments,2010,"  We investigate the quantitative impact that data from the second oscillation maximum has on the performance of wide band beam neutrino oscillation experiments. We present results for the physics sensitivities to standard three flavor oscillation, as well as results for the sensitivity to non-standard interactions. The quantitative study is performed using an experimental setup similar to the Fermilab to DUSEL Long Baseline Neutrino Experiment (LBNE). We find that, with the single exception of sensitivity to the mass hierarchy, the second maximum plays only a marginal role due to the experimental difficulties to obtain a statistically significant and sufficiently background-free event sample at low energies. This conclusion is valid for both water Cherenkov and liquid argon detectors. Moreover, we confirm that non-standard neutrino interactions are very hard to distinguish experimentally from standard three-flavor effects and can lead to a considerable loss of sensitivity to \theta_{13}, the mass hierarchy and CP violation. ",https://doi.org/10.1007/JHEP03(2011)013 10.1007/JHEP05(2011)024,1010.3706v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Statistical interpretation of sterile neutrino oscillation searches at   reactors,2020,"  A considerable experimental effort is currently under way to test the persistent hints for oscillations due to an eV-scale sterile neutrino in the data of various reactor neutrino experiments. The assessment of the statistical significance of these hints is usually based on Wilks' theorem, whereby the assumption is made that the log-likelihood is $\chi^2$-distributed. However, it is well known that the preconditions for the validity of Wilks' theorem are not fulfilled for neutrino oscillation experiments. In this work we derive a simple asymptotic form of the actual distribution of the log-likelihood based on reinterpreting the problem as fitting white Gaussian noise. From this formalism we show that, even in the absence of a sterile neutrino, the expectation value for the maximum likelihood estimate of the mixing angle remains non-zero with attendant large values of the log-likelihood. Our analytical results are then confirmed by numerical simulations of a toy reactor experiment. Finally, we apply this framework to the data of the Neutrino-4 experiment and show that the null hypothesis of no-oscillation is rejected at the 2.6\,$\sigma$ level, compared to 3.2\,$\sigma$ obtained under the assumption that Wilks' theorem applies. ",https://doi.org/10.1140/epjc/s10052-020-08774-2,2008.06083v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Faraday Instability in a Surface-Frozen Liquid,2005,"  Faraday surface instability measurements of the critical acceleration, a_c, and wavenumber, k_c, for standing surface waves on a tetracosanol (C_24H_50) melt exhibit abrupt changes at T_s=54degC above the bulk freezing temperature. The measured variations of a_c and k_c vs. temperature and driving frequency are accounted for quantitatively by a hydrodynamic model, revealing a change from a free-slip surface flow, generic for a free liquid surface (T>T_s), to a surface-pinned, no-slip flow, characteristic of a flow near a wetted solid wall (T < T_s). The change at T_s is traced to the onset of surface freezing, where the steep velocity gradient in the surface-pinned flow significantly increases the viscous dissipation near the surface. ",https://doi.org/10.1103/PhysRevLett.94.184504,physics/0504056v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Melting and freezing of argon in a granular packing of linear mesopore   arrays,2008,"  Freezing and melting of Ar condensed in a granular packing of template-grown arrays of linear mesopores (SBA-15, mean pore diameter 8 nanometer) has been studied by specific heat measurements C as a function of fractional filling of the pores. While interfacial melting leads to a single melting peak in C, homogeneous and heterogeneous freezing along with a delayering transition for partial fillings of the pores result in a complex freezing mechanism explainable only by a consideration of regular adsorption sites (in the cylindrical mesopores) and irregular adsorption sites (in niches of the rough external surfaces of the grains, and at points of mutual contact of the powder grains). The tensile pressure release upon reaching bulk liquid/vapor coexistence quantitatively accounts for an upward shift of the melting/freeezing temperature observed while overfilling the mesopores. ",https://doi.org/10.1103/PhysRevLett.100.175701,0803.4256v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Continuous Paranematic-to-Nematic Ordering Transitions of Liquid   Crystals in Tubular Silica Nanochannels,2008,"  The optical birefringence of rod-like nematogens (7CB, 8CB), imbibed in parallel silica channels with 10 nm diameter and 300 micrometer length, is measured and compared to the thermotropic bulk behavior. The orientational order of the confined liquid crystals, quantified by the uniaxial nematic ordering parameter, evolves continuously between paranematic and nematic states, in contrast to the discontinuous isotropic-to-nematic bulk phase transitions. A Landau-de Gennes model reveals that the strength of the orientational ordering fields, imposed by the silica walls, is beyond a critical threshold, that separates discontinuous from continuous paranematic-to-nematic behavior. Quenched disorder effects, attributable to wall irregularities, leave the transition temperatures affected only marginally, despite the strong ordering fields in the channels. ",https://doi.org/10.1103/PhysRevLett.101.187801,0810.0509v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Preferred orientation of n-hexane crystallized in silicon nanochannels:   A combined x-ray diffraction and sorption isotherm study,2009,"  We present an x-ray diffraction study on n-hexane in tubular silicon channels of approximately 10 nm diameter both as a function of the filling fraction f of the channels and as a function of temperature. Upon cooling, confined n-hexane crystallizes in a triclinic phase typical of the bulk crystalline state. However, the anisotropic spatial confinement leads to a preferred orientation of the confined crystallites, where the <001> crystallographic direction coincides with the long axis of the channels. The magnitude of this preferred orientation increases with the filling fraction, which corroborates the assumption of a Bridgman-type crystallization process being responsible for the peculiar crystalline texture. This growth process predicts for a channel-like confinement an alignment of the fastest crystallization direction parallel to the long channel axis. It is expected to be increasingly effective with the length of solidifying liquid parcels and thus with increasing f. In fact, the fastest solidification front is expected to sweep over the full silicon nanochannel for f=1, in agreement with our observation of a practically perfect texture for entirely filled nanochannels. ",https://doi.org/10.1103/PhysRevE.79.032601,0903.3379v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Capillary rise of water in hydrophilic nanopores,2009,"  We report on the capillary rise of water in three-dimensional networks of hydrophilic silica pores with 3.5nm and 5nm mean radii, respectively (porous Vycor monoliths). We find classical square root of time Lucas-Washburn laws for the imbibition dynamics over the entire capillary rise times of up to 16h investigated. Provided we assume two preadsorbed strongly bound layers of water molecules resting at the silica walls, which corresponds to a negative velocity slip length of -0.5nm for water flow in silica nanopores, we can describe the filling process by a retained fluidity and capillarity of water in the pore center. This anticipated partitioning in two dynamic components reflects the structural-thermodynamic partitioning in strongly silica bound water layers and capillary condensed water in the pore center which is documented by sorption isotherm measurements. ",https://doi.org/10.1103/PhysRevE.79.067301,0906.4614v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,A new approach to anti-neutrino running in long baseline neutrino   oscillation experiments,2010,"  We study the possibility to replace the anti-neutrino run of a long baseline neutrino oscillation experiment, with anti-neutrinos from muon decay at rest. The low energy of these neutrinos allows the use of inverse beta decay for detection in a Gadolinium-doped water Cerenkov detector. We show that this approach yields a factor of five times larger anti-neutrino event sample. The resulting discovery reaches in theta_13, the mass hierarchy and leptonic CP violation are compared with those from a conventional superbeam experiment with combined neutrino and anti-neutrino running. We find that this approach yields a greatly improved reach for CP violation and theta_13 while leaving the ability to measure the mass hierarchy intact. ",https://doi.org/10.1007/JHEP04(2011)099,1005.4055v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Transition from van-der-Waals to H Bonds dominated Interaction in   n-Propanol physisorbed on Graphite,2011,  Multilayer sorption isotherms of 1-propanol on graphite have been measured by means of high-resolution ellipsometry within the liquid regime of the adsorbed film for temperatures ranging from 180 to 260 K. In the first three monolayers the molecules are oriented parallel to the substrate and the growth is roughly consistent with the Frenkel-Halsey-Hill-model (FHH) that is obeyed in van-der-Waals systems on strong substrates. The condensation of the fourth and higher layers is delayed with respect to the FHH-model. The fourth layer is actually a bilayer. Furthermore there is indication of a wetting transition. The results are interpreted in terms of hydrogen-bridge bonding within and between the layers. ,https://doi.org/10.1103/PhysRevLett.106.156103,1104.3319v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Hydraulic Transport Across Hydrophilic and Hydrophobic Nanopores: Flow   Experiments with Water and n-Hexane,2015,"  We experimentally explore pressure-driven flow of water and n-hexane across nanoporous silica (Vycor glass monoliths with 7 or 10 nm pore diameters, respectively) as a function of temperature and surface functionalization (native and silanized glass surfaces). Hydraulic flow rates are measured by applying hydrostatic pressures via inert gases (argon and helium, pressurized up to 70 bar) on the upstream side in a capacitor-based membrane permeability setup. For the native, hydrophilic silica walls, the measured hydraulic permeabilities can be quantitatively accounted for by bulk fluidity provided we assume a sticking boundary layer, i.e. a negative velocity slip length of molecular dimensions. The thickness of this boundary layer is discussed with regard to previous capillarity-driven flow experiments (spontaneous imbibition) and with regard to velocity slippage at the pore walls resulting from dissolved gas. Water flow across the silanized, hydrophobic nanopores is blocked up to a hydrostatic pressure of at least 70 bar. The absence of a sticking boundary layer quantitatively accounts for an enhanced n-hexane permeability in the hydrophobic compared to the hydrophilic nanopores. ",https://doi.org/10.1103/PhysRevE.93.013102,1512.03908v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutrino Detectors as Tools for Nuclear Security,2019,"  For over 40 years, physicists have considered possible uses for neutrino detectors in nuclear nonproliferation, arms control, and fissile materials security. Neutrinos are an attractive fission signature because they readily pass through matter. The same property makes neutrinos challenging to detect in systems that would be practical for nuclear security applications. This colloquium presents a broad overview of several potential neutrino applications, including the near-field monitoring of known reactors, far-field monitoring of known or discovery of undeclared reactors, detection of reactor waste streams, and detection of nuclear explosions. We conclude that recent detector advances have made near-field monitoring feasible. Farther-field reactor detection and waste stream detection monitoring are possible in some cases with further research and development. Very long-range reactor monitoring and nuclear explosion detection do not appear feasible for the foreseeable future due to considerable physical and/or practical constraints. ",https://doi.org/10.1103/RevModPhys.92.011003,1908.07113v3,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Silicon Flexes Muscles: Giant Electrochemical Actuation in a Nanoporous   Silicon-Polypyrrole Hybrid Material,2020,"  The absence of piezoelectricity in silicon makes direct electro-mechanical applications of this mainstream semiconductor impossible. Integrated electrical control of the silicon mechanics, however, would open up new perspectives for on-chip actuorics. Here, we combine wafer-scale nanoporosity in single-crystalline silicon with polymerization of an artificial muscle material inside pore space to synthesize a composite that shows macroscopic electrostrain in aqueous electrolyte. The voltage-strain coupling is 3 orders of magnitude larger than the best-performing ceramics in terms of piezoelectric actuation. We trace this huge electroactuation to the concerted action of 100 billions of nanopores per square centimetre cross-section and to potential-dependent pressures of up to 150 atmospheres at the single-pore scale. The exceptionally small operation voltages (0.4-0.9 V) along with the sustainable and biocompatible base materials make this hybrid promising for bio-actuator applications. ",https://doi.org/10.1126/sciadv.aba1483,2010.03878v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,New features in the simulation of neutrino oscillation experiments with   GLoBES 3.0,2007,"  We present Version 3.0 of the GLoBES (``General Long Baseline Experiment Simulator'') software, which is a simulation tool for short- and long-baseline neutrino oscillation experiments. As a new feature, GLoBES 3.0 allows for user-defined systematical errors, which can also be used to simulate experiments with multiple discrete sources and detectors. In addition, the combination with external information, such as from different experiment classes, is simplified. As far as the probability calculation is concerned, GLoBES now provides an interface for the inclusion of non-standard physics without re-compilation of the software. The set of experiment prototypes coming with GLoBES has been updated. For example, built-in fluxes are now provided for the simulation of beta beams. ",https://doi.org/10.1016/j.cpc.2007.05.004,hep-ph/0701187v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Quantifying the sensitivity of oscillation experiments to the neutrino   mass ordering,2013,"  Determining the type of the neutrino mass ordering (normal versus inverted) is one of the most important open questions in neutrino physics. In this paper we clarify the statistical interpretation of sensitivity calculations for this measurement. We employ standard frequentist methods of hypothesis testing in order to precisely define terms like the median sensitivity of an experiment. We consider a test statistic $T$ which in a certain limit will be normal distributed. We show that the median sensitivity in this limit is very close to standard sensitivities based on $\Delta\chi^2$ values from a data set without statistical fluctuations, such as widely used in the literature. Furthermore, we perform an explicit Monte Carlo simulation of the INO, JUNO, LBNE, NOvA, and PINGU experiments in order to verify the validity of the Gaussian limit, and provide a comparison of the expected sensitivities for those experiments. ",https://doi.org/10.1007/JHEP03(2014)028,1311.1822v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutrino-nucleus interaction models and their impact on oscillation   analyses,2013,"  In neutrino oscillation experiments, neutrino interactions at the detector are simulated using event generators which attempt to reflect our understanding of nuclear physics. We study the impact of different neutrino interactions and nuclear models on the determination of neutrino oscillation parameters. We use two independent neutrino event generators, GENIE and GiBUU, and apply them to a setup with a conventional neutrino beam aiming at a water \v{C}erenkov detector, for which only the QE-like sample is selected. Subsequently, we perform a fit to the oscillation parameters in the $\nu_\mu$ disappearance channel. ",https://doi.org/10.1103/PhysRevD.89.073015,1311.4506v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Future searches for light sterile neutrinos at nuclear reactors,2021,"  We study the optimization of a green-field, two-baseline reactor experiment with respect to the sensitivity for electron antineutrino disappearance in search of a light sterile neutrino. We consider both commercial and research reactors and identify as key factors the distance of closest approach and detector energy resolution. We find that a total of 5 tons of detectors deployed at a commercial reactor with a closest approach of 25 m can probe the mixing angle $\sin^22\theta$ down to $\sim5\times10^{-3}$ around $\Delta m^2\sim 1$ eV$^2$. The same detector mass deployed at a research reactor can be sensitive up to $\Delta m^2\sim20-30$ eV$^2$ assuming a closest approach of 3 m and excellent energy resolution, such as that projected for the Taishan Antineutrino Observatory. We also find that lithium doping of the reactor could be effective in increasing the sensitivity for higher $\Delta m^2$ values. ",https://doi.org/10.1103/PhysRevD.105.035002,2104.00005v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Neutrino Oscillations at JUNO, the Born Rule, and Sorkin's Triple Path   Interference",2021,"  We argue that neutrino oscillations at JUNO offer a unique opportunity to study Sorkin's triple-path interference, which is predicted to be zero in canonical quantum mechanics by virtue of the Born rule. In particular, we compute the expected bounds on triple-path interference at JUNO and demonstrate that they are comparable to those already available from electromagnetic probes. Furthermore, the neutrino probe of the Born rule is much more direct due to an intrinsic independence from any boundary conditions, whereas such dependence on boundary conditions is always present in the case of electromagnetic probes. Thus, neutrino oscillations present an ideal probe of this aspect of the foundations of quantum mechanics. ",https://doi.org/10.1103/PhysRevD.105.115013,2105.14061v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Low-Energy Supernovae Bounds on Sterile Neutrinos,2023,"  Sterile neutrinos can be produced through mixing with active neutrinos in the hot and dense core of a core-collapse supernova (SN). The standard bounds on the active-sterile mixing ($\sin^2 \theta$) from SN arise from SN1987A energy-loss, requiring $E_{\text{loss}}<10^{52}~{\rm erg}$. In this letter, we discuss a novel bound on sterile neutrino parameter space arising from the energy deposition through its decays inside the SN envelope. Using the observed underluminous SN IIP population, this energy deposition is constrained to be below $\sim 10^{50}~{\rm erg}$. Focusing on sterile neutrino mixing only with the tau neutrino, for heavy sterile masses $m_s$ in the range $100$-$500$ MeV, we find stringent constraints on $\sin^2 \theta_\tau$ reaching two orders of magnitude lower than those from the SN1987A energy-loss argument. Similar bounds will also be applicable to sterile mixing only with muons ($\sin^2 \theta_\mu$). ",Kein DOI-Link verfügbar,2309.05860v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Probing the Sterile Neutrino Dipole Portal with SN1987A and Low-Energy   Supernovae,2024,"  BSM electromagnetic properties of neutrinos may lead to copious production of sterile neutrinos in the hot and dense core of a core-collapse supernova. In this work, we focus on the active-sterile transition magnetic moment portal for heavy sterile neutrinos. Firstly, we revisit the SN1987A cooling bounds for dipole portal using the integrated luminosity method, which yields more reliable results (especially in the trapping regime) compared to the previously explored via emissivity loss, aka the Raffelt criterion. Secondly, we obtain strong bounds on the dipole coupling strength reaching as low as $10^{-11} \text{ GeV}^{-1}$ from energy deposition, i.e., constrained from the observation of explosion energies of underluminous Type IIP supernovae. In addition, we find that sterile neutrino production from Primakoff upscattering off of proton dominates over scattering off of electron for low sterile neutrino masses. ",Kein DOI-Link verfügbar,2402.01624v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Opportunities in Neutrino Theory -- a Snowmass White Paper,2013,"  Neutrino masses are clear evidence for physics beyond the standard model and much more remains to be understood about the neutrino sector. We highlight some of the outstanding questions and research opportunities in neutrino theory. We show that most of these questions are directly connected to the very rich experimental program currently being pursued (or at least under serious consideration) in the United States and worldwide. Finally, we also comment on the state of the theoretical neutrino physics community in the U.S. ",Kein DOI-Link verfügbar,1309.7338v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Application of retardation-modulation polarimetry in studies of   nanocomposite materials,2019,"  We demonstrate an application of retardation-modulation polarimetry in studies of nanocomposite materials. Molecular ordering is explored on both nonchiral and chiral liquid crystals (LCs) in the bulk state and embedded into parallel-arrays of cylindrical channels of alumina or silica membranes of different channel sizes (12-42 nm). Two arms polarimetry serves for simultaneous measurements of the birefringence retardation and optical activity characterizing, respectively, orientational molecular ordering and chiral structuring inside nanochannels. ",https://doi.org/10.1109/TCSET.2018.8336248,1904.10268v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Molecular dynamics of n-hexane: A quasi-elastic neutron scattering study   on the bulk and spatially nanochannel-confined liquid,2012,"  We present incoherent quasi-elastic neutron scattering measurements in a wavevector transfer range from 0.4 AA^{-1} to 1.6AA^{-1} on liquid n-hexane confined in cylindrical, parallel-aligned nanochannels of 6 nm mean diameter and 260 micrometer length in monolithic, mesoporous silicon. They are complemented with, and compared to, measurements on the bulk system in a temperature range from 50K to 250K. The time-of-flight spectra of the bulk liquid can be modeled by microscopic translational as well as fast localized rotational, thermally-excited, stochastic motions of the molecules. In the nano-confined state of the liquid, which was prepared by vapor condensation, we find two molecular populations with distinct dynamics, a fraction which is immobile on the time scale of 1ps to 100ps probed in our experiments and a second component with a self-diffusion dynamics slightly slower than observed for the bulk liquid. No hints of an anisotropy of the translational diffusion with regard to the orientation of the channels' long axes have been found. The immobile fraction amounts to about 5% at 250K, gradually increases upon cooling and exhibits an abrupt increase at 160K (20K below bulk crystallization), which indicates pore freezing ",https://doi.org/10.1063/1.3696684,1204.2362v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Paranematic-to-nematic ordering of a binary mixture of rod-like liquid   crystals confined in cylindrical nanochannels,2014,"  We explore the optical birefringence of the nematic binary mixtures 6CB$_{1-x}$7CB$_x$ ($0~\le~x~\le~1$) imbibed into parallel-aligned nanochannels of mesoporous alumina and silica membranes for channel radii of $3.4~\le~R~\le 21.0$ nm. The results are compared with the bulk behavior and analyzed with a Landau-de-Gennes model. Depending on the channel radius the nematic ordering in the cylindrical nanochannels evolves either discontinuously (subcritical regime, nematic ordering field $\sigma<1/2$) or continuously (overcritical regime, $\sigma>1/2$), but in both cases with a characteristic paranematic precursor behavior. The strength of the ordering field, imposed by the channel walls, and the magnitude of quenched disorder varies linearly with the mole fraction $x$ and scales inversely proportionally with $R$ for channel radii larger than 4 nm. The critical pore radius, $R_c$, separating a continuous from a discontinuous paranematic-to-nematic evolution, varies linearly with $x$ and differs negligibly between the silica and alumina membranes. We find no hints of preferred adsorption of one species at the channels walls. By contrast, a linear variation of the nematic-to-paranematic transition point $T_{\rm PN}$ and of the nematic ordering field $\sigma$ vs. $x$ suggest that the binary mixtures of cyanobiphenyls 6CB and 7CB keep their homogeneous bulk stoichiometry also in nanoconfinement, at least for channel diameters larger than $\sim$7 nm. ",https://doi.org/10.1103/PhysRevE.89.062501,1406.5050v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Elastic Response of Mesoporous Silicon to Capillary Pressures in the   Pores,2015,"  We study water adsorption-induced deformation of a monolithic, mesoporous silicon membrane traversed by independent channels of $\sim$8 nm diameter. We focus on the elastic constant associated with the Laplace pressure-induced deformation of the membrane upon capillary condensation, i.e. the pore-load modulus. We perform finite-element method (FEM) simulations of the adsorption-induced deformation of hexagonal and square lattices of cylindrical pores representing the membrane. We find that the pore-load modulus weakly depends on the geometrical arrangement of pores, and can be expressed as a function of porosity. We propose an analytical model which relates the pore-load modulus to the porosity and to the elastic properties of bulk silicon (Young's modulus and Poisson's ratio), and provides an excellent agreement with FEM results. We find good agreement between our experimental data and the predictions of the analytical model, with the Young's modulus of the pore walls slightly lower than the bulk value. This model is applicable to a large class of materials with morphologies similar to mesoporous silicon. Moreover, our findings suggest that liquid condensation experiments allow one to elegantly access the elastic constants of a mesoporous medium. ",https://doi.org/10.1063/1.4923240,1507.06482v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Ionic liquid dynamics in nanoporous carbon: A pore-size- and   temperature-dependent neutron spectroscopy study on supercapacitor materials,2020,"  The influence of spatial confinement on the thermally excited stochastic cation dynamics of the room-temperature ionic liquid 1-N-butylpyridinium bis-((trifluoromethyl)sulfonyl)imide ([BuPy][Tf_2N]) inside porous carbide-derived carbons with various pore sizes in the sub- to a few nanometer range are investigated by quasi-elastic neutron spectroscopy. Using the potential of fixed window scans, i.e. scanning a sample parameter, while observing solely one specific energy transfer value, an overview of the dynamic landscape within a wide temperature range is obtained. It is shown that already these data provide a quite comprehensive understanding of the confinement-induced alteration of the molecular mobility in comparison to the bulk. A complementary, more detailed analysis of full energy transfer spectra at selected temperatures reveals two translational diffusive processes on different time scales. Both are considerably slower than in the bulk liquid and show a decrease of the respective self-diffusion coefficients with decreasing nanopore size. Different thermal activation energies for molecular self-diffusion in nanoporous carbons with similar pore size indicate the importance of pore morphology on the molecular mobility, beyond the pure degree of confinement. In spite of the dynamic slowing down we can show that the temperature range of the liquid state upon nanoconfinement is remarkably extended to much lower temperatures, which is beneficial for potential technical applications of such systems. ",https://doi.org/10.1103/PhysRevMaterials.4.055401,2005.02851v1,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Systematic uncertainties in long-baseline neutrino oscillations for   large $θ_{13}$,2012,"  We study the physics potential of future long-baseline neutrino oscillation experiments at large $\theta_{13}$, focusing especially on systematic uncertainties. We discuss superbeams, \bbeams, and neutrino factories, and for the first time compare these experiments on an equal footing with respect to systematic errors. We explicitly simulate near detectors for all experiments, we use the same implementation of systematic uncertainties for all experiments, and we fully correlate the uncertainties among detectors, oscillation channels, and beam polarizations as appropriate. As our primary performance indicator, we use the achievable precision in the measurement of the CP violating phase $\deltacp$. We find that a neutrino factory is the only instrument that can measure $\deltacp$ with a precision similar to that of its quark sector counterpart. All neutrino beams operating at peak energies $\gtrsim 2$ GeV are quite robust with respect to systematic uncertainties, whereas especially \bbeams and \thk suffer from large cross section uncertainties in the quasi-elastic regime, combined with their inability to measure the appearance signal cross sections at the near detector. A noteworthy exception is the combination of a $\gamma=100$ \bbeam with an \spl-based superbeam, in which all relevant cross sections can be measured in a self-consistent way. This provides a performance, second only to the neutrino factory. For other superbeam experiments such as \lbno and the setups studied in the context of the \lbne reconfiguration effort, statistics turns out to be the bottleneck. In almost all cases, the near detector is not critical to control systematics since the combined fit of appearance and disappearance data already constrains the impact of systematics to be small provided that the three active flavor oscillation framework is valid. ",https://doi.org/10.1103/PhysRevD.87.033004,1209.5973v1,Yes,"noteworthy(1), potent(1)"
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Observation of Reactor Antineutrinos with a Rapidly-Deployable   Surface-Level Detector,2018,"  We deployed a small, 80kg, antineutrino detector based on solid plastic scintillator, called MiniCHANDLER for nearly three months at a distance of 25m from a 2.9GW thermal power reactor core at the North Anna Nuclear Generating Station. We report the detection of an antineutrino signal resulting from inverse beta decay at 5.5 sigma significance with no overburden and minimal shielding. This result also demonstrates that 3D segmentation can be used to significantly improve the signal to noise ratio, in this case by a factor of 4. In addition, this measurement represents an observation of the positron spectrum in a small, surface-deployed detector; this observation of reactor antineutrinos was achieved with a mobile neutrino detector mounted in an ordinary, small trailer. ",https://doi.org/10.1103/PhysRevApplied.13.034028,1812.02163v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training,2021,"  With the rise of large-scale pre-trained language models, open-domain question-answering (ODQA) has become an important research topic in NLP. Based on the popular pre-training fine-tuning approach, we posit that an additional in-domain pre-training stage using a large-scale, natural, and diverse question-answering (QA) dataset can be beneficial for ODQA. Consequently, we propose a novel QA dataset based on the Common Crawl project in this paper. Using the readily available schema.org annotation, we extract around 130 million multilingual question-answer pairs, including about 60 million English data-points. With this previously unseen number of natural QA pairs, we pre-train popular language models to show the potential of large-scale in-domain pre-training for the task of question-answering. In our experiments, we find that pre-training question-answering models on our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks. ",Kein DOI-Link verfügbar,2110.07731v2,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Wafer-scale fabrication of mesoporous silicon functionalized with   electrically conductive polymers,2024,"  The fabrication of hybrid materials consisting of nanoporous hosts with conductive polymers is a challenging task, since the extreme spatial confinement often conflicts with the stringent physico-chemical requirements for polymerization of organic constituents. Here, several low-threshold and scalable synthesis routes for such hybrids are presented. First, the electrochemical synthesis of composites based on mesoporous silicon (pore size of 7 nm) and the polymers PANI, PPy and PEDOT is discussed and validated by scanning electron microscopy (SEM) and energy-dispersive X-ray spectroscopy (EDX). Polymer filling degrees of 74% are achieved. Second, the production of PEDOT/pSi hybrids, based on the solid-state polymerization (SSP) of DBEDOT to PEDOT is shown. The resulting amorphous structure of the nanopore-embedded PEDOT is investigated via in-situ synchrotron-based X-ray scattering. In addition, a twofold increase in the electrical conductivity of the hybrid compared to the porous silicon host is shown, making this system particularly promising for thermoelectric applications. ",Kein DOI-Link verfügbar,2401.09276v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,CoDi: Conversational Distillation for Grounded Question Answering,2024,"  Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge compared to larger models. Secondly, high-quality conversational datasets are often scarce, small, and domain-specific. Addressing these challenges, we introduce a novel data distillation framework named CoDi (short for Conversational Distillation, pronounced ""Cody""), allowing us to synthesize large-scale, assistant-style datasets in a steerable and diverse manner. Specifically, while our framework is task agnostic at its core, we explore and evaluate the potential of CoDi on the task of conversational grounded reasoning for question answering. This is a typical on-device scenario for specialist SLMs, allowing for open-domain model responses, without requiring the model to ""memorize"" world knowledge in its limited weights. Our evaluations show that SLMs trained with CoDi-synthesized data achieve performance comparable to models trained on human-annotated data in standard metrics. Additionally, when using our framework to generate larger datasets from web data, our models surpass larger, instruction-tuned models in zero-shot conversational grounded reasoning tasks. ",Kein DOI-Link verfügbar,2408.11219v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Thermotropic interface and core relaxation dynamics of liquid crystals   in silica glass nanochannels: A dielectric spectroscopy study,2015,"  We report dielectric relaxation spectroscopy experiments on two rod-like liquid crystals of the cyanobiphenyl family (5CB and 6CB) confined in tubular nanochannels with 7 nm radius and 340 micrometer length in a monolithic, mesoporous silica membrane. The measurements were performed on composites for two distinct regimes of fractional filling: monolayer coverage at the pore walls and complete filling of the pores. For the layer coverage a slow surface relaxation dominates the dielectric properties. For the entirely filled channels the dielectric spectra are governed by two thermally-activated relaxation processes with considerably different relaxation rates: A slow relaxation in the interface layer next to the channel walls and a fast relaxation in the core region of the channel filling. The strengths and characteristic frequencies of both relaxation processes have been extracted and analysed as a function of temperature. Whereas the temperature dependence of the static capacitance reflects the effective (average) molecular ordering over the pore volume and is well described within a Landau-de Gennes theory, the extracted relaxation strengths of the slow and fast relaxation processes provide an access to distinct local molecular ordering mechanisms. The order parameter in the core region exhibits a bulk-like behaviour with a strong increase in the nematic ordering just below the paranematic-to-nematic transition temperature T_PN and subsequent saturation during cooling. By contrast, the surface ordering evolves continuously with a kink near T_PN. A comparison of the thermotropic behaviour of the monolayer with the complete filling reveals that the molecular order in the core region of the pore filling affects the order of the peripheral molecular layers at the wall. ",https://doi.org/10.1039/C5CP03039K,1508.03827v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Capillary rise dynamics of liquid hydrocarbons in mesoporous silica as   explored by gravimetry, optical and neutron imaging: Nano-rheology and   determination of pore size distributions from the shape of imbibition fronts",2015,"  We present gravimetrical, optical, and neutron imaging measurements of the capillarity-driven infiltration of mesoporous silica glass by hydrocarbons. Square-root-of-time Lucas-Washburn invasion kinetics are found for linear alkanes from n-decane (C10) to n-hexacontane (C60) and for squalane, a branched alkane, in porous Vycor with 6.5 nm or 10 nm pore diameter, respectively. Humidity-dependent experiments allow us to study the influence on the imbibition kinetics of water layers adsorbed on the pore walls. Except for the longest molecule studied, C60, the invasion kinetics can be described by bulk fluidity and bulk capillarity, provided we assume a sticking, pore-wall adsorbed boundary layer, i.e. a monolayer of water covered by a monolayer of flat-laying hydrocarbons. For C60, however, an enhanced imbibition speed compared to the value expected in the bulk is found. This suggests the onset of velocity slippage at the silica walls or a reduced shear viscosity due to the transition towards a polymer-like flow in confined geometries. Both, light scattering and neutron imaging indicate a pronounced roughening of the imbibition fronts. Their overall shape and width can be resolved by neutron imaging. The fronts can be described by a superposition of independent wetting fronts moving with pore size-dependent square-root-of-time laws and weighted according to the pore size distributions obtained from nitrogen gas sorption isotherms. This finding indicates that the shape of the imbibition front in a porous medium, such as Vycor glass, with interconnected, elongated pores, is solely determined by independent movements of liquid menisci. These are dictated by the Laplace pressure and hydraulic permeability variations and thus the pore size variation at the invasion front. Our results suggest that pore size distributions can be derived from the broadening of imbibition fronts. ",https://doi.org/10.1016/j.colsurfa.2015.09.055,1511.08728v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Missing energy and the measurement of the CP-violating phase in neutrino   oscillations,2015,"  In the next generation of long-baseline neutrino oscillation experiments, aiming to determine the charge-parity violating phase $\delta_{CP}$ in the appearance channel, fine-grained time-projection chambers are expected to play an important role. In this Letter, we analyze an influence of realistic detector capabilities on the $\delta_{CP}$ sensitivity for a setup similar to that of the Deep Underground Neutrino Experiment. We find that the effect of the missing energy, carried out by undetected particles, is sizable. Although the reconstructed neutrino energy can be corrected for the missing energy, the accuracy of such procedure has to exceed 20\%, to avoid a sizable bias in the extracted $\delta_{CP}$ value. ",https://doi.org/10.1103/PhysRevD.92.091301,1507.08561v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Precursor Film Spreading during Liquid Imbibition in Nanoporous Photonic   Crystals,2020,"  When a macroscopic droplet spreads, a thin precursor film of liquid moves ahead of the advancing liquid-solid-vapor contact line. Whereas this phenomenon has been explored extensively for planar solid substrates, its presence in nanostructured geometries has barely been studied so far, despite its importance for many natural and technological fluid transport processes. Here we use porous photonic crystals in silicon to resolve by light interferometry capillarity-driven spreading of liquid fronts in pores of few nanometers in radius. Upon spatiotemporal rescaling the fluid profiles collapse on master curves indicating that all imbibition fronts follow a square-root-of-time broadening dynamics. For the simple liquid (glycerol) a sharp front with a widening typical of Lucas-Washburn capillary-rise dynamics in a medium with pore-size distribution occurs. By contrast, for a polymer (PDMS) a precursor film moving ahead of the main menisci entirely alters the nature of the nanoscale transport, in agreement with predictions of computer simulations. ",https://doi.org/10.1103/PhysRevLett.125.234502,2011.05401v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language   Models in Dialogues,2023,"  Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to build discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple tasks for fine-tuning and show that the dialogue-tailored Sentence Ordering task performs best. To locate and exploit discourse information in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals achieve encouraging results on the STAC corpus, with F1 scores of 57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When restricted to projective trees, our scores improved to 63.3 and 68.1. ",Kein DOI-Link verfügbar,2302.05895v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Small But Funny: A Feedback-Driven Approach to Humor Distillation,2024,"  The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a ""teacher"" generating data, as well as a ""critic"" evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation. ",Kein DOI-Link verfügbar,2402.18113v1,Yes,"intricate(1), potent(1)"
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,High-resolution dielectric study reveals pore size-dependent   orientational order of a discotic liquid crystal confined in tubular   nanopores,2015,"  We report a high-resolution dielectric study on a pyrene-based discotic liquid crystal (DLC) in the bulk state and confined in parallel tubular nanopores of monolithic silica and alumina membranes. The positive dielectric anisotropy of the DLC molecule at low frequencies (in the quasi-static case) allows us to explore the thermotropic collective orientational order. A face-on arrangement of the molecular discs on the pore walls and a corresponding radial arrangement of the molecules is found. In contrast to the bulk, the isotropic-to-columnar transition of the confined DLC is continuous, shifts with decreasing pore diameter to lower temperatures and exhibits a pronounced hysteresis between cooling and heating. These findings corroborate conclusions from previous neutron and X-ray scattering experiments as well as optical birefringence measurements. Our study also indicates that the relative simple dielectric technique presented here is a quite efficient method in order to study the thermotropic orientational order of DLC based nanocomposites. ",https://doi.org/10.1103/PhysRevE.92.012503,1507.02627v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Dynamic Kerr and Pockels Electro-Optics of Liquid Crystals in Nanopores   for Active Photonic Metamaterials,2021,"  Photonic metamaterials with properties unattainable in base materials are already beginning to revolutionize optical component design. However, their exceptional characteristics are often static, as artificially engineered into the material during the fabrication process. This limits their application for in-operando adjustable optical devices and active optics in general. Here, for a hybrid material consisting of a liquid crystal-infused nanoporous solid, we demonstrate active and dynamic control of its meta-optics by applying alternating electric fields parallel to the long axes of its cylindrical pores. First-harmonic Pockels and second-harmonic Kerr birefringence responses, strongly depending on the excitation frequency- and temperature, are observed in a frequency range from 50 Hz to 50 kHz. This peculiar behavior is quantitatively traced by a Landau-De Gennes free energy analysis to an order-disorder orientational transition of the rod-like mesogens and intimately related changes in the molecular mobilities and polar anchoring at the solid walls on the single-pore, meta-atomic scale. Thus, our study evidences that liquid crystal-infused nanopores exhibit integrated multi-physical couplings and reversible phase changes that make them particularly promising for the design of photonic metamaterials with thermo-electrically tunable birefringence in the emerging field of spacetime metamaterials aiming at a full spatio-temporal control of light. ",Kein DOI-Link verfügbar,2107.01363v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Wafer-Scale Fabrication of Hierarchically Porous Silicon and Silica   Glass by Active Nanoparticle-Assisted Chemical Etching and Pseudomorphic   Thermal Oxidation,2022,"  Many biological materials exhibit a multiscale porosity with small, mostly nanoscale pores as well as large, macroscopic capillaries to simultaneously achieve optimized mass transport capabilities and lightweight structures with large inner surfaces. Realizing such a hierarchical porosity in artificial materials necessitates often sophisticated and expensive top-down processing that limits scalability. Here we present an approach that combines self-organized porosity based on metal-assisted chemical etching (MACE) with photolithographically induced macroporosity for the synthesis of single-crystalline silicon with a bimodal pore-size distribution, i.e., hexagonally arranged cylindrical macropores with 1 micrometer diameter separated by walls that are traversed by mesopores 60 nm across. The MACE process is mainly guided by a metal-catalyzed reduction-oxidation reaction, where silver nanoparticles (AgNPs) serve as the catalyst. In this process, the AgNPs act as self-propelled particles that are constantly removing silicon along their trajectories. High-resolution X-ray imaging and electron tomography reveal a resulting large open porosity and inner surface for potential applications in high-performance energy storage, harvesting and conversion or for on-chip sensorics and actuorics. Finally, the hierarchically porous silicon membranes can be transformed structure-conserving by thermal oxidation into hierarchically porous amorphous silica, a material that could be of particular interest for opto-fluidic and (bio-)photonic applications due to its multiscale artificial vascularization. ",https://doi.org/10.1002/smll.202206842,2212.10160v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Statistical significance of the sterile-neutrino hypothesis in the   context of reactor and gallium data,2021,"  We evaluate the statistical significance of the 3+1 sterile-neutrino hypothesis using $\nu_e$ and $\bar\nu_e$ disappearance data from reactor, solar and gallium radioactive source experiments. Concerning the latter, we investigate the implications of the recent BEST results. For reactor data we focus on relative measurements independent of flux predictions. For the problem at hand, the usual $\chi^2$-approximation to hypothesis testing based on Wilks' theorem has been shown in the literature to be inaccurate. We therefore present results based on Monte Carlo simulations, and find that this typically reduces the significance by roughly $1\,\sigma$ with respect to the na\""ive expectation. We find no significant indication in favor of sterile-neutrino oscillations from reactor data. On the other hand, gallium data (dominated by the BEST result) show more than $5\,\sigma$ of evidence supporting the sterile-neutrino hypothesis, favoring oscillation parameters in agreement with constraints from reactor data. This explanation is, however, in significant tension ($\sim 3\,\sigma$) with solar neutrino experiments. In order to assess the robustness of the signal for gallium experiments we present a discussion of the impact of cross-section uncertainties on the results. ",https://doi.org/10.1007/JHEP02(2022)055,2111.12530v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"Optimization of the Neutrino Factory, revisited",2010,"  We perform the baseline and energy optimization of the Neutrino Factory including the latest simulation results on the magnetized iron detector (MIND). We also consider the impact of tau decays, generated by nu_mu to nu_tau or nu_e to nu_tau appearance, on the mass hierarchy, CP violation, and theta_{13} discovery reaches, which we find to be negligible for the considered detector. For the baseline-energy optimization for small theta_{13}, we qualitatively recover the results with earlier simulations of the MIND detector. We find optimal baselines of about 2500 km to 5000 km for the CP violation measurement, where now values of E_mu as low as about 12 GeV may be possible. However, for large theta_{13}, we demonstrate that the lower threshold and the backgrounds reconstructed at lower energies allow in fact for muon energies as low as 5 GeV at considerably shorter baselines, such as FNAL-Homestake. This implies that with the latest MIND analysis, low- and high-energy versions of the Neutrino Factory are just two different versions of the same experiment optimized for different parts of the parameter space. Apart from a green-field study of the updated detector performance, we discuss specific implementations for the two-baseline Neutrino Factory, where the considered detector sites are taken to be currently discussed underground laboratories. We find that reasonable setups can be found for the Neutrino Factory source in Asia, Europe, and North America, and that a triangular-shaped storage ring is possible in all cases based on geometrical arguments only. ",https://doi.org/10.1007/JHEP01(2011)120,1012.1872v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Tetra Point Wetting at the Free Surface of Liquid Ga-Bi,2002,"  A continuous surface wetting transition, pinned to a solid/liquid/liquid/vapor tetra coexistence point, is studied by x-ray reflectivity in liquid Ga-Bi binary alloys. The short-range surface potential is determined from the measured temperature evolution of the wetting film. The thermal fluctuations are shown to be insufficient to induce a noticeable breakdown of mean-field behavior, expected in short-range-interacting systems due to their $d_u=3$ upper critical dimensionality. ",https://doi.org/10.1103/PhysRevLett.89.035502,cond-mat/0205506v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,The Physics Case for a Neutrino Factory,2022,"  Neutrino factories, neutrino beams produced in the decay of a muon or antimuon beam inside a storage ring, yield cleaner, richer, and more flexible neutrino beams relative to super-beams. We explore the physics case for this type of beam both for standard oscillation as well as new physics searches and present some machine options. We argue that there is a rich program beyond what the current neutrino program can cover and a string synergy with the muon collider program. ",Kein DOI-Link verfügbar,2203.08094v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Deformation Dynamics of Nanopores upon Water Imbibition,2023,"  Capillarity-driven transport in nanoporous solids is ubiquitous in nature and is of increasing importance for the functionality of modern liquid-infused engineering materials. During imbibition, highly curved menisci are driven by negative Laplace pressures of several hundred atmospheres, exerting an enormous contractile load on an increasing portion of the porous matrix. Due to the challenge of simultaneously monitoring imbibition and deformation with high spatial resolution, the resulting coupling of solid elasticity to liquid capillarity has remained largely unexplored. Here, we study water imbibition in mesoporous silica using optical imaging, gravimetry, and high-resolution dilatometry. In contrast to an expected Laplace pressure-induced contraction, we find a square-root-of-time expansion and an additional abrupt length increase when the menisci reach the top surface. The final expansion is absent when we stop the imbibition front inside the porous medium in a dynamic imbibition-evaporation equilibrium, as is typical for water transport and transpiration in plants. These peculiar deformation behaviors are validated by single-nanopore molecular dynamics simulations and described by a continuum model that highlights the importance of expansive surface stresses at the pore walls (Bangham effect) and the buildup or release of contractile Laplace pressures as nanoscale menisci collectively advance, arrest, or disappear. Our model predicts that these observations are valid not only for water imbibition in silica, but for any imbibition process in nanopores, regardless of the liquid/solid combination. This also suggests that simple deformation measurements can be used to quantify surface stresses and Laplace pressures or transport in a wide variety of natural and artificial porous media. ",Kein DOI-Link verfügbar,2311.13025v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Polymeric Liquids in Nanoporous Photonic Structures: From Precursor Film   Spreading to Imbibition Dynamics at the Nanoscale,2023,"  Polymers are known to wet nanopores with high surface energy through an atomically thin precursor film followed by slower capillary filling. We present here light interference spectroscopy using a nanoporous membrane-based chip that allows us to observe the dynamics of these phenomena in situ with sub-nanometer spatial and milli- to microsecond temporal resolution. The device consists of a mesoporous silicon film (average pore size 6 nm) with an integrated photonic crystal, which permits to simultaneously measure the phase shift of the thin-film interference and the resonance of the photonic crystal upon imbibition. For a styrene dimer, we find a flat fluid front without a precursor film, while the pentamer forms an expanding molecular thin film moving in front of the menisci of the capillary filling. These different behaviors are attributed to a significantly faster pore-surface diffusion compared to the imbibition dynamics for the pentamer and vice versa for the dimer. In addition, both oligomers exhibit anomalously slow imbibition dynamics, which could be explained by apparent viscosities of six and eleven times the bulk value, respectively. However, a more consistent description of the dynamics is achieved by a constriction model that emphasizes the increasing importance of local undulations in the pore radius with the molecular size and includes a sub-nanometer hydrodynamic dead, immobile zone at the pore wall, but otherwise uses bulk fluid parameters. Overall, our study illustrates that interferometric, opto-fluidic experiments with nanoporous media allow for a remarkably detailed exploration of the nano-rheology of polymeric liquids. ",Kein DOI-Link verfügbar,2312.00142v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,X-ray study of the liquid potassium surface: structure and capillary   wave excitations,2004,"  We present x-ray reflectivity and diffuse scattering measurements from the liquid surface of pure potassium. They strongly suggest the existence of atomic layering at the free surface of a pure liquid metal with low surface tension. Prior to this study, layering was observed only for metals like Ga, In and Hg, the surface tensions of which are 5-7 fold higher than that of potassium, and hence closer to inducing an ideal ""hard wall"" boundary condition. The experimental result requires quantitative analysis of the contribution to the surface scattering from thermally excited capillary waves. Our measurements confirm the predicted form for the differential cross section for diffuse scattering, $d\sigma /d\Omega \sim 1/q_{xy}^{2-\eta}$ where $\eta = k_BT q_z^2/2\pi \gamma $, over a range of $\eta$ and $q_{xy}$ that is larger than any previous measurement. The partial measure of the surface structure factor that we obtained agrees with computer simulations and theoretical predictions. ",https://doi.org/10.1103/PhysRevB.67.115405,cond-mat/0406585v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Rich polymorphism of a rod-like liquid crystal (8CB) confined in two   types of unidirectional nanopores,2008,"  We present a neutron and X-rays scattering study of the phase transitions of 4-n-octyl-4'-cyanobiphenyl (8CB) confined in unidirectional nanopores of porous alumina and porous silicon (PSi) membranes with an average diameter of 30 nm. Spatial confinement reveals a rich polymorphism, with at least four different low temperature phases in addition to the smectic A phase. The structural study as a function of thermal treatments and conditions of spatial confinement allows us to get insights into the formation of these phases and their relative stability. It gives the first description of the complete phase behavior of 8CB confined in PSi and provides a direct comparison with results obtained in bulk conditions and in similar geometric conditions of confinement but with reduced quenched disorder effects using alumina anopore membranes ",https://doi.org/10.1140/epje/i2007-10323-0,0804.1540v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Criticality of an isotropic-to-smectic transition induced by anisotropic   quenched disorder,2010,"  We report combined optical birefringence and neutron scattering measurements on the liquid crystal 12CB nanoconfined in mesoporous silicon layers. This liquid crystal exhibits strong nematic-smectic coupling responsible for a discontinuous isotropic-to-smectic phase transition in the bulk state. Confined in porous silicon, 12CB is subjected to strong anisotropic quenched disorder: a short-ranged smectic state evolves out of a paranematic phase. This transformation appears continuous, losing its bulk first order character. This contrasts with previously reported observations on liquid crystals under isotropic quenched disorder. In the low temperature phase, both orientational and translational order parameters obey the same power-law. ",https://doi.org/10.1103/PhysRevE.81.031703,1005.0240v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,How nanoporous silicon-polypyrrole hybrids flex their muscles in aqueous   electrolytes: In operando high-resolution x-ray diffraction and electron   tomography-based micromechanical computer simulations,2022,"  Macroscopic strain experiments revealed that Si crystals traversed by parallel, channel-like nanopores functionalized with the muscle polymer polypyrrole exhibit large and reversible electrochemo-mechanical actuation in aqueous electrolytes. On the microscopical level this system still bears open questions, as to how the electrochemical expansion and contraction of PPy acts on to np-Si pore walls and how the collective motorics of the pore array emerges from the single-nanopore behavior. An analysis of in operando X-ray diffraction experiments with micromechanical finite element simulations, based on a 3D reconstruction of the nanoporous medium by TEM tomography, shows that the in-plane mechanical response is dominantly isotropic despite the anisotropic elasticity of the single crystalline host matrix. However, the structural anisotropy originating from the parallel alignment of the nanopores lead to significant differences between the in- and out-of-plane electromechanical response. This response is not describable by a simple 2D arrangement of parallel cylindrical channels. Rather, the simulations highlight that the dendritic shape of the Si pore walls, including pore connections between the main channels, cause complex, inhomogeneous stress-strain fields in the crystalline host. Time-dependent X-ray scattering on the dynamics of the actuator properties hint towards the importance of diffusion limitations, plastic deformation and creep in the nanoconfined polymer upon (counter-)ion adsorption and desorption, the very pore-scale processes causing the macroscopic electroactuation. From a more general perspective, our study demonstrates that the combination of TEM tomography-based micromechanical modeling with high-resolution X-ray scattering experiments provides a powerful approach for in operando analysis of nanoporous composites from the single-nanopore up to the porous-medium scale. ",https://doi.org/10.1103/PhysRevMaterials.6.116002,2211.15496v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,"A ferroelectric liquid crystal confined in cylindrical nanopores:   Reversible smectic layer buckling, enhanced light rotation and extremely fast   electro-optically active Goldstone excitations",2017,"  The orientational and translational order of a thermotropic ferroelectric liquid crystal (2MBOCBC) imbibed in self-organized, parallel, cylindrical pores with radii of 10, 15, or 20 nm in anodic aluminium oxide monoliths (AAO) are explored by high-resolution linear and circular optical birefringence as well as neutron diffraction texture analysis. The results are compared to experiments on the bulk system. The native oxidic pore walls do not provide a stable smectogen wall anchoring. By contrast, a polymeric wall grafting enforcing planar molecular anchoring results in a thermal-history independent formation of smectic C* helices and a reversible chevron-like layer buckling. An enhancement of the optical rotatory power by up to one order of magnitude of the confined compared to the bulk liquid crystal is traced to the pretransitional formation of helical structures at the smectic-A*-to-smectic-C* transformation. A linear electro-optical birefringence effect evidences collective fluctuations in the molecular tilt vector direction along the confined helical superstructures, i.e. the Goldstone phason excitations typical of the para-to-ferroelectric transition. Their relaxation frequencies increase with the square of the inverse pore radii as characteristic of plane-wave excitations and are two orders of magnitude larger than in the bulk, evidencing an exceptionally fast electro-optical functionality of the liquid-crystalline-AAO nanohybrids. ",https://doi.org/10.1039/C7NR07273B,1711.09673v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Anisotropic Confinement of Chromophores Induces Second-Order Nonlinear   Optics in a Nanoporous Photonic Metamaterial,2021,"  Second-order nonlinear optics is the base for a large variety of devices aimed at the active manipulation of light. However, physical principles restrict its occurrence to non-centrosymmetric, anisotropic matter. This significantly limits the number of base materials exhibiting nonlinear optics. Here, we show that embedding chromophores in an array of conical channels 13 nm across in monolithic silica results in mesoscopic anisotropic matter and thus in a hybrid material showing second-harmonic generation (SHG). This non-linear optics is compared to the one achieved in corona-poled polymer films containing the identical chromophores. It originates in confinement-induced orientational order of the elongated guest molecules in the nanochannels. This leads to a non-centrosymmetric dipolar order and hence to a non-linear light-matter interaction on the sub-wavelength, single-pore scale. Our study demonstrates that the advent of large-scale, self-organised nanoporosity in monolithic solids along with confinement-controllable orientational order of chromophores at the single-pore scale provides a reliable and accessible tool to design materials with a nonlinear meta-optics. ",https://doi.org/10.1364/OL.416948,2102.05322v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Quantized Self-Assembly of Discotic Rings in a Liquid Crystal Confined   in Nanopores,2018,"  Disklike molecules with aromatic cores spontaneously stack up in linear columns with high, one-dimensional charge carrier mobilities along the columnar axes making them prominent model systems for functional, self-organized matter. We show by high-resolution optical birefringence and synchrotron-based X-ray diffraction that confining a thermotropic discotic liquid crystal in cylindrical nanopores induces a quantized formation of annular layers consisting of concentric circular bent columns, unknown in the bulk state. Starting from the walls this ring self-assembly propagates layer by layer towards the pore center in the supercooled domain of the bulk isotropic-columnar transition and thus allows one to switch on and off reversibly single, nanosized rings through small temperature variations. By establishing a Gibbs free energy phase diagram we trace the phase transition quantization to the discreteness of the layers' excess bend deformation energies in comparison to the thermal energy, even for this near room-temperature system. Monte Carlo simulations yielding spatially resolved nematic order parameters, density maps and bond-forientational order parameters corroborate the universality and robustness of the confinement-induced columnar ring formation as well as its quantized nature. ",https://doi.org/10.1103/PhysRevLett.120.067801,1801.07605v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,How water wets and self-hydrophilizes nanopatterns of physisorbed   hydrocarbons,2021,"  Weakly bound, physisorbed hydrocarbons could in principle provide a similar water-repellency as obtained by chemisorption of strongly bound hydrophobic molecules at surfaces. Here we present experiments and computer simulations on the wetting behavior of water on molecularly thin, self-assembled alkane carpets of dotriacontane (n-C32H66 or C32) physisorbed on the hydrophilic native oxide layer of silicon surfaces during dip-coating from a binary alkane solution. By changing the dip-coating velocity we control the initial C32 surface coverage and achieve distinct film morphologies, encompassing homogeneous coatings with self-organised nanopatterns that range from dendritic nano-islands to stripes. These patterns exhibit a good water wettability even though the carpets are initially prepared with a high coverage of hydrophobic alkane molecules. Using in-liquid atomic force microscopy, along with molecular dynamics simulations, we trace this to a rearrangement of the alkane layers upon contact with water. This restructuring is correlated to the morphology of the C32 coatings, i.e. their fractal dimension. Water molecules displace to a large extent the first adsorbed alkane monolayer and thereby reduce the hydrophobic C32 surface coverage. Thus, our experiments evidence that water molecules can very effectively hydrophilize initially hydrophobic surfaces that consist of weakly bound hydrocarbon carpets. ",Kein DOI-Link verfügbar,2107.12129v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,On the Issue of Textured Crystallization of Ba(NO$_3$)$_2$ in Mesoporous   SiO$_2$: Raman Spectroscopy and Lattice Dynamics Analysis,2022,"  The lattice dynamics of preferentially aligned nanocrystals formed upon drying of aqueous Ba(NO$_3$)$_2$ solutions in a mesoporous silica glass traversed by tubular pores of approximately 12 nm are explored by Raman scattering. To interpret the experiments on the confined nanocrystals polarized Raman spectra of bulk single crystals and X-ray diffraction experiments are also performed. Since a cubic symmetry is inherent to Ba(NO$_3$)$_2$, a special Raman scattering geometry was utilized to separate the phonon modes of A$_g$ and E$_g$ species. Combining group-theory analysis and \textit{ab initio} lattice dynamics calculations a full interpretation of all Raman lines of the bulk single crystal is achieved. Apart from a small confinement-induced line broadening, the peak positions and normalized peak intensities of the Raman spectra of the nanoconfined and macroscopic crystals are identical. Interestingly, the Raman scattering experiment indicates the existence of comparatively large, $\sim$10-20 $\mu$m, single-crystalline regions of Ba(NO$_3$)$_2$ embedded in the porous host, near three orders of magnitude larger than the average size of single nanopores. This is contrast to the initial assumption of non-interconnected pores. It rather indicates an inter-pore propagation of the crystallization front, presumably via microporosity in the pore walls. ",https://doi.org/10.1016/j.saa.2022.121157,2205.01452v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Anomalous Front Broadening During Spontaneous Imbibition in a Matrix   with Elongated Pores,2012,"  During spontaneous imbibition a wetting liquid is drawn into a porous medium by capillary forces. In systems with comparable pore length and diameter, such as paper and sand, the front of the propagating liquid forms a continuous interface. Sections of this interface advance in a highly correlated manner due to an effective surface tension, which restricts front broadening. Here we investigate water imbibition in a nanoporous glass (Vycor) in which the pores are much longer than they are wide. In this case, no continuous liquid-vapor interface with coalesced menisci can form. Anomalously fast imbibition front roughening is experimentally observed by neutron imaging.We propose a theoretical pore network model, whose structural details are adapted to the microscopic pore structure of Vycor glass, and show that it displays the same large scale roughening characteristics as observed in the experiment. The model predicts that menisci movements are uncorrelated. This indicates that despite the connectivity of the network the smoothening effect of surface tension on the imbibition front roughening is negligible. These results suggest a new universality class of imbibition behavior which is expected to occur in any matrix with elongated, interconnected pores of random radii. ",https://doi.org/10.1073/pnas.1119352109,1207.1868v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Comparison of the calorimetric and kinematic methods of neutrino energy   reconstruction in disappearance experiments,2015,"  To be able to achieve their physics goals, future neutrino-oscillation experiments will need to reconstruct the neutrino energy with very high accuracy. In this work, we analyze how the energy reconstruction may be affected by realistic detection capabilities, such as energy resolutions, efficiencies, and thresholds. This allows us to estimate how well the detector performance needs to be determined a priori in order to avoid a sizable bias in the measurement of the relevant oscillation parameters. We compare the kinematic and calorimetric methods of energy reconstruction in the context of two muon-neutrino disappearance experiments operating in different energy regimes. For the calorimetric reconstruction method, we find that the detector performance has to be estimated with a ~10% accuracy to avoid a significant bias in the extracted oscillation parameters. On the other hand, in the case of kinematic energy reconstruction, we observe that the results exhibit less sensitivity to an overestimation of the detector capabilities. ",https://doi.org/10.1103/PhysRevD.92.073014,1507.08560v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Influence of Pore Surface Chemistry on the Rotational Dynamics of   Nanoconfined Water,2021,"  We have investigated the dynamics of water confined in mesostructured porous silicas (SBA-15, MCM-41) and four periodic mesoporous organosilicas (PMOs) by dielectric relaxation spectroscopy. The influence of water-surface interaction has been controlled by the carefully designed surface chemistry of PMOs that involved organic bridges connecting silica moieties with different repetition lengths, hydrophilicity and H-bonding capability. Relaxation processes attributed to the rotational motions of non-freezable water located in the vicinity of the pore surface were studied in the temperature range from 140 K to 225 K. Two distinct situations were achieved depending on the hydration level: at low relative humidity (33% RH), water formed a non-freezable layer adsorbed on the pore surface. At 75% RH, water formed an interfacial liquid layer sandwiched between the pore surface and the ice crystallized in the pore center. In the two cases, the study revealed different water dynamics and different dependence on the surface chemistry. We infer that these findings illustrate the respective importance of water-water and water-surface interactions in determining the dynamics of the interfacial liquid-like water and the adsorbed water molecules, as well as the nature of the different H-bonding sites present on the pore surface. ",https://doi.org/10.1021/acs.jpcc.1c05502,2107.12798v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Structure of Water at Hydrophilic and Hydrophobic Interfaces: Raman   Spectroscopy of Water Confined in Periodic Mesoporous (Organo)Silicas,2022,"  The temperature dependence of the structure of water confined in hydrophilic mesostructured porous silica (MCM-41) and hydrophobic benzene-bridged periodic mesoporous organosilicas (PMO) is studied by Raman vibrational spectroscopy. For capillary filled pores (75% relative humidity, RH), the OH-stretching region is dominated by the contribution from liquid water situated in the core part of the pore. It adopts a bulk-like structure that is modestly disrupted by confinement and surface hydrophobicity. For partially filled pores (33% RH), the structure of the non-freezable adsorbed film radically differs from that found in capillary filled pores. A first remarkable feature is the absence of the Raman spectral fingerprint of low density amorphous ice, even at low temperature (-120{\textdegree}C). Secondly, additional bands reveal water hydroxyls groups pointing towards the different water/solid and water/vapor interfaces. For MCM-41, they correspond to water molecules acting as weak H-bond donors with silica, and dangling hydroxyl groups oriented towards the empty center of the pore. For benzene-bridged PMO, we found an additional type of dangling hydroxyl groups, which we attribute to water at hydrophobic solid interface. ",Kein DOI-Link verfügbar,2201.13057v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Assessment of nanoparticle immersion depth at liquid interfaces from   chemically equivalent macroscopic surfaces,2022,"  Hypothesis: We test whether the wettability of nanoparticles (NPs) straddling at an air/water surface or oil/water interface can be extrapolated from sessile drop-derived macroscopic contact angles (mCAs) on planar substrates, assuming that both the nanoparticles and the macroscopic substrates are chemically equivalent and feature the same electrokinetic potential. Experiments: Pure silica (SiO2) and amino-terminated silica (APTES-SiO2) NPs are compared to macroscopic surfaces with extremely low roughness (root mean square [RMS] roughness <= 2 nm) or a roughness determined by a close-packed layer of NPs (RMS roughness about 35 nm). Equivalence of the surface chemistry is assessed by comparing the electrokinetic potentials of the NPs via electrophoretic light scattering and of the macroscopic substrates via streaming current analysis. The wettability of the macroscopic substrates is obtained from advancing (ACAs) and receding contact angles (RCAs) and in situ synchrotron X-ray reflectivity (XRR) provided by the NP wettability at the liquid interfaces. Findings: Generally, the RCA on smooth surfaces provides a good estimate of NP wetting properties. However, mCAs alone cannot predict adsorption barriers that prevent NP segregation to the interface, as is the case with the pure SiO2 nanoparticles. This strategy greatly facilitates assessing the wetting properties of NPs for applications such as emulsion formulation, flotation, or water remediation. ",https://doi.org/10.1016/j.jcis.2021.12.113,2202.03727v2,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural   Network in Analyzing Geospatial Resilience of Multicommodity Food Flows,2023,"  Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and future directions for improvement. The results show the advancements of this approach to quantifying the resilience of multicommodity food flow networks, contributing to efforts towards ensuring global food security using AI methods. The developed FLEE-GNN has the potential to be applied in other spatial networks with spatially heterogeneous sub-network distributions. ",https://doi.org/10.1145/3615886.3627742,2310.13248v1,Yes,"innovative(1), potent(1)"
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Passive low energy nuclear recoil detection with color centers --   PALEOCCENE,2022,"  The PALEOCCENE concept offers the potential for room-temperature, passive and robust detectors in the gram to kilogram range for the detection of low-energy nuclear recoil events. Nuclear recoil events can be caused by neutron scattering, coherent elastic neutrino nucleus scattering (CEvNS) or dark matter scattering and therefore, PALEOCCENE could find applications in all three areas. In this white paper we present current and planned R&D efforts to study the feasibility of this technique. ",Kein DOI-Link verfügbar,2203.05525v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Thermotropic Orientational Order of Discotic Liquid Crystals in   Nanochannels: An Optical Polarimetry Study and a Landau-de Gennes Analysis,2015,"  Optical polarimetry measurements of the orientational order of a discotic liquid crystal based on a pyrene derivative and confined in parallel-aligned nanochannels of monolithic mesoporous alumina, silica, and silicon as a function of temperature, channel radius (3 - 22 nm) and surface chemistry reveal a competition of radial and axial columnar order. The evolution of the orientational order parameter of the confined systems is continuous, in contrast to the discontinuous transition in the bulk. For channel radii larger than 10 nm we suggest several, alternative defect structures, which are compatible both with the optical experiments on the collective molecular orientation presented here and with a translational, radial columnar order reported in previous diffraction studies. For smaller channel radii our observations can semi-quantitatively be described by a Landau-de Gennes model with a nematic shell of radially ordered columns (affected by elastic splay deformations) that coexists with an orientationally disordered, isotropic core. For these structures, the cylindrical phase boundaries are predicted to move from the channel walls to the channel centres upon cooling, and vice-versa upon heating, in accord with the pronounced cooling/heating hystereses observed and the scaling behavior of the transition temperatures with channel diameter. The absence of experimental hints of a paranematic state is consistent with a biquadratic coupling of the splay deformations to the order parameter. ",https://doi.org/10.1039/c4sm00211c,1504.02078v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Self-Assembly of Liquid Crystals in Nanoporous Solids for Adaptive   Photonic Metamaterials,2019,"  Nanoporous media exhibit structures significantly smaller than the wavelengths of visible light and can thus act as photonic metamaterials. Their optical functionality is not determined by the properties of the base materials, but rather by tailored, multiscale structures, in terms of precise pore shape, geometry, and orientation. Embedding liquid crystals in pore space provides additional opportunities to control light-matter interactions at the single-pore, meta-atomic scale. Here, we present temperature-dependent 3D reciprocal space mapping using synchrotron-based X-ray diffraction in combination with high-resolution birefringence experiments on disk-like mesogens (HAT6) imbibed in self-ordered arrays of parallel cylindrical pores 17 to 160 nm across in monolithic anodic aluminium oxide (AAO). In agreement with Monte Carlo computer simulations we observe a remarkably rich self-assembly behaviour, unknown from the bulk state. It encompasses transitions between the isotropic liquid state and discotic stacking in linear columns as well as circular concentric ring formation perpendicular and parallel to the pore axis. These textural transitions underpin an optical birefringence functionality, tuneable in magnitude and in sign from positive to negative via pore size, pore surface-grafting and temperature. Our study demonstrates that the advent of large-scale, self-organised nanoporosity in monolithic solids along with confinement-controllable phase behaviour of liquid-crystalline matter at the single-pore scale provides a reliable and accessible tool to design materials with adjustable optical anisotropy, and thus offers versatile pathways to fine-tune polarisation-dependent light propagation speeds in materials. Such a tailorability is at the core of the emerging field of transformative optics, allowing, e.g., adjustable light absorbers and extremely thin metalenses. ",https://doi.org/10.1039/C9NR07143A,1911.10052v1,Yes,versatile(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Paraelectric KH$_2$PO$_4$ Nanocrystals in Monolithic Mesoporous Silica:   Structure and Lattice Dynamics,2021,"  Combining dielectric crystals with mesoporous solids allows a versatile design of functional nanomaterials, where the porous host provides a mechanical rigid scaffold structure and the molecular filling adds the functionalization. Here, we report a study of the complex lattice dynamics of a SiO$_2$:KH$_2$PO$_4$ nanocomposite consisting of a monolithic, mesoporous silica glass host with KH$_2$PO$_4$ nanocrystals embedded in its tubular channels $\sim$12 nm across. A micro-Raman investigation performed in the spectral range of 70-1600 cm$^{-1}$ reveals the complex lattice dynamics of the confined crystals. Their Raman spectrum resembles the one taken from bulk KH$_2$PO$_4$ crystals and thus, along with X-ray diffraction experiments, corroborates the successful solution-based synthesis of KH$_2$PO$_4$ nanocrystals with a structure analogous to the bulk material. We succeeded in observing not only the high-frequency internal modes ($\sim$900-1200 cm$^{-1}$), typical of internal vibrations of the PO$_4$ tetrahedra, but, more importantly, also the lowest frequency modes typical of bulk KH$_2$PO$_4$ crystals. The experimental Raman spectrum was interpreted with a group theory analysis and first-principle lattice dynamics calculations. The analysis of calculated eigen-vectors indicates the involvement of hydrogen atoms in most phonon modes corroborating the substantial significance of the hydrogen subsystem in the lattice dynamics of paraelectric bulk and of KH$_2$PO$_4$ crystals in extreme spatial confinement. A marginal redistribution of relative Raman intensities of the confined compared to unconfined crystals presumably originates in slightly changed crystal fields and interatomic interactions, in particular for the parts of the nanocrystals in close proximity to the silica pore surfaces. ",Kein DOI-Link verfügbar,2102.06268v1,Yes,versatile(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,How do ionic superdiscs self-assemble in nanopores?,2024,"  Discotic ionic liquid crystals (DILCs) consist of self-assembled superdiscs of cations and anions that spontaneously stack in linear columns with high one-dimensional ionic and electronic charge mobility, making them prominent model systems for functional soft matter. Unfortunately, a homogeneous alignment of DILCs on the macroscale is often not achievable, which significantly limits their applicability. Infiltration into nanoporous solid scaffolds can in principle overcome this drawback. However, due to the extreme experimental challenges to scrutinise liquid crystalline order in extreme spatial confinement, little is known about the structures of DILCs in nanopores. Here, we present temperature-dependent high-resolution optical birefringence measurement and 3D reciprocal space mapping based on synchrotron-based X-ray scattering to investigate the thermotropic phase behaviour of dopamine-based ionic liquid crystals confined in cylindrical channels of 180~nm diameter in macroscopic anodic aluminum oxide (AAO) membranes. As a function of the membranes' hydrophilicity and thus the molecular anchoring to the pore walls (edge-on or face-on) and the variation of the hydrophilic-hydrophobic balance between the aromatic cores and the alkyl side chain motifs of the superdiscs by tailored chemical synthesis, we find a particularly rich phase behaviour, which is not present in the bulk state. It is governed by a complex interplay of liquid crystalline elastic energies (bending and splay deformations), polar interactions and pure geometric confinement, and includes textural transitions between radial and axial alignment of the columns with respect to the long nanochannel axis. ",Kein DOI-Link verfügbar,2401.12663v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Wetting behavior at the free surface of a liquid gallium-bismuth alloy:   An X-ray reflectivity study close to the bulk monotectic point,2004,  We present x-ray reflectivity measurements from the free surface of a liquid gallium-bismuth alloy (Ga-Bi) in the temperature range close to the bulk monotectic temperature $T_{mono} = 222$C. Our measurements indicate a continuous formation of a thick wetting film at the free surface of the binary system driven by the first order transition in the bulk at the monotectic point. We show that the behavior observed is that of a complete wetting at a tetra point of solid-liquid-liquid-vapor coexistance. ,Kein DOI-Link verfügbar,cond-mat/0406659v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Pairing Interactions and Gibbs Adsorption at the Liquid Bi-In Surface: A   Resonant X-Ray Reflectivity Study,2004,"  Resonant x-ray reflectivity measurements from the surface of liquid Bi22In78 find only a modest surface Bi enhancement, with 35 atomic % Bi in the first atomic layer. This is in contrast to the Gibbs adsorption in all liquid alloys studied to date, which show surface segregation of a complete monolayer of the low surface tension component. This suggests that surface adsorption in Bi-In is dominated by attractive interactions that increase the number of Bi-In neighbors at the surface. These are the first measurements in which resonant x-ray scattering has been used to quantify compositional changes induced at a liquid alloy surface. ",https://doi.org/10.1103/PhysRevLett.86.1538,cond-mat/0412103v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Resonant X-Ray Scattering from the Surface of a Dilute Hg-Au Alloy,2004,"  We present the first resonant x-ray reflectivity measurements from a liquid surface. The surface structure of the liquid Hg-Au alloy system just beyond the solubility limit of 0.14at% Au in Hg had previously been shown to exhibit a unique surface phase characterized by a low-density surface region with a complicated temperature dependence. In this paper we present reflectivity measurements near the Au LIII edge, for 0.2at% Au in Hg at room temperature. The data are consistent with a concentration of Au in the surface region that can be no larger than about 30at%. These results rule out previous suggestions that pure Au layers segregate at the alloy surface. ",Kein DOI-Link verfügbar,cond-mat/0412113v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Acoustically Induced Giant Synthetic Hall Voltages in Graphene,2021,"  Any departure from graphene's flatness leads to the emergence of artificial gauge fields that act on the motion of the Dirac fermions through an associated pseudomagnetic field. Here, we demonstrate the tunability of strong gauge fields in non-local experiments using a large planar graphene sheet that conforms to the deformation of a piezoelectric layer by a surface acoustic wave. The acoustic wave induces a longitudinal and a giant synthetic Hall voltage in the absence of external magnetic fields. The superposition of a synthetic Hall potential and a conventional Hall voltage can annihilate the sample's transversal potential at large external magnetic fields. Surface acoustic waves thus provide a promising and facile avenue for the exploit of gauge fields in large planar graphene systems. ",https://doi.org/10.1103/PhysRevLett.128.256601,2112.11888v2,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Microscopic View on Short-Range Wetting at the Free Surface of the   Binary Metallic Liquid Gallium-Bismuth: An X-ray Reflectivity and Square   Gradient Theory Study,2004,"  We present an x-ray reflectivity study of wetting at the free surface of the binary liquid metal gallium-bismuth (Ga-Bi) in the region where the bulk phase separates into Bi-rich and Ga-rich liquid phases. The measurements reveal the evolution of the microscopic structure of wetting films of the Bi-rich, low-surface-tension phase along different paths in the bulk phase diagram. A balance between the surface potential preferring the Bi-rich phase and the gravitational potential which favors the Ga-rich phase at the surface pins the interface of the two demixed liquid metallic phases close to the free surface. This enables us to resolve it on an Angstrom level and to apply a mean-field, square gradient model extended by thermally activated capillary waves as dominant thermal fluctuations. The sole free parameter of the gradient model, i.e. the so-called influence parameter, $\kappa$, is determined from our measurements. Relying on a calculation of the liquid/liquid interfacial tension that makes it possible to distinguish between intrinsic and capillary wave contributions to the interfacial structure we estimate that fluctuations affect the observed short-range, complete wetting phenomena only marginally. A critical wetting transition that should be sensitive to thermal fluctuations seems to be absent in this binary metallic alloy. ",https://doi.org/10.1103/PhysRevB.68.085409,cond-mat/0406661v1,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Statistical Analysis of Submicron X-Ray Tomography Data on Polymer   Imbibition into Arrays of Cylindrical Nanopores,2021,"  Frozen transient imbibition states in arrays of straight cylindrical pores 400 nm in diameter were imaged by phase-contrast X-ray computed tomography with single-pore resolution. A semi-automatic algorithm yielding brightness profiles along all pores identified within the probed sample volume is described. Imbibition front positions are determined by descriptive statistics. A first approach involves the evaluation of frequency densities of single-pore imbibition lengths, a second one the evaluation of the statistical brightness dispersion within the probed volume as a function of the distance from the pore mouths. We plotted average imbibition front positions against systematically varied powers of the imbibition time and determined the optimal exponent of the imbibition time by considering the correlation coefficients of the corresponding linear fits. Thus, slight deviations from the proportionality of the average imbibition front position to the square root of the imbibition time predicted by the Lucas-Washburn theory were found. A meaningful preexponential factor in the power law relating imbibition front position and imbibition time may only be determined after ambiguities regarding the exponent of the imbibition time are resolved. The dispersion of peaks representing the imbibition front in frequency densities of single-pore imbibition lengths and in brightness dispersion profiles plotted against the pore depth is suggested as measure of the imbibition front width. Phase-contrast X-ray computed tomography allows the evaluation of a large number of infiltrated submicron pores taking advantage of phase-contrast imaging; artifacts related to sample damage by tomography requiring physical ablation of sample material are avoided. ",https://doi.org/10.1021/acs.jpcc.1c06798,2111.12192v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Large Language Models as Zero-shot Dialogue State Tracker through   Function Calling,2024,"  Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD ",Kein DOI-Link verfügbar,2402.10466v4,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Nu Tools: Exploring Practical Roles for Neutrinos in Nuclear Energy and   Security,2021,"  For decades, physicists have used neutrinos from nuclear reactors to advance basic science. These pursuits have inspired many ideas for application of neutrino detectors in nuclear energy and security. While developments in neutrino detectors are now making some of these ideas technically feasible, their value in the context of real needs and constraints has been unclear. This report seeks to help focus the picture of where neutrino technology may find practical roles in nuclear energy and security.   This report is the final product of the Nu Tools study, commissioned in 2019 by the DOE National Nuclear Security Administration (NNSA) Office of Defense Nuclear Nonproliferation Research and Development (DNN R&D). The study was conducted over two years by a group of neutrino physicists and nuclear engineers. A central theme of the study and this report is that useful application of neutrinos will depend not only on advancing physics and technology but also on understanding the needs and constraints of potential end-users.   The Study Approach emphasized broad end-user engagement. The major effort, undertaken from May to December 2020, was a series of engagements with the wider nuclear energy and security communities. Interviews with 41 experts revealed points of common understanding, which this report captures in three Cross-Cutting Findings, a Framework for Evaluating Utility, and seven Use Case Findings. The report concludes with two Recommendations. The findings and recommendations are summarized below. The respective ordering within each category does not represent a prioritization or implied value judgement. ",https://doi.org/10.2172/1826602,2112.12593v1,Yes,potent(1)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Dynamics of Water Confined in Mesopores with Variable Surface   Interaction,2020,"  We have investigated the dynamics of liquid water confined in mesostructured porous silica (MCM-41) and periodic mesoporous organosilicas (PMOs) by incoherent quasielastic neutron scattering experiments. The effect of tuning the water/surface interaction from hydrophilic to more hydrophobic on the water mobility, while keeping the pore size in the range 3.5-4.1 nm, was assessed from the comparative study of three PMOs comprising different organic bridging units and the purely siliceous MCM-41 case. An extended dynamical range was achieved by combining time-of-flight (IN5B) and backscattering (IN16B) quasielastic neutron spectrometers providing complementary energy resolutions. Liquid water was studied at regularly spaced temperatures ranging from 300 K to 243 K. In all systems, the molecular dynamics could be described consistently by the combination of two independent motions resulting from fast local motion around the average molecule position and the confined translational jump diffusion of its center of mass. All the molecules performed local relaxations, whereas the translational motion of a fraction of molecules was frozen on the experimental timescale. This study provides a comprehensive microscopic view on the dynamics of liquid water confined in mesopores, with distinct surface chemistries, in terms of non-mobile/mobile fraction, self-diffusion coefficient, residence time, confining radius, local relaxation time, and their temperature dependence. Importantly, it demonstrates that the strength of the water/surface interaction determines the long-time tail of the dynamics, which we attributed to the translational diffusion of interfacial molecules, while the water dynamics in the pore center is barely affected by the interface hydrophilicity. ",https://doi.org/10.1063/5.0040705,2012.14664v3,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Nuclear Data to Reduce Uncertainties in Reactor Antineutrino   Measurements: Summary Report of the Workshop on Nuclear Data for Reactor   Antineutrino Measurements (WoNDRAM),2022,"  The large quantities of antineutrinos produced through the decay of fission fragments in nuclear reactors provide an opportunity to study the properties of these particles and investigate their use in reactor monitoring. The reactor antineutrino spectra are measured using specialized, large area detectors that detect antineutrinos through inverse beta decay, electron elastic scattering, or coherent elastic neutrino nucleus scattering; although, inverse beta decay is the only demonstrated method so far. Reactor monitoring takes advantage of the differences in the antineutrino yield and spectra resulting from uranium and plutonium fission providing an opportunity to estimate the fissile material composition in the reactor. Recent experiments reveal a deviation between the measured and calculated antineutrino flux and spectra indicating either the existence of yet undiscovered neutrino physics, uncertainties in the reactor source term calculation, incorrect nuclear data, or a combination of all three.   To address the nuclear data that impact the antineutrino spectrum calculations and measurements, an international group of over 180 experts in antineutrino physics, reactor analysis, detector development, and nuclear data came together during the Workshop on Nuclear Data for Reactor Antineutrino Measurements (WoNDRAM) to discuss nuclear data needs and achieve concordance on a set of recommended priorities for nuclear data improvements. Three topical sessions provided a forum to gain consensus amongst the participants on the most important data improvements to address two goals: 1) understand the reactor anomaly and 2) improve the ability to monitor reactors using antineutrinos. This report summarizes the outcomes of the workshop discussions and the recommendations for nuclear data efforts that reduce reactor antineutrino measurement uncertainties. ",https://doi.org/10.2172/1842423,2202.08241v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Straight versus Spongy -- Effect of Tortuosity on Polymer Imbibition   into Nanoporous Matrices Assessed by Segmentation-Free Analysis of 3D Sample   Reconstructions,2024,"  We comparatively analyzed imbibition of polystyrene (PS) into two complementary pore models having pore diameters of about 380 nm and hydroxyl-terminated inorganic-oxidic pore walls, controlled porous glass (CPG) and self-ordered porous alumina (AAO), by X-ray computed tomography and EDX spectroscopy. CPG contains continuous spongy-tortuous pore systems. AAO containing arrays of isolated straight cylindrical pores is a reference pore model with a tortuosity close to 1. Comparative evaluation of the spatiotemporal imbibition front evolution yields important information on the pore morphology of a probed tortuous matrix like CPG and on the imbibition mechanism. To this end, pixel brightness dispersions in tomographic 3D reconstructions and 2D EDX maps of infiltrated AAO and CPG samples were condensed into 1D brightness dispersion profiles normal to the membrane surfaces. Their statistical analysis yielded positions and widths of the imbibition fronts without segmentation or determination of pore positions. The retardation of the imbibition front movement with respect to AAO reference samples may be used as a descriptor for the tortuosity of a tested porous matrix. The velocity of the imbibition front movements in CPG equaled two-thirds of the velocity of the imbibition front movements in AAO. Moreover, the dynamics of the imbibition front broadening discloses whether porous matrices are dominated by cylindrical neck-like pore segments or by nodes. Independent single-meniscus movements in cylindrical AAO pores result in faster imbibition front broadening than in CPG, in which a morphology dominated by nodes results in slower cooperative imbibition front movements involving several menisci. ",https://doi.org/10.1021/acs.jpcc.2c01991,2401.14950v1,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,A Staged Muon Accelerator Facility For Neutrino and Collider Physics,2015,"  Muon-based facilities offer unique potential to provide capabilities at both the Intensity Frontier with Neutrino Factories and the Energy Frontier with Muon Colliders. They rely on a novel technology with challenging parameters, for which the feasibility is currently being evaluated by the Muon Accelerator Program (MAP). A realistic scenario for a complementary series of staged facilities with increasing complexity and significant physics potential at each stage has been developed. It takes advantage of and leverages the capabilities already planned for Fermilab, especially the strategy for long-term improvement of the accelerator complex being initiated with the Proton Improvement Plan (PIP-II) and the Long Baseline Neutrino Facility (LBNF). Each stage is designed to provide an R&D platform to validate the technologies required for subsequent stages. The rationale and sequence of the staging process and the critical issues to be addressed at each stage, are presented. ",Kein DOI-Link verfügbar,1502.01647v1,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Supernova Physics at DUNE,2016,"  The DUNE/LBNF program aims to address key questions in neutrino physics and astroparticle physics. Realizing DUNE's potential to reconstruct low-energy particles in the 10-100 MeV energy range will bring significant benefits for all DUNE's science goals. In neutrino physics, low-energy sensitivity will improve neutrino energy reconstruction in the GeV range relevant for the kinematics of DUNE's long-baseline oscillation program. In astroparticle physics, low-energy capabilities will make DUNE's far detectors the world's best apparatus for studying the electron-neutrino flux from a supernova. This will open a new window to unrivaled studies of the dynamics and neutronization of a star's central core in real time, the potential discovery of the neutrino mass hierarchy, provide new sensitivity to physics beyond the Standard Model, and evidence of neutrino quantum-coherence effects. The same capabilities will also provide new sensitivity to `boosted dark matter' models that are not observable in traditional direct dark matter detectors. ",Kein DOI-Link verfügbar,1608.07853v1,Yes,potent(2)
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Neutrino-based tools for nuclear verification and diplomacy in North   Korea,2018,"  We present neutrino-based options for verifying that the nuclear reactors at North Korea's Yongbyon Nuclear Research Center are no longer operating or that they are operating in an agreed manner, precluding weapons production. Neutrino detectors may be a mutually agreeable complement to traditional verification protocols because they do not require access inside reactor buildings, could be installed collaboratively, and provide persistent and specific observations. At Yongbyon, neutrino detectors could passively verify reactor shutdowns or monitor power levels and plutonium contents, all from outside the reactor buildings. The monitoring options presented here build on recent successes in basic particle physics. Following a dedicated design study, these tools could be deployed in as little as one year at a reasonable cost. In North Korea, cooperative deployment of neutrino detectors could help redirect a limited number of scientists and engineers from military applications to peaceful technical work in an international community. Opportunities for scientific collaboration with South Korea are especially strong. We encourage policymakers to consider collaborative neutrino projects within a broader program of action toward stability and security on the Korean Peninsula. ",https://doi.org/10.1080/08929882.2019.1603007,1811.04737v2,No,
0000-0002-0669-5654,Patrick Huber,Kempten Universität der Angewandte Wissenschaften,Exploring the Quantum Universe: Pathways to Innovation and Discovery in   Particle Physics,2024,"  This is the report from the 2023 Particle Physics Project Prioritization Panel (P5) approved by High Energy Physics Advisory Panel (HEPAP) on December 8, 2023. The final version was made public on May 8, 2024 and submitted to DOE SC and NSF MPS. ",https://doi.org/10.2172/2368847,2407.19176v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Prototype Discovery using Quality-Diversity,2018,"  An iterative computer-aided ideation procedure is introduced, building on recent quality-diversity algorithms, which search for diverse as well as high-performing solutions. Dimensionality reduction is used to define a similarity space, in which solutions are clustered into classes. These classes are represented by prototypes, which are presented to the user for selection. In the next iteration, quality-diversity focuses on searching within the selected class. A quantitative analysis is performed on a 2D airfoil, and a more complex 3D side view mirror domain shows how computer-aided ideation can help to enhance engineers' intuition while allowing their design decisions to influence the design process. ",Kein DOI-Link verfügbar,1807.09488v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,An Analysis of Phenotypic Diversity in Multi-Solution Optimization,2021,"  More and more, optimization methods are used to find diverse solution sets. We compare solution diversity in multi-objective optimization, multimodal optimization, and quality diversity in a simple domain. We show that multiobjective optimization does not always produce much diversity, multimodal optimization produces higher fitness solutions, and quality diversity is not sensitive to genetic neutrality and creates the most diverse set of solutions. An autoencoder is used to discover phenotypic features automatically, producing an even more diverse solution set with quality diversity. Finally, we make recommendations about when to use which approach. ",https://doi.org/10.1007/978-3-030-63710-1_4,2105.04252v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Evolving Parsimonious Networks by Mixing Activation Functions,2017,"  Neuroevolution methods evolve the weights of a neural network, and in some cases the topology, but little work has been done to analyze the effect of evolving the activation functions of individual nodes on network size, which is important when training networks with a small number of samples. In this work we extend the neuroevolution algorithm NEAT to evolve the activation function of neurons in addition to the topology and weights of the network. The size and performance of networks produced using NEAT with uniform activation in all nodes, or homogenous networks, is compared to networks which contain a mixture of activation functions, or heterogenous networks. For a number of regression and classification benchmarks it is shown that, (1) qualitatively different activation functions lead to different results in homogeneous networks, (2) the heterogeneous version of NEAT is able to select well performing activation functions, (3) producing heterogeneous networks that are significantly smaller than homogeneous networks. ",Kein DOI-Link verfügbar,1703.07122v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Modeling User Selection in Quality Diversity,2019,"  The initial phase in real world engineering optimization and design is a process of discovery in which not all requirements can be made in advance, or are hard to formalize. Quality diversity algorithms, which produce a variety of high performing solutions, provide a unique chance to support engineers and designers in the search for what is possible and high performing. In this work we begin to answer the question how a user can interact with quality diversity and turn it into an interactive innovation aid. By modeling a user's selection it can be determined whether the optimization is drifting away from the user's preferences. The optimization is then constrained by adding a penalty to the objective function. We present an interactive quality diversity algorithm that can take into account the user's selection. The approach is evaluated in a new multimodal optimization benchmark that allows various optimization tasks to be performed. The user selection drift of the approach is compared to a state of the art alternative on both a planning and a neuroevolution control task, thereby showing its limits and possibilities. ",https://doi.org/10.1145/3321707.3321823,1907.06912v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Designing Air Flow with Surrogate-assisted Phenotypic Niching,2021,"  In complex, expensive optimization domains we often narrowly focus on finding high performing solutions, instead of expanding our understanding of the domain itself. But what if we could quickly understand the complex behaviors that can emerge in said domains instead? We introduce surrogate-assisted phenotypic niching, a quality diversity algorithm which allows to discover a large, diverse set of behaviors by using computationally expensive phenotypic features. In this work we discover the types of air flow in a 2D fluid dynamics optimization problem. A fast GPU-based fluid dynamics solver is used in conjunction with surrogate models to accurately predict fluid characteristics from the shapes that produce the air flow. We show that these features can be modeled in a data-driven way while sampling to improve performance, rather than explicitly sampling to improve feature models. Our method can reduce the need to run an infeasibly large set of simulations while still being able to design a large diversity of air flows and the shapes that cause them. Discovering diversity of behaviors helps engineers to better understand expensive domains and their solutions. ",https://doi.org/10.1007/978-3-030-58112-1_10,2105.04256v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Expressivity of Parameterized and Data-driven Representations in Quality   Diversity Search,2021,"  We consider multi-solution optimization and generative models for the generation of diverse artifacts and the discovery of novel solutions. In cases where the domain's factors of variation are unknown or too complex to encode manually, generative models can provide a learned latent space to approximate these factors. When used as a search space, however, the range and diversity of possible outputs are limited to the expressivity and generative capabilities of the learned model. We compare the output diversity of a quality diversity evolutionary search performed in two different search spaces: 1) a predefined parameterized space and 2) the latent space of a variational autoencoder model. We find that the search on an explicit parametric encoding creates more diverse artifact sets than searching the latent space. A learned model is better at interpolating between known data points than at extrapolating or expanding towards unseen examples. We recommend using a generative model's latent space primarily to measure similarity between artifacts rather than for search and generation. Whenever a parametric encoding is obtainable, it should be preferred over a learned representation as it produces a higher diversity of solutions. ",https://doi.org/10.1145/3449639.3459287,2105.04247v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Data-efficient Neuroevolution with Kernel-Based Surrogate Models,2018,"  Surrogate-assistance approaches have long been used in computationally expensive domains to improve the data-efficiency of optimization algorithms. Neuroevolution, however, has so far resisted the application of these techniques because it requires the surrogate model to make fitness predictions based on variable topologies, instead of a vector of parameters. Our main insight is that we can sidestep this problem by using kernel-based surrogate models, which require only the definition of a distance measure between individuals. Our second insight is that the well-established Neuroevolution of Augmenting Topologies (NEAT) algorithm provides a computationally efficient distance measure between dissimilar networks in the form of ""compatibility distance"", initially designed to maintain topological diversity. Combining these two ideas, we introduce a surrogate-assisted neuroevolution algorithm that combines NEAT and a surrogate model built using a compatibility distance kernel. We demonstrate the data-efficiency of this new algorithm on the low dimensional cart-pole swing-up problem, as well as the higher dimensional half-cheetah running task. In both tasks the surrogate-assisted variant achieves the same or better results with several times fewer function evaluations as the original NEAT. ",https://doi.org/10.1145/3205455.3205510,1804.05364v2,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Data-Efficient Design Exploration through Surrogate-Assisted   Illumination,2018,"  Design optimization techniques are often used at the beginning of the design process to explore the space of possible designs. In these domains illumination algorithms, such as MAP-Elites, are promising alternatives to classic optimization algorithms because they produce diverse, high-quality solutions in a single run, instead of only a single near-optimal solution. Unfortunately, these algorithms currently require a large number of function evaluations, limiting their applicability. In this article we introduce a new illumination algorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate modeling techniques to create a map of the design space according to user-defined features while minimizing the number of fitness evaluations. On a 2-dimensional airfoil optimization problem SAIL produces hundreds of diverse but high-performing designs with several orders of magnitude fewer evaluations than MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of producing maps of high-performing designs in realistic 3-dimensional aerodynamic tasks with an accurate flow simulation. Data-efficient design exploration with SAIL can help designers understand what is possible, beyond what is optimal, by considering more than pure objective-based optimization. ",Kein DOI-Link verfügbar,1806.05865v1,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,"Data-Efficient Exploration, Optimization, and Modeling of Diverse   Designs through Surrogate-Assisted Illumination",2017,"  The MAP-Elites algorithm produces a set of high-performing solutions that vary according to features defined by the user. This technique has the potential to be a powerful tool for design space exploration, but is limited by the need for numerous evaluations. The Surrogate-Assisted Illumination algorithm (SAIL), introduced here, integrates approximative models and intelligent sampling of the objective function to minimize the number of evaluations required by MAP-Elites.   The ability of SAIL to efficiently produce both accurate models and diverse high performing solutions is illustrated on a 2D airfoil design problem. The search space is divided into bins, each holding a design with a different combination of features. In each bin SAIL produces a better performing solution than MAP-Elites, and requires several orders of magnitude fewer evaluations. The CMA-ES algorithm was used to produce an optimal design in each bin: with the same number of evaluations required by CMA-ES to find a near-optimal solution in a single bin, SAIL finds solutions of similar quality in every bin. ",https://doi.org/10.1145/3071178.3071282,1702.03713v2,Yes,potent(1)
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Discovering Representations for Black-box Optimization,2020,"  The encoding of solutions in black-box optimization is a delicate, handcrafted balance between expressiveness and domain knowledge -- between exploring a wide variety of solutions, and ensuring that those solutions are useful. Our main insight is that this process can be automated by generating a dataset of high-performing solutions with a quality diversity algorithm (here, MAP-Elites), then learning a representation with a generative model (here, a Variational Autoencoder) from that dataset. Our second insight is that this representation can be used to scale quality diversity optimization to higher dimensions -- but only if we carefully mix solutions generated with the learned representation and those generated with traditional variation operators. We demonstrate these capabilities by learning an low-dimensional encoding for the inverse kinematics of a thousand joint planar arm. The results show that learned representations make it possible to solve high-dimensional problems with orders of magnitude fewer evaluations than the standard MAP-Elites, and that, once solved, the produced encoding can be used for rapid optimization of novel, but similar, tasks. The presented techniques not only scale up quality diversity algorithms to high dimensions, but show that black-box optimization encodings can be automatically learned, rather than hand designed. ",https://doi.org/10.1145/3377930.3390221,2003.04389v2,No,
0000-0003-1133-9424,Alexander Asteroth,Hochschule Bonn-Rhein-Sieg,Efficient Quality Diversity Optimization of 3D Buildings through 2D   Pre-optimization,2023,"  Quality diversity algorithms can be used to efficiently create a diverse set of solutions to inform engineers' intuition. But quality diversity is not efficient in very expensive problems, needing 100.000s of evaluations. Even with the assistance of surrogate models, quality diversity needs 100s or even 1000s of evaluations, which can make it use infeasible. In this study we try to tackle this problem by using a pre-optimization strategy on a lower-dimensional optimization problem and then map the solutions to a higher-dimensional case. For a use case to design buildings that minimize wind nuisance, we show that we can predict flow features around 3D buildings from 2D flow features around building footprints. For a diverse set of building designs, by sampling the space of 2D footprints with a quality diversity algorithm, a predictive model can be trained that is more accurate than when trained on a set of footprints that were selected with a space-filling algorithm like the Sobol sequence. Simulating only 16 buildings in 3D, a set of 1024 building designs with low predicted wind nuisance is created. We show that we can produce better machine learning models by producing training data with quality diversity instead of using common sampling techniques. The method can bootstrap generative design in a computationally expensive 3D domain and allow engineers to sweep the design space, understanding wind nuisance in early design phases. ",Kein DOI-Link verfügbar,2303.15896v1,No,
0000-0003-0418-4938,Iman Awaad,Hochschule Bonn-Rhein-Sieg,b-it-bots RoboCup@Work Team Description Paper 2023,2023,"  This paper presents the b-it-bots RoboCup@Work team and its current hardware and functional architecture for the KUKA youBot robot. We describe the underlying software framework and the developed capabilities required for operating in industrial environments including features such as reliable and precise navigation, flexible manipulation, robust object recognition and task planning. New developments include an approach to grasp vertical objects, placement of objects by considering the empty space on a workstation, and the process of porting our code to ROS2. ",Kein DOI-Link verfügbar,2312.17643v1,No,
0000-0002-2722-8254,Daniel Klein,Hochschule Bonn-Rhein-Sieg - Campus Rheinbach,Optimal Immunization Policy Using Dynamic Programming,2019,"  Decisions in public health are almost always made in the context of uncertainty. Policy makers are responsible for making important decisions, faced with the daunting task of choosing from amongst many possible options. This task is called planning under uncertainty, and is particularly acute when addressing complex systems, such as issues of global health and development. Uncertainty leads to cautious or incorrect decisions that cost time, money, and human life. It is with this understanding that we pursue greater clarity on, and methods to address optimal policy making in health. Decision making under uncertainty is a challenging task, and all too often this uncertainty is averaged away to simplify results for policy makers. Our goal in this work is to implement dynamic programming which provides basis for compiling planning results into reactive strategies. We present here a description of an AI-based method and illustrate how this method can improve our ability to find an optimal vaccination strategy. We model the problem as a partially observable Markov decision process, POMDP and show how a re-active policy can be computed using dynamic programming. In this paper, we developed a framework for optimal health policy design in an uncertain dynamic setting. We apply a stochastic dynamic programming approach to identify the optimal time to change the health intervention policy and the value of decision relevant information for improving the impact of the policy. ",Kein DOI-Link verfügbar,1910.08677v2,No,
0000-0002-2722-8254,Daniel Klein,Hochschule Bonn-Rhein-Sieg - Campus Rheinbach,FACTTRACK: Time-Aware World State Tracking in Story Outlines,2024,"  While accurately detecting and correcting factual contradictions in language model outputs has become increasingly important as their capabilities improve, doing so is highly challenging. We propose a novel method, FACTTRACK, for tracking atomic facts and addressing factual contradictions. Crucially, FACTTRACK also maintains time-aware validity intervals for each fact, allowing for change over time. At a high level, FACTTRACK consists of a four-step pipeline to update a world state data structure for each new event: (1) decompose the event into directional atomic facts; (2) determine the validity interval of each atomic fact using the world state; (3) detect contradictions with existing facts in the world state; and finally (4) add new facts to the world state and update existing atomic facts. When we apply FACTTRACK to contradiction detection on structured story outlines, we find that FACTTRACK using LLaMA2-7B-Chat substantially outperforms a fair baseline using LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline. Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4 baseline. ",Kein DOI-Link verfügbar,2407.16347v1,No,
0000-0002-2722-8254,Daniel Klein,Hochschule Bonn-Rhein-Sieg - Campus Rheinbach,Investigating Complex HPV Dynamics Using Emulation and History Matching,2024,"  The study of transmission and progression of human papillomavirus (HPV) is crucial for understanding the incidence of cervical cancers, and has been identified as a priority worldwide. The complexity of the disease necessitates a detailed model of HPV transmission and its progression to cancer; to infer properties of the above we require a careful process that can match to imperfect or incomplete observational data. In this paper, we describe the HPVsim simulator to satisfy the former requirement; to satisfy the latter we couple this stochastic simulator to a process of emulation and history matching using the R package hmer. With these tools, we are able to obtain a comprehensive collection of parameter combinations that could give rise to observed cancer data, and explore the implications of the variability of these parameter sets as it relates to future health interventions. ",Kein DOI-Link verfügbar,2408.15805v1,No,
0000-0002-9281-2027,Mario Christopher Bedrunka,Bonn-Rhein-Sieg Universität der Angewandte Wissenschaften Sankt Augustin DE,Lettuce: PyTorch-based Lattice Boltzmann Framework,2021,"  The lattice Boltzmann method (LBM) is an efficient simulation technique for computational fluid mechanics and beyond. It is based on a simple stream-and-collide algorithm on Cartesian grids, which is easily compatible with modern machine learning architectures. While it is becoming increasingly clear that deep learning can provide a decisive stimulus for classical simulation techniques, recent studies have not addressed possible connections between machine learning and LBM. Here, we introduce Lettuce, a PyTorch-based LBM code with a threefold aim. Lettuce enables GPU accelerated calculations with minimal source code, facilitates rapid prototyping of LBM models, and enables integrating LBM simulations with PyTorch's deep learning and automatic differentiation facility. As a proof of concept for combining machine learning with the LBM, a neural collision model is developed, trained on a doubly periodic shear layer and then transferred to a different flow, a decaying turbulence. We also exemplify the added benefit of PyTorch's automatic differentiation framework in flow control and optimization. To this end, the spectrum of a forced isotropic turbulence is maintained without further constraining the velocity field. The source code is freely available from https://github.com/lettucecfd/lettuce. ",https://doi.org/10.1007/978-3-030-90539-2_3,2106.12929v2,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Decoupling Lock-Free Data Structures from Memory Reclamation for Static   Analysis,2018,"  Verification of concurrent data structures is one of the most challenging tasks in software verification. The topic has received considerable attention over the course of the last decade. Nevertheless, human-driven techniques remain cumbersome and notoriously difficult while automated approaches suffer from limited applicability. The main obstacle for automation is the complexity of concurrent data structures. This is particularly true in the absence of garbage collection. The intricacy of lock-free memory management paired with the complexity of concurrent data structures makes automated verification prohibitive.   In this work we present a method for verifying concurrent data structures and their memory management separately. We suggest two simpler verification tasks that imply the correctness of the data structure. The first task establishes an over-approximation of the reclamation behavior of the memory management. The second task exploits this over-approximation to verify the data structure without the need to consider the implementation of the memory management itself. To make the resulting verification tasks tractable for automated techniques, we establish a second result. We show that a verification tool needs to consider only executions where a single memory location is reused. We implemented our approach and were able to verify linearizability of Michael&Scott's queue and the DGLM queue for both hazard pointers and epoch-based reclamation. To the best of our knowledge, we are the first to verify such implementations fully automatically. ",Kein DOI-Link verfügbar,1810.10807v2,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Pointer Life Cycle Types for Lock-Free Data Structures with Memory   Reclamation,2019,"  We consider the verification of lock-free data structures that manually manage their memory with the help of a safe memory reclamation (SMR) algorithm. Our first contribution is a type system that checks whether a program properly manages its memory. If the type check succeeds, it is safe to ignore the SMR algorithm and consider the program under garbage collection. Intuitively, our types track the protection of pointers as guaranteed by the SMR algorithm. There are two design decisions. The type system does not track any shape information, which makes it extremely lightweight. Instead, we rely on invariant annotations that postulate a protection by the SMR. To this end, we introduce angels, ghost variables with an angelic semantics. Moreover, the SMR algorithm is not hard-coded but a parameter of the type system definition. To achieve this, we rely on a recent specification language for SMR algorithms. Our second contribution is to automate the type inference and the invariant check. For the type inference, we show a quadratic-time algorithm. For the invariant check, we give a source-to-source translation that links our programs to off-the-shelf verification tools. It compiles away the angelic semantics. This allows us to infer appropriate annotations automatically in a guess-and-check manner. To demonstrate the effectiveness of our type-based verification approach, we check linearizability for various list and set implementations from the literature with both hazard pointers and epoch-based memory reclamation. For many of the examples, this is the first time they are verified automatically. For the ones where there is a competitor, we obtain a speed-up of up to two orders of magnitude. ",https://doi.org/10.1145/3371136,1910.11714v3,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Make flows small again: revisiting the flow framework,2023,"  We present a new flow framework for separation logic reasoning about programs that manipulate general graphs. The framework overcomes problems in earlier developments: it is based on standard fixed point theory, guarantees least flows, rules out vanishing flows, and has an easy to understand notion of footprint as needed for soundness of the frame rule. In addition, we present algorithms for automating the frame rule, which we evaluate on graph updates extracted from linearizability proofs for concurrent data structures. The evaluation demonstrates that our algorithms help to automate key aspects of these proofs that have previously relied on user guidance or heuristics. ",Kein DOI-Link verfügbar,2304.04886v1,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,A Concurrent Program Logic with a Future and History,2022,"  Verifying fine-grained optimistic concurrent programs remains an open problem. Modern program logics provide abstraction mechanisms and compositional reasoning principles to deal with the inherent complexity. However, their use is mostly confined to pencil-and-paper or mechanized proofs. We devise a new separation logic geared towards the lacking automation. While local reasoning is known to be crucial for automation, we are the first to show how to retain this locality for (i) reasoning about inductive properties without the need for ghost code, and (ii) reasoning about computation histories in hindsight. We implemented our new logic in a tool and used it to automatically verify challenging concurrent search structures that require inductive properties and hindsight reasoning, such as the Harris set. ",https://doi.org/10.1145/3563337,2207.02355v2,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Embedding Hindsight Reasoning in Separation Logic,2022,"  Proving linearizability of concurrent data structures remains a key challenge for verification. We present temporal interpolation as a new proof principle to conduct such proofs using hindsight arguments within concurrent separation logic. Temporal reasoning offers an easy-to-use alternative to prophecy variables and has the advantage of structuring proofs into easy-to-discharge hypotheses. To hindsight theory, our work brings the formal rigor and proof machinery of concurrent program logics. We substantiate the usefulness of our development by verifying the linearizability of the Logical Ordering (LO-)tree and RDCSS. Both of these involve complex proof arguments due to future-dependent linearization points. The LO-tree additionally features complex structure overlays. Our proof of the LO-tree is the first formal proof of this data structure. Interestingly, our formalization revealed an unknown bug and an existing informal proof as erroneous. ",https://doi.org/10.1145/3591296,2209.13692v3,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Effect Summaries for Thread-Modular Analysis,2017,"  We propose a novel guess-and-check principle to increase the efficiency of thread-modular verification of lock-free data structures. We build on a heuristic that guesses candidates for stateless effect summaries of programs by searching the code for instances of a copy-and-check programming idiom common in lock-free data structures. These candidate summaries are used to compute the interference among threads in linear time. Since a candidate summary need not be a sound effect summary, we show how to fully automatically check whether the precision of candidate summaries is sufficient. We can thus perform sound verification despite relying on an unsound heuristic. We have implemented our approach and found it up to two orders of magnitude faster than existing ones. ",Kein DOI-Link verfügbar,1705.03701v1,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Context-Aware Separation Logic,2023,"  Separation logic is often praised for its ability to closely mimic the locality of state updates when reasoning about them at the level of assertions. The prover only needs to concern themselves with the footprint of the computation at hand, i.e., the part of the state that is actually being accessed and manipulated. Modern concurrent separation logics lift this local reasoning principle from the physical state to abstract ghost state. For instance, these logics allow one to abstract the state of a fine-grained concurrent data structure by a predicate that provides a client the illusion of atomic access to the underlying state. However, these abstractions inadvertently increase the footprint of a computation: when reasoning about a local low-level state update, one needs to account for its effect on the abstraction, which encompasses a possibly unbounded portion of the low-level state. Often this gives the reasoning a global character.   We present context-aware (concurrent) separation logic (Co(Co)SL) to provide new opportunities for local reasoning in the presence of rich ghost state abstractions. Co(Co)SL introduces the notion of a context of a computation, the part of the concrete state that is only affected on the abstract level. Contexts give rise to a new proof rule that allows one to reduce the footprint by the context, provided the computation preserves the context as an invariant. The context rule complements the frame rule of separation logic by enabling more local reasoning in cases where the predicate to be framed is known in advance. We instantiate our developed theory for the flow framework, enabling contextual reasoning about programs manipulating general heap graphs, and describe two other applications of the logic. We have implemented the flow instantiation of the logic in a proof outline checker and used it to verify two highly-concurrent binary search trees. ",Kein DOI-Link verfügbar,2307.15549v2,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Pointer Race Freedom,2015,"  We propose a novel notion of pointer race for concurrent programs manipulating a shared heap. A pointer race is an access to a memory address which was freed, and it is out of the accessor's control whether or not the cell has been re-allocated. We establish two results. (1) Under the assumption of pointer race freedom, it is sound to verify a program running under explicit memory management as if it was running with garbage collection. (2) Even the requirement of pointer race freedom itself can be verified under the garbage-collected semantics. We then prove analogues of the theorems for a stronger notion of pointer race needed to cope with performance-critical code purposely using racy comparisons and even racy dereferences of pointers. As a practical contribution, we apply our results to optimize a thread-modular analysis under explicit memory management. Our experiments confirm a speed-up of up to two orders of magnitude. ",Kein DOI-Link verfügbar,1511.00184v3,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Realizability in Semantics-Guided Synthesis Done Eagerly,2024,"  We present realizability and realization logic, two program logics that jointly address the problem of finding solutions in semantics-guided synthesis. What is new is that we proceed eagerly and not only analyze a single candidate program but a whole set. Realizability logic computes information about the set of candidate programs in a forward fashion. Realization logic uses this information as guidance to identify a suitable candidate in a backward fashion. Realizability logic is able to analyze a set of programs due to a new form of assertions that tracks synthesis alternatives. Realizability logic then picks alternatives to arrive at a program, and we give the guarantee that this process will not need backtracking. We show how to implement the program logics using verification conditions, and report on experiments with a prototype in the context of safe memory reclamation for lock-free data structures. ",Kein DOI-Link verfügbar,2403.05607v1,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Robustness investigation of quality measures for the assessment of   machine learning models,2024,"  In this paper the accuracy and robustness of quality measures for the assessment of machine learning models are investigated. The prediction quality of a machine learning model is evaluated model-independent based on a cross-validation approach, where the approximation error is estimated for unknown data. The presented measures quantify the amount of explained variation in the model prediction. The reliability of these measures is assessed by means of several numerical examples, where an additional data set for the verification of the estimated prediction error is available. Furthermore, the confidence bounds of the presented quality measures are estimated and local quality measures are derived from the prediction residuals obtained by the cross-validation approach. ",Kein DOI-Link verfügbar,2408.04391v1,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Arithmetizing Shape Analysis,2024,"  Memory safety is an essential correctness property of software systems. For programs operating on linked heap-allocated data structures, the problem of proving memory safety boils down to analyzing the possible shapes of data structures, leading to the field of shape analysis. This paper presents a novel reduction-based approach to memory safety analysis that relies on two forms of abstraction: flow abstraction, representing global properties of the heap graph through local flow equations; and view abstraction, which enable verification tools to reason symbolically about an unbounded number of heap objects. In combination, the two abstractions make it possible to reduce memory-safety proofs to proofs about heap-less imperative programs that can be discharged using off-the-shelf software verification tools without built-in support for heap reasoning. Using an empirical evaluation on a broad range of programs, the paper shows that the reduction approach can effectively verify memory safety for sequential and concurrent programs operating on different kinds of linked data structures, including singly-linked, doubly-linked, and nested lists as well as trees. ",Kein DOI-Link verfügbar,2408.09037v1,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Model-based Fault Classification for Automotive Software,2022,"  Intensive testing using model-based approaches is the standard way of demonstrating the correctness of automotive software. Unfortunately, state-of-the-art techniques leave a crucial and labor intensive task to the test engineer: identifying bugs in failing tests. Our contribution is a model-based classification algorithm for failing tests that assists the engineer when identifying bugs. It consists of three steps. (i) Fault localization replays the test on the model to identify the moment when the two diverge. (ii) Fault explanation then computes the reason for the divergence. The reason is a subset of actions from the test that is sufficient for divergence. (iii) Fault classification groups together tests that fail for similar reasons. Our approach relies on machinery from formal methods: (i) symbolic execution, (ii) Hoare logic and a new relationship between the intermediary assertions constructed for a test, and (iii) a new relationship among Hoare proofs. A crucial aspect in automotive software is timing requirements, for which we develop appropriate Hoare logic theory. We also briefly report on our prototype implementation for the CAN bus Unified Diagnostic Services in an industrial project. ",https://doi.org/10.1007/978-3-031-21037-2_6,2208.14290v2,No,
0000-0002-9049-1311,Thomas Kolb,Hochschule RheinMain Fachbereich Wiesbaden Business School,Active binary mixtures of fast and slow hard spheres,2019,"  We computationally studied the phase behavior and dynamics of binary mixtures of active particles, where each 'species' had distinct activities leading to distinct velocities, fast and slow. We obtained phase diagrams demonstrating motility-induced phase separation (MIPS) upon varying the activity and concentration of each species, and extended current kinetic theory of active/passive mixtures to active/active mixtures. We discovered two regimes of behavior quantified through the participation of each species in the dense phase compared to their monodisperse counterparts. In regime I (active/passive and active/weakly-active), we found that the dense phase was segregated by particle type into domains of fast and slow particles. Moreover, fast particles were suppressed from entering the dense phase while slow particles were enhanced entering the dense phase, compared to monodisperse systems of all-fast or all-slow particles. These effects decayed asymptotically as the activity of the slow species increased, approaching the activity of the fast species until they were negligible (regime II). In regime II, the dense phase was homogeneously mixed and each species participated in the dense phase as if it were it a monodisperse system; each species behaved as if it weren't mixed at all. Finally, we collapsed our data defining two quantities that measure the total activity of the system. We thus found expressions that predict, a priori, the percentage of each particle type that participates in the dense phase through MIPS. ",Kein DOI-Link verfügbar,1909.02919v1,No,
0000-0002-9049-1311,Thomas Kolb,Hochschule RheinMain Fachbereich Wiesbaden Business School,Dependence of phase behavior and surface tension on particle stiffness   for active Brownian particles,2021,"  We study quasi two-dimensional, monodisperse systems of active Brownian particles (ABPs) for a range of activities, stiffnesses, and densities. We develop a microscopic, analytical method for predicting the dense phase structure formed after motility-induced phase separation (MIPS) has occurred, including the dense cluster's area fraction, interparticle pressure, and radius. Our predictions are in good agreement with our Brownian dynamics simulations. We, then, derive a continuum model to investigate the relationship between the predicted interparticle pressure, the swim pressure, and the macroscopic pressure in the momentum equation. We find that formulating the point-wise macroscopic pressure as the interparticle pressure and modeling the particle activity through a spatially variant body force -- as opposed to a volume-averaged swim pressure -- results in consistent predictions of pressure in both the continuum model and the microscopic theory. This formulation of pressure also results in nearly zero surface tension for the phase separated domains, irrespective of activity, stiffness, and area fraction. Furthermore, using Brownian dynamics simulations and our continuum model, we showed that both the interface width and surface tension, are intrinsic characteristics of the system. On the other hand, if we were to exclude the body force induced by activity, we find that the resulting surface tension values are linearly dependent on the size of the simulation, in contrast to the statistical mechanical definition of surface tension. ",Kein DOI-Link verfügbar,2106.02008v1,No,
0000-0002-9049-1311,Thomas Kolb,Hochschule RheinMain Fachbereich Wiesbaden Business School,Increased dose rate precision in combined $α$ and $β$ counting   in the $μ$Dose system - a probabilistic approach to data analysis,2018,"  The $\mu$Dose system was developed to allow the measurement of environmental levels of natural radioactive isotopes. The system records $\alpha$ and $\beta$ particles along with four decay pairs arising from subsequent decays of $^{214}$Bi/$^{214}$Po, $^{220}$Rn/$^{216}$Po, $^{212}$Bi/$^{212}$Po and $^{219}$Rn/$^{215}$Po. Under the assumption of secular equilibrium this allows to assess the specific radioactivities of $^{238}$U, $^{235}$U, $^{232}$Th decay chains and $^{40}$K. This assessment provides results with uncertainties which are correlated and, thus, require the development of an error estimation methodology which considers this issue. Here we present two different approaches for uncertainty propagation based on Monte Carlo and Bayesian methods. Both approaches produce statistically indistinguishable results and allow significantly better dose rate precision than when the correlations are not accounted for. In the given example, the dose rate precision is improved by a factor of two. ",Kein DOI-Link verfügbar,1811.03664v1,No,
0000-0002-9049-1311,Thomas Kolb,Hochschule RheinMain Fachbereich Wiesbaden Business School,"Growth, characterization, and magnetic properties of a Li(Mn,Ni)PO4   single crystal",2013,"  Ni-doped LiMn0.95Ni0.05PO4 single crystals have been grown for the first time by the travelling-solvent floating-zone method at low Argon pressure. The grown sample exhibits large single crystalline grains as revealed by means of polarization microscopy and X-ray Laue back scattering. The composition of the crystal was determined by Energy-dispersive X-ray spectroscopy. LiMn0.95Ni0.05PO4 orders in an orthorhombic olivine-like structure as expected and phase purity was confirmed by powder X-ray diffraction. An oriented cuboid with size of 2.4 x 2.5 x 2.7 mm3 along a, b, and c crystalline directions, respectively, was used for anisotropic magnetic measurements. ",https://doi.org/10.1016/j.jcrysgro.2013.08.032,1304.4174v1,No,
0009-0009-9629-8430,Bodo Igler,RheinMain Universität der Angewandte Wissenschaften,RDF2Vec-based Classification of Ontology Alignment Changes,2018,"  When ontologies cover overlapping topics, the overlap can be represented using ontology alignments. These alignments need to be continuously adapted to changing ontologies. Especially for large ontologies this is a costly task often consisting of manual work. Finding changes that do not lead to an adaption of the alignment can potentially make this process significantly easier. This work presents an approach to finding these changes based on RDF embeddings and common classification techniques. To examine the feasibility of this approach, an evaluation on a real-world dataset is presented. In this evaluation, the best classifiers reached a precision of 0.8. ",Kein DOI-Link verfügbar,1805.09145v1,Yes,potent(1)
0000-0003-0396-0341,Marco Wrzalik,RheinMain Universität der Angewandte Wissenschaften,CoRT: Complementary Rankings from Transformers,2020,"  Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies. ",Kein DOI-Link verfügbar,2010.10252v2,Yes,potent(1)
0000-0003-4313-9259,Veronika Weiß,RheinMain Universität der Angewandte Wissenschaften,Narrative Visualization to Communicate Neurological Diseases,2022,"  While narrative visualization has been used successfully in various applications to communicate scientific data in the format of a story to a general audience, the same has not been true for medical data. There are only a few exceptions that present tabular medical data to non-experts. However, a key component of medical visualization is the interactive analysis of 3D data, such as 3D models of anatomical structures, which were rarely included in narrative visualizations so far. In this design study, we investigate how neurological disease data can be communicated through narrative visualization techniques to a general audience in an understandable way. We designed a narrative visualization explaining cerebral small vessel disease. Learning about its avoidable risk factors serves to motivate the audience watching the resulting visual data story. Using this example, we discuss the adaption of basic narrative components. This includes the conflict and characters of a story, as well as the story's structure and content to address and communicate specific characteristics of medical data. Furthermore, we explore the extent to which complex medical relationships need to be simplified to be understandable to a general audience without distorting the underlying data and evidence. In particular, the data needs to be preprocessed for non-experts and appropriate forms of interaction must be found. We explore approaches to make the data more personally relatable, such as including a fictional patient. We evaluated our approach in a user study with 40 participants in a web-based implementation of the designed story. We found that the combination of a carefully thought-out storyline with a clear key message, appealing visualizations combined with easy-to-use interactions, and credible references are crucial for creating a narrative visualization about a neurological disease that engages an audience. ",Kein DOI-Link verfügbar,2212.10121v1,No,
0000-0002-7539-0432,Felix Hamann,RheinMain Universität der Angewandte Wissenschaften,Hamming Sentence Embeddings for Information Retrieval,2019,"  In retrieval applications, binary hashes are known to offer significant improvements in terms of both memory and speed. We investigate the compression of sentence embeddings using a neural encoder-decoder architecture, which is trained by minimizing reconstruction error. Instead of employing the original real-valued embeddings, we use latent representations in Hamming space produced by the encoder for similarity calculations.   In quantitative experiments on several benchmarks for semantic similarity tasks, we show that our compressed hamming embeddings yield a comparable performance to uncompressed embeddings (Sent2Vec, InferSent, Glove-BoW), at compression ratios of up to 256:1. We further demonstrate that our model strongly decorrelates input features, and that the compressor generalizes well when pre-trained on Wikipedia sentences. We publish the source code on Github and all experimental results. ",Kein DOI-Link verfügbar,1908.05541v1,No,
0000-0002-7539-0432,Felix Hamann,RheinMain Universität der Angewandte Wissenschaften,Neural Entity Linking on Technical Service Tickets,2020,"  Entity linking, the task of mapping textual mentions to known entities, has recently been tackled using contextualized neural networks. We address the question whether these results -- reported for large, high-quality datasets such as Wikipedia -- transfer to practical business use cases, where labels are scarce, text is low-quality, and terminology is highly domain-specific. Using an entity linking model based on BERT, a popular transformer network in natural language processing, we show that a neural approach outperforms and complements hand-coded heuristics, with improvements of about 20% top-1 accuracy. Also, the benefits of transfer learning on a large corpus are demonstrated, while fine-tuning proves difficult. Finally, we compare different BERT-based architectures and show that a simple sentence-wise encoding (Bi-Encoder) offers a fast yet efficient search in practice. ",Kein DOI-Link verfügbar,2005.07604v2,No,
0000-0002-7539-0432,Felix Hamann,RheinMain Universität der Angewandte Wissenschaften,IRT2: Inductive Linking and Ranking in Knowledge Graphs of Varying Scale,2023,"  We address the challenge of building domain-specific knowledge models for industrial use cases, where labelled data and taxonomic information is initially scarce. Our focus is on inductive link prediction models as a basis for practical tools that support knowledge engineers with exploring text collections and discovering and linking new (so-called open-world) entities to the knowledge graph. We argue that - though neural approaches to text mining have yielded impressive results in the past years - current benchmarks do not reflect the typical challenges encountered in the industrial wild properly. Therefore, our first contribution is an open benchmark coined IRT2 (inductive reasoning with text) that (1) covers knowledge graphs of varying sizes (including very small ones), (2) comes with incidental, low-quality text mentions, and (3) includes not only triple completion but also ranking, which is relevant for supporting experts with discovery tasks.   We investigate two neural models for inductive link prediction, one based on end-to-end learning and one that learns from the knowledge graph and text data in separate steps. These models compete with a strong bag-of-words baseline. The results show a significant advance in performance for the neural approaches as soon as the available graph data decreases for linking. For ranking, the results are promising, and the neural approaches outperform the sparse retriever by a wide margin. ",Kein DOI-Link verfügbar,2301.00716v1,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,Face Morphing Attacks and Face Image Quality: The Effect of Morphing and   the Unsupervised Attack Detection by Quality,2022,"  Morphing attacks are a form of presentation attacks that gathered increasing attention in recent years. A morphed image can be successfully verified to multiple identities. This operation, therefore, poses serious security issues related to the ability of a travel or identity document to be verified to belong to multiple persons. Previous works touched on the issue of the quality of morphing attack images, however, with the main goal of quantitatively proofing the realistic appearance of the produced morphing attacks. We theorize that the morphing processes might have an effect on both, the perceptual image quality and the image utility in face recognition (FR) when compared to bona fide samples. Towards investigating this theory, this work provides an extensive analysis of the effect of morphing on face image quality, including both general image quality measures and face image utility measures. This analysis is not limited to a single morphing technique, but rather looks at six different morphing techniques and five different data sources using ten different quality measures. This analysis reveals consistent separability between the quality scores of morphing attack and bona fide samples measured by certain quality measures. Our study goes further to build on this effect and investigate the possibility of performing unsupervised morphing attack detection (MAD) based on quality scores. Our study looks intointra and inter-dataset detectability to evaluate the generalizability of such a detection concept on different morphing techniques and bona fide sources. Our final results point out that a set of quality measures, such as MagFace and CNNNIQA, can be used to perform unsupervised and generalized MAD with a correct classification accuracy of over 70%. ",Kein DOI-Link verfügbar,2208.05864v2,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,Towards Explaining Demographic Bias through the Eyes of Face Recognition   Models,2022,"  Biases inherent in both data and algorithms make the fairness of widespread machine learning (ML)-based decision-making systems less than optimal. To improve the trustfulness of such ML decision systems, it is crucial to be aware of the inherent biases in these solutions and to make them more transparent to the public and developers. In this work, we aim at providing a set of explainability tool that analyse the difference in the face recognition models' behaviors when processing different demographic groups. We do that by leveraging higher-order statistical information based on activation maps to build explainability tools that link the FR models' behavior differences to certain facial regions. The experimental results on two datasets and two face recognition models pointed out certain areas of the face where the FR models react differently for certain demographic groups compared to reference groups. The outcome of these analyses interestingly aligns well with the results of studies that analyzed the anthropometric differences and the human judgment differences on the faces of different demographic groups. This is thus the first study that specifically tries to explain the biased behavior of FR models on different demographic groups and link it directly to the spatial facial features. The code is publicly available here. ",Kein DOI-Link verfügbar,2208.13400v1,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,Explainability of the Implications of Supervised and Unsupervised Face   Image Quality Estimations Through Activation Map Variation Analyses in Face   Recognition Models,2021,"  It is challenging to derive explainability for unsupervised or statistical-based face image quality assessment (FIQA) methods. In this work, we propose a novel set of explainability tools to derive reasoning for different FIQA decisions and their face recognition (FR) performance implications. We avoid limiting the deployment of our tools to certain FIQA methods by basing our analyses on the behavior of FR models when processing samples with different FIQA decisions. This leads to explainability tools that can be applied for any FIQA method with any CNN-based FR solution using activation mapping to exhibit the network's activation derived from the face embedding. To avoid the low discrimination between the general spatial activation mapping of low and high-quality images in FR models, we build our explainability tools in a higher derivative space by analyzing the variation of the FR activation maps of image sets with different quality decisions. We demonstrate our tools and analyze the findings on four FIQA methods, by presenting inter and intra-FIQA method analyses. Our proposed tools and the analyses based on them point out, among other conclusions, that high-quality images typically cause consistent low activation on the areas outside of the central face region, while low-quality images, despite general low activation, have high variations of activation in such areas. Our explainability tools also extend to analyzing single images where we show that low-quality images tend to have an FR model spatial activation that strongly differs from what is expected from a high-quality image where this difference also tends to appear more in areas outside of the central face region and does correspond to issues like extreme poses and facial occlusions. The implementation of the proposed tools is accessible here [link]. ",Kein DOI-Link verfügbar,2112.04827v1,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,The Effect of Wearing a Face Mask on Face Image Quality,2021,"  Due to the COVID-19 situation, face masks have become a main part of our daily life. Wearing mouth-and-nose protection has been made a mandate in many public places, to prevent the spread of the COVID-19 virus. However, face masks affect the performance of face recognition, since a large area of the face is covered. The effect of wearing a face mask on the different components of the face recognition system in a collaborative environment is a problem that is still to be fully studied. This work studies, for the first time, the effect of wearing a face mask on face image quality by utilising state-of-the-art face image quality assessment methods of different natures. This aims at providing better understanding on the effect of face masks on the operation of face recognition as a whole system. In addition, we further studied the effect of simulated masks on face image utility in comparison to real face masks. We discuss the correlation between the mask effect on face image quality and that on the face verification performance by automatic systems and human experts, indicating a consistent trend between both factors. The evaluation is conducted on the database containing (1) no-masked faces, (2) real face masks, and (3) simulated face masks, by synthetically generating digital facial masks on no-masked faces. Finally, a visual interpretation of the face areas contributing to the quality score of a selected set of quality assessment methods is provided to give a deeper insight into the difference of network decisions in masked and non-masked faces, among other variations. ",Kein DOI-Link verfügbar,2110.11283v4,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,Generalization of Fitness Exercise Recognition from Doppler Measurements   by Domain-adaption and Few-Shot Learning,2023,"  In previous works, a mobile application was developed using an unmodified commercial off-the-shelf smartphone to recognize whole-body exercises. The working principle was based on the ultrasound Doppler sensing with the device built-in hardware. Applying such a lab-environment trained model on realistic application variations causes a significant drop in performance, and thus decimate its applicability. The reason of the reduced performance can be manifold. It could be induced by the user, environment, and device variations in realistic scenarios. Such scenarios are often more complex and diverse, which can be challenging to anticipate in the initial training data. To study and overcome this issue, this paper presents a database with controlled and uncontrolled subsets of fitness exercises. We propose two concepts to utilize small adaption data to successfully improve model generalization in an uncontrolled environment, increasing the recognition accuracy by two to six folds compared to the baseline for different users. ",Kein DOI-Link verfügbar,2311.11910v1,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,A Deep Insight into Measuring Face Image Utility with General and   Face-specific Image Quality Metrics,2021,"  Quality scores provide a measure to evaluate the utility of biometric samples for biometric recognition. Biometric recognition systems require high-quality samples to achieve optimal performance. This paper focuses on face images and the measurement of face image utility with general and face-specific image quality metrics. While face-specific metrics rely on features of aligned face images, general image quality metrics can be used on the global image and relate to human perceptions. In this paper, we analyze the gap between the general image quality metrics and the face image quality metrics. Our contribution lies in a thorough examination of how different the image quality assessment algorithms relate to the utility for the face recognition task. The results of image quality assessment algorithms are further compared with those of dedicated face image quality assessment algorithms. In total, 25 different quality metrics are evaluated on three face image databases, BioSecure, LFW, and VGGFace2 using three open-source face recognition solutions, SphereFace, ArcFace, and FaceNet. Our results reveal a clear correlation between learned image metrics to face image utility even without being specifically trained as a face utility measure. Individual handcrafted features lack general stability and perform significantly worse than general face-specific quality metrics. We additionally provide a visual insight into the image areas contributing to the quality score of a selected set of quality assessment methods. ",Kein DOI-Link verfügbar,2110.11111v2,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,A Survey on Drowsiness Detection -- Modern Applications and Methods,2024,"  Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection plays a critical role in preventing accidents, enhancing safety, and ultimately saving lives across various sectors and scenarios. This comprehensive review explores the significance of drowsiness detection in various areas of application, transcending the conventional focus solely on driver drowsiness detection. We delve into the current methodologies, challenges, and technological advancements in drowsiness detection schemes, considering diverse contexts such as public transportation, healthcare, workplace safety, and beyond. By examining the multifaceted implications of drowsiness, this work contributes to a holistic understanding of its impact and the crucial role of accurate and real-time detection techniques in enhancing safety and performance. We identified weaknesses in current algorithms and limitations in existing research such as accurate and real-time detection, stable data transmission, and building bias-free systems. Our survey frames existing works and leads to practical recommendations like mitigating the bias issue by using synthetic data, overcoming the hardware limitations with model compression, and leveraging fusion to boost model performance. This is a pioneering work to survey the topic of drowsiness detection in such an entirely and not only focusing on one single aspect. We consider the topic of drowsiness detection as a dynamic and evolving field, presenting numerous opportunities for further exploration. ",Kein DOI-Link verfügbar,2408.12990v1,No,
0000-0003-4848-1256,Biying Fu,RheinMain Universität der Angewandte Wissenschaften,CR-FIQA: Face Image Quality Assessment by Learning Sample Relative   Classifiability,2021,"  The quality of face images significantly influences the performance of underlying face recognition algorithms. Face image quality assessment (FIQA) estimates the utility of the captured image in achieving reliable and accurate recognition performance. In this work, we propose a novel learning paradigm that learns internal network observations during the training process. Based on that, our proposed CR-FIQA uses this paradigm to estimate the face image quality of a sample by predicting its relative classifiability. This classifiability is measured based on the allocation of the training sample feature representation in angular space with respect to its class center and the nearest negative class center. We experimentally illustrate the correlation between the face image quality and the sample relative classifiability. As such property is only observable for the training dataset, we propose to learn this property from the training dataset and utilize it to predict the quality measure on unseen samples. This training is performed simultaneously while optimizing the class centers by an angular margin penalty-based softmax loss used for face recognition model training. Through extensive evaluation experiments on eight benchmarks and four face recognition models, we demonstrate the superiority of our proposed CR-FIQA over state-of-the-art (SOTA) FIQA algorithms. ",Kein DOI-Link verfügbar,2112.06592v2,No,
0000-0003-2519-6794,Christian Herta,HTW Berlin,Radial Prediction Layer,2019,"  For a broad variety of critical applications, it is essential to know how confident a classification prediction is. In this paper, we discuss the drawbacks of softmax to calculate class probabilities and to handle uncertainty in Bayesian neural networks. We introduce a new kind of prediction layer called radial prediction layer (RPL) to overcome these issues. In contrast to the softmax classification, RPL is based on the open-world assumption. Therefore, the class prediction probabilities are much more meaningful to assess the uncertainty concerning the novelty of the input. We show that neural networks with RPLs can be learned in the same way as neural networks using softmax. On a 2D toy data set (spiral data), we demonstrate the fundamental principles and advantages. On the real-world ImageNet data set, we show that the open-world properties are beneficially fulfilled. Additionally, we show that RPLs are less sensible to adversarial attacks on the MNIST data set. Due to its features, we expect RPL to be beneficial in a broad variety of applications, especially in critical environments, such as medicine or autonomous driving. ",Kein DOI-Link verfügbar,1905.11150v2,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Deep Aggregation of Regional Convolutional Activations for Content Based   Image Retrieval,2019,"  One of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for the Oxford Buildings and Paris data sets while reducing the training time significantly. ",Kein DOI-Link verfügbar,1909.09420v2,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Deep Metric Learning using Similarities from Nonlinear Rank   Approximations,2019,"  In recent years, deep metric learning has achieved promising results in learning high dimensional semantic feature embeddings where the spatial relationships of the feature vectors match the visual similarities of the images. Similarity search for images is performed by determining the vectors with the smallest distances to a query vector. However, high retrieval quality does not depend on the actual distances of the feature vectors, but rather on the ranking order of the feature vectors from similar images. In this paper, we introduce a metric learning algorithm that focuses on identifying and modifying those feature vectors that most strongly affect the retrieval quality. We compute normalized approximated ranks and convert them to similarities by applying a nonlinear transfer function. These similarities are used in a newly proposed loss function that better contracts similar and disperses dissimilar samples. Experiments demonstrate significant improvement over existing deep feature embedding methods on the CUB-200-2011, Cars196, and Stanford Online Products data sets for all embedding sizes. ",Kein DOI-Link verfügbar,1909.09427v2,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs,2019,"  Nowadays stock photo agencies often have millions of images. Non-stop viewing of 20 million images at a speed of 10 images per second would take more than three weeks. This demonstrates the impossibility to inspect all images and the difficulty to get an overview of the entire collection. Although there has been a lot of effort to improve visual image search, there is little research and support for visual image exploration. Typically, users start ""exploring"" an image collection with a keyword search or an example image for a similarity search. Both searches lead to long unstructured lists of result images. In earlier publications, we introduced the idea of graph-based image navigation and proposed an efficient algorithm for building hierarchical image similarity graphs for dynamically changing image collections. In this demo we showcase real-time visual exploration of millions of images with a standard web browser. Subsets of images are successively retrieved from the graph and displayed as a visually sorted 2D image map, which can be zoomed and dragged to explore related concepts. Maintaining the positions of previously shown images creates the impression of an ""endless map"". This approach allows an easy visual image-based navigation, while preserving the complex image relationships of the graph. ",Kein DOI-Link verfügbar,1910.06005v1,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval,2021,"  Even though it has extensively been shown that retrieval specific training of deep neural networks is beneficial for nearest neighbor image search quality, most of these models are trained and tested in the domain of landmarks images. However, some applications use images from various other domains and therefore need a network with good generalization properties - a general-purpose CBIR model. To the best of our knowledge, no testing protocol has so far been introduced to benchmark models with respect to general image retrieval quality. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with a broad range of image categories. This benchmark is subsequently used to evaluate various pretrained models of different architectures on their generalization qualities. We show that large-scale pretraining significantly improves retrieval performance and present experiments on how to further increase these properties by appropriate fine-tuning. With these promising results, we hope to increase interest in the research topic of general-purpose CBIR. ",Kein DOI-Link verfügbar,2111.13122v1,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,"PicArrange -- Visually Sort, Search, and Explore Private Images on a Mac   Computer",2021,  The native macOS application PicArrange integrates state-of-the-art image sorting and similarity search to enable users to get a better overview of their images. Many file and image management features have been added to make it a tool that addresses a full image management workflow. A modification of the Self Sorting Map algorithm enables a list-like image arrangement without loosing the visual sorting. Efficient calculation and storage of visual features as well as the use of many macOS APIs result in an application that is fluid to use. ,Kein DOI-Link verfügbar,2111.13363v1,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Improved Evaluation and Generation of Grid Layouts using Distance   Preservation Quality and Linear Assignment Sorting,2022,"  Images sorted by similarity enables more images to be viewed simultaneously, and can be very useful for stock photo agencies or e-commerce applications. Visually sorted grid layouts attempt to arrange images so that their proximity on the grid corresponds as closely as possible to their similarity. Various metrics exist for evaluating such arrangements, but there is low experimental evidence on correlation between human perceived quality and metric value. We propose Distance Preservation Quality (DPQ) as a new metric to evaluate the quality of an arrangement. Extensive user testing revealed stronger correlation of DPQ with user-perceived quality and performance in image retrieval tasks compared to other metrics. In addition, we introduce Fast Linear Assignment Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts. FLAS achieves very good sorting qualities while improving run time and computational resources. ",Kein DOI-Link verfügbar,2205.04255v2,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Fast Approximate Nearest Neighbor Search with a Dynamic Exploration   Graph using Continuous Refinement,2023,"  For approximate nearest neighbor search, graph-based algorithms have shown to offer the best trade-off between accuracy and search time. We propose the Dynamic Exploration Graph (DEG) which significantly outperforms existing algorithms in terms of search and exploration efficiency by combining two new ideas: First, a single undirected even regular graph is incrementally built by partially replacing existing edges to integrate new vertices and to update old neighborhoods at the same time. Secondly, an edge optimization algorithm is used to continuously improve the quality of the graph. Combining this ongoing refinement with the graph construction process leads to a well-organized graph structure at all times, resulting in: (1) increased search efficiency, (2) predictable index size, (3) guaranteed connectivity and therefore reachability of all vertices, and (4) a dynamic graph structure. In addition we investigate how well existing graph-based search systems can handle indexed queries where the seed vertex of a search is the query itself. Such exploration tasks, despite their good starting point, are not necessarily easy. High efficiency in approximate nearest neighbor search (ANNS) does not automatically imply good performance in exploratory search. Extensive experiments show that our new Dynamic Exploration Graph outperforms existing algorithms significantly for indexed and unindexed queries. ",Kein DOI-Link verfügbar,2307.10479v2,No,
0000-0003-0461-2058,Carsten Thomas,HTW Berlin - Universität der Angewandte Wissenschaften,Hexagonal High-Entropy Alloys,2014,  We report on the discovery of a high-entropy alloy with a hexagonal crystal structure. Equiatomic samples in the alloy system Ho-Dy-Y-Gd-Tb were found to solidify as homogeneous single-phase high-entropy alloys. The results of our electron diffraction investigations and high-resolution scanning transmission electron microscopy are consistent with a Mg-type hexagonal structure. The possibility of hexagonal high-entropy alloys in other alloy systems is discussed. ,https://doi.org/10.1080/21663831.2014.951493,1408.0100v3,No,
0000-0003-0461-2058,Carsten Thomas,HTW Berlin - Universität der Angewandte Wissenschaften,Influence of chemical composition on the room temperature plas-ticity of   C15 Ca-Al-Mg Laves phases,2024,"  The influence of chemical composition changes on the room temperature mechanical proper-ties in the C15 CaAl2 Laves phase were investigated in two off-stoichiometric compositions with 5.7 at.-% Mg addition (Ca33Al61Mg6) and 10.8 at.-% Mg and 3.0 at.-% Ca addition (Ca36Al53Mg11) and compared to the stoichiometric (Ca33Al67) composition. Cubic Ca-Al-Mg Laves phases with multiple crystallographic orientations were characterised and deformed using nanoindentation. The hardness and indentation modulus were measured to be 4.1 +- 0.3 GPa and 71.3 +- 1.5 GPa for Ca36Al53Mg11, 4.6 +- 0.2 GPa and 80.4 +- 3.8 GPa for Ca33Al61Mg6 and 4.9 +- 0.3 GPa and 85.5 +- 4.0 GPa for Ca33Al67, respectively. The resulting surface traces as well as slip and crack planes, were distinguished on the indentation surfac-es, revealing the activation of several different {11n} slip systems, as further confirmed by conventional transmission electron microscopic observations. Additionally, the deformation mechanisms and corresponding energy barriers of activated slip systems were evaluated by atomistic simulations. ",Kein DOI-Link verfügbar,2403.13432v1,No,
0000-0001-6809-6895,Hermann Hessling,HTW Berlin - Universität der Angewandte Wissenschaften,On particle--like jets,1995,"  Under which conditions does a jet appear as a particle--like signal from the hidden realm of quarks and gluons? Motivated by this question jet clustering conditions are formulated, in order to characterize jet clustering algorithms, which can be used for a determination of particle--like jets. Jets are understood as particle--like, if they behave like free particles. The simplest solution to the jet clustering conditions leads to a new jet algorithm: a Lorentz invariant generalization of the JADE algorithm. It is found that this generalization amplifies hadronization effects in certain phase space regions in such a way, that hadronization models might become testable in jet physics at the electron--proton collider HERA. Moreover, a method is suggested, which can be used at HERA, in order to determine a region in the phase space, where hadronization effects from the proton remnant are small and where parton jets are particle--like. ",Kein DOI-Link verfügbar,hep-ph/9504297v2,No,
0000-0001-6809-6895,Hermann Hessling,HTW Berlin - Universität der Angewandte Wissenschaften,On the Local Equilibrium Condition,1994,"  A physical system is in local equilibrium if it cannot be distinguished from a global equilibrium by ``infinitesimally localized measurements''. This should be a natural characterization of local equilibrium, but the problem is to give a precise meaning to the qualitative phrase ``infinitesimally localized measurements''. A solution is suggested in form of a Local Equilibrium Condition, which can be applied to linear relativistic quantum field theories but not directly to selfinteracting quantum fields. The concept of local temperature resulting from LEC is compared to an old approach to local temperature based on the principle of maximal entropy. It is shown that the principle of maximal entropy does not always lead to physical states if it is applied to relativistic quantum field theories. ",Kein DOI-Link verfügbar,hep-th/9411094v1,No,
0000-0001-6809-6895,Hermann Hessling,HTW Berlin - Universität der Angewandte Wissenschaften,On the Local Equilibrium Principle,2001,"  A physical system should be in a local equilibrium if it cannot be distinguished from a global equilibrium by ``infinitesimally localized measurements''. This seems to be a natural characterization of local equilibrium, however the problem is to give a precise meaning to the qualitative phrase ``infinitesimally localized measurements''.   A solution is suggested in form of a {\em Local Equilibrium Condition} (LEC) which can be applied to non-interacting quanta.   The Unruh temperature of massless quanta is derived by applying LEC to an arbitrary point inside the Rindler Wedge.   Massless quanta outside a hot sphere are analyzed. A stationary spherically symmetric local equilibrium does only exist according to LEC if the temperature is globally constant.   Using LEC a non-trivial stationary local equilibrium is found for rotating massless quanta between two concentric cylinders of different temperatures. This shows that quanta may behave like a fluid with a B\'enard instability. ",Kein DOI-Link verfügbar,hep-th/0106039v2,No,
0000-0002-0343-0730,Florian Koch,HTW Berlin - Universität der Angewandte Wissenschaften,Twist-Deformed Lorentzian Heisenberg-Algebras,2006,  The Moyal-Weyl quantization procedure is embedded into the twist formalism of vector fields on phase space. Double application of twists provide most general deformations of Minkowskian Heisenberg-algebras and corresponding quantizations of the Lorentz-algebra. Such deformations deliver high-energy extensions of standard relativistic quantum mechanics. These are required to obtain minimal uncertainty properties for high-energy spacetime measurements that standard quantum mechanics lacks. The procedure of double twist application is outlined. We give an instructive and genuine example. ,Kein DOI-Link verfügbar,hep-th/0608064v1,No,
0000-0002-0343-0730,Florian Koch,HTW Berlin - Universität der Angewandte Wissenschaften,Vector Field Twisting of Lie-Algebras,2006,"  In quantum groups coproducts of Lie-algebras are twisted in terms of generators of the corresponding universal enveloping algebra. If representations are considered, twists also serve as starproducts that accordingly quantize representation spaces. In physics, requirements turn out to be the other way around. Physics comes up with noncommutative spaces in terms of starproducts that miss a suiting quantum symmetry. In general the classical limit is known, i.e. there exists a representation of the Lie-algebra on a corresponding finitely generated commutative space. In this setup quantization can be considered independently from any representation theoretic issue. We construct an algebra of vector fields from a left cross-product algebra of the representation space and its Hopf-algebra of momenta. The latter can always be defined. The suitingly devided cross-product algebra is then lifted to a Hopf-algebra that carries the required genuine structure to accomodate a matrix representation of the universal enveloping algebra as a subalgebra. We twist the Hopf-algebra of vector fields and thereby obtain the desired twisting of the Lie-algebra. Since we twist with vector fields and not with generators of the Lie-algebra, this is the most general twisting that can possibly be obtained. In other words, we push starproducts to twists of the desired symmetry algebra and to this purpose solve the problem of turning vector fields into a Hopf-algebra. We give some genuine example. ",Kein DOI-Link verfügbar,hep-th/0607063v2,No,
0000-0002-0343-0730,Florian Koch,HTW Berlin - Universität der Angewandte Wissenschaften,Construction of $θ$-Poincaré Algebras and their Invariants on   $\mathcal{M}_θ$,2004,"  In the present paper we construct deformations of the Poincar\'e algebra as representations on a noncommutative spacetime with canonical commutation relations. These deformations are obtained by solving a set of conditions by an appropriate ansatz for the deformed Lorentz generator. They turn out to be Hopf algebras of quantum universal enveloping algebra type with nontrivial antipodes. In order to present a notion of $\theta$-deformed Minkowski space $\mathcal{M}_\theta$, we introduce Casimir operators and spacetime invariants for all deformations obtained. ",https://doi.org/10.1016/j.nuclphysb.2005.04.019,hep-th/0409012v1,No,
0000-0002-0343-0730,Florian Koch,HTW Berlin - Universität der Angewandte Wissenschaften,Quantum Non-Hermitian Topological Sensors,2021,"  We investigate in the framework of quantum noise theory how the striking boundary-sensitivity recently discovered in the context of non-Hermitian (NH) topological phases may be harnessed to devise novel quantum sensors. Specifically, we study a quantum-optical setting of coupled modes arranged in an array with broken ring geometry that would realize a NH topological phase in the classical limit. Using methods from quantum-information theory of Gaussian states, we show that a small coupling induced between the ends of the broken ring may be detected with a precision that increases exponentially in the number of coupled modes, e.g. by heterodyne detection of two output modes. While this robust effect only relies on reaching a NH topological regime, we identify a resonance phenomenon without direct classical counterpart that provides an experimental knob for drastically enhancing the aforementioned exponential growth. Our findings pave the way towards designing quantum NH topological sensors (QUANTOS) that may observe with high precision any physical observable that couples to the boundary conditions of the device. ",https://doi.org/10.1103/PhysRevResearch.4.013113,2106.05297v2,No,
0000-0002-0343-0730,Florian Koch,HTW Berlin - Universität der Angewandte Wissenschaften,Dissipative frequency converter: from Lindblad dynamics to non-Hermitian   topology,2024,"  A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency. Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter. To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis. The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description. For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit. This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case. Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state. Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics. ",Kein DOI-Link verfügbar,2403.07991v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Coherent Design of Wind Turbine Controllers Considering Transitions   between Operating Regions using Fuzzy Membership Functions,2024,"  This paper presents a coherent design of wind turbine controllers with explicit consideration of transitions between operating regions by fuzzy membership functions. In improving the design process of wind turbines, the transitions between partial-load operation by torque control and full-load operation by pitch control need to be systematically considered. From the first view, fuzzy methods for blending separately designed control laws are an obvious choice. However, valid design rules must be developed to ensure stability and performance during the transition. A model-based control design procedure in the Takagi-Sugeno fuzzy framework using the sector nonlinearity method is proposed to achieve the above control design objectives. In addition to a detailed mathematical analysis of the design, the method's applicability is verified by simulation studies using a high-fidelity reference wind turbine model. ",Kein DOI-Link verfügbar,2405.06817v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Load Mitigation and Power Tracking Control for Multi-Rotor Turbines,2022,"  A model-based feasible control strategy for multi-rotor systems is presented, pursuing two control objectives simultaneously: Mechanical loads on the main tower are to be mitigated, and an externally determined power change is to be followed to obtain fast power reference response in power systems. For this purpose, a scalable control strategy consisting of two levels is proposed: The first level consists of the decentralized control of each rotor unit. By using an LPV formalism, it is shown how the nonlinearities of the controlled system are considered in the design using a decentralized wind speed observer of each rotor to improve the overall closed-loop performance. To mitigate the lateral loads on the multi-rotor main tower caused by asymmetric rotor thrust forces, a higher-level controller is introduced. Finally, the applicability of the controller structure is demonstrated by simulation studies. ",Kein DOI-Link verfügbar,2212.09135v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Model Reference Control for Wind Turbine Systems in Full Load Region   based on Takagi-Sugeno Fuzzy Systems,2024,"  This paper presents a novel Model Reference Control (MRC) approach for wind turbine (WT) systems in the full load region employing a fuzzy Parallel Distribution Compensation Controller (PDC-C) derived using a Takagi-Sugeno (TS) fuzzy System approach. Through first-order Taylor series expansion, local linear submodels are generated and combined via triangular membership functions to develop a TS descriptor model. From here, the MRC PDC-C is synthesized by a constrained LMI optimization procedure, including damping characteristics of the elastic drive train, to track the desired rotor speed and generator torque based on the reference model dynamics. The controller is tested on the nonlinear WT model in simulation studies under various wind conditions, such as turbulent wind, wind gusts, and a Fault Ride Through (FRT) scenario where the generator torque is set to 0 p.u. for 150 ms. ",Kein DOI-Link verfügbar,2405.06829v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Wind Turbine Model and Observer in Takagi-Sugeno Model Structure,2014,"  Based on a reduced-order, dynamic nonlinear wind turbine model in Takagi-Sugeno (TS) model structure, a TS state observer is designed as a disturbance observer to estimate the unknown effective wind speed. The TS observer model is an exact representation of the underlying nonlinear model, obtained by means of the sector-nonlinearity approach. The observer gain matrices are obtained by means of a linear matrix inequality (LMI) design approach for optimal fuzzy control, where weighting matrices for the individual system states and outputs are included. The observer is tested in simulations with the aero-elastic code FAST for the NREL 5 MW reference turbine, where it shows a stable behaviour both for IEC wind gusts and turbulent wind input. ",https://doi.org/10.1088/1742-6596/555/1/012042,1401.8199v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Experimental Validation of a Dynamic Virtual Power Plant Concept Based   on Multiple-Converter Power Hardware-In-the-Loop Test Bench,2023,"  Recently, the concept of dynamic virtual power plants (DVPP) has been proposed to collectively provide desired dynamic ancillary services such as fast frequency and voltage control by a heterogeneous ensemble of distributed energy resources (DER). This paper presents an experimental validation of a recent DVPP control design approach on a multi-converter power hardware-in-the-loop (PHIL) test bed system. More specifically, we consider a DVPP composed of a wind generation system, a photovoltaic (PV) system, and a STATCOM with small storage capacity to collectively provide grid-following fast frequency regulation in the presence of grid-frequency and load variations. The performance of the aggregated DVPP response is evaluated with respect to its ability to match a desired dynamic behavior while taking practical limitations of the individual DVPP units into account. ",Kein DOI-Link verfügbar,2309.00882v1,No,
0000-0001-5851-3616,Horst Schulte,HTW Berlin - Universität der Angewandte Wissenschaften,Modelling Approaches of Power Systems Considering Grid-Connected   Converters and Renewable Generation Dynamics,2021,"  This paper presents a comparative analysis of several modelling approaches of key elements used in simulations of power systems with renewable energy sources. Different models of synchronous generators, transmission lines, converters, wind generators and photovoltaic (PV) power plants are compared to assess the most suitable models for grid-connection studies. It also analyses how the dynamics of PV power plants and the mechanical dynamics of wind generators affect the electrical variables on the grid side. The models were compared in terms of precision and computational time through simulations of load connection, short-circuits, disconnection of generators and lines in a benchmark system modelled in Simulink. ",Kein DOI-Link verfügbar,2112.00867v1,No,
0009-0007-1222-0747,Jan Schroeder,HTW Berlin - Universität der Angewandte Wissenschaften,Predicting and Evaluating Software Model Growth in the Automotive   Industry,2017,"  The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical.   Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations.   Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best. ",Kein DOI-Link verfügbar,1708.02884v1,No,
0000-0003-0589-7550,Felix Richter,HTW Berlin - Universität der Angewandte Wissenschaften,Poynting's theorem and energy conservation in the propagation of light   in bounded media,2007,"  Starting from the Maxwell-Lorentz equations, Poynting's theorem is reconsidered. The energy flux vector is introduced as S_e=(E x B)/mu_0 instead of E x H, because only by this choice the energy dissipation can be related to the balance of the kinetic energy of the matter subsystem. Conservation of the total energy as the sum of kinetic and electromagnetic energy follows. In our discussion, media and their microscopic nature are represented exactly by their susceptibility functions, which do not necessarily have to be known. On this footing, it can be shown that energy conservation in the propagation of light through bounded media is ensured by Maxwell's boundary conditions alone, even for some frequently used approximations. This is demonstrated for approaches using additional boundary conditions and the dielectric approximation in detail, the latter of which suspected to violate energy conservation for decades. ",https://doi.org/10.1209/0295-5075/81/67005,0710.0515v3,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,"PicArrange -- Visually Sort, Search, and Explore Private Images on a Mac   Computer",2021,  The native macOS application PicArrange integrates state-of-the-art image sorting and similarity search to enable users to get a better overview of their images. Many file and image management features have been added to make it a tool that addresses a full image management workflow. A modification of the Self Sorting Map algorithm enables a list-like image arrangement without loosing the visual sorting. Efficient calculation and storage of visual features as well as the use of many macOS APIs result in an application that is fluid to use. ,Kein DOI-Link verfügbar,2111.13363v1,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,Fast Approximate Nearest Neighbor Search with a Dynamic Exploration   Graph using Continuous Refinement,2023,"  For approximate nearest neighbor search, graph-based algorithms have shown to offer the best trade-off between accuracy and search time. We propose the Dynamic Exploration Graph (DEG) which significantly outperforms existing algorithms in terms of search and exploration efficiency by combining two new ideas: First, a single undirected even regular graph is incrementally built by partially replacing existing edges to integrate new vertices and to update old neighborhoods at the same time. Secondly, an edge optimization algorithm is used to continuously improve the quality of the graph. Combining this ongoing refinement with the graph construction process leads to a well-organized graph structure at all times, resulting in: (1) increased search efficiency, (2) predictable index size, (3) guaranteed connectivity and therefore reachability of all vertices, and (4) a dynamic graph structure. In addition we investigate how well existing graph-based search systems can handle indexed queries where the seed vertex of a search is the query itself. Such exploration tasks, despite their good starting point, are not necessarily easy. High efficiency in approximate nearest neighbor search (ANNS) does not automatically imply good performance in exploratory search. Extensive experiments show that our new Dynamic Exploration Graph outperforms existing algorithms significantly for indexed and unindexed queries. ",Kein DOI-Link verfügbar,2307.10479v2,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,Deep Aggregation of Regional Convolutional Activations for Content Based   Image Retrieval,2019,"  One of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for the Oxford Buildings and Paris data sets while reducing the training time significantly. ",Kein DOI-Link verfügbar,1909.09420v2,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,Deep Metric Learning using Similarities from Nonlinear Rank   Approximations,2019,"  In recent years, deep metric learning has achieved promising results in learning high dimensional semantic feature embeddings where the spatial relationships of the feature vectors match the visual similarities of the images. Similarity search for images is performed by determining the vectors with the smallest distances to a query vector. However, high retrieval quality does not depend on the actual distances of the feature vectors, but rather on the ranking order of the feature vectors from similar images. In this paper, we introduce a metric learning algorithm that focuses on identifying and modifying those feature vectors that most strongly affect the retrieval quality. We compute normalized approximated ranks and convert them to similarities by applying a nonlinear transfer function. These similarities are used in a newly proposed loss function that better contracts similar and disperses dissimilar samples. Experiments demonstrate significant improvement over existing deep feature embedding methods on the CUB-200-2011, Cars196, and Stanford Online Products data sets for all embedding sizes. ",Kein DOI-Link verfügbar,1909.09427v2,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs,2019,"  Nowadays stock photo agencies often have millions of images. Non-stop viewing of 20 million images at a speed of 10 images per second would take more than three weeks. This demonstrates the impossibility to inspect all images and the difficulty to get an overview of the entire collection. Although there has been a lot of effort to improve visual image search, there is little research and support for visual image exploration. Typically, users start ""exploring"" an image collection with a keyword search or an example image for a similarity search. Both searches lead to long unstructured lists of result images. In earlier publications, we introduced the idea of graph-based image navigation and proposed an efficient algorithm for building hierarchical image similarity graphs for dynamically changing image collections. In this demo we showcase real-time visual exploration of millions of images with a standard web browser. Subsets of images are successively retrieved from the graph and displayed as a visually sorted 2D image map, which can be zoomed and dragged to explore related concepts. Maintaining the positions of previously shown images creates the impression of an ""endless map"". This approach allows an easy visual image-based navigation, while preserving the complex image relationships of the graph. ",Kein DOI-Link verfügbar,1910.06005v1,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval,2021,"  Even though it has extensively been shown that retrieval specific training of deep neural networks is beneficial for nearest neighbor image search quality, most of these models are trained and tested in the domain of landmarks images. However, some applications use images from various other domains and therefore need a network with good generalization properties - a general-purpose CBIR model. To the best of our knowledge, no testing protocol has so far been introduced to benchmark models with respect to general image retrieval quality. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with a broad range of image categories. This benchmark is subsequently used to evaluate various pretrained models of different architectures on their generalization qualities. We show that large-scale pretraining significantly improves retrieval performance and present experiments on how to further increase these properties by appropriate fine-tuning. With these promising results, we hope to increase interest in the research topic of general-purpose CBIR. ",Kein DOI-Link verfügbar,2111.13122v1,No,
0000-0002-3957-4672,Nico Hezel,HTW Berlin - Universität der Angewandte Wissenschaften,Improved Evaluation and Generation of Grid Layouts using Distance   Preservation Quality and Linear Assignment Sorting,2022,"  Images sorted by similarity enables more images to be viewed simultaneously, and can be very useful for stock photo agencies or e-commerce applications. Visually sorted grid layouts attempt to arrange images so that their proximity on the grid corresponds as closely as possible to their similarity. Various metrics exist for evaluating such arrangements, but there is low experimental evidence on correlation between human perceived quality and metric value. We propose Distance Preservation Quality (DPQ) as a new metric to evaluate the quality of an arrangement. Extensive user testing revealed stronger correlation of DPQ with user-perceived quality and performance in image retrieval tasks compared to other metrics. In addition, we introduce Fast Linear Assignment Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts. FLAS achieves very good sorting qualities while improving run time and computational resources. ",Kein DOI-Link verfügbar,2205.04255v2,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,"PicArrange -- Visually Sort, Search, and Explore Private Images on a Mac   Computer",2021,  The native macOS application PicArrange integrates state-of-the-art image sorting and similarity search to enable users to get a better overview of their images. Many file and image management features have been added to make it a tool that addresses a full image management workflow. A modification of the Self Sorting Map algorithm enables a list-like image arrangement without loosing the visual sorting. Efficient calculation and storage of visual features as well as the use of many macOS APIs result in an application that is fluid to use. ,Kein DOI-Link verfügbar,2111.13363v1,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,Deep Aggregation of Regional Convolutional Activations for Content Based   Image Retrieval,2019,"  One of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for the Oxford Buildings and Paris data sets while reducing the training time significantly. ",Kein DOI-Link verfügbar,1909.09420v2,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,Deep Metric Learning using Similarities from Nonlinear Rank   Approximations,2019,"  In recent years, deep metric learning has achieved promising results in learning high dimensional semantic feature embeddings where the spatial relationships of the feature vectors match the visual similarities of the images. Similarity search for images is performed by determining the vectors with the smallest distances to a query vector. However, high retrieval quality does not depend on the actual distances of the feature vectors, but rather on the ranking order of the feature vectors from similar images. In this paper, we introduce a metric learning algorithm that focuses on identifying and modifying those feature vectors that most strongly affect the retrieval quality. We compute normalized approximated ranks and convert them to similarities by applying a nonlinear transfer function. These similarities are used in a newly proposed loss function that better contracts similar and disperses dissimilar samples. Experiments demonstrate significant improvement over existing deep feature embedding methods on the CUB-200-2011, Cars196, and Stanford Online Products data sets for all embedding sizes. ",Kein DOI-Link verfügbar,1909.09427v2,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs,2019,"  Nowadays stock photo agencies often have millions of images. Non-stop viewing of 20 million images at a speed of 10 images per second would take more than three weeks. This demonstrates the impossibility to inspect all images and the difficulty to get an overview of the entire collection. Although there has been a lot of effort to improve visual image search, there is little research and support for visual image exploration. Typically, users start ""exploring"" an image collection with a keyword search or an example image for a similarity search. Both searches lead to long unstructured lists of result images. In earlier publications, we introduced the idea of graph-based image navigation and proposed an efficient algorithm for building hierarchical image similarity graphs for dynamically changing image collections. In this demo we showcase real-time visual exploration of millions of images with a standard web browser. Subsets of images are successively retrieved from the graph and displayed as a visually sorted 2D image map, which can be zoomed and dragged to explore related concepts. Maintaining the positions of previously shown images creates the impression of an ""endless map"". This approach allows an easy visual image-based navigation, while preserving the complex image relationships of the graph. ",Kein DOI-Link verfügbar,1910.06005v1,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval,2021,"  Even though it has extensively been shown that retrieval specific training of deep neural networks is beneficial for nearest neighbor image search quality, most of these models are trained and tested in the domain of landmarks images. However, some applications use images from various other domains and therefore need a network with good generalization properties - a general-purpose CBIR model. To the best of our knowledge, no testing protocol has so far been introduced to benchmark models with respect to general image retrieval quality. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with a broad range of image categories. This benchmark is subsequently used to evaluate various pretrained models of different architectures on their generalization qualities. We show that large-scale pretraining significantly improves retrieval performance and present experiments on how to further increase these properties by appropriate fine-tuning. With these promising results, we hope to increase interest in the research topic of general-purpose CBIR. ",Kein DOI-Link verfügbar,2111.13122v1,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,Fast Approximate Nearest Neighbor Search with a Dynamic Exploration   Graph using Continuous Refinement,2023,"  For approximate nearest neighbor search, graph-based algorithms have shown to offer the best trade-off between accuracy and search time. We propose the Dynamic Exploration Graph (DEG) which significantly outperforms existing algorithms in terms of search and exploration efficiency by combining two new ideas: First, a single undirected even regular graph is incrementally built by partially replacing existing edges to integrate new vertices and to update old neighborhoods at the same time. Secondly, an edge optimization algorithm is used to continuously improve the quality of the graph. Combining this ongoing refinement with the graph construction process leads to a well-organized graph structure at all times, resulting in: (1) increased search efficiency, (2) predictable index size, (3) guaranteed connectivity and therefore reachability of all vertices, and (4) a dynamic graph structure. In addition we investigate how well existing graph-based search systems can handle indexed queries where the seed vertex of a search is the query itself. Such exploration tasks, despite their good starting point, are not necessarily easy. High efficiency in approximate nearest neighbor search (ANNS) does not automatically imply good performance in exploratory search. Extensive experiments show that our new Dynamic Exploration Graph outperforms existing algorithms significantly for indexed and unindexed queries. ",Kein DOI-Link verfügbar,2307.10479v2,No,
0000-0001-6309-572X,Kai Uwe Barthel,HTW Berlin - Universität der Angewandte Wissenschaften,Improved Evaluation and Generation of Grid Layouts using Distance   Preservation Quality and Linear Assignment Sorting,2022,"  Images sorted by similarity enables more images to be viewed simultaneously, and can be very useful for stock photo agencies or e-commerce applications. Visually sorted grid layouts attempt to arrange images so that their proximity on the grid corresponds as closely as possible to their similarity. Various metrics exist for evaluating such arrangements, but there is low experimental evidence on correlation between human perceived quality and metric value. We propose Distance Preservation Quality (DPQ) as a new metric to evaluate the quality of an arrangement. Extensive user testing revealed stronger correlation of DPQ with user-perceived quality and performance in image retrieval tasks compared to other metrics. In addition, we introduce Fast Linear Assignment Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts. FLAS achieves very good sorting qualities while improving run time and computational resources. ",Kein DOI-Link verfügbar,2205.04255v2,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Asymptotic Normality of Support Vector Machine Variants and Other   Regularized Kernel Methods,2010,"  In nonparametric classification and regression problems, regularized kernel methods, in particular support vector machines, attract much attention in theoretical and in applied statistics. In an abstract sense, regularized kernel methods (simply called SVMs here) can be seen as regularized M-estimators for a parameter in a (typically infinite dimensional) reproducing kernel Hilbert space. For smooth loss functions, it is shown that the difference between the estimator, i.e.\ the empirical SVM, and the theoretical SVM is asymptotically normal with rate $\sqrt{n}$. That is, the standardized difference converges weakly to a Gaussian process in the reproducing kernel Hilbert space. As common in real applications, the choice of the regularization parameter may depend on the data. The proof is done by an application of the functional delta-method and by showing that the SVM-functional is suitably Hadamard-differentiable. ",Kein DOI-Link verfügbar,1010.0535v3,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Asymptotic Confidence Sets for General Nonparametric Regression and   Classification by Regularized Kernel Methods,2012,"  Regularized kernel methods such as, e.g., support vector machines and least-squares support vector regression constitute an important class of standard learning algorithms in machine learning. Theoretical investigations concerning asymptotic properties have manly focused on rates of convergence during the last years but there are only very few and limited (asymptotic) results on statistical inference so far. As this is a serious limitation for their use in mathematical statistics, the goal of the article is to fill this gap. Based on asymptotic normality of many of these methods, the article derives a strongly consistent estimator for the unknown covariance matrix of the limiting normal distribution. In this way, we obtain asymptotically correct confidence sets for $\psi(f_{P,\lambda_0})$ where $f_{P,\lambda_0}$ denotes the minimizer of the regularized risk in the reproducing kernel Hilbert space $H$ and $\psi:H\rightarrow\mathds{R}^m$ is any Hadamard-differentiable functional. Applications include (multivariate) pointwise confidence sets for values of $f_{P,\lambda_0}$ and confidence sets for gradients, integrals, and norms. ",Kein DOI-Link verfügbar,1203.4354v1,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Practical Tikhonov Regularized Estimators in Reproducing Kernel Hilbert   Spaces for Statistical Inverse Problems,2013,"  Regularized kernel methods such as support vector machines (SVM) and support vector regression (SVR) constitute a broad and flexible class of methods which are theoretically well investigated and commonly used in nonparametric classification and regression problems. As these methods are based on a Tikhonov regularization which is also common in inverse problems, this article investigates the use of regularized kernel methods for inverse problems in a unifying way. Regularized kernel methods are based on the use of reproducing kernel Hilbert spaces (RKHS) which lead to very good computational properties. It is shown that similar properties remain true in solving statistical inverse problems and that standard software implementations developed for ordinary regression problems can still be used for inverse regression problems. Consistency of these methods and a rate of convergence for the risk is shown under quite weak assumptions and rates of convergence for the estimator are shown under somehow stronger assumptions. The applicability of these methods is demonstrated in a simulation. ",Kein DOI-Link verfügbar,1305.1137v1,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Estimation of scale functions to model heteroscedasticity by support   vector machines,2011,"  A main goal of regression is to derive statistical conclusions on the conditional distribution of the output variable Y given the input values x. Two of the most important characteristics of a single distribution are location and scale. Support vector machines (SVMs) are well established to estimate location functions like the conditional median or the conditional mean. We investigate the estimation of scale functions by SVMs when the conditional median is unknown, too. Estimation of scale functions is important e.g. to estimate the volatility in finance. We consider the median absolute deviation (MAD) and the interquantile range (IQR) as measures of scale. Our main result shows the consistency of MAD-type SVMs. ",Kein DOI-Link verfügbar,1111.1830v1,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Qualitative Robustness of Support Vector Machines,2009,"  Support vector machines have attracted much attention in theoretical and in applied statistics. Main topics of recent interest are consistency, learning rates and robustness. In this article, it is shown that support vector machines are qualitatively robust. Since support vector machines can be represented by a functional on the set of all probability measures, qualitative robustness is proven by showing that this functional is continuous with respect to the topology generated by weak convergence of probability measures. Combined with the existence and uniqueness of support vector machines, our results show that support vector machines are the solutions of a well-posed mathematical problem in Hadamard's sense. ",Kein DOI-Link verfügbar,0912.0874v2,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,On the Consistency of the Bootstrap Approach for Support Vector Machines   and Related Kernel Based Methods,2013,  It is shown that bootstrap approximations of support vector machines (SVMs) based on a general convex and smooth loss function and on a general kernel are consistent. This result is useful to approximate the unknown finite sample distribution of SVMs by the bootstrap approach. ,Kein DOI-Link verfügbar,1301.6944v1,No,
0000-0002-3719-8992,Robert Hable,Technische Hochschule Deggendorf,Support Vector Machines for Additive Models: Consistency and Robustness,2010,"  Support vector machines (SVMs) are special kernel based methods and belong to the most successful learning methods since more than a decade. SVMs can informally be described as a kind of regularized M-estimators for functions and have demonstrated their usefulness in many complicated real-life problems. During the last years a great part of the statistical research on SVMs has concentrated on the question how to design SVMs such that they are universally consistent and statistically robust for nonparametric classification or nonparametric regression purposes. In many applications, some qualitative prior knowledge of the distribution P or of the unknown function f to be estimated is present or the prediction function with a good interpretability is desired, such that a semiparametric model or an additive model is of interest.   In this paper we mainly address the question how to design SVMs by choosing the reproducing kernel Hilbert space (RKHS) or its corresponding kernel to obtain consistent and statistically robust estimators in additive models. We give an explicit construction of kernels - and thus of their RKHSs - which leads in combination with a Lipschitz continuous loss function to consistent and statistically robust SMVs for additive models. Examples are quantile regression based on the pinball loss function, regression based on the epsilon-insensitive loss function, and classification based on the hinge loss function. ",Kein DOI-Link verfügbar,1007.4062v1,No,
0000-0003-0173-2900,Michael Scholz,Technische Hochschule Deggendorf,Mira science with interferometry: a review,2004,"  Model-predicted and observed properties of the brightness distribution on M-type Mira disks are discussed. Fundamental issues of limb-darkening and diameter definition, of assigning observational data to diameter-type quantities and of interpreting such quantities in terms of model diameters are outlined. The influence of model properties upon interpretation of measured data is clarified. The dependence of the centre-to-limb variation (CLV) of intensity on wavelength, on stellar parameters and on variablity phase and cycle may be used for analyzing the geometrical and physical structure of the Mira atmosphere, for determining fundamental stellar parameters, and for investigating the quality of models. Desirable future observations include simultaneous observations in different spectral features at different phases and cycles, observation of the position of the shock front and observation of the time- and wavelength-dependence of deviations from spherical symmetry. ",https://doi.org/10.1117/12.456978,astro-ph/0409555v1,No,
0000-0003-0173-2900,Michael Scholz,Technische Hochschule Deggendorf,Dynamical Opacity-Sampling Models of Mira Variables. II: Time-Dependent   Atmospheric Structure and Observable Properties of 4 M-Type Model Series,2011,"  We present 4 model series of the CODEX dynamical opacity-sampling models of Mira variables with solar abundances, designed to have parameters similar to $o$ Cet, R Leo and R Cas. We demonstrate that the CODEX models provide a clear physical basis for the molecular shell scenario used to explain interferometric observations of Mira variables. We show that these models generally provide a good match to photometry and interferometry at wavelengths between the near-infrared and the radio, and make the model outputs publicly available. These model also demonstrate that, in order to match visible and infrared observations, the Fe-poor silicate grains that form within 3 continuum radii must have small grain radii and therefore can not drive the winds from O-rich Mira variables. ",https://doi.org/10.1111/j.1365-2966.2011.19469.x,1107.3619v1,No,
0000-0003-0173-2900,Michael Scholz,Technische Hochschule Deggendorf,Effects of moderate abundance changes on the atmospheric structure and   colours of Mira variables (Research Note),2014,"  Aims. We study the effects of moderate deviations from solar abundances upon the atmospheric structure and colours of typical Mira variables. Methods. We present two model series of dynamical opacity-sampling models of Mira variables which have (1) 1 solar metallicity 3 and (2) ""mild"" S-type C/O abundance ratio ([C/O]=0.9) with typical Zr enhancement (solar +1.0). These series are compared to a previously studied solar-abundance series which has similar fundamental parameters (mass, luminosity, period, radius) that are close to those of o Cet. Results. Both series show noticeable effects of abundance upon stratifications and infrared colours but cycle-to-cycle differences mask these effects at most pulsation phases, with the exception of a narrow-water-filter colour near minimum phase. ",https://doi.org/10.1051/0004-6361/201323183,1404.6735v1,No,
0000-0003-0173-2900,Michael Scholz,Technische Hochschule Deggendorf,Silicate condensation in Mira variables,2016,"  We study whether the condensation of silicate dust in Mira envelopes could be caused by cluster formation by the abundant SiO molecules. For a simplified model of the pulsational motions of matter in the the outer layers of a Mira variable which is guided by a numerical model for Mira pulsations, the equations of dust nucleation and growth are solved in the co-moving frame of a fixed mass element. It is assumed that seed particles form by clustering of SiO molecules. The calculation of the nucleation rate is based on the experimental data of Nuth and Donn (1982). The quantity of dust formed is calculated by a moment method and the calculation of radiation pressure on the dusty gas is based on a dirty silicate model. Dust nucleation occurs in the model at the upper culmination of the trajectory of a gas parcel where it stays for a considerable time at low temperatures while subsequent dust growth occurs during the descending part of the motion and continues after the next shock reversed motion. It is found that sufficient dust forms that radiation pressure exceeds gravitational pull of the stars such that the mass element is finally driven out of the star. Nucleation of dust particles by clustering of the abundant SiO molecules could be the mechanism that triggers silicate dust formation in Miras. ",https://doi.org/10.1051/0004-6361/201628113,1604.04636v1,No,
0000-0003-0173-2900,Michael Scholz,Technische Hochschule Deggendorf,Coordinated AMBER and MIDI observations of the Mira variable RR Aql,2008,"  We have used near- and mid-infrared interferometry to investigate the pulsating atmosphere and the circumstellar environment of the Mira variable RR Aql. Observations were taken with the VLTI/AMBER (near infrared) and the VLTI/MIDI (mid infrared) instruments. We have obtained a total of 15 MIDI epochs between Apr 9, 2004 and Jul 28, 2007 covering 4 pulsation cycles and one AMBER epoch on Sep 9, 2006 at phase 2.82. This work is also part of an ongoing project of joint VLTI and VLBA observations to study the connection between stellar pulsation and the mass loss process. Here we present a comparison of the AMBER visibility data to a simple uniform disk model as well as to predictions by recent self-excited dynamic model atmospheres. The best fitting photospheric angular diameter of the model atmosphere at phase 2.82 is 9.9 +/- 2.4 mas. ",https://doi.org/10.1063/1.3099285,0809.4619v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,A Security Architecture for Mobile Wireless Sensor Networks,2014,"  Wireless sensor networks increasingly become viable solutions to many challenging problems and will successively be deployed in many areas in the future. However, deploying new technology without security in mind has often proved to be unreasonably dangerous. We propose a security architecture for self-organizing mobile wireless sensor networks that prevents many attacks these networks are exposed to. Furthermore, it limits the security impact of some attacks that cannot be prevented. We analyse our security architecure and show that it provides the desired security aspects while still being a lightweight solution and thus being applicable for self-organizing mobile wireless sensor networks. ",https://doi.org/10.1007/978-3-540-30496-8_14,1409.6606v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Lifting DecPOMDPs for Nanoscale Systems -- A Work in Progress,2021,"  DNA-based nanonetworks have a wide range of promising use cases, especially in the field of medicine. With a large set of agents, a partially observable stochastic environment, and noisy observations, such nanoscale systems can be modelled as a decentralised, partially observable, Markov decision process (DecPOMDP). As the agent set is a dominating factor, this paper presents (i) lifted DecPOMDPs, partitioning the agent set into sets of indistinguishable agents, reducing the worst-case space required, and (ii) a nanoscale medical system as an application. Future work turns to solving and implementing lifted DecPOMDPs. ",Kein DOI-Link verfügbar,2110.09152v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Towards Fault Localization via Probabilistic Software Modeling,2020,"  Software testing helps developers to identify bugs. However, awareness of bugs is only the first step. Finding and correcting the faulty program components is equally hard and essential for high-quality software. Fault localization automatically pinpoints the location of an existing bug in a program. It is a hard problem, and existing methods are not yet precise enough for widespread industrial adoption. We propose fault localization via Probabilistic Software Modeling (PSM). PSM analyzes the structure and behavior of a program and synthesizes a network of Probabilistic Models (PMs). Each PM models a method with its inputs and outputs and is capable of evaluating the likelihood of runtime data. We use this likelihood evaluation to find fault locations and their impact on dependent code elements. Results indicate that PSM is a robust framework for accurate fault localization. ",Kein DOI-Link verfügbar,2001.07409v2,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Exploring a Test Data-Driven Method for Selecting and Constraining   Metamorphic Relations,2023,"  Identifying and selecting high-quality Metamorphic Relations (MRs) is a challenge in Metamorphic Testing (MT). While some techniques for automatically selecting MRs have been proposed, they are either domain-specific or rely on strict assumptions about the applicability of a pre-defined MRs. This paper presents a preliminary evaluation of MetaTrimmer, a method for selecting and constraining MRs based on test data. MetaTrimmer comprises three steps: generating random test data inputs for the SUT (Step 1), performing test data transformations and logging MR violations (Step 2), and conducting manual inspections to derive constraints (Step 3). The novelty of MetaTrimmer is its avoidance of complex prediction models that require labeled datasets regarding the applicability of MRs. Moreover, MetaTrimmer facilitates the seamless integration of MT with advanced fuzzing for test data generation. In a preliminary evaluation, MetaTrimmer shows the potential to overcome existing limitations and enhance MR effectiveness. ",https://doi.org/10.1109/SEAA60479.2023.00063,2307.15522v1,Yes,potent(1)
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Using Source Code Metrics for Predicting Metamorphic Relations at Method   Level,2022,"  Metamorphic testing (TM) examines the relations between inputs and outputs of test runs. These relations are known as metamorphic relations (MR). Currently, MRs are handpicked and require in-depth knowledge of the System Under Test (SUT), as well as its problem domain. As a result, the identification and selection of high-quality MRs is a challenge. \citeauthor{PMR1} suggested the Predicting Metamorphic Relations (PMR) approach for automatic prediction of applicable MRs picked from a predefined list. PMR is based on a Support Vector Machine (SVM) model using features derived from the Control Flow Graphs (CFGs) of 100 Java methods. The original study of \citeauthor{PMR1} showed encouraging results, but developing classification models from CFG-related features is costly. In this paper, we aim at developing a PMR approach that is less costly without losing performance. We complement the original PMR approach by considering other than CFG-related features. We define 21 features that can be directly extracted from source code and build several classifiers, including SVM models. Our results indicate that using the original CFG-based method-level features, in particular for a SVM with random walk kernel (RWK), achieve better predictions in terms of AUC-ROC for most of the candidate MRs than our models. However, for one of the candidate MRs, using source code features achieved the best AUC-ROC result (greater than 0.8). ",https://doi.org/10.1109/SANER53432.2022.0013,2205.15835v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Towards Automatic Generation of Amplified Regression Test Oracles,2023,"  Regression testing is crucial in ensuring that pure code refactoring does not adversely affect existing software functionality, but it can be expensive, accounting for half the cost of software maintenance. Automated test case generation reduces effort but may generate weak test suites. Test amplification is a promising solution that enhances tests by generating additional or improving existing ones, increasing test coverage, but it faces the test oracle problem. To address this, we propose a test oracle derivation approach that uses object state data produced during System Under Test (SUT) test execution to amplify regression test oracles. The approach monitors the object state during test execution and compares it to the previous version to detect any changes in relation to the SUT's intended behaviour. Our preliminary evaluation shows that the proposed approach can enhance the detection of behaviour changes substantially, providing initial evidence of its effectiveness. ",https://doi.org/10.1109/SEAA60479.2023.00058,2307.15527v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Shawn: A new approach to simulating wireless sensor networks,2005,"  We consider the simulation of wireless sensor networks (WSN) using a new approach. We present Shawn, an open-source discrete-event simulator that has considerable differences to all other existing simulators. Shawn is very powerful in simulating large scale networks with an abstract point of view. It is, to the best of our knowledge, the first simulator to support generic high-level algorithms as well as distributed protocols on exactly the same underlying networks. ",Kein DOI-Link verfügbar,cs/0502003v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Bug or not Bug? Analysing the Reasons Behind Metamorphic Relation   Violations,2023,"  Metamorphic Testing (MT) is a testing technique that can effectively alleviate the oracle problem. MT uses Metamorphic Relations (MRs) to determine if a test case passes or fails. MRs specify how the outputs should vary in response to specific input changes when executing the System Under Test (SUT). If a particular MR is violated for at least one test input (and its change), there is a high probability that the SUT has a fault. On the other hand, if a particular MR is not violated, it does not guarantee that the SUT is fault free. However, deciding if the MR is being violated due to a bug or because the MR does not hold/fit for particular conditions generated by specific inputs remains a manual task and unexplored. In this paper, we develop a method for refining MRs to offer hints as to whether a violation results from a bug or arises from the MR not being matched to certain test data under specific circumstances. In our initial proof-of-concept, we derive the relevant information from rules using the Association Rule Mining (ARM) technique. In our initial proof-of-concept, we validate our method on a toy example and discuss the lessons learned from our experiments. Our proof-of-concept demonstrates that our method is applicable and that we can provide suggestions that help strengthen the test suite for regression testing purposes. ",https://doi.org/10.1109/SANER56733.2023.00109,2305.09640v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Neighborhood-Based Topology Recognition in Sensor Networks,2004,"  We consider a crucial aspect of self-organization of a sensor network consisting of a large set of simple sensor nodes with no location hardware and only very limited communication range. After having been distributed randomly in a given two-dimensional region, the nodes are required to develop a sense for the environment, based on a limited amount of local communication. We describe algorithmic approaches for determining the structure of boundary nodes of the region, and the topology of the region. We also develop methods for determining the outside boundary, the distance to the closest boundary for each point, the Voronoi diagram of the different boundaries, and the geometric thickness of the network. Our methods rely on a number of natural assumptions that are present in densely distributed sets of nodes, and make use of a combination of stochastics, topology, and geometry. Evaluation requires only a limited number of simple local computations. ",Kein DOI-Link verfügbar,cs/0405058v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Deterministic boundary recognition and topology extraction for large   sensor networks,2005,"  We present a new framework for the crucial challenge of self-organization of a large sensor network. The basic scenario can be described as follows: Given a large swarm of immobile sensor nodes that have been scattered in a polygonal region, such as a street network. Nodes have no knowledge of size or shape of the environment or the position of other nodes. Moreover, they have no way of measuring coordinates, geometric distances to other nodes, or their direction. Their only way of interacting with other nodes is to send or to receive messages from any node that is within communication range. The objective is to develop algorithms and protocols that allow self-organization of the swarm into large-scale structures that reflect the structure of the street network, setting the stage for global routing, tracking and guiding algorithms. ",Kein DOI-Link verfügbar,cs/0510048v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Koordinatenfreies Lokationsbewusstsein (Localization without   Coordinates),2005,"  Localization is one of the fundamental issues in sensor networks. It is almost always assumed that it must be solved by assigning coordinates to the nodes. This article discusses positioning algorithms from a theoretical, practical and simulative point of view, and identifies difficulties and limitations. Ideas for more abstract means of location awareness are presented and the resulting possible improvements for applications are shown. Nodes with certain topological or environmental properties are clustered, and the neighborhood structure of the clusters is modeled as a graph. Eines der fundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer die Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon ausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In diesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein kritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen aufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine abstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird gezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf einer graphenbasierten Modellierung des Netzes: Knoten mit bestimmten topologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und Clusternachbarschaften dann als Graphen modelliert. ",Kein DOI-Link verfügbar,cs/0502069v1,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute der Technology,Empowered by Wireless Communication: Self-Organizing Traffic Collectives,2010,"  In recent years, tremendous progress has been made in understanding the dynamics of vehicle traffic flow and traffic congestion by interpreting traffic as a multi-particle system. This helps to explain the onset and persistence of many undesired phenomena, e.g., traffic jams. It also reflects the apparent helplessness of drivers in traffic, who feel like passive particles that are pushed around by exterior forces; one of the crucial aspects is the inability to communicate and coordinate with other traffic participants. We present distributed methods for solving these fundamental problems, employing modern wireless, ad-hoc, multi-hop networks. The underlying idea is to use these capabilities as the basis for self-organizing methods for coordinating data collection and processing, recognizing traffic phenomena, and changing their structure by coordinated behavior. The overall objective is a multi-level approach that reaches from protocols for local wireless communication, data dissemination, pattern recognition, over hierarchical structuring and coordinated behavior, all the way to large-scale traffic regulation. In this article we describe three types of results: (i) self-organizing and distributed methods for maintaining and collecting data (using our concept of Hovering Data Clouds); (ii) adaptive data dissemination for traffic information systems; (iii) methods for self-recognition of traffic jams. We conclude by describing higher-level aspects of our work. ",Kein DOI-Link verfügbar,1005.0675v1,No,
0009-0003-6744-8010,Christoph Schober,Deggendorf Institute der Technology,Critical analysis of fragment-orbital DFT schemes for the calculation of   electronic coupling values,2015,"  We present a critical analysis of the popular fragment-orbital density-functional theory (FO-DFT) scheme for the calculation of electronic coupling values. We discuss the characteristics of different possible formulations or 'flavors' of the scheme which differ by the number of electrons in the calculation of the fragments and the construction of the Hamiltonian. In addition to two previously described variants based on neutral fragments, we present a third version taking a different route to the approximate diabatic state by explicitly considering charged fragments. In applying these FO-DFT flavors to the two molecular test sets HAB7 (electron transfer) and HAB11 (hole transfer) we find that our new scheme gives improved electronic couplings for HAB7 (-6.2% decrease in mean relative signed error) and greatly improved electronic couplings for HAB11 (-15.3% decrease in mean relative signed error). A systematic investigation of the influence of exact exchange on the electronic coupling values shows that the use of hybrid functionals in FO-DFT calculations improves the electronic couplings, giving values close to or even better than more sophisticated constrained DFT calculations. Comparing the accuracy and computational cost of each variant we devise simple rules to choose the best possible flavor depending on the task. For accuracy, our new scheme with charged-fragment calculations performs best, while numerically more efficient at reasonable accuracy is the variant with neutral fragments. ",https://doi.org/10.1063/1.4940920,1512.00200v1,No,
0009-0003-6744-8010,Christoph Schober,Deggendorf Institute der Technology,Finding the Right Bricks for Molecular Lego: A Data Mining Approach to   Organic Semiconductor Design,2021,"  Improving charge carrier mobilities in organic semiconductors is a challenging task that has hitherto primarily been tackled by empirical structural tuning of promising core compounds. Knowledge-based methods can greatly accelerate such local exploration, while a systematic analysis of large chemical databases can point towards promising design strategies. Here, we demonstrate such data mining by clustering an in-house database of >64.000 organic molecular crystals for which two charge-transport descriptors, the electronic coupling and the reorganization energy, have been calculated from first principles. The clustering is performed according to the Bemis-Murcko scaffolds of the constituting molecules and according to the sidegroups with which these molecular backbones are functionalized. In both cases, we obtain statistically significant structure-property relationships with certain scaffolds (sidegroups) consistently leading to favorable charge-transport properties. Functionalizing promising scaffolds with favorable sidegroups results in engineered molecular crystals for which we indeed compute improved charge-transport properties. ",https://doi.org/10.1021/acs.chemmater.8b04436,2110.09149v1,No,
0009-0003-6744-8010,Christoph Schober,Deggendorf Institute der Technology,Genarris: Random Generation of Molecular Crystal Structures and Fast   Screening with a Harris Approximation,2018,"  We present Genarris, a Python package that performs configuration space screening for molecular crystals of rigid molecules by random sampling with physical constraints. For fast energy evaluations Genarris employs a Harris approximation, whereby the total density of a molecular crystal is constructed via superposition of single molecule densities. Dispersion-inclusive density functional theory (DFT) is then used for the Harris density without performing a self-consistency cycle. Genarris uses machine learning for clustering, based on a relative coordinate descriptor (RCD) developed specifically for molecular crystals, which is shown to be robust in identifying packing motif similarity. In addition to random structure generation, Genarris offers three workflows based on different sequences of successive clustering and selection steps: the ""Rigorous"" workflow is an exhaustive exploration of the potential energy landscape, the ""Energy"" workflow produces a set of low energy structures, and the ""Diverse"" workflow produces a maximally diverse set of structures. The latter is recommended for generating initial populations for genetic algorithms. Here, the implementation of Genarris is reported and its application is demonstrated for three test cases. ",https://doi.org/10.1063/1.5014038,1803.02145v1,Yes,potent(1)
0000-0002-7232-5256,Marco Schmidt,Hochschule Bochum,Detection and Annotation of Plant Organs from Digitized Herbarium Scans   using Deep Learning,2020,"  As herbarium specimens are increasingly becoming digitized and accessible in online repositories, advanced computer vision techniques are being used to extract information from them. The presence of certain plant organs on herbarium sheets is useful information in various scientific contexts and automatic recognition of these organs will help mobilize such information. In our study we use deep learning to detect plant organs on digitized herbarium specimens with Faster R-CNN. For our experiment we manually annotated hundreds of herbarium scans with thousands of bounding boxes for six types of plant organs and used them for training and evaluating the plant organ detection model. The model worked particularly well on leaves and stems, while flowers were also present in large numbers in the sheets, but not equally well recognized. ",Kein DOI-Link verfügbar,2007.13106v2,No,
0009-0000-1041-9559,Herbert Schmidt,Bochum Universität der Angewandte Wissenschaften,Influence of Iron Losses on Switching Dynamics of an Electromagnet from   Experiment and Simulation,2020,"  The switching behaviour of electromagnets is determined by the inertia of the armature, the stiffness of the return spring and the magnetostatic forces between armature and yoke. For highly dynamic systems, hysteresis and eddy current losses have a slowing effect. In this paper we consider the experimentally observed behaviour of a switching magnet and compare it with simulation results including hysteresis and eddy current losses. ",Kein DOI-Link verfügbar,2001.09638v1,No,
0009-0000-1041-9559,Herbert Schmidt,Bochum Universität der Angewandte Wissenschaften,Energy Gap from Tunneling and Metallic Sharvin Contacts onto MgB2:   Evidence for a Weakened Surface Layer,2001,"  Point-contact tunnel junctions using a Au tip on sintered MgB2 pellets reveal a sharp superconducting energy gap that is confirmed by subsequent metallic Sharvin contacts made on the same sample. The peak in the tunneling conductance and the Sharvin contact conductance follow the BCS form, but the gap values of 4.3 meV are less than the weak-coupling BCS value of 5.9 meV for the bulk Tc of 39 K. The low value of Delta compared to the BCS value for the bulk Tc is possibly due to chemical reactions at the surface. ",https://doi.org/10.1103/PhysRevB.63.220504,cond-mat/0102389v1,No,
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,Emergence of a Renormalized $1/N$ Expansion in Quenched Critical   Many-Body Systems,2020,"  We consider the fate of $1/N$ expansions in unstable many-body quantum systems, as realized by a quench across criticality, and show the emergence of ${\rm e}^{2\lambda t}/N$ as a renormalized parameter ruling the quantum-classical transition and accounting nonperturbatively for the local divergence rate $\lambda$ of mean-field solutions. In terms of ${\rm e}^{2\lambda t}/N$, quasiclassical expansions of paradigmatic examples of criticality, like the self-trapping transition in an integrable Bose-Hubbard dimer and the generic instability of attractive bosonic systems toward soliton formation, are pushed to arbitrarily high orders. The agreement with numerical simulations supports the general nature of our results in the appropriately combined long-time $\lambda t\to \infty$ quasiclassical $N\to \infty$ regime, out of reach of expansions in the bare parameter $1/N$. For scrambling in many-body hyperbolic systems, our results provide formal grounds to a conjectured multiexponential form of out-of-time-ordered correlators. ",https://doi.org/10.1103/PhysRevLett.126.110602,2010.08364v3,No,
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,"Classical, semiclassical and quantum signatures of quantum phase   transitions in a (pseudo) relativistic many-body system",2020,"  We identify a (pseudo) relativistic spin-dependent analogue of the celebrated quantum phase transition driven by the formation of a bright soliton in attractive one-dimensional bosonic gases. In this new scenario, due to the simultaneous existence of the linear dispersion and the bosonic nature of the system, special care must be taken with the choice of energy region where the transition takes place. Still, due to a crucial adiabatic separation of scales, and identified through extensive numerical diagonalization, a suitable effective model describing the transition is found. The corresponding mean-field analysis based on this effective model provides accurate predictions for the location of the quantum phase transition when compared against extensive numerical simulations. Furthermore, we numerically investigate the dynamical exponents characterizing the approach from its finite-size precursors to the sharp quantum phase transition in the thermodynamic limit. ",https://doi.org/10.3390/condmat5020026,2007.04650v1,No,
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,Semiclassics in a system without classical limit: The few-body spectrum   of two interacting bosons in one dimension,2017,"  We present a semiclassical study of the spectrum of a few-body system consisting of two short-range interacting bosonic particles in one dimension, a particular case of a general class of integrable many-body systems where the energy spectrum is given by the solution of algebraic transcendental equations. By an exact mapping between $\delta$-potentials and boundary conditions on the few-body wave functions, we are able to extend previous semiclassical results for single-particle systems with mixed boundary conditions to the two-body problem. The semiclassical approach allows us to derive explicit analytical results for the smooth part of the two-body density of states that are in excellent agreement with numerical calculations. It further enables us to include the effect of bound states in the attractive case. Remarkably, for the particular case of two particles in one dimension, the discrete energy levels obtained through a requantization condition of the smooth density of states are essentially in perfect agreement with the exact ones. ",https://doi.org/10.1103/PhysRevE.96.022204,1705.09637v2,Yes,potent(1)
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,Nonlocal pair correlations in Lieb-Liniger gases: A unified   nonperturbative approach from weak degeneracy to high temperatures,2018,"  We present analytical results for the nonlocal pair correlations in one-dimensional bosonic systems with repulsive contact interactions that are uniformly valid from the classical regime of high temperatures down to weak quantum degeneracy entering the regime of ultralow temperatures. By using the information contained in the short-time approximations of the full many-body propagator, we derive results that are nonperturbative in the interaction parameter while covering a wide range of temperatures and densities. For the case of three particles we give a simple formula for arbitrary couplings that is exact in the dilute limit while remaining valid up to the regime where the thermal de Broglie wavelength $\lambda_T$ is of the order of the characteristic length $L$ of the system. We then show how to use this result to find analytical expressions for the nonlocal correlations for arbitrary but fixed particle numbers $N$ including finite-size corrections. Neglecting the latter in the thermodynamic limit provides an expansion in the quantum degeneracy parameter $N\lambda_T/L$. We compare our analytical results with numerical Bethe ansatz calculations, finding excellent agreement. ",https://doi.org/10.1103/PhysRevA.97.063612,1801.07666v2,No,
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,Cell reorientation under cyclic stretching,2014,"  Mechanical cues from the extracellular microenvironment play a central role in regulating the structure, function and fate of living cells. Nevertheless, the precise nature of the mechanisms and processes underlying this crucial cellular mechanosensitivity remains a fundamental open problem. Here we provide a novel framework for addressing cellular sensitivity and response to external forces by experimentally and theoretically studying one of its most striking manifestations -- cell reorientation to a uniform angle in response to cyclic stretching of the underlying substrate. We first show that existing approaches are incompatible with our extensive measurements of cell reorientation. We then propose a fundamentally new theory that shows that dissipative relaxation of the cell's passively-stored, two-dimensional, elastic energy to its minimum actively drives the reorientation process. Our theory is in excellent quantitative agreement with the complete temporal reorientation dynamics of individual cells, measured over a wide range of experimental conditions, thus elucidating a basic aspect of mechanosensitivity. ",https://doi.org/10.1038/ncomms4938,1412.0369v1,No,
0009-0001-5660-4044,Benjamin Geiger,Bochum Universität der Angewandte Wissenschaften,Reversible quantum information spreading in many-body systems near   criticality,2018,"  Quantum chaotic interacting $N$-particle systems are assumed to show fast and irreversible spreading of quantum information on short (Ehrenfest) time scales $\sim\!\log N$. Here we show that, near criticality, certain many-body systems exhibit fast initial scrambling, followed subsequently by oscillatory behavior between reentrant localization and delocalization of information in Hilbert space. We consider both integrable and nonintegrable quantum critical bosonic systems with attractive contact interaction that exhibit locally unstable dynamics in the corresponding many-body phase space of the large-$N$ limit. Semiclassical quantization of the latter accounts for many-body correlations in excellent agreement with simulations. Most notably, it predicts an asymptotically constant local level spacing $\hbar/\tau$, again given by $\tau\! \sim\! \log N$. This unique timescale governs the long-time behavior of out-of-time-order correlators that feature quasi-periodic recurrences indicating reversibility. ",https://doi.org/10.1103/PhysRevLett.123.160401,1812.09237v2,No,
0000-0003-1857-0186,Andreas Brunnert,Hochschule München Universität der Angewandte Wissenschaften,Flexibility Is Key in Organizing a Global Professional Conference   Online: The ICPE 2020 Experience in the COVID-19 Era,2020,"  Organizing professional conferences online has never been more timely. Responding to the new challenges raised by COVID-19, the organizers of the ACM/SPEC International Conference on Performance Engineering 2020 had to address the question: How should we organize these conferences online? This article summarizes their successful answer. ",Kein DOI-Link verfügbar,2005.09085v1,No,
0000-0003-1857-0186,Andreas Brunnert,Hochschule München Universität der Angewandte Wissenschaften,Performance-oriented DevOps: A Research Agenda,2015,"  DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance.   Performance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area. ",Kein DOI-Link verfügbar,1508.04752v1,No,
0000-0003-4075-984X,Markus Plattner,Hochschule München Universität der Angewandte Wissenschaften,The Wide Field Imager Instrument for Athena,2017,"  The WFI (Wide Field Imager) instrument is planned to be one of two complementary focal plane cameras on ESA's next X-ray observatory Athena. It combines unprecedented survey power through its large field of view of 40 amin x 40 amin together with excellent count rate capability (larger than 1 Crab). The energy resolution of the silicon sensor is state-of-the-art in the energy band of interest from 0.2 keV to 15 keV, e.g. the full width at half maximum of a line at 7 keV will be better than 170 eV until the end of the nominal mission phase. This performance is accomplished by using DEPFET active pixel sensors with a pixel size of 130 x 130 microns is well suited to the on-axis angular resolution of 5 arcsec half energy width (HEW) of the mirror system. Each DEPFET pixel is a combined sensor-amplifier structure with a MOSFET integrated onto a fully depleted 450 micron thick silicon bulk. Two detectors are planned for the WFI instrument: A large-area detector comprising four sensors with a total of 1024 x 1024 pixels and a fast detector optimized for high count rate observations. This high count rate capable detector permits for bright point sources with an intensity of 1 Crab a throughput of more than 80% and a pile-up of less than 1 %. The fast readout of the DEPFET pixel matrices is facilitated by an ASIC development, called VERITAS-2. Together with the Switcher-A, a control ASIC that allows for operation of the DEPFET in rolling shutter mode, these elements form the key components of the WFI detectors. The detectors are surrounded by a graded-Z shield, which has in particular the purpose to avoid fluorescence lines that would contribute to the instrument background...[Abridged] ",Kein DOI-Link verfügbar,1702.01079v1,No,
0000-0003-4075-984X,Markus Plattner,Hochschule München Universität der Angewandte Wissenschaften,The metrology system of the VLTI instrument GRAVITY,2016,"  The VLTI instrument GRAVITY combines the beams from four telescopes and provides phase-referenced imaging as well as precision-astrometry of order 10 microarcseconds by observing two celestial objects in dual-field mode. Their angular separation can be determined from their differential OPD (dOPD) when the internal dOPDs in the interferometer are known. Here, we present the general overview of the novel metrology system which performs these measurements. The metrology consists of a three-beam laser system and a homodyne detection scheme for three-beam interference using phase-shifting interferometry in combination with lock-in amplifiers. Via this approach the metrology system measures dOPDs on a nanometer-level. ",https://doi.org/10.1117/12.2232272,1608.04888v1,No,
0000-0003-4075-984X,Markus Plattner,Hochschule München Universität der Angewandte Wissenschaften,ERIS: revitalising an adaptive optics instrument for the VLT,2018,"  ERIS is an instrument that will both extend and enhance the fundamental diffraction limited imaging and spectroscopy capability for the VLT. It will replace two instruments that are now being maintained beyond their operational lifetimes, combine their functionality on a single focus, provide a new wavefront sensing module that makes use of the facility Adaptive Optics System, and considerably improve their performance. The instrument will be competitive with respect to JWST in several regimes, and has outstanding potential for studies of the Galactic Center, exoplanets, and high redshift galaxies. ERIS had its final design review in 2017, and is expected to be on sky in 2020. This contribution describes the instrument concept, outlines its expected performance, and highlights where it will most excel. ",Kein DOI-Link verfügbar,1807.05089v1,Yes,potent(1)
0000-0003-4075-984X,Markus Plattner,Hochschule München Universität der Angewandte Wissenschaften,"The MICADO first light imager for the ELT: overview, operation,   simulation",2018,"  MICADO will enable the ELT to perform diffraction limited near-infrared observations at first light. The instrument's capabilities focus on imaging (including astrometric and high contrast) as well as single object spectroscopy. This contribution looks at how requirements from the observing modes have driven the instrument design and functionality. Using examples from specific science cases, and making use of the data simulation tool, an outline is presented of what we can expect the instrument to achieve. ",Kein DOI-Link verfügbar,1807.10003v1,No,
0000-0002-1314-8157,Georg Braun,München Universität der Angewandte Wissenschaften,On Supercritical Branching Processes with Emigration,2020,"  We study supercritical branching processes under the influence of an i.i.d. emigration component. We provide conditions, under which the lifetime of the process is finite respectively has a finite expectation. A new version of the Kesten-Stigum theorem is obtained and the extinction probability for a large initial population size is related to the tail behaviour of the emigration. ",Kein DOI-Link verfügbar,2008.05178v1,No,
0000-0002-1314-8157,Georg Braun,München Universität der Angewandte Wissenschaften,On the Growth of a Ballistic Deposition Model on Finite Graphs,2020,"  We revisit a ballistic deposition process introduced by Atar, Athreya and Kang. Let $\mathcal{G}=(V,E)$ be a finite connected graph. We choose independently and uniformly vertices in $\mathcal{G}$. If a vertex $x$ is chosen and the previous height configuration is given by $h=(h_y)_{y \in V} \in \mathbb{N}_0^V$, the height $h_x$ is replaced by \[ \tilde{h}_x := 1 + \max_{y \sim x} h_y. \] We study asymptotic properties of this growth model. We determine the asymptotic growth parameter $\gamma(\mathcal{G} )$ for some graphs and prove a central limit theorem for the fluctuations around $\gamma ( \mathcal{G})$. We also give a new graph-theoretic interpretation of an inequality obtained by Atar et al.. ",Kein DOI-Link verfügbar,2001.09836v1,No,
0000-0002-1314-8157,Georg Braun,München Universität der Angewandte Wissenschaften,Boolean percolation on digraphs and random exchange processes,2021,"  We study, in a general graph-theoretic formulation, a long-range percolation model introduced by Lamperti. For various underlying directed graphs, we discuss connections between this model and random exchange processes. We clarify, for $n \in \mathbb{N}$, under which conditions the lattices $\mathbb{N}_0^n$ and $\mathbb{Z}^n$ are essentially covered in this model. Moreover, for all $n \geq 2$, we establish that it is impossible to cover the directed $n$-ary tree in our model. ",Kein DOI-Link verfügbar,2111.04772v2,No,
0000-0002-8217-6055,Martin Leitner,München Universität der Angewandte Wissenschaften,Heliospheric Evolution of Magnetic Clouds,2019,"  Interplanetary evolution of eleven magnetic clouds (MCs) recorded by at least two radially aligned spacecraft is studied. The in situ magnetic field measurements are fitted to a cylindrically symmetric Gold-Hoyle force-free uniform-twist flux-rope configuration. The analysis reveals that in a statistical sense the expansion of studied MCs is compatible with self-similar behavior. However, individual events expose a large scatter of expansion rates, ranging from very weak to very strong expansion. Individually, only four events show an expansion rate compatible with the isotropic self-similar expansion. The results indicate that the expansion has to be much stronger when MCs are still close to the Sun than in the studied 0.47 - 4.8 AU distance range. The evolution of the magnetic field strength shows a large deviation from the behavior expected for the case of an isotropic self-similar expansion. In the statistical sense, as well as in most of the individual events, the inferred magnetic field decreases much slower than expected. Only three events show a behavior compatible with a self-similar expansion. There is also a discrepancy between the magnetic field decrease and the increase of the MC size, indicating that magnetic reconnection and geometrical deformations play a significant role in the MC evolution. About half of the events show a decay of the electric current as expected for the self-similar expansion. Statistically, the inferred axial magnetic flux is broadly consistent with it remaining constant. However, events characterized by large magnetic flux show a clear tendency of decreasing flux. ",https://doi.org/10.3847/1538-4357/ab190a,1904.08266v1,No,
0000-0002-8217-6055,Martin Leitner,München Universität der Angewandte Wissenschaften,Unusual plasma and particle signatures at Mars and STEREO-A related to   CME-CME interaction,2019,"  On July 25 2017 a multi-step Forbush decrease (FD) with the remarkable total amplitude of more than 15\% was observed by MSL/RAD at Mars. We find that these particle signatures are related to very pronounced plasma and magnetic field signatures detected in situ by STEREO-A on July 24 2017, with a higher than average total magnetic field strength reaching more than 60 nT. In the observed time period STEREO-A was at a relatively small longitudinal separation (46 degrees) to Mars and both were located at the back side of the Sun as viewed from Earth. We analyse a number of multi-spacecraft and multi-instrument (both in situ and remote-sensing) observations, and employ modelling to understand these signatures. We find that the solar sources are two CMEs which erupted on July 23 2017 from the same source region on the back side of the Sun as viewed from Earth. Moreover, we find that the two CMEs interact non-uniformly, inhibiting the expansion of one of the CMEs in STEREO-A direction, whereas allowing it to expand more freely in the Mars direction. The interaction of the two CMEs with the ambient solar wind adds up to the complexity of the event, resulting in a long, sub-structured interplanetary disturbance at Mars, where different sub-structures correspond to different steps of the FD, adding-up to a globally large-amplitude FD. ",https://doi.org/10.3847/1538-4357/ab27ca,1906.02532v1,No,
0000-0002-8217-6055,Martin Leitner,München Universität der Angewandte Wissenschaften,Linking remote imagery of a coronal mass ejection to its in situ   signatures at 1 AU,2009,"  In a case study (June 6-7, 2008) we report on how the internal structure of a coronal mass ejection (CME) at 1 AU can be anticipated from remote observations of white-light images of the heliosphere. Favorable circumstances are the absence of fast equatorial solar wind streams and a low CME velocity which allow us to relate the imaging and in-situ data in a straightforward way. The STEREO-B spacecraft encountered typical signatures of a magnetic flux rope inside an interplanetary CME (ICME) whose axis was inclined at 45 degree to the solar equatorial plane. Various CME direction-finding techniques yield consistent results to within 15 degree. Further, remote images from STEREO-A show that (1) the CME is unambiguously connected to the ICME and can be tracked all the way to 1 AU, (2) the particular arc-like morphology of the CME points to an inclined axis, and (3) the three-part structure of the CME may be plausibly related to the in situ data. This is a first step in predicting both the direction of travel and the internal structure of CMEs from complete remote observations between the Sun and 1 AU, which is one of the main requirements for forecasting the geo-effectiveness of CMEs. ",https://doi.org/10.1088/0004-637X/705/2/L180,0910.1188v1,No,
0000-0002-8970-9264,Michael Krämer,München Universität der Angewandte Wissenschaften,Inelastic $J/ψ$ photoproduction,1995,"  Inelastic photoproduction of $J/\psi$ particles at high energies is one of the processes to determine the gluon distribution in the nucleon. The QCD radiative corrections to the color-singlet model of this reaction have recently been calculated. They are large at moderate photon energies, but decrease with increasing energies. I compare the cross section and the ${J/\psi}$ energy spectrum with the available fixed-target photoproduction data. Predictions for the HERA energy range are given which demonstrate the sensitivity of the result to the parametrization of the gluon distribution in the small-$x$ region. (Talk presented at the Workshop on ""Heavy Quark Physics"", Bad Honnef, FRG, Dec. 1994) ",Kein DOI-Link verfügbar,hep-ph/9504255v1,No,
0000-0002-8970-9264,Michael Krämer,München Universität der Angewandte Wissenschaften,Photoproduction of Heavy Quarks,1995,"  Heavy quarks are copiously produced in two-photon collisions at $e^+e^-$ colliders. The theoretical predictions including QCD radiative corrections are compared to recent experimental data on $\gamma\gamma$ production of charm quarks at PETRA, PEP, TRISTAN and LEP. Photoproduction of heavy quarks at HERA is an important tool to measure the gluon distribution in the proton. New theoretical results on heavy quark photoproduction at large transverse momenta are discussed and NLO predictions for inelastic $J/\psi$ photoproduction in the HERA energy range are given. The sensitivity of the results to the parametrization of the gluon distribution in the small-$x$ region is demonstrated. ",Kein DOI-Link verfügbar,hep-ph/9505380v1,No,
0000-0002-8970-9264,Michael Krämer,München Universität der Angewandte Wissenschaften,Quarkonium Production at High-Energy Colliders,2001,"  The theoretical description of heavy quarkonium production at high-energy p-pbar and e-p colliders is reviewed. Predictions based on non-relativistic QCD factorisation are confronted with recent charmonium and bottomonium data from the Tevatron and HERA. Potential shortcomings of the present theoretical analyses are discussed, and the prospects for quarkonium physics at the upgraded Tevatron and HERA colliders and at the LHC are summarised. ",https://doi.org/10.1016/S0146-6410(01)00154-5,hep-ph/0106120v1,Yes,potent(1)
0000-0002-8970-9264,Michael Krämer,München Universität der Angewandte Wissenschaften,Associated Higgs production with bottom quarks at hadron colliders,2004,"  Higgs-boson production in association with bottom quarks is an important discovery channel for supersymmetric Higgs particles at the Tevatron and the LHC. We present higher-order QCD predictions for inclusive cross sections and for the production of a Higgs boson in association with high-p_T bottom quarks. We compare calculations performed in a four-flavour scheme based on the parton processes gg,qqbar -> b bbar H with five-flavour scheme calculations based on bottom-quark scattering. ",https://doi.org/10.1016/j.nuclphysbps.2004.09.037,hep-ph/0407080v1,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Irregular Sampling of the Radon Transform of Bandlimited Functions,2013,"  We provide conditions for exact reconstruction of a bandlimited function from irregular polar samples of its Radon transform. First, we prove that the Radon transform is a continuous L2-operator for certain classes of bandlimited signals. We then show that the Beurling-Malliavin condition for the radial sampling density ensures existence and uniqueness of a solution. Moreover, Jaffard's density condition is sufficient for stable reconstruction. ",Kein DOI-Link verfügbar,1307.1151v1,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Complexity $L^0$-penalized M-Estimation: Consistency in More Dimensions,2013,"  We study the asymptotics in $L^2$ for complexity penalized least squares regression for the discrete approximation of finite-dimensional signals on continuous domains - e.g. images - by piecewise smooth functions.   We introduce a fairly general setting which comprises most of the presently popular partitions of signal- or image- domains like interval-, wedgelet- or related partitions, as well as Delaunay triangulations. Then we prove consistency and derive convergence rates. Finally, we illustrate by way of relevant examples that the abstract results are useful for many applications. ",Kein DOI-Link verfügbar,1301.5492v2,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,The L1-Potts functional for robust jump-sparse reconstruction,2012,"  We investigate the non-smooth and non-convex $L^1$-Potts functional in discrete and continuous time. We show $\Gamma$-convergence of discrete $L^1$-Potts functionals towards their continuous counterpart and obtain a convergence statement for the corresponding minimizers as the discretization gets finer. For the discrete $L^1$-Potts problem, we introduce an $O(n^2)$ time and $O(n)$ space algorithm to compute an exact minimizer. We apply $L^1$-Potts minimization to the problem of recovering piecewise constant signals from noisy measurements $f.$ It turns out that the $L^1$-Potts functional has a quite interesting blind deconvolution property. In fact, we show that mildly blurred jump-sparse signals are reconstructed by minimizing the $L^1$-Potts functional. Furthermore, for strongly blurred signals and known blurring operator, we derive an iterative reconstruction algorithm. ",https://doi.org/10.1137/120896256,1207.4642v3,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Signal Analysis based on Complex Wavelet Signs,2012,"  We propose a signal analysis tool based on the sign (or the phase) of complex wavelet coefficients, which we call a signature. The signature is defined as the fine-scale limit of the signs of a signal's complex wavelet coefficients. We show that the signature equals zero at sufficiently regular points of a signal whereas at salient features, such as jumps or cusps, it is non-zero. At such feature points, the orientation of the signature in the complex plane can be interpreted as an indicator of local symmetry and antisymmetry. We establish that the signature rotates in the complex plane under fractional Hilbert transforms. We show that certain random signals, such as white Gaussian noise and Brownian motions, have a vanishing signature. We derive an appropriate discretization and show the applicability to signal analysis. ",https://doi.org/10.1016/j.acha.2015.08.005,1208.4578v2,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Jump-sparse and sparse recovery using Potts functionals,2013,"  We recover jump-sparse and sparse signals from blurred incomplete data corrupted by (possibly non-Gaussian) noise using inverse Potts energy functionals. We obtain analytical results (existence of minimizers, complexity) on inverse Potts functionals and provide relations to sparsity problems. We then propose a new optimization method for these functionals which is based on dynamic programming and the alternating direction method of multipliers (ADMM). A series of experiments shows that the proposed method yields very satisfactory jump-sparse and sparse reconstructions, respectively. We highlight the capability of the method by comparing it with classical and recent approaches such as TV minimization (jump-sparse signals), orthogonal matching pursuit, iterative hard thresholding, and iteratively reweighted $\ell^1$ minimization (sparse signals). ",https://doi.org/10.1109/TSP.2014.2329263,1304.4373v2,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Total variation regularization for manifold-valued data,2013,"  We consider total variation minimization for manifold valued data. We propose a cyclic proximal point algorithm and a parallel proximal point algorithm to minimize TV functionals with $\ell^p$-type data terms in the manifold case. These algorithms are based on iterative geodesic averaging which makes them easily applicable to a large class of data manifolds. As an application, we consider denoising images which take their values in a manifold. We apply our algorithms to diffusion tensor images, interferometric SAR images as well as sphere and cylinder valued images. For the class of Cartan-Hadamard manifolds (which includes the data space in diffusion tensor imaging) we show the convergence of the proposed TV minimizing algorithms to a global minimizer. ",https://doi.org/10.1137/130951075,1312.7710v1,No,
0000-0002-1228-9396,Laurent Demaret,München Universität der Angewandte Wissenschaften,Mumford-Shah and Potts Regularization for Manifold-Valued Data with   Applications to DTI and Q-Ball Imaging,2014,"  Mumford-Shah and Potts functionals are powerful variational models for regularization which are widely used in signal and image processing; typical applications are edge-preserving denoising and segmentation. Being both non-smooth and non-convex, they are computationally challenging even for scalar data. For manifold-valued data, the problem becomes even more involved since typical features of vector spaces are not available. In this paper, we propose algorithms for Mumford-Shah and for Potts regularization of manifold-valued signals and images. For the univariate problems, we derive solvers based on dynamic programming combined with (convex) optimization techniques for manifold-valued data. For the class of Cartan-Hadamard manifolds (which includes the data space in diffusion tensor imaging), we show that our algorithms compute global minimizers for any starting point. For the multivariate Mumford-Shah and Potts problems (for image regularization) we propose a splitting into suitable subproblems which we can solve exactly using the techniques developed for the corresponding univariate problems. Our method does not require any a priori restrictions on the edge set and we do not have to discretize the data space. We apply our method to diffusion tensor imaging (DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentation of the corpus callosum. ",https://doi.org/10.1007/s10851-015-0628-2,1410.1699v1,No,
0000-0002-2430-1655,Markus Lindner,München Universität der Angewandte Wissenschaften,Single Photon Counting X-ray Imaging with Si and CdTe Single Chip Pixel   Detectors and Multichip Pixel Modules,2003,"  Multichip modules (MCM) with 4 single photon counting MPEC 2.3 chips bump bonded to 1.3 cm x 1.3 cm large CdTe and Si semiconductor sensors as well as to single chip pixel detectors have been successfully built and operated. The MPEC 2.3 chip provides a pixel count rate up to 1 MHz with a large dynamic range of 18 bit, 2 counters and energy windowing with continuously adjustable thresholds. Each MPEC has 32 x 32 pixels of 200 um x 200 um pixel size. For a MCM the 4 chips are arranged in a 2 x 2 array which leads to a 64 x 64 sensor pixel geometry. The MCM construction is described, and the imaging performance of the different detectors is shown. As readout system a newly developed USB system has been used. ",https://doi.org/10.1109/TNS.2004.832610,physics/0312070v1,No,
0000-0002-4272-8353,Thomas Herzog,München Universität der Angewandte Wissenschaften,Wavelength-tunable open double-microcavity to enhance two closely spaced   optical transitions,2022,"  Microcavities have long been recognized as indispensable elements in quantum photonic research due to their usefulness for enhanced light extraction and light-matter interaction. A conventional high-Q cavity structure typically allows only a single optical transition to be tuned into resonance with a specific mode. The transition to a more advanced double-cavity structure, however, introduces new and interesting possibilities such as enhancing two spectrally close optical transitions at the same time with two distinct cavity modes. Here, we investigate a cavity structure composed of a monolithic planar cavity enclosed between two semiconductor distributed Bragg reflectors (DBR) and a top dielectric mirror deposited on a fiber tip. While the bottom cavity is formed by the two DBRs, the mirror on the fiber tip and the top DBR of the semiconductor chip create a second tunable cavity. These coupled cavities exhibit mode hybridization when tuned into resonance and their splitting can be adjusted to match with the spectral separation of closely spaced optical transitions by a suitable sample design. Furthermore, we report on the simultaneous resonance tuning of the exciton and biexciton transition of a semiconductor quantum dot, each to a separate mode of the open fiber-based double cavity. Decay time measurements at simultaneous resonance showed a Purcell-factor of $F_P^X$=1.9$\pm$0.4 for the exciton transition. ",Kein DOI-Link verfügbar,2208.14790v2,No,
0000-0002-4272-8353,Thomas Herzog,München Universität der Angewandte Wissenschaften,3D printed micro-optics for quantum technology: Optimized coupling of   single quantum dot emission into a single mode fiber,2020,"  Future quantum technology relies crucially on building quantum networks with high fidelity. To achieve this challenging goal, it is of utmost importance to connect single quantum systems in a way such that their emitted single-photons overlap with the highest possible degree of coherence. This requires perfect mode overlap of the emitted light of different emitters, which necessitates the use of single mode fibers. Here we present an advanced manufacturing approach to accomplish this task: we combine 3D printed complex micro-optics such as hemispherical and Weierstrass solid immersion lenses as well as total internal reflection solid immersion lenses on top of single InAs quantum dots with 3D printed optics on single mode fibers and compare their key features. Interestingly, the use of hemispherical solid immersion lenses further increases the localization accuracy of the emitters to below 1 nm when acquiring micro-photoluminescence maps. The system can be joined together and permanently fixed. This integrated system can be cooled by dipping into liquid helium, by a Stirling cryocooler or by a closed-cycle helium cryostat without the necessity for optical windows, as all access is through the integrated single mode fiber. We identify the ideal optical designs and present experiments that prove excellent high-rate single-photon emission by high-contrast Hanbury Brown and Twiss experiments. ",Kein DOI-Link verfügbar,2007.06369v1,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,An euclidean affine invariant of quadrilaterals,2010,"  A certain real number, depending on two neighbouring sides of a quadrilateral and the diagonal meeting these two sides at their common point, is shown to be invariant under affinity. As an application we demonstrate a nice formula for the area of a finite sector at centre of a planar quadric with point symmetry. ",Kein DOI-Link verfügbar,1004.2829v3,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,The loss value of multilinear regression,2022,"  Determinant formulas are presented for: a certain positive semidefinite, hermitian matrix; the loss value of multilinear regression; the multiple linear regression coefficient. ",Kein DOI-Link verfügbar,2204.02686v4,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,On the final limit of a transition matrix,2018,  For a finite intensity matrix $B$ the final limit of its transition matrix $\exp(t B)$ exists. This is a well-known fact in the realm of continous-time Markov processes where it is proven by probability theoretic means. A simple proof is presented with help of a Tauberian theorem of complex analytic functions which is used also in \cite{Newman} to proof the prime number theorem. Furthermore the final limit is computed. ,Kein DOI-Link verfügbar,1901.01108v4,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,The area of a sector at centre of a planar quadric,2009,"  Three linearly dependent and pairwise linearly independent vectors of an euclidian space uniquely determine a planar quadric with symmetry centre in the origin. A rather simple formula for the area of an arbitrary sector at centre of such a quadric will be shown by classical methods. The formula describes that area in dependence of   1. the lengths of the two straight lines that bound the sector at two sides,   2. the length of an arbitrary straight line from the centre to the quadric arc that bounds the sector at the third side,   3. the two angles in between these three straight lines. ",Kein DOI-Link verfügbar,0903.5448v7,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,Measuring quadric sectors at centre,2010,"  Sectors at centre of affine quadrics with point symmetry are investigated over arbitrary fields of characteristic different from two. As an application we demonstrate nice formulas for the area and the volume of such planar and spatial sectors, respectively, in euclidean space. It seems that up to now there has been atmost little research in this field up to very special cases. ",Kein DOI-Link verfügbar,1007.0152v14,No,
0000-0002-1572-0364,Helmut Kahl,München Universität der Angewandte Wissenschaften,Symmetric Matrices: Theory and Applications,2014,  This text is a survey on symmetric matrices. It serves as a script for a module to be taught at university. ,Kein DOI-Link verfügbar,1408.5923v22,No,
0000-0002-5157-8799,Rainer Fischer,München Universität der Angewandte Wissenschaften,Bayesian mixture models for Poisson astronomical images,2012,"  Astronomical images in the Poisson regime are typically characterized by a spatially varying cosmic background, large variety of source morphologies and intensities, data incompleteness, steep gradients in the data, and few photon counts per pixel. The Background-Source separation technique is developed with the aim to detect faint and extended sources in astronomical images characterized by Poisson statistics. The technique employs Bayesian mixture models to reliably detect the background as well as the sources with their respective uncertainties. Background estimation and source detection is achieved in a single algorithm. A large variety of source morphologies is revealed. The technique is applied in the X-ray part of the electromagnetic spectrum on ROSAT and Chandra data sets and it is under a feasibility study for the forthcoming eROSITA mission. ",Kein DOI-Link verfügbar,1202.0390v1,No,
0000-0002-5157-8799,Rainer Fischer,München Universität der Angewandte Wissenschaften,"GPEC, a real-time capable Tokamak equilibrium code",2015,"  A new parallel equilibrium reconstruction code for tokamak plasmas is presented. GPEC allows to compute equilibrium flux distributions sufficiently accurate to derive parameters for plasma control within 1 ms of runtime which enables real-time applications at the ASDEX Upgrade experiment (AUG) and other machines with a control cycle of at least this size. The underlying algorithms are based on the well-established offline-analysis code CLISTE, following the classical concept of iteratively solving the Grad-Shafranov equation and feeding in diagnostic signals from the experiment. The new code adopts a hybrid parallelization scheme for computing the equilibrium flux distribution and extends the fast, shared-memory-parallel Poisson solver which we have described previously by a distributed computation of the individual Poisson problems corresponding to different basis functions. The code is based entirely on open-source software components and runs on standard server hardware and software environments. The real-time capability of GPEC is demonstrated by performing an offline-computation of a sequence of 1000 flux distributions which are taken from one second of operation of a typical AUG discharge and deriving the relevant control parameters with a time resolution of a millisecond. On current server hardware the new code allows employing a grid size of 32x64 zones for the spatial discretization and up to 15 basis functions. It takes into account about 90 diagnostic signals while using up to 4 equilibrium iterations and computing more than 20 plasma-control parameters, including the computationally expensive safety-factor q on at least 4 different levels of the normalized flux. ",https://doi.org/10.13182/FST15-154,1511.04203v2,No,
0000-0002-6672-0145,David Spieler,München Universität der Angewandte Wissenschaften,Bounding the Equilibrium Distribution of Markov Population Models,2010,"  Arguing about the equilibrium distribution of continuous-time Markov chains can be vital for showing properties about the underlying systems. For example in biological systems, bistability of a chemical reaction network can hint at its function as a biological switch. Unfortunately, the state space of these systems is infinite in most cases, preventing the use of traditional steady state solution techniques. In this paper we develop a new approach to tackle this problem by first retrieving geometric bounds enclosing a major part of the steady state probability mass, followed by a more detailed analysis revealing state-wise bounds. ",Kein DOI-Link verfügbar,1007.3130v1,No,
0000-0002-6672-0145,David Spieler,München Universität der Angewandte Wissenschaften,Parameter Identification for Markov Models of Biochemical Reactions,2011,"  We propose a numerical technique for parameter inference in Markov models of biological processes. Based on time-series data of a process we estimate the kinetic rate constants by maximizing the likelihood of the data. The computation of the likelihood relies on a dynamic abstraction of the discrete state space of the Markov model which successfully mitigates the problem of state space largeness. We compare two variants of our method to state-of-the-art, recently published methods and demonstrate their usefulness and efficiency on several case studies from systems biology. ",Kein DOI-Link verfügbar,1102.2819v1,No,
0000-0002-6672-0145,David Spieler,München Universität der Angewandte Wissenschaften,Model Checking CSL for Markov Population Models,2011,"  Markov population models (MPMs) are a widely used modelling formalism in the area of computational biology and related areas. The semantics of a MPM is an infinite-state continuous-time Markov chain. In this paper, we use the established continuous stochastic logic (CSL) to express properties of Markov population models. This allows us to express important measures of biological systems, such as probabilistic reachability, survivability, oscillations, switching times between attractor regions, and various others. Because of the infinite state space, available analysis techniques only apply to a very restricted subset of CSL properties. We present a full algorithm for model checking CSL for MPMs, and provide experimental evidence showing that our method is effective. ",https://doi.org/10.4204/EPTCS.154.7,1111.4385v2,No,
0000-0002-6672-0145,David Spieler,München Universität der Angewandte Wissenschaften,"Deep learning for brake squeal: vibration detection, characterization   and prediction",2020,"  Despite significant advances in modeling of friction-induced vibrations and brake squeal, the majority of industrial research and design is still conducted experimentally, since many aspects of squeal and its mechanisms involved remain unknown. We report here for the first time on novel strategies for handling data-intensive vibration testings to gain better insights into friction brake system vibrations and noise generation mechanisms. Machine learning-based methods to detect and characterize vibrations, to understand sensitivities and to predict brake squeal are applied with the aim to illustrate how interdisciplinary approaches can leverage the potential of data science techniques for classical mechanical engineering challenges. In the first part, a deep learning brake squeal detector is developed to identify several classes of typical friction noise recordings. The detection method is rooted in recent computer vision techniques for object detection based on convolutional neural networks. It allows to overcome limitations of classical approaches that solely rely on instantaneous spectral properties of the recorded noise. Results indicate superior detection and characterization quality when compared to a state-of-the-art brake squeal detector. In the second part, a recurrent neural network is employed to learn the parametric patterns that determine the dynamic stability of an operating brake system. Given a set of multivariate loading conditions, the RNN learns to predict the noise generation of the structure. The validated RNN represents a virtual twin model for the squeal behavior of a specific brake system. It is found that this model can predict the occurrence and the onset of brake squeal with high accuracy and that it can identify the complicated patterns and temporal dependencies in the loading conditions that drive the dynamical structure into regimes of instability. ",Kein DOI-Link verfügbar,2001.01596v2,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Uncertainties and Ambiguities in Percentiles and how to Avoid Them,2012,  The recently proposed fractional scoring scheme is used to attribute publications to percentile rank classes. It is shown that in this way uncertainties and ambiguities in the evaluation of percentile ranks do not occur. Using the fractional scoring the total score of all papers exactly reproduces the theoretical value. ,Kein DOI-Link verfügbar,1205.3588v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,How to improve the outcome of performance evaluations in terms of   percentiles for citation frequencies of my papers,2014,  Using empirical data I demonstrate that the result of performance evaluations by percentiles can be drastically influenced by the proper choice of the journal in which a manuscript is published. ,https://doi.org/10.1016/j.joi.2014.09.002,1408.2946v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Self-citation corrections for the Hirsch index,2007,"  I propose to sharpen the index h, proposed by Hirsch as a useful index to characterize the scientific output of a researcher, by excluding the self-citations. Performing a self-experiment and also analyzing two anonymous data sets, it is shown that self-citations can significantly reduce the $h$ index in contrast to Hirsch's expectations. This result is confirmed by an analysis of 13 further data sets. ",Kein DOI-Link verfügbar,physics/0701231v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The influence of self-citation corrections on Egghe's g index,2007,"  The g index was introduced by Leo Egghe as an improvement of Hirsch's index h for measuring the overall citation record of a set of articles. It better takes into account the highly skewed frequency distribution of citations than the h index. I propose to sharpen this g index by excluding the self-citations. I have worked out nine practical cases in physics and compare the h and g values with and without self-citations. As expected, the g index characterizes the data set better than the h index. The influence of the self-citations appears to be more significant for the g index than for the h index. ",https://doi.org/10.1007/s11192-007-1886-6,0707.4577v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A case study of the Hirsch index for 26 non-prominent physicists,2007,"  The h index was introduced by Hirsch to quantify an individual's scientific research output. It has been widely used in different fields to show the relevance of the research work of prominent scientists. I have worked out 26 practical cases of physicists which are not so prominent. Therefore this case study should be more relevant to discuss various features of the Hirsch index which are interesting or disturbing or both for the more average situation. In particular, I investigate quantitatively some pitfalls in the evaluation and the influence of self-citations. ",https://doi.org/10.1002/andp.200710252,0708.0120v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A modification of the h-index: the hm-index accounts for multi-authored   manuscripts,2008,"  In order to take multiple co-authorship appropriately into account, a straightforward modification of the Hirsch index was recently proposed. Fractionalised counting of the papers yields an appropriate measure which is called the hm-index. The effect of this procedure is compared in the present work with other variants of the h-index and found to be superior to the fractionalised counting of citations and to the normalization of the h-index with the average number of authors in the h-core. Three fictitious examples for model cases and one empirical case are analysed. ",Kein DOI-Link verfügbar,0805.2000v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,How relevant is the predictive power of the h-index? A case study of the   time-dependent Hirsch index,2013,  The h-index has been shown to have predictive power. Here I report results of an empirical study showing that the increase of the h-index with time often depends for a long time on citations to rather old publications. This inert behavior of the h-index means that it is difficult to use it as a measure for predicting future scientific output. ,https://doi.org/10.1016/j.joi.2013.01.001,1301.2060v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Do we need the g-index?,2013,  Using a very small sample of 8 datasets it was recently shown by De Visscher (2011) that the g-index is very close to the square root of the total number of citations. It was argued that there is no bibliometrically meaningful difference. Using another somewhat larger empirical sample of 26 datasets I show that the difference may be larger and I argue in favor of the g-index. ,Kein DOI-Link verfügbar,1301.4028v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Inconsistencies of the Highly-Cited-Publications Indicator,2013,"  One way of evaluating individual scientists is the determination of the number of highly cited publications, where the threshold is given by a large reference set. It is shown that this indicator behaves in a counterintuitive way, leading to inconsistencies in the ranking of different scientists. ",https://doi.org/10.1002/asi.22824,1302.6391v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The predictability of the Hirsch index evolution,2013,"  The h-index can be used as a predictor of itself. However, the evolution of the h-index with time is shown in the present investigation to be dominated for several years by citations to previous publications rather than by new scientific achievements. This inert behaviour of the h-index raises questions, whether the h-index can be used profitably in academic appointment processes or for the allocation of research resources. ",Kein DOI-Link verfügbar,1307.5964v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A variant of the h-index to measure recent performance,2014,"  The predictive power of the h-index has been shown to depend for a long time on citations to rather old publications. This has raised doubts about its usefulness for predicting future scientific achievements. Here I investigate a variant which considers only the recent publications and is therefore more useful in academic hiring processes and for the allocation of research resources. It is simply defined in analogy to the usual h-index, but taking into account only the publications from recent years, and it can easily be determined from the ISI Web of Knowledge. ",https://doi.org/10.1002/asi.23438,1409.3379v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Is the new citation-rank approach P100' in bibliometrics really new?,2014,  The percentile-based rating scale P100 describes the citation impact in terms of the distribution of unique citation values. This approach has recently been refined by considering also the frequency of papers with the same citation counts. Here I compare the resulting P100' with P100 for an empirical dataset and a simple fictitious model dataset. It is shown that P100' is not much different from standard percentile-based ratings in terms of citation frequencies. A new indicator P100'' is introduced. ,https://doi.org/10.1016/j.joi.2014.10.001,1409.4899v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Restricting the h-index to a citation time window: A case study of a   timed Hirsch index,2014,  The h-index has been shown to increase in many cases mostly because of citations to rather old publications. This inertia can be circumvented by restricting the evaluation to a citation time window. Here I report results of an empirical study analyzing the evolution of the thus defined timed h-index in dependence on the length of the citation time window. ,https://doi.org/10.1016/j.joi.2014.12.005,1412.5050v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A Case Study of the Modified Hirsch Index hm Accounting for Multiple   Co-authors,2009,"  J. E. Hirsch (2005) introduced the h-index to quantify an individual's scientific research output by the largest number h of a scientist's papers, that received at least h citations. This so-called Hirsch index can be easily modified to take multiple co-authorship into account by counting the papers fractionally according to (the inverse of) the number of authors. I have worked out 26 empirical cases of physicists to illustrate the effect of this modification. Although the correlation between the original and the modified Hirsch index is relatively strong, the arrangement of the datasets is significantly different depending on whether they are put into order according to the values of either the original or the modified index. ",https://doi.org/10.1002/asi.21057,0903.4960v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Twenty Hirsch index variants and other indicators giving more or less   preference to highly cited papers,2010,"  The Hirsch index or h-index is widely used to quantify the impact of an individual's scientific research output, determining the highest number h of a scientist's papers that received at least h citations. Several variants of the index have been proposed in order to give more or less preference to highly cited papers. I analyse the citation records of 26 physicists discussing various suggestions, in particular A, e, f, g, h(2), h_w, h_T, \hbar, m, {\pi}, R, s, t, w, and maxprod. The total number of all and of all cited publications as well as the highest and the average number of citations are also compared. Advantages and disadvantages of these indices and indicators are discussed. Correlation coefficients are determined quantifying which indices and indicators yield similar and which yield more deviating rankings of the 26 datasets. For 6 datasets the determination of the indices and indicators is visualized. ",https://doi.org/10.1002/andp.201000046,1005.5227v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Empirical Evidence for the Relevance of Fractional Scoring in the   Calculation of Percentile Rank Scores,2013,  Fractional scoring has been proposed to avoid inconsistencies in the attribution of publications to percentile rank classes. Uncertainties and ambiguities in the evaluation of percentile ranks can be demonstrated most easily with small datasets. But for larger datasets an often large number of papers with the same citation count leads to the same uncertainties and ambiguities which can be avoided by fractional scoring. This is demonstrated for four different empirical datasets with several thousand publications each which are assigned to 6 percentile rank classes. Only by utilizing fractional scoring the total score of all papers exactly reproduces the theoretical value in each case. ,https://doi.org/10.1002/asi.22774,1301.7192v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,How much do different ways of calculating percentiles influence the   derived performance indicators? - A case study,2013,"  Bibliometric indicators can be determined by comparing specific citation records with the percentiles of a reference set. However, there exists an ambiguity in the computation of percentiles because usually a significant number of papers with the same citation count are found at the border between percentile rank classes. The present case study of the citations to the journal Europhysics Letters (EPL) in comparison with all physics papers from the Web of Science shows the deviations which occur due to the different ways of treating the tied papers in the evaluation of the percentage of highly cited publications. A strong bias can occur, if the papers tied at the threshold number of citations are all considered as highly cited or all considered as not highly cited. ",https://doi.org/10.1007/s11192-013-0984-x,1302.6392v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,How to derive an advantage from the arbitrariness of the g-index,2013,"  The definition of the g-index is as arbitrary as that of the h-index, because the threshold number g^2 of citations to the g most cited papers can be modified by a prefactor at one's discretion, thus taking into account more or less of the highly cited publications within a dataset. In a case study I investigate the citation records of 26 physicists and show that the prefactor influences the ranking in terms of the generalized g-index less than for the generalized h-index. I propose specifically a prefactor of 2 for the g-index, because then the resulting values are of the same order of magnitude as for the common h-index. In this way one can avoid the disadvantage of the original g-index, namely that the values are usually substantially larger than for the h-index and thus the precision problem is substantially larger; while the advantages of the g-index over the h-index are kept. Like for the generalized h-index, also for the generalized g-index different prefactors might be more useful for investigations which concentrate only on top scientists with high citation frequencies or on junior researchers with small numbers of citations. ",https://doi.org/10.1016/j.joi.2013.02.003,1302.6396v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A Case Study of the Arbitrariness of the h-Index and the   Highly-Cited-Publications Indicator,2013,"  The arbitrariness of the h-index becomes evident, when one requires q*h instead of h citations as the threshold for the definition of the index, thus changing the size of the core of the most influential publications of a dataset. I analyze the citation records of 26 physicists in order to determine how much the prefactor q influences the ranking. Likewise, the arbitrariness of the highly-cited-publications indicator is due to the threshold value, given either as an absolute number of citations or as a percentage of highly cited papers. The analysis of the 26 citation records shows that the changes in the rankings in dependence on these thresholds are rather large and comparable with the respective changes for the h-index. ",https://doi.org/10.1016/j.joi.2012.12.006,1302.6582v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Examples for counterintuitive behavior of the new citation-rank   indicator P100 for bibliometric evaluations,2014,"  A new percentile-based rating scale P100 has recently been proposed to describe the citation impact in terms of the distribution of the unique citation values. Here I investigate P100 for 5 example datasets, two simple fictitious models and three larger empirical samples. Counterintuitive behavior is demonstrated in the model datasets, pointing to difficulties when the evolution with time of the indicator is analyzed or when different fields or publication years are compared. It is shown that similar problems can occur for the three larger datasets of empirical citation values. Further, it is observed that the performance evalution result in terms of percentiles can be influenced by selecting different journals for publication of a manuscript. ",Kein DOI-Link verfügbar,1407.3268v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"An empirical investigation of the g-index for 26 physicists in   comparison with the h-index, the A-index, and the R-index",2008,"  Hirsch has introduced the h-index to quantify an individual's scientific research output by the largest number h of a scientist's papers that received at least h citations. In order to take into account the highly skewed frequency distribution of citations, Egghe proposed the g-index as an improvement of the h-index. I have worked out 26 practical cases of physicists from the Institute of Physics at Chemnitz University of Technology and compare the h and g values. It is demonstrated that the g-index discriminates better between different citation patterns. This can also be achieved by evaluating Jin's A-index which reflects the average number of citations in the h-core and interpreting it in conjunction with the h-index. h and A can be combined into the R-index to measure the h-core's citation intensity. I have also determined the A and R values for the 26 data sets. For a better comparison, I utilize interpolated indices. The correlations between the various indices as well as with the total number of papers and the highest citation counts are discussed. The largest Pearson correlation coefficient is found between g and R. Although the correlation between g and h is relatively strong, the arrangement of the data set is significantly different, depending on whether they are put into order according to the values of either h or g. ",Kein DOI-Link verfügbar,0802.1820v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Inconsistencies of Recently Proposed Citation Impact Indicators and how   to Avoid Them,2012,"  It is shown that under certain circumstances in particular for small datasets the recently proposed citation impact indicators I3(6PR) and R(6,k) behave inconsistently when additional papers or citations are taken into consideration. Three simple examples are presented, in which the indicators fluctuate strongly and the ranking of scientists in the evaluated group is sometimes completely mixed up by minor changes in the data base. The erratic behavior is traced to the specific way in which weights are attributed to the six percentile rank classes, specifically for the tied papers. For 100 percentile rank classes the effects will be less serious. For the 6 classes it is demonstrated that a different way of assigning weights avoids these problems, although the non-linearity of the weights for the different percentile rank classes can still lead to (much less frequent) changes in the ranking. This behavior is not undesired, because it can be used to correct for differences in citation behavior in different fields. Remaining deviations from the theoretical value R(6,k) = 1.91 can be avoided by a new scoring rule, the fractional scoring. Previously proposed consistency criteria are amended by another property of strict independence which a performance indicator should aim at. ",Kein DOI-Link verfügbar,1202.3861v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Aperiodic Tilings on the Computer,1999,"  We briefly review the standard methods used to construct quasiperiodic tilings, such as the projection, the inflation, and the grid method. A number of sample Mathematica programs, implementing the different approaches for one- and two-dimensional examples, are discussed. Apart from small examples, the corresponding programs are not contained in the text, but will be made available in electronic form. ",Kein DOI-Link verfügbar,cond-mat/9903010v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The Hartree-Fock based diagonalization - an efficient algorithm for the   treatment of interacting electrons in disordered solids,2001,"  The Hartree-Fock based diagonalization is a computational method for the investigation of the low-energy properties of correlated electrons in disordered solids. The method is related to the quantum-chemical configuration interaction approach. It consists in diagonalizing the Hamiltonian in a reduced Hilbert space built of the low-energy states of the corresponding disordered Hartree-Fock Hamiltonian. The properties of the method are discussed for the example of the quantum Coulomb glass, a lattice model of electrons in a random potential interacting via long-range Coulomb interaction. Particular attention is paid to the accuracy of the results as a function of the dimension of the reduced Hilbert space. It is argued that disorder actually helps the approximation. ",Kein DOI-Link verfügbar,cond-mat/0111062v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Correlation-Strength Driven Anderson Metal-Insulator Transition,2012,  The possibility of driving an Anderson metal-insulator transition in the presence of scale-free disorder by changing the correlation exponent is numerically investigated. We calculate the localization length for quasi-one-dimensional systems at fixed energy and fixed disorder strength using a standard transfer matrix method. From a finite-size scaling analysis we extract the critical correlation exponent and the critical exponent characterizing the phase transition. ,https://doi.org/10.1103/PhysRevB.85.205147,1201.3334v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Differences between regular and random order of updates in damage   spreading simulations,1998,"  We investigate the spreading of damage in the three-dimensional Ising model by means of large-scale Monte-Carlo simulations. Within the Glauber dynamics we use different rules for the order in which the sites are updated. We find that the stationary damage values and the spreading temperature are different for different update order. In particular, random update order leads to larger damage and a lower spreading temperature than regular order. Consequently, damage spreading in the Ising model is non-universal not only with respect to different update algorithms (e.g. Glauber vs. heat-bath dynamics) as already known, but even with respect to the order of sites. ",https://doi.org/10.1103/PhysRevE.58.7998,cond-mat/9807229v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Localization and conductance in the quantum Coulomb glass,2001,"  We consider the combined influence of disorder, electron-electron interactions and quantum hopping on the properties of electronic systems in a localized phase, approaching an insulator-metal transition. The generic models in this regime are the quantum Coulomb glass and its generalization to electrons with spin. After introducing these models we explain our computational method, the Hartree-Fock based diagonalization. We then discuss the conductance and compare spinless fermions and electrons. It turns out that spin degrees of freedom do not play an essential role in the systems considered. Finally, we analyze localization and decay of single-particle excitations. We find that interactions generically tend to localize these excitations which is a result of the Coulomb gap in the single-particle density of states. ",https://doi.org/10.1080/13642810108205795,cond-mat/0109551v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A Matrix Model for Bilayered Quantum Hall Systems,2003,"  We develop a matrix model to describe bilayered quantum Hall fluids for a series of filling factors. Considering two coupling layers, and starting from a corresponding action, we construct its vacuum configuration at \nu=q_iK_{ij}^{-1}q_j, where K_{ij} is a 2\times 2 matrix and q_i is a vector. Our model allows us to reproduce several well-known wave functions. We show that the wave function \Psi_{(m,m,n)} constructed years ago by Yoshioka, MacDonald and Girvin for the fractional quantum Hall effect at filling factor {2\over m+n} and in particular \Psi_{(3,3,1)} at filling {1\over 2} can be obtained from our vacuum configuration. The unpolarized Halperin wave function and especially that for the fractional quantum Hall state at filling factor {2\over 5} can also be recovered from our approach. Generalization to more than 2 layers is straightforward. ",https://doi.org/10.1088/0305-4470/37/9/007,hep-th/0304207v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Magnetic susceptibility of the two-dimensional Hubbard model using a   power series for the hopping constant,2007,"  The magnetic susceptibility of the two-dimensional repulsive Hubbard model with nearest-neighbor hopping is investigated using the diagram technique developed for the case of strong correlations. In this technique a power series in the hopping constant is used. At half-filling the calculated zero-frequency susceptibility and the square of the site spin reproduce adequately results of Monte Carlo simulations. Also in agreement with numerical simulations no evidence of ferromagnetic correlations was found in the considered range of electron concentrations $0.8\alt\bar{n}\alt 1.2$ for the repulsion parameters $8|t|\leq U\leq 16|t|$. However, for larger $U/|t|$ and $|1-\bar{n}|\approx 0.2$ the nearest neighbor correlations become ferromagnetic. For $\bar{n}\alt 0.94$ and $\bar{n}\agt 1.06$ the imaginary part of the real-frequency susceptibility becomes incommensurate for small frequencies. The incommensurability parameter grows with departure from half-filling and decreases with increasing the frequency. This behavior of the susceptibility can explain the observed low-frequency incommensurate response observed in normal-state lanthanum cuprates. ",https://doi.org/10.1103/PhysRevB.76.245112,0707.0584v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Partitioning Schemes and Non-Integer Box Sizes for the Box-Counting   Algorithm in Multifractal Analysis,2012,"  We compare different partitioning schemes for the box-counting algorithm in the multifractal analysis by computing the singularity spectrum and the distribution of the box probabilities. As model system we use the Anderson model of localization in two and three dimensions. We show that a partitioning scheme which includes unrestricted values of the box size and an average over all box origins leads to smaller error bounds than the standard method using only integer ratios of the linear system size and the box size which was found by Rodriguez et al. (Eur. Phys. J. B 67, 77-82 (2009)) to yield the most reliable results. ",https://doi.org/10.1140/epjb/e2012-30410-x,1204.3755v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Generalized Inverse Participation Numbers in Metallic-Mean Quasiperiodic   Systems,2012,"  From the quantum mechanical point of view, the electronic characteristics of quasicrystals are determined by the nature of their eigenstates. A practicable way to obtain information about the properties of these wave functions is studying the scaling behavior of the generalized inverse participation numbers $Z_q \sim N^{-D_q(q-1)}$ with the system size $N$. In particular, we investigate $d$-dimensional quasiperiodic models based on different metallic-mean quasiperiodic sequences. We obtain the eigenstates of the one-dimensional metallic-mean chains by numerical calculations for a tight-binding model. Higher dimensional solutions of the associated generalized labyrinth tiling are then constructed by a product approach from the one-dimensional solutions. Numerical results suggest that the relation $D_q^{d\mathrm{d}} = d D_q^\mathrm{1d}$ holds for these models. Using the product structure of the labyrinth tiling we prove that this relation is always satisfied for the silver-mean model and that the scaling exponents approach this relation for large system sizes also for the other metallic-mean systems. ",https://doi.org/10.1140/epjb/e2011-20323-7,1204.3764v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Renormalization Group Approach for the Wave Packet Dynamics in   Golden-Mean and Silver-Mean Labyrinth Tilings,2012,"  We study the quantum diffusion in quasiperiodic tight-binding models in one, two, and three dimensions. First, we investigate a class of one-dimensional quasiperiodic chains, in which the atoms are coupled by weak and strong bonds aligned according to the metallic-mean sequences. The associated generalized labyrinth tilings in d dimensions are then constructed from the direct product of d such chains, which allows us to consider rather large systems numerically. The electronic transport is studied by computing the scaling behavior of the mean-square displacement of the wave packets with respect to the time. The results reveal the occurrence of anomalous diffusion in these systems. By extending a renormalization group approach, originally proposed for the golden-mean chain, we show also for the silver-mean chain as well as for the higher-dimensional labyrinth tilings that in the regime of strong quasiperiodic modulation the wave-packet dynamics are governed by the underlying quasiperiodic structure. ",https://doi.org/10.1103/PhysRevB.85.224205,1204.4017v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,On the calculation of percentile-based bibliometric indicators,2012,"  A percentile-based bibliometric indicator is an indicator that values publications based on their position within the citation distribution of their field. The most straightforward percentile-based indicator is the proportion of frequently cited publications, for instance the proportion of publications that belong to the top 10% most frequently cited of their field. Recently, more complex percentile-based indicators were proposed. A difficulty in the calculation of percentile-based indicators is caused by the discrete nature of citation distributions combined with the presence of many publications with the same number of citations. We introduce an approach to calculating percentile-based indicators that deals with this difficulty in a more satisfactory way than earlier approaches suggested in the literature. We show in a formal mathematical framework that our approach leads to indicators that do not suffer from biases in favor of or against particular fields of science. ",https://doi.org/10.1002/asi.22775,1205.0646v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Quantum Diffusion in Separable d-Dimensional Quasiperiodic Tilings,2012,"  We study the electronic transport in quasiperiodic separable tight-binding models in one, two, and three dimensions. First, we investigate a one-dimensional quasiperiodic chain, in which the atoms are coupled by weak and strong bonds aligned according to the Fibonacci chain. The associated d-dimensional quasiperiodic tilings are constructed from the product of d such chains, which yields either the square/cubic Fibonacci tiling or the labyrinth tiling. We study the scaling behavior of the mean square displacement and the return probability of wave packets with respect to time. We also discuss results of renormalization group approaches and lower bounds for the scaling exponent of the width of the wave packet. ",Kein DOI-Link verfügbar,1212.6337v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Photonic Properties of Metallic-Mean Quasiperiodic Chains,2012,"  The light propagation through a stack of two media with different refractive indices, which are aligned according to different quasiperiodic sequences determined by metallic means, is studied using the transfer matrix method. The focus lies on the investigation of the influence of the underlying quasiperiodic sequence as well as the dependence of the transmission on the frequency, the incidence angle of the light wave and different ratios of the refractive indices. In contrast to a periodically aligned stack we find complete transmission for the quasiperiodic systems for a wide range of different refraction indices for small incidence angles. Additional bands of moderate transmission occur for frequencies in the range of the photonic band gaps of the periodic system. Further, for fixed indices of refraction we find a range of almost perfect transmission for angles close to the angle of total reflection, which is caused by the bending of photonic transmission bands towards higher frequencies for increasing incidence angles. Comparing with the results of a periodic stack the quasiperiodicity seems to have only an influence in the region around the midgap frequency of a periodic stack. ",https://doi.org/10.1140/epjb/e2010-00226-y,1212.6605v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Monte-Carlo Simulations of the Dynamical Behavior of the Coulomb Glass,1996,"  We study the dynamical behavior of disordered many-particle systems with long-range Coulomb interactions by means of damage-spreading simulations. In this type of Monte-Carlo simulations one investigates the time evolution of the damage, i.e. the difference of the occupation numbers of two systems, subjected to the same thermal noise. We analyze the dependence of the damage on temperature and disorder strength. For zero disorder the spreading transition coincides with the equilibrium phase transition, whereas for finite disorder, we find evidence for a dynamical phase transition well below the transition temperature of the pure system. ",https://doi.org/10.1103/PhysRevB.55.6272,cond-mat/9610089v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,No enhancement of the localization length for two interacting particles   in a random potential,1996,  We study two interacting particles in a random potential chain by means of the transfer matrix method. The dependence of the two-particle localization length $\lambda_2$ on disorder and interaction strength is investigated. Our results demonstrate that the recently proposed enhancement of $\lambda_2$ as compared to the results for single particles is entirely due to the finite size of the systems considered. This is shown for a Hubbard-like onsite interaction and also a long-range interaction. ,Kein DOI-Link verfügbar,cond-mat/9612034v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The two-dimensional Anderson model of localization with random hopping,1997,"  We examine the localization properties of the 2D Anderson Hamiltonian with off-diagonal disorder. Investigating the behavior of the participation numbers of eigenstates as well as studying their multifractal properties, we find states in the center of the band which show critical behavior up to the system size $N= 200 \times 200$ considered. This result is confirmed by an independent analysis of the localization lengths in quasi-1D strips with the help of the transfer-matrix method. Adding a very small additional onsite potential disorder, the critical states become localized. ",https://doi.org/10.1007/s100510050149,cond-mat/9706265v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Exact Eigenstates of Tight-Binding Hamiltonians on the Penrose Tiling,1998,"  We investigate exact eigenstates of tight-binding models on the planar rhombic Penrose tiling. We consider a vertex model with hopping along the edges and the diagonals of the rhombi. For the wave functions, we employ an ansatz, first introduced by Sutherland, which is based on the arrow decoration that encodes the matching rules of the tiling. Exact eigenstates are constructed for particular values of the hopping parameters and the eigenenergy. By a generalized ansatz that exploits the inflation symmetry of the tiling, we show that the corresponding eigenenergies are infinitely degenerate. Generalizations and applications to other systems are outlined. ",https://doi.org/10.1103/PhysRevB.58.13482,cond-mat/9805321v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Planar quasiperiodic Ising models,1999,"  We investigate zero-field Ising models on periodic approximants of planar quasiperiodic tilings by means of partition function zeros and high-temperature expansions. These are obtained by employing a determinant expression for the partition function. The partition function zeros in the complex temperature plane yield precise estimates of the critical temperature of the quasiperiodic model. Concerning the critical behaviour, our results are compatible with Onsager universality, in agreement with the Harris-Luck criterion based on scaling arguments. ",https://doi.org/10.1016/S0921-5093(00)01153-9,cond-mat/9908088v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Exponents of the localization lengths in the bipartite Anderson model   with off-diagonal disorder,2000,"  We investigate the scaling properties of the two-dimensional (2D) Anderson model of localization with purely off-diagonal disorder (random hopping). In particular, we show that for small energies the infinite-size localization lengths as computed from transfer-matrix methods together with finite-size scaling diverge with a power-law behavior. The corresponding exponents seem to depend on the strength and the type of disorder chosen. ",https://doi.org/10.1016/S0921-4526(00)00777-8,cond-mat/0007102v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Disorder and Two-Particle Interaction in Low-Dimensional Quantum Systems,2000,"  We review some of the recent results on two-interacting particles (TIP) in low-dimensional disordered quantum models. Special attention is given to the mapping of the problem onto random band matrices. In particular, we construct two simple, seemingly closely related examples for which an analogous mapping leads to incorrect results. We briefly discuss possible reasons for this discrepancy based on the physical differences between the TIP problem and our examples. ",https://doi.org/10.1016/S1386-9477(00)00236-8,cond-mat/0009378v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Localization properties of two interacting particles in a quasi-periodic   potential with a metal-insulator transition,2001,"  We study the influence of many-particle interactions on a metal-insulator transition. We consider the two-interacting-particle problem for onsite interacting particles on a one-dimensional quasiperiodic chain, the so-called Aubry-Andr\'{e} model. We show numerically by the decimation method and finite-size scaling that the interaction does not modify the critical parameters such as the transition point and the localization-length exponent. We compare our results to the case of finite density systems studied by means of the density-matrix renormalization scheme. ",https://doi.org/10.1007/s100510170072,cond-mat/0106603v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Exciton scattering in light-harvesting systems of purple bacteria,2001,  Using the reduced density matrix formalism the exciton scattering in light-harvesting systems of purple bacteria is calculated. The static disorder (fluctuations of the site energies) as well as the dynamic disorder (dissipation) is taken into account in this work. Circular aggregates with 18 pigments are studied to model the B850 ring of bacteriochlorophylls with LH2 complexes. It can be shown that the influence of dissipation may not be neglected in the simulation of the time-dependent anisotropy of fluorescence. Also an elliptical deformation of the ring could be essential. ,https://doi.org/10.1016/S0022-2313(01)00334-9,physics/0109011v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,A density matrix approach to photoinduced electron injection,2001,"  Electron injection from an adsorbed molecule to the substrate (heterogeneous electron transfer) is studied. One reaction coordinate is used to model this process. The surface phonons and/or the electron-hole pairs together with the internal degrees of freedom of the adsorbed molecule as well as possibly a liquid surrounding the molecule provide a dissipative environment, which may lead to dephasing, relaxation, and sometimes excitation of the relevant system. In the process studied the adsorbed molecule is excited by a light pulse. This is followed by an electron transfer from the excited donor state to the quasi-continuum of the substrate. It is assumed that the substrate is a semiconductor. The effects of dissipation on electron injection are investigated. ",https://doi.org/10.1016/S0022-2313(01)00341-6,physics/0109012v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Anderson Localization in 1D Systems with Correlated Disorder,2011,"  Anderson localization has been a subject of intense studies for many years. In this context, we study numerically the influence of long-range correlated disorder on the localization behavior in one dimensional systems. We investigate the localization length and the density of states and compare our numerical results with analytical predictions. Specifically, we find two distinct characteristic behaviors in the vicinity of the band center and at the unperturbed band edge, respectively. Furthermore we address the effect of the intrinsic short-range correlations. ",https://doi.org/10.1140/epjb/e2011-20212-1,1103.2866v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Diamagnetism of Confined Dirac Fermions in Disordered Graphene,2011,"  The diamagnetism of confined Dirac fermions submitted to a uniform magnetic field in disordered graphene is investigated. The solutions of the energy spectrum are used to discuss the orbital magnetism from a statistical mechanical point of view. More precisely, by the technique of Green functions the self-energy for short and long-ranged disorders is obtained. This allows us to determine the susceptibility for short and long-ranged disorders together with confinement. We compare our results with already published work and point out the relevance of these findings to a systematic formulation of the diamagnetism in a confining potential. ",https://doi.org/10.1088/1751-8113/44/27/275001,1105.3277v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The Role of Power-Law Correlated Disorder in the Anderson   Metal-Insulator Transition,2011,  We study the influence of scale-free correlated disorder on the metal-insulator transition in the Anderson model of localization. We use standard transfer matrix calculations and perform finite-size scaling of the largest inverse Lyapunov exponent to obtain the localization length for respective 3D tight-binding systems. The density of states is obtained from the full spectrum of eigenenergies of the Anderson Hamiltonian. We discuss the phase diagram of the metal-insulator transition and the influence of the correlated disorder on the critical exponents. ,https://doi.org/10.1140/epjb/e2012-21059-6,1112.4469v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Wave Functions, Quantum Diffusion, and Scaling Exponents in Golden-Mean   Quasiperiodic Tilings",2012,"  We study the properties of wave functions and the wave-packet dynamics in quasiperiodic tight-binding models in one, two, and three dimensions. The atoms in the one-dimensional quasiperiodic chains are coupled by weak and strong bonds aligned according to the Fibonacci sequence. The associated d-dimensional quasiperiodic tilings are constructed from the direct product of d such chains, which yields either the hypercubic tiling or the labyrinth tiling. This approach allows us to consider rather large systems numerically. We show that the wave functions of the system are multifractal and that their properties can be related to the structure of the system in the regime of strong quasiperiodic modulation by a renormalization group (RG) approach. We also study the dynamics of wave packets to get information about the electronic transport properties. In particular, we investigate the scaling behaviour of the return probability of the wave packet with time. Applying again the RG approach we show that in the regime of strong quasiperiodic modulation the return probability is governed by the underlying quasiperiodic structure. Further, we also discuss lower bounds for the scaling exponent of the width of the wave packet and propose a modified lower bound for the absolute continuous regime. ",https://doi.org/10.1088/0953-8984/25/7/075503,1204.4211v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Similarity of eigenstates in generalized labyrinth tilings,2012,"  The eigenstates of d-dimensional quasicrystalline models with a separable Hamiltonian are studied within the tight-binding model. The approach is based on mathematical sequences, constructed by an inflation rule P = {w -> s, s -> sws^(b-1)} describing the weak/strong couplings of atoms in a quasiperiodic chain. Higher-dimensional quasiperiodic tilings are constructed as a direct product of these chains and their eigenstates can be directly calculated by multiplying the energies or wave functions of the chain, respectively.   Applying this construction rule, the grid in d dimensions splits into 2^(d-1) different tilings, for which we investigated the characteristics of the wave functions. For the standard two-dimensional labyrinth tiling constructed from the octonacci sequence (b=2) the lattice breaks up into two identical lattices, which consequently yield the same eigenstates. While this is not the case for other b, our numerical results show that the wave functions of the different grids become increasingly similar for large system sizes. This can be explained by the fact that the structure of the 2^(d-1) grids mainly differs at the boundaries and thus for large systems the eigenstates approach each other. This property allows us to analytically derive properties of the higher-dimensional generalized labyrinth tilings from the one-dimensional results. In particular participation numbers and corresponding scaling exponents have been determined. ",https://doi.org/10.1088/1742-6596/226/1/012029,1212.6754v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Two interacting particles in a random potential: The random matrix model   revisited,1997,  We reinvestigate the validity of mapping the problem of two onsite interacting particles in a random potential onto an effective random matrix model. To this end we first study numerically how the non-interacting basis is coupled by the interaction. Our results indicate that the typical coupling matrix element decreases significantly faster with increasing single-particle localization length than is assumed in the random matrix model. We further show that even for models where the dependency of the coupling matrix element on the single-particle localization length is correctly described by the corresponding random matrix model its predictions for the localization length can be qualitatively incorrect. These results indicate that the mapping of an interacting random system onto an effective random matrix model is potentially dangerous. We also discuss how Imry's block-scaling picture for two interacting particles is influenced by the above arguments. ,https://doi.org/10.1002/(SICI)1521-3951(199902)211:2<681::AID-PSSB681>3.0.CO;2-I,cond-mat/9702241v2,Yes,potent(2)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The quantum Coulomb glass within the Hartree-Fock approximation,1997,"  We study the influence of electron-electron interactions on the electronic properties of disordered materials. In particular, we consider the insulating side of a metal-insulator transition where screening breaks down and the electron-electron interaction remains long-ranged. The investigations are based on the quantum Coulomb glass, a generalization of the classical Coulomb glass model of disordered insulators. The quantum Coulomb glass is studied by decoupling the Coulomb interaction by means of a Hartree-Fock approximation and exactly diagonalizing the remaining localization problem. We investigate the behavior of the Coulomb gap in the density of states when approaching the metal-insulator transition and study the influence of the interaction on the localization of the electrons. We find that the interaction leads to an enhancement of localization at the Fermi level. ",https://doi.org/10.1103/PhysRevB.56.5890,cond-mat/9704068v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Quantum Coulomb Glass: Anderson localization in an interacting system,1997,  The quantum Coulomb glass model describes disordered interacting electrons on the insulating side of a metal-insulator transition. By taking quantum fluctuations into account it can describe not only the localized limit but also the weakly localized regime. We discuss several possibilities to generalize the concept of Anderson localization to interacting electron systems such as the quantum Coulomb glass and define criteria for localization. The corresponding physical quantities are calculated by numerically exact diagonalization. The results indicate that single-particle excitations close to the Fermi energy become more strongly localized under the influence of interaction. ,https://doi.org/10.1002/(SICI)1521-3951(199801)205:1<53::AID-PSSB53>3.3.CO;2-9,cond-mat/9708192v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Quantum Coulomb glass - Hartree-Fock approximation versus exact   diagonalization,1997,"  We investigate the behavior of disordered interacting electrons in the insulating regime. Our study is based on the quantum Coulomb glass model which is obtained from the classical Coulomb glass by adding hopping matrix elements between neighboring sites. We use two different numerical methods, viz. a Hartree-Fock approximation and an exact diagonalization and compare the results for the tunneling density of states and the localization properties in order to determine the range of validity of the Hartree-Fock method. We find that the Hartree-Fock method gives a good approximation for the density of states for all energies but represents the localization properties correctly close to the Fermi level only. Some consequences for the localization of disordered interacting electrons are discussed. ",https://doi.org/10.1002/(SICI)1521-3951(199801)205:1<233::AID-PSSB233>3.3.CO;2-2,cond-mat/9708193v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Do interactions increase or reduce the conductance of disordered   electrons? It depends!,1998,  We investigate the influence of electron-electron interactions on the conductance of two-dimensional disordered spinless electrons. By using an efficient numerical method which is based on exact diagonalization in a truncated basis of Hartree-Fock states we are able to determine the exact low-energy properties of comparatively large systems in the diffusive as well as in the localized regimes. We find that weak interactions increase the d.c. conductance in the localized regime while they decrease the d.c. conductance in the diffusive regime. Strong interactions always decrease the conductance. We also study the localization of single-particle excitations close to the Fermi energy which turns out to be only weakly influenced by the interactions. ,https://doi.org/10.1103/PhysRevLett.81.4212,cond-mat/9806194v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Interaction-dependent enhancement of the localisation length for two   interacting particles in a one-dimensional random potential,1998,"  We present calculations of the localisation length, $\lambda_{2}$, for two interacting particles (TIP) in a one-dimensional random potential, presenting its dependence on disorder, interaction strength $U$ and system size. $\lambda_{2}(U)$ is computed by a decimation method from the decay of the Green function along the diagonal of finite samples. Infinite sample size estimates $\xi_{2}(U)$ are obtained by finite-size scaling. For U=0 we reproduce approximately the well-known dependence of the one-particle localisation length on disorder while for finite $U$, we find that $ \xi_{2}(U) \sim \xi_2(0)^{\beta(U)} $ with $\beta(U)$ varying between $\beta(0)=1$ and $\beta(1) \approx 1.5$. We test the validity of various other proposed fit functions and also study the problem of TIP in two different random potentials corresponding to interacting electron-hole pairs. As a check of our method and data, we also reproduce well-known results for the two-dimensional Anderson model without interaction. ",https://doi.org/10.1007/s100510050732,cond-mat/9806255v1,Yes,potent(2)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Formation of electron-hole pairs in a one-dimensional random environment,1998,"  We study the formation of electron-hole pairs for disordered systems in the limit of weak electron-hole interactions. We find that both attractive and repulsive interactions lead to electron-hole pair states with large localization length $\lambda_{2}$ even when we are in this non-excitonic limit. Using a numerical decimation method to calculate the decay of the Green function along the diagonal of finite samples, we investigate the dependence of $\lambda_2(U)$ on disorder, interaction strength $U$ and system size. Infinite sample size estimates $\xi_{2}(U)$ are obtained by finite-size scaling. The results show a great similarity to the problem of two interacting electrons in the same random one-dimensional potential. ",Kein DOI-Link verfügbar,cond-mat/9806350v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Transport in disordered interacting systems: Numerical results for   one-dimensional spinless electrons,1998,"  The combined influence of disorder and interactions on the transport properties of electrons in one dimension is investigated. The numerical simulations are carried out by means of the Hartree-Fock-based diagonalization (HFD), a very efficient method to determine the low-energy properties of a disordered many-particle system. We find that the conductance of a strongly localized system can become considerably enhanced by the interactions. The enhancement for long-range interactions is significantly larger than for short-range interactions. In contrast, the conductance of weakly localized systems becomes suppressed by the interactions. ",https://doi.org/10.1016/S0378-4371(98)00628-1,cond-mat/9807385v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Magnetotransport in Quasilattices,1998,"  The dc conductance and the Hall voltage of planar arrays of interconnected quantum wires are calculated numerically. Our systems are derived from finite patches of aperiodic graphs, with completely symmetric scatterers placed on their vertices which are interconnected by ideal quantum wires. Already in a periodic square lattice arrangement, quantum interference effects lead to complicated magnetotransport properties related to the Hofstadter butterfly. For rectangular Fibonacci grids and other quasiperiodic lattices, we obtain still more complex fractal patterns. In particular, irrational ratios of edge lengths and of tile areas in our samples destroy the periodicities with respect to the Fermi wave vector and the magnetic flux, respectively. ",Kein DOI-Link verfügbar,cond-mat/9809114v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Localization of Electronic Wave Functions on Quasiperiodic Lattices,1998,"  We study electronic eigenstates on quasiperiodic lattices using a tight-binding Hamiltonian in the vertex model. In particular, the two-dimensional Penrose tiling and the three-dimensional icosahedral Ammann-Kramer tiling are considered. Our main interest concerns the decay form and the self-similarity of the electronic wave functions, which we compute numerically for periodic approximants of the perfect quasiperiodic structure. In order to investigate the suggested power-law localization of states, we calculate their participation numbers and structural entropy. We also perform a multifractal analysis of the eigenstates by standard box-counting methods. Our results indicate a rather different behaviour of the two- and the three-dimensional systems. Whereas the eigenstates on the Penrose tiling typically show power-law localization, this was not observed for the icosahedral tiling. ",https://doi.org/10.1088/0953-8984/10/4/008,cond-mat/9809117v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Hartree-Fock based diagonalization: an efficient method for simulating   disordered interacting electrons,1998,"  We present an efficient numerical method for simulating the low-energy properties of disordered many-particle systems. The method which is based on the quantum-chemical configuration interaction approach consists in diagonalizing the Hamiltonian in an energetically truncated basis build of the low-energy states of the corresponding Hartree-Fock Hamiltonian. As an example we investigate the quantum Coulomb glass, a model of spinless electrons in a random potential interacting via long-range Coulomb interaction. We find that the Coulomb interaction increases the conductance of strongly disordered systems but reduces the conductance of weakly disordered systems. ",https://doi.org/10.1016/S0010-4655(99)00388-4,cond-mat/9809171v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Scaling the localisation lengths for two interacting particles in   one-dimensional random potentials,1998,"  Using a numerical decimation method, we compute the localisation length $\lambda_{2}$ for two onsite interacting particles (TIP) in a one-dimensional random potential. We show that an interaction $U>0$ does lead to $\lambda_2(U) > \lambda_2(0)$ for not too large $U$ and test the validity of various proposed fit functions for $\lambda_2(U)$. Finite-size scaling allows us to obtain infinite sample size estimates $\xi_{2}(U)$ and we find that $ \xi_{2}(U) \sim \xi_2(0)^{\alpha(U)} $ with $\alpha(U)$ varying between $\alpha(0)\approx 1$ and $\alpha(1) \approx 1.5$. We observe that all $\xi_2(U)$ data can be made to coalesce onto a single scaling curve. We also present results for the problem of TIP in two different random potentials corresponding to interacting electron-hole pairs. ",https://doi.org/10.1016/S0378-4371(98)00635-9,cond-mat/9809369v1,Yes,potent(2)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,High-temperature expansion for Ising models on quasiperiodic tilings,1999,"  We consider high-temperature expansions for the free energy of zero-field Ising models on planar quasiperiodic graphs. For the Penrose and the octagonal Ammann-Beenker tiling, we compute the expansion coefficients up to 18th order. As a by-product, we obtain exact vertex-averaged numbers of self-avoiding polygons on these quasiperiodic graphs. In addition, we analyze periodic approximants by computing the partition function via the Kac-Ward determinant. For the critical properties, we find complete agreement with the commonly accepted conjecture that the models under consideration belong to the same universality class as those on periodic two-dimensional lattices. ",https://doi.org/10.1088/0305-4470/32/24/306,cond-mat/9901001v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Energy-level statistics at the metal-insulator transition in anisotropic   systems,1999,"  We study the three-dimensional Anderson model of localization with anisotropic hopping, i.e. weakly coupled chains and weakly coupled planes. In our extensive numerical study we identify and characterize the metal-insulator transition using energy-level statistics. The values of the critical disorder $W_c$ are consistent with results of previous studies, including the transfer-matrix method and multifractal analysis of the wave functions. $W_c$ decreases from its isotropic value with a power law as a function of anisotropy. Using high accuracy data for large system sizes we estimate the critical exponent $\nu=1.45\pm0.2$. This is in agreement with its value in the isotropic case and in other models of the orthogonal universality class. The critical level statistics which is independent of the system size at the transition changes from its isotropic form towards the Poisson statistics with increasing anisotropy. ",https://doi.org/10.1103/PhysRevB.61.6028,cond-mat/9909210v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Crossover from interaction induced localization to delocalization in   disordered electron systems,1999,"  We numerically investigate the transport properties of interacting spinless electrons in disordered systems. We use an efficient method which is based on the diagonalization of the Hamiltonian in the subspace of the many-particle Hilbert space which is spanned by the low-energy Slater states. Low-energy properties can be calculated with an accuracy comparable to that of exact diagonalization but for larger system sizes. The method works well in the entire parameter space, and it can handle long-range as well as short-range interactions. Using this method we calculate the combined effect of disorder and interactions on the Kubo-Greenwood conductance and on the sensitivity of the ground state energy to a twist in the boundary conditions. We find that the influence of the interactions on the transport properties is opposite for large and small disorder. In the strongly localized regime (small kinetic energy, large disorder) interactions increase the transport whereas for weak disorder (large kinetic energy) interactions decrease the transport. ",Kein DOI-Link verfügbar,cond-mat/9910321v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Interacting particles at a metal-insulator transition,2001,"  We study the influence of many-particle interaction in a system which, in the single particle case, exhibits a metal-insulator transition induced by a finite amount of onsite pontential fluctuations. Thereby, we consider the problem of interacting particles in the one-dimensional quasiperiodic Aubry-Andre chain. We employ the density-matrix renormalization scheme to investigate the finite particle density situation. In the case of incommensurate densities, the expected transition from the single-particle analysis is reproduced. Generally speaking, interaction does not alter the incommensurate transition. For commensurate densities, we map out the entire phase diagram and find that the transition into a metallic state occurs for attractive interactions and infinite small fluctuations -- in contrast to the case of incommensurate densities. Our results for commensurate densities also show agreement with a recent analytic renormalization group approach. ",https://doi.org/10.1103/PhysRevB.65.115114,cond-mat/0102251v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Energy spectra and eigenstates of quasiperiodic tight-binding   Hamiltonians,2002,"  Analytic and numerical results for quasiperiodic tight-binding models are reviewed, with emphasis on two and three-dimensional models which so far are beyond a mathematically rigorous treatment. In particular, we consider energy spectra of aperiodic tight-binding models and the corresponding level statistics, which are well reproduced by random matrix theory. The eigenstates are characterised by multifractal analysis, and a construction of peculiar multifractal states on the Penrose tiling is discussed. We also consider quantum diffusion, and present some results on interacting electron systems in one dimension. ",Kein DOI-Link verfügbar,cond-mat/0212140v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The influence of ultra-fast laser pulses on electron transfer in   molecular wires studied by a non-Markovian density matrix approach,2005,"  New features of molecular wires can be observed when they are irradiated by laser fields. These effects can be achieved by periodically oscillating fields but also by short laser pulses. The theoretical foundation used for these investigations is a density matrix formalism where the full system is partitioned into a relevant part and a thermal fermionic bath. The derivation of a quantum master equation, either based on a time-convolutionless or time-convolution projection-operator approach, incorporates the interaction with time-dependent laser fields non-perturbatively and is valid at low temperatures for weak system-bath coupling. From the population dynamics the electrical current through the molecular wire is determined. This theory including further extensions is used for the determination of electron transport through molecular wires. As examples, we show computations of coherent destruction of tunneling in asymmetric periodically driven quantum systems, alternating currents and the suppression of the directed current by using a short laser pulse. ",https://doi.org/10.1063/1.2162537,cond-mat/0509442v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Localization of electronic states in amorphous materials: recursive   Green's function method and the metal-insulator transition at E<>0,2006,"  In this paper we will investigate whether the scaling assumptions made in previous studies for the transition at energies outside the band centre can be reconfirmed in numerical calculations, and in particular whether the conductivity sigma follows a power law close to the critical energy E_c. For this purpose we will use the recursive Green's function method to calculate the four-terminal conductance of a disordered system for fixed disorder strength at temperature T=0. Applying the finite-size scaling analysis we will compute the critical exponent and determine the mobility edge, i.e. the MIT outside the band centre. ",https://doi.org/10.1007/3-540-33541-2_11,cond-mat/0602300v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Anisotropic Quantum Hall Matrix Model,2003,"  We consider the anisotropic effect in the quantum Hall systems by applying a confining potential that is not of parabolic type. This can be done by extending Susskind--Polychronakos's approach to involve the matrices of two coupled harmonic oscillators. Starting from its action, we employ a unitary transformation to diagonalize the model. The operators for building up the anisotropic ground state and creating the collective excitations can be constructed explicitly. Evaluating the area of the quantum Hall droplet, we obtain the corresponding filling factor which is found to depend on the anisotropy parameter and to vary with the magnetic field strength. This can be used to obtain the observed anisotropic filling factors, i.e. {9\over 2}, {11\over 2} and others. ",Kein DOI-Link verfügbar,hep-th/0309085v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Photo-induced intermolecular charge transfer in porphyrin complexes,2000,  Optical excitation of the sequential supermolecule H_2P-ZnP-Q induces an electron transfer from the free-base porphyrin (H_2P) to the quinone (Q) via the zinc porphyrin (ZnP). This process is modeled by equations of motion for the reduced density matrix which are solved numerically and approximately analytically. These two solutions agree very well in a great region of parameter space. It is shown that for the majority of solvents the electron transfer occurs with the superexchange mechanism. ,Kein DOI-Link verfügbar,physics/0001014v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Efficiency of different numerical methods for solving Redfield equations,2000,"  The numerical efficiency of different schemes for solving the Liouville-von Neumann equation within multilevel Redfield theory has been studied. Among the tested algorithms are the well-known Runge-Kutta scheme in two different implementations as well as methods especially developed for time propagation: the Short Iterative Arnoldi, Chebyshev and Newtonian propagators. In addition, an implementation of a symplectic integrator has been studied. For a simple example of a two-center electron transfer system we discuss some aspects of the efficiency of these methods to integrate the equations of motion. Overall for time-independent potentials the Newtonian method is recommended. For time-dependent potentials implementations of the Runge-Kutta algorithm are very efficient. ",https://doi.org/10.1063/1.1335656,physics/0009059v1,Yes,potent(2)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Perturbative treatment of intercenter coupling in Redfield theory,2000,"  The quantum dynamics of coupled subsystems connected to a thermal bath is studied. In some of the earlier work the effect of intercenter coupling on the dissipative part was neglected. This is equivalent to a zeroth-order perturbative expansion of the damping term with respect to the intercenter coupling. It is shown numerically for two coupled harmonic oscillators that this treatment can lead to artifacts and a completely wrong description, for example, of a charge transfer processes even for very weak intercenter coupling. Here we perform a first-order treatment and show that these artifacts disappear. In addition, we demonstrate that the thermodynamic equilibrium population is almost reached even for strong intercenter coupling strength. ",https://doi.org/10.1016/S0301-0104(01)00288-9,physics/0012012v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Influence of Static and Dynamic Disorder on the Anisotropy of Emission   in the Ring Antenna Subunits of Purple Bacteria Photosynthetic Systems,2001,  Using the reduced density matrix formalism the time dependence of the exciton scattering in light-harvesting ring systems of purple bacteria is calculated. In contrast to the work of Kumble and Hochstrasser (J. Chem. Phys. 109 (1998) 855) static disorder (fluctuations of the site energies) as well as dynamic disorder (dissipation) is taken into account. For the description of dissipation we use Redfield theory in exciton eigenstates without secular approximation. This is shown to be equivalent to the Markovian limit of Capek's theory in local states. Circular aggregates with 18 pigments are studied to model the B850 ring of bacteriochlorophyls within LH2 complexes. It can be demonstrated that the dissipation is important for the time-dependent anisotropy of the fluorescence. Smaller values of static disorder are sufficient to produce the same decay rates in the anisotropy in comparison with the results by Kumble and Hochstrasser. ,https://doi.org/10.1016/S0301-0104(01)00520-1,physics/0106043v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Stochastic unraveling of Redfield master equations and its application   to electron transfer problems,2003,"  A method for stochastic unraveling of general time-local quantum master equations (QMEs) is proposed. The present kind of jump algorithm allows a numerically efficient treatment of QMEs which are not in Lindblad form, i.e. are not positive semidefinite by definition. The unraveling can be achieved by allowing for trajectories with negative weights. Such a property is necessary, e.g. to unravel the Redfield QME and to treat various related problems with high numerical efficiency. The method is successfully tested on the damped harmonic oscillator and on electron transfer models including one and two reaction coordinates. The obtained results are compared to those from a direct propagation of the reduced density matrix (RDM) as well as from the standard quantum jump method. Comparison of the numerical efficiency is performed considering both the population dynamics and the RDM in the Wigner phase space representation. ",https://doi.org/10.1063/1.1605095,physics/0307050v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Stochastic unraveling of time-local quantum master equations beyond the   Lindblad class,2002,  A new method for stochastic unraveling of general time-local quantum master equations (QME) which involve the reduced density operator at time t only is proposed. The present kind of jump algorithm enables a numerically efficient treatment of QMEs which are not of Lindblad form. So it opens new large fields of application for stochastic methods. The unraveling can be achieved by allowing for trajectories with negative weight. We present results for the quantum Brownian motion and the Redfield QMEs as test examples. The algorithm can also unravel non-Markovian QMEs when they are in a time-local form like in the time-convolutionless formalism. ,https://doi.org/10.1103/PhysRevE.66.037701,quant-ph/0208084v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Coherent laser control of the current through molecular junctions,2007,"  The electron tunneling through a molecular junction modeled by a single site weakly coupled to two leads is studied in the presence of a time-dependent external field using a master equation approach. In the case of small bias voltages and high carrier frequencies of the external field, we observe the phenomenon of coherent destruction of tunneling, i.e. the current through the molecular junction vanishes completely for certain parameters of the external field. In previous studies the tunneling within isolated and open multi-site systems was suppressed; it is shown here that the tunneling between a single site and electronic reservoirs, i.e. the leads, can be suppressed as well. For larger bias voltages the current does not vanish any more since further tunneling channels participate in the electron conduction and we also observe photon-assisted tunneling which leads to steps in the current-voltage characteristics. The described phenomena are demonstrated not only for monochromatic fields but also for laser pulses and therefore could be used for ultrafast optical switching of the current through molecular junctions. ",https://doi.org/10.1209/0295-5075/79/27006,0708.3429v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Wave Packet Dynamics, Ergodicity, and Localization in Quasiperiodic   Chains",2009,"  In this paper, we report results for the wave packet dynamics in a class of quasiperiodic chains consisting of two types of weakly coupled clusters. The dynamics are studied by means of the return probability and the mean square displacement. The wave packets show anomalous diffusion in a stepwise process of fast expansion followed by time intervals of confined wave packet width. Applying perturbation theory, where the coupling parameter v is treated as perturbation, the properties of the eigenstates of the system are investigated and related to the structure of the chains. The results show the appearance of non-localized states only in sufficiently high orders of the perturbation expansions. Further, we compare these results to the exact solutions obtained by numerical diagonalization. This shows that eigenstates spread across the entire chain for v>0, while in the limit v->0 ergodicity is broken and eigenstates only spread across clusters of the same type, in contradistinction to trivial localization for v=0. Caused by this ergodicity breaking, the wave packet dynamics change significantly in the presence of an impurity offering the possibility to control its long-term dynamics. ",https://doi.org/10.1103/PhysRevB.80.214203,0912.0705v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Light Transmission Through Metallic-Mean Quasiperiodic Stacks with   Oblique Incidence,2010,"  The propagation of s- and p-polarized light through quasiperiodic multilayers, consisting of layers with different refractive indices, is studied by the transfer matrix method. In particular, we focus on the transmission coefficient of the systems in dependency on the incidence angle and on the ratio of the refractive indices. We obtain additional bands with almost complete transmission in the quasiperiodic systems at frequencies in the range of the photonic band gap of a system with a periodic alignment of the two materials for both types of light polarization. With increasing incidence angle these bands bend towards higher frequencies, where the curvature of the transmission bands in the quasiperiodic stack depends on the metallic mean of the construction rule. Additionally, in the quasiperiodic systems for p-polarized light the bands show almost complete transmission near the Brewster's angle in contrast to the results for s-polarized light. Further, we present results for the influence of the refractive indices at the midgap frequency of the periodic stack, where the quasiperiodicity was found to be most effective. ",https://doi.org/10.1080/14786435.2010.523721,1010.1396v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Analysis of localization-delocalization transitions in corner-sharing   tetrahedral lattices,2015,"  We study the critical behavior of the Anderson localization-delocalization transition in corner-sharing tetrahedral lattices. We compare our results obtained by three different numerical methods namely the multifractal analysis, the Green resolvent method, and the energy-level statistics which yield the singularity strength, the decay length of the wave functions, and the (integrated) energy-level distribution, respectively. From these measures a finite-size scaling approach allows us to determine the critical parameters simultaneously. With particular emphasis we calculate the propagation of the statistical errors by a Monte-Carlo method. We find a high agreement between the results of all methods and we can estimate the highest critical disorder $W_\mathrm{c}=14.474(8)$ at energy $E_\mathrm{c}=-4.0$ and the critical exponent $\nu=1.565(11)$. Our results agree with a previous study by Fazileh et al. but improve accuracy significantly. ",https://doi.org/10.1140/epjb/e2015-60562-x,1507.03434v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Multifractal analysis of electronic states on random Voronoi-Delaunay   lattices,2015,"  We consider the transport of non-interacting electrons on two- and three-dimensional random Voronoi-Delaunay lattices. It was recently shown that these topologically disordered lattices feature strong disorder anticorrelations between the coordination numbers that qualitatively change the properties of continuous and first-order phase transitions. To determine whether or not these unusual features also influence Anderson localization, we study the electronic wave functions by multifractal analysis and finite-size scaling. We observe only localized states for all energies in the two-dimensional system. In three dimensions, we find two Anderson transitions between localized and extended states very close to the band edges. The critical exponent of the localization length is about 1.6. All these results agree with the usual orthogonal universality class. Additional generic energetic randomness introduced via random potentials does not lead to qualitative changes but allows us to obtain a phase diagram by varying the strength of these potentials. ",https://doi.org/10.1140/epjb/e2015-60698-7,1508.04284v2,Yes,potent(2)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Electronic transport in metallic carbon nanotubes with mixed defects   within the strong localization regime,2017,"  We study the electron transport in metallic carbon nanotubes (CNTs) with realistic defects of different types. We focus on large CNTs with many defects in the mesoscopic range. In a recent paper we demonstrated that the electronic transport in those defective CNTs is in the regime of strong localization. We verify by quantum transport simulations that the localization length of CNTs with defects of mixed types can be related to the localization lengths of CNTs with identical defects by taking the weighted harmonic average. Secondly, we show how to use this result to estimate the conductance of arbitrary defective CNTs, avoiding time consuming transport calculations. ",https://doi.org/10.1016/j.commatsci.2017.06.001,1705.01749v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Strong localization in defective carbon nanotubes: a recursive Green's   function study,2017,"  We study the transport properties of defective single-walled armchair carbon nanotubes (CNTs) on a mesoscopic length scale. Monovacancies and divancancies are positioned randomly along the CNT. The calculations are based on a fast, linearly scaling recursive Green's function formalism that allows us to treat large systems quantum-mechanically. The electronic structure of the CNT is described by a density-functional-based tight-binding model. We determine the influence of the defects on the transmission function for a given defect density by statistical analysis. We show that the system is in the regime of strong localization (i.e. Anderson localization). In the limit of large disorder the conductance scales exponentially with the number of defects. This allows us to extract the localization length. Furthermore, we study in a systematic and comprehensive way, how the conductance, the conductance distribution, and the localization length depend on defect probability, CNT diameter, and temperature. ",https://doi.org/10.1088/1367-2630/16/12/123026,1705.01757v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Improved recursive Green's function formalism for quasi one-dimensional   systems with realistic defects,2017,"  We derive an improved version of the recursive Green's function formalism (RGF), which is a standard tool in the quantum transport theory. We consider the case of disordered quasi one-dimensional materials where the disorder is applied in form of randomly distributed realistic defects, leading to partly periodic Hamiltonian matrices. The algorithm accelerates the common RGF in the recursive decimation scheme, using the iteration steps of the renormalization decimation algorithm. This leads to a smaller effective system, which is treated using the common forward iteration scheme. The computational complexity scales linearly with the number of defects, instead of linearly with the total system length for the conventional approach. We show that the scaling of the calculation time of the Green's function depends on the defect density of a random test system. Furthermore, we discuss the calculation time and the memory requirement of the whole transport formalism applied to defective carbon nanotubes. ",https://doi.org/10.1016/j.jcp.2017.01.024,1705.02178v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,An improved Green's function algorithm applied to quantum transport in   carbon nanotubes,2018,"  The renormalization-decimation algorithm (RDA) of L\'opez Sancho et al. is used in quantum transport theory to calculate bulk and surface Green's functions. We derive an improved version of the RDA for the case of very long quasi one-dimensional unit cells (in transport direction). This covers not only long unit cells but also supercell-like calculations for structures with disorder or defects. In such large systems, short-range interactions lead to sparse real-space Hamiltonian matrices. We show how this and a corresponding subdivision of the unit cell in combination with the decimation technique can be used to reduce the calculation time. Within the resulting algorithm, separate RDA calculations of much smaller effective Hamiltonian matrices must be done for each Green's function, which enables the treatment of systems too large for the common RDA. Finally, we discuss the performance properties of our improved algorithm as well as some exemplary results for chiral carbon nanotubes. ",https://doi.org/10.1016/j.commatsci.2019.05.012,1806.02039v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Electronic transport through defective semiconducting carbon nanotubes,2018,"  We investigate the electronic transport properties of semiconducting ($m$,$n$) carbon nanotubes (CNTs) on the mesoscopic length scale with arbitrarily distributed realistic defects. The study is done by performing quantum transport calculations based on recursive Green's function techniques and an underlying density-functional-based tight-binding model for the description of the electronic structure. Zigzag CNTs as well as chiral CNTs of different diameter are considered. Different defects are exemplarily represented by monovacancies and divacancies. We show the energy-dependent transmission and the temperature-dependent conductance as a function of the number of defects. In the limit of many defetcs, the transport is described by strong localization. Corresponding localization lengths are calculated (energy dependent and temperature dependent) and systematically compared for a large number of CNTs. It is shown, that a distinction by $(m-n)\,\mathrm{mod}\,3$ has to be drawn in order to classify CNTs with different bandgaps. Besides this, the localization length for a given defect probability per unit cell depends linearly on the CNT diameter, but not on the CNT chirality. Finally, elastic mean free paths in the diffusive regime are computed for the limit of few defects, yielding qualitatively same statements. ",https://doi.org/10.1088/2399-6528/aae4cb,1806.02737v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Transport properties of hybrid single-bilayer graphene interfaces in   magnetic field,2023,"  We investigate the electronic properties of a hybrid system that comprises single-bilayer graphene structures subjected to a perpendicular magnetic field. Specifically, our focus is on the behavior exhibited by the zigzag boundaries of the junction, namely Zigzag-1 (ZZ1) and Zigzag-2 (ZZ2), using the continuum Dirac model for rigorous analysis. Our findings reveal a striking dependence of conductance on the width of the bilayer graphene at ZZ1, providing essential insights into the transport behavior of this boundary. Moreover, we observe a captivating phenomenon where the conductance at ZZ2 exhibits prominent maxima, demonstrating a robust correlation with the applied magnetic field. Additionally, our investigation uncovers the profound impact of interfaces on transmission probability, with ZZ1 being notably more affected compared to ZZ2. The variation of the Fermi energy further highlights the significant influence of magnetic field strength on the system's conductive properties, resulting in distinct conductance characteristics between the two regions. The combined results of ZZ1 and ZZ2 provide valuable insights into the system's transport properties. Notably, a clear exponential-like trend in conductance variation with the applied magnetic field underscores the system's strong sensitivity to magnetic changes. ",https://doi.org/10.1103/PhysRevB.108.245419,2305.14284v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Interaction between ionic lattices and superconducting condensates,2007,  The interaction of the ionic lattice with the superconducting condensate is treated in terms of the electrostatic force in superconductors. It is shown that this force is similar but not identical to the force suggested by the volume difference of the normal and superconducting states. The BCS theory shows larger deviations than the two-fluid model. ,https://doi.org/10.1103/PhysRevB.77.014506,0708.3760v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The enhancement of the localization length for two interacting particles   is vanishingly small in transfer-matrix calculations,1997,"  In response to a recent Comment by Frahm et al. regarding our Letter [Phys. Rev. Lett. {\bf 78}, 515 (1997)], we point out that no ``consistent picture'' exists for the enhancement of the localization length $\lambda_2$ for two interacting particles (TIP) proposed previously by Shepelyansky. In fact there are at least 3 different proposals for the dependence of $\lambda_2$ on interaction and disorder. Most analytical and numerical work following Shepelyansky's original approach neglected the phase correlations inherent in the interference phenomena of localization and thus appears at least questionable. In our Letter, we avoided this problem. Our results based on the transfer matrix method (TMM) led us to ``conclude that the TMM ... measures an enhancement ... which is ... due to the finiteness of the systems ''. In particular, we did not question the results of v. Oppen et al. reproduced in the Comment. We also note that in our Letter we explored the limit of large system size M and not the ``thermodynamic limit''. The latter implies of course a finite particle density quite different from the TIP problem. In any case, $\lambda_2$ does not correspond to ``extended states'', because it remains finite and smaller than M. Finally, to the best of our knowledge, there is no ``scaling theory of localization'' for TIP. ",https://doi.org/10.1103/PhysRevLett.78.4890,cond-mat/9702246v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Energy spectra, wavefunctions and quantum diffusion for quasiperiodic   systems",1999,"  We study energy spectra, eigenstates and quantum diffusion for one- and two-dimensional quasiperiodic tight-binding models. As our one-dimensional model system we choose the silver mean or `octonacci' chain. The two-dimensional labyrinth tiling, which is related to the octagonal tiling, is derived from a product of two octonacci chains. This makes it possible to treat rather large systems numerically. For the octonacci chain, one finds singular continuous energy spectra and critical eigenstates which is the typical behaviour for one-dimensional Schr""odinger operators based on substitution sequences. The energy spectra for the labyrinth tiling can, depending on the strength of the quasiperiodic modulation, be either band-like or fractal-like. However, the eigenstates are multifractal. The temporal spreading of a wavepacket is described in terms of the autocorrelation function C(t) and the mean square displacement d(t). In all cases, we observe power laws for C(t) and d(t) with exponents -delta and beta, respectively. For the octonacci chain, 0<delta<1, whereas for the labyrinth tiling a crossover is observed from delta=1 to 0<delta<1 with increasing modulation strength. Corresponding to the multifractal eigenstates, we obtain anomalous diffusion with 0<beta<1 for both systems. Moreover, we find that the behaviour of C(t) and d(t) is independent of the shape and the location of the initial wavepacket. We use our results to check several relations between the diffusion exponent beta and the fractal dimensions of energy spectra and eigenstates that were proposed in the literature. ",https://doi.org/10.1103/PhysRevB.62.15569,cond-mat/9912176v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Trace and antitrace maps for aperiodic sequences, their extensions and   applications",2000,"  We study aperiodic systems based on substitution rules by means of a transfer-matrix approach. In addition to the well-known trace map, we investigate the so-called `antitrace' map, which is the corresponding map for the difference of the off-diagonal elements of the 2x2 transfer matrix. The antitrace maps are obtained for various binary, ternary and quaternary aperiodic sequences, such as the Fibonacci, Thue-Morse, period-doubling, Rudin-Shapiro sequences, and certain generalizations. For arbitrary substitution rules, we show that not only trace maps, but also antitrace maps exist. The dimension of the our antitrace map is r(r+1)/2, where r denotes the number of basic letters in the aperiodic sequence. Analogous maps for specific matrix elements of the transfer matrix can also be constructed, but the maps for the off-diagonal elements and for the difference of the diagonal elements coincide with the antitrace map. Thus, from the trace and antitrace map, we can determine any physical quantity related to the global transfer matrix of the system. As examples, we employ these dynamical maps to compute the transmission coefficients for optical multilayers, harmonic chains, and electronic systems. ",https://doi.org/10.1103/PhysRevB.62.14020,cond-mat/0005463v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Numerical investigations of scaling at the Anderson transition,2002,"  At low temperature T, a significant difference between the behavior of crystals on the one hand and disordered solids on the other is seen: sufficiently strong disorder can give rise to a transition of the transport properties from conducting behavior with finite resistance R to insulating behavior with R=infinity as T -> 0. This well-studied phenomenon is called the disorder-driven metal-insulator transition and it is characteristic to non-crystalline solids. In this review of recent advances, we have presented results of transport studies in disordered systems, ranging from modifications of the standard Anderson model of localization to effects of a two-body interaction. Of paramount importance in these studies was always the highest possible accuracy of the raw data combined with the careful subsequent application of the finite-size scaling technique. In fact, it is this scaling method that has allowed numerical studies to move beyond simple extrapolations and reliably construct estimates of quantities as if one were studying an infinite system. ",https://doi.org/10.1007/978-3-540-45202-7_1,cond-mat/0212569v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Magneto-transport in periodic and quasiperiodic arrays of mesoscopic   rings,2003,"  We study theoretically the transmission properties of serially connected mesoscopic rings threaded by a magnetic flux. Within a tight-binding formalism we derive exact analytical results for the transmission through periodic and quasiperiodic Fibonacci arrays of rings of two different sizes. The role played by the number of scatterers in each arm of the ring is analyzed in some detail. The behavior of the transmission coefficient at a particular value of the energy of the incident electron is studied as a function of the magnetic flux (and vice versa) for both the periodic and quasiperiodic arrays of rings having different number of atoms in the arms. We find interesting resonance properties at specific values of the flux, as well as a power-law decay in the transmission coefficient as the number of rings increases, when the magnetic field is switched off. For the quasiperiodic Fibonacci sequence we discuss various features of the transmission characteristics as functions of energy and flux, including one special case where, at a special value of the energy and in the absence of any magnetic field, the transmittivity changes periodically as a function of the system size. ",https://doi.org/10.1103/PhysRevB.68.195417,cond-mat/0304364v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Magnetic phase diagram of the spin-1 two-dimensional J1-J3 Heisenberg   model on a triangular lattice,2012,"  The spin-1 Heisenberg model on a triangular lattice with the ferromagnetic nearest, $J_1=-(1-p)J,$ $J>0$, and antiferromagnetic third-nearest-neighbor, $J_3=pJ$, exchange interactions is studied in the range of the parameter $0 \leqslant p \leqslant 1$. Mori's projection operator technique is used as a method, which retains the rotation symmetry of spin components and does not anticipate any magnetic ordering. For zero temperature several phase transitions are observed. At $p\approx 0.2$ the ground state is transformed from the ferromagnetic spin structure into a disordered state, which in its turn is changed to an antiferromagnetic long-range ordered state with the incommensurate ordering vector ${\bf Q = Q^\prime} \approx (1.16, 0)$ at $p\approx 0.31$. With the further growth of $p$ the ordering vector moves along the line ${\bf Q^\prime-Q_c}$ to the commensurate point ${\bf Q_c}=(\frac{2\pi}{3}, 0)$, which is reached at $p = 1$. The final state with an antiferromagnetic long-range order can be conceived as four interpenetrating sublattices with the $120^\circ$ spin structure on each of them. Obtained results are used for interpretation of the incommensurate magnetic ordering observed in NiGa$_2$S$_4$. ",https://doi.org/10.1016/j.physleta.2012.01.045,1201.6042v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,VR Facial Animation for Immersive Telepresence Avatars,2023,"  VR Facial Animation is necessary in applications requiring clear view of the face, even though a VR headset is worn. In our case, we aim to animate the face of an operator who is controlling our robotic avatar system. We propose a real-time capable pipeline with very fast adaptation for specific operators. In a quick enrollment step, we capture a sequence of source images from the operator without the VR headset which contain all the important operator-specific appearance information. During inference, we then use the operator keypoint information extracted from a mouth camera and two eye cameras to estimate the target expression and head pose, to which we map the appearance of a source still image. In order to enhance the mouth expression accuracy, we dynamically select an auxiliary expression frame from the captured sequence. This selection is done by learning to transform the current mouth keypoints into the source camera space, where the alignment can be determined accurately. We, furthermore, demonstrate an eye tracking pipeline that can be trained in less than a minute, a time efficient way to train the whole pipeline given a dataset that includes only complete faces, show exemplary results generated by our method, and discuss performance at the ANA Avatar XPRIZE semifinals. ",https://doi.org/10.1109/IROS47612.2022.9981892,2304.12051v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Shape Analysis of the Level Spacing Distribution around the Metal   Insulator Transition in the Three Dimensional Anderson Model,1994,"  We present a new method for the numerical treatment of second order phase transitions using the level spacing distribution function $P(s)$. We show that the quantities introduced originally for the shape analysis of eigenvectors can be properly applied for the description of the eigenvalues as well. The position of the metal--insulator transition (MIT) of the three dimensional Anderson model and the critical exponent are evaluated. The shape analysis of $P(s)$ obtained numerically shows that near the MIT $P(s)$ is clearly different from both the Brody distribution and from Izrailev's formula, and the best description is of the form $P(s)=c_1\,s\exp(-c_2\,s^{1+\beta})$, with $\beta\approx 0.2$. This is in good agreement with recent analytical results. ",https://doi.org/10.1103/PhysRevB.52.7783,cond-mat/9407058v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Multifractal analysis of the metal-insulator transition in anisotropic   systems,1996,"  We study the Anderson model of localization with anisotropic hopping in three dimensions for weakly coupled chains and weakly coupled planes. The eigenstates of the Hamiltonian, as computed by Lanczos diagonalization for systems of sizes up to $48^3$, show multifractal behavior at the metal-insulator transition even for strong anisotropy. The critical disorder strength $W_c$ determined from the system size dependence of the singularity spectra is in a reasonable agreement with a recent study using transfer matrix methods. But the respective spectrum at $W_c$ deviates from the ``characteristic spectrum'' determined for the isotropic system. This indicates a quantitative difference of the multifractal properties of states of the anisotropic as compared to the isotropic system. Further, we calculate the Kubo conductivity for given anisotropies by exact diagonalization. Already for small system sizes of only $12^3$ sites we observe a rapidly decreasing conductivity in the directions with reduced hopping if the coupling becomes weaker. ",https://doi.org/10.1103/PhysRevB.55.9463,cond-mat/9609276v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Two interacting particles at the metal-insulator transition,1998,"  To investigate the influence of electronic interaction on the metal-insulator transition (MIT), we consider the Aubry-Andr\'{e} (or Harper) model which describes a quasiperiodic one-dimensional quantum system of non-interacting electrons and exhibits an MIT. For a two-particle system, we study the effect of a Hubbard interaction on the transition by means of the transfer-matrix method and finite-size scaling. In agreement with previous studies we find that the interaction localizes some states in the otherwise metallic phase of the system. Nevertheless, the MIT remains unaffected by the interaction. For a long-range interaction, many more states become localized for sufficiently large interaction strength and the MIT appears to shift towards smaller quasiperiodic potential strength. ",https://doi.org/10.1007/s100510050721,cond-mat/9808192v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Smoothed universal correlations in the two-dimensional Anderson model,1998,"  We report on calculations of smoothed spectral correlations in the two-dimensional Anderson model for weak disorder. As pointed out in (M. Wilkinson, J. Phys. A: Math. Gen. 21, 1173 (1988)), an analysis of the smoothing dependence of the correlation functions provides a sensitive means of establishing consistency with random matrix theory. We use a semiclassical approach to describe these fluctuations and offer a detailed comparison between numerical and analytical calculations for an exhaustive set of two-point correlation functions. We consider parametric correlation functions with an external Aharonov-Bohm flux as a parameter and discuss two cases, namely broken time-reversal invariance and partial breaking of time-reversal invariance. Three types of correlation functions are considered: density-of-states, velocity and matrix element correlation functions. For the values of smoothing parameter close to the mean level spacing the semiclassical expressions and the numerical results agree quite well in the whole range of the magnetic flux. ",https://doi.org/10.1103/PhysRevB.59.4080,cond-mat/9811258v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Critical properties of the metal-insulator transition in anisotropic   systems,1999,"  We study the three-dimensional Anderson model of localization with anisotropic hopping, i.e., weakly coupled chains and weakly coupled planes. In our extensive numerical study we identify and characterize the metal-insulator transition by means of the transfer-matrix method. The values of the critical disorder $W_c$ obtained are consistent with results of previous studies, including multifractal analysis of the wave functions and energy level statistics. $W_c$ decreases from its isotropic value with a power law as a function of anisotropy. Using high accuracy data for large system sizes we estimate the critical exponent as $\nu=1.62\pm0.07$. This is in agreement with its value in the isotropic case and in other models of the orthogonal universality class. ",https://doi.org/10.1007/s100510051173,cond-mat/9911029v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,From localization to delocalization in the quantum Coulomb glass,2000,"  We numerically investigate how electron-electron interactions influence the transport properties of disordered electrons in two dimensions. Our study is based on the quantum Coulomb glass model appropriately generalized to include the spin degrees of freedom. In order to obtain the low-energy properties of this model we employ the Hartree-Fock based diagonalization, an efficient numerical method similar to the configuration interaction approach in quantum chemistry. We calculate the d.c. conductance by means of the Kubo-Greenwood formula and pay particular attention to the spin degrees of freedom. In agreement with earlier results we find that electron-electron interactions can cause delocalization. For spinful electrons this delocalization is significantly larger than for spinless electrons. ",https://doi.org/10.1002/(SICI)1521-3951(200003)218:1<31::AID-PSSB31>3.0.CO;2-U,cond-mat/0003395v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Effects of Scale-Free Disorder on the Anderson Metal-Insulator   Transition,2004,"  We investigate the three-dimensional Anderson model of localization via a modified transfer-matrix method in the presence of scale-free diagonal disorder characterized by a disorder correlation function $g(r)$ decaying asymptotically as $r^{-\alpha}$. We study the dependence of the localization-length exponent $\nu$ on the correlation-strength exponent $\alpha$. % For fixed disorder $W$, there is a critical $\alpha_{\rm c}$, such that for $\alpha < \alpha_{\rm c}$, $\nu=2/\alpha$ and for $\alpha > \alpha_{\rm c}$, $\nu$ remains that of the uncorrelated system in accordance with the extended Harris criterion. At the band center, $\nu$ is independent of $\alpha$ but equal to that of the uncorrelated system. The physical mechanisms leading to this different behavior are discussed. ",https://doi.org/10.1209/epl/i2004-10267-5,cond-mat/0402018v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Integrable impurities for an open fermion chain,2000,"  Employing the graded versions of the Yang-Baxter equation and the reflection equations, we construct two kinds of integrable impurities for a small-polaron model with general open boundary conditions: (a) we shift the spectral parameter of the local Lax operator at arbitrary sites in the bulk, and (b) we embed the impurity fermion vertex at each boundary of the chain. The Hamiltonians with different types of impurity terms are given explicitly. The Bethe ansatz equations, as well as the eigenvalues of the Hamiltonians, are constructed by means of the quantum inverse scattering method. In addition, we discuss the ground-state properties in the thermodynamic limit. ",https://doi.org/10.1088/0305-4470/33/21/302,nlin/0001040v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Optimal control of molecular electronics by femtosecond laser pulses,2007,  Combining the features of molecular wires and femtosecond laser pulses gives the unique opportunity to optically switch electron currents in molecular devices with very high speed. Based on a weak-coupling approximation between wire and leads a quantum master equation for the population dynamics and the electric current through the molecular wire has been developed which allows for arbitrary time-dependent laser fields interacting with the wire. This formalism is combined with the theory of optimal control. For a tight-binding approximation of the wire we show how to compute the laser pulses to switch the current through the wire on and off. With this approach the desired pattern of the current in time can be chosen in an almost arbitrary fashion. ,Kein DOI-Link verfügbar,0708.2908v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Switching the current through molecular wires,2007,"  The influence of Gaussian laser pulses on the transport through molecular wires is investigated within a tight-binding model for spinless electrons including correlation. Motivated by the phenomenon of coherent destruction of tunneling for monochromatic laser fields, situations are studied in which the maximum amplitude of the electric field fulfills the conditions for the destructive quantum effect. It is shown that, as for monochromatic laser pulses, the average current through the wire can be suppressed. For parameters of the model, which do not show a net current without any optical field, a Gaussian laser pulse can establish a temporary current. In addition, the effect of electron correlation on the current is investigated. ",https://doi.org/10.1209/epl/i2006-10074-0,0708.3432v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Coherent destruction of the current through molecular wires using short   laser pulses,2007,"  A molecular wire coupled to two electron reservoirs is investigated within a tight-binding approach including spin and Coulomb interaction. Under the assumption of weak coupling to the electron reservoirs a quantum master equation can be derived for the electron transport through the wire. Motivated by the phenomenon of coherent destruction of tunneling for monochromatic laser fields, the influence of Gaussian laser pulses on the transport through the wires is studied. For situations in which the maximum amplitude of the electric field fulfills the conditions for the destructive quantum effect the average current through the system can be suppressed even for a wire consisting of only one site. Turning on the electron correlation does not destroy the suppression of the current by the laser. ",Kein DOI-Link verfügbar,0708.3433v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Child-sized 3D Printed igus Humanoid Open Platform,2018,"  The use of standard platforms in the field of humanoid robotics can accelerate research, and lower the entry barrier for new research groups. While many affordable humanoid standard platforms exist in the lower size ranges of up to 60cm, beyond this the few available standard platforms quickly become significantly more expensive, and difficult to operate and maintain. In this paper, the igus Humanoid Open Platform is presented---a new, affordable, versatile and easily customisable standard platform for humanoid robots in the child-sized range. At 90cm, the robot is large enough to interact with a human-scale environment in a meaningful way, and is equipped with enough torque and computing power to foster research in many possible directions. The structure of the robot is entirely 3D printed, allowing for a lightweight and appealing design. The electrical and mechanical designs of the robot are presented, and the main features of the corresponding open-source ROS software are discussed. The 3D CAD files for all of the robot parts have been released open-source in conjunction with this paper. ",Kein DOI-Link verfügbar,1809.10701v1,Yes,versatile(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Edge state critical behavior of the integer quantum Hall transition,2020,"  The integer quantum Hall effect features a paradigmatic quantum phase transition. Despite decades of work, experimental, numerical, and analytical studies have yet to agree on a unified understanding of the critical behavior. Based on a numerical Green function approach, we consider the quantum Hall transition in a microscopic model of non-interacting disordered electrons on a simple square lattice. In a strip geometry, topologically induced edge states extend along the system rim and undergo localization-delocalization transitions as function of energy. We investigate the boundary critical behavior in the lowest Landau band and compare it with a recent tight-binding approach to the bulk critical behavior [Phys. Rev. B 99, 121301(R) (2019)] as well as other recent studies of the quantum Hall transition with both open and periodic boundary conditions. ",https://doi.org/10.1140/epjs/s11734-021-00064-6,2004.01611v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Integer quantum Hall transition on a tight-binding lattice,2018,"  Even though the integer quantum Hall transition has been investigated for nearly four decades its critical behavior remains a puzzle. The best theoretical and experimental results for the localization length exponent $\nu$ differ significantly from each other, casting doubt on our fundamental understanding. While this discrepancy is often attributed to long-range Coulomb interactions, Gruzberg et al. [Phys. Rev. B 95, 125414 (2017)] recently suggested that the semiclassical Chalker-Coddington model, widely employed in numerical simulations, is incomplete, questioning the established central theoretical results. To shed light on the controversy, we perform a high-accuracy study of the integer quantum Hall transition for a microscopic model of disordered electrons. We find a localization length exponent $\nu=2.58(3)$ validating the result of the Chalker-Coddington network. ",https://doi.org/10.1103/PhysRevB.99.121301,1805.09958v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Transport properties in gapped graphene through magnetic barrier in a   laser field,2023,"  We study the transport properties of Dirac fermions through gapped graphene through a magnetic barrier irradiated by a laser field oscillating in time. We use Floquet theory and the solution of Weber's differential equation to determine the energy spectrum corresponding to the three regions composing the system. The boundary conditions and the transfer matrix approach {are} employed to explicitly determine the transmission probabilities for multi-energy bands and the associated conductance. As an illustration, we focus only on the three first bands: the central band $T_0$ (zero photon exchange) and the two first side bands $T_{\pm1}$ (photon emission or absorption). It is found that the laser field activates the process of translation through photon exchange. Furthermore, we show that varying the incident angle and energy gap strongly affects the transmission process. The conductance increases when the number of electrons that cross the barrier increases, namely when there is a significant transmission. ",https://doi.org/10.1016/j.physe.2023.115865,2307.03999v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Self-centering 3-DOF feet controller for hands-free locomotion control   in telepresence and virtual reality,2024,"  We present a novel seated foot controller for handling 3-DOF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering foot controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction. ",Kein DOI-Link verfügbar,2408.02319v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Two Coupled Harmonic Oscillators on Non-commutative Plane,2003,"  We investigate a system of two coupled harmonic oscillators on the non-commutative plane \RR^2_{\theta} by requiring that the spatial coordinates do not commute. We show that the system can be diagonalized by a suitable transformation, i.e. a rotation with a mixing angle \alpha. The obtained eigenstates as well as the eigenvalues depend on the non-commutativity parameter \theta. Focusing on the ground state wave function before the transformation, we calculate the density matrix \rho_0(\theta) and find that its traces {\rm Tr}(\rho_{0}(\theta)) and {\rm Tr}(\rho_0^2(\theta)) are not affected by the non-commutativity. Evaluating the Wigner function on \RR^2_{\theta} confirms this. The uncertainty relation is explicitly determined and found to depend on \theta. For small values of \theta, the relation is shifted by a \theta^2 term, which can be interpreted as a quantum correction. The calculated entropy does not change with respect to the normal case. We consider the limits \alpha=1 and \alpha={\pi\over 2}. In first case, by identifying \theta to the squared magnetic length, one can recover basic features of the Hall system. ",https://doi.org/10.1142/S0217751X05020835,hep-th/0309105v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Application of random matrix theory to quasiperiodic systems,1998,"  We study statistical properties of energy spectra of a tight-binding model on the two-dimensional quasiperiodic Ammann-Beenker tiling. Taking into account the symmetries of finite approximants, we find that the underlying universal level-spacing distribution is given by the Gaussian orthogonal random matrix ensemble, and thus differs from the critical level-spacing distribution observed at the metal-insulator transition in the three-dimensional Anderson model of disorder. Our data allow us to see the difference to the Wigner surmise. ",https://doi.org/10.1016/S0378-4371(98)00634-7,cond-mat/9809370v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Energy Levels of Quasiperiodic Hamiltonians, Spectral Unfolding, and   Random Matrix Theory",1998,"  We consider a tight-binding Hamiltonian defined on the quasiperiodic Ammann-Beenker tiling. Although the density of states (DOS) is rather spiky, the integrated DOS is quite smooth and can be used to perform spectral unfolding. The effect of unfolding on the integrated level-spacing distribution is investigated for various parts of the spectrum which show different behaviour of the DOS. For energy intervals with approximately constant DOS, we find good agreement with the distribution of the Gaussian orthogonal random matrix ensemble (GOE) even without unfolding. For energy ranges with fluctuating DOS, we observe deviations from the GOE result. After unfolding, we always recover the GOE distribution. ",https://doi.org/10.1016/S0010-4655(99)00391-4,cond-mat/9811359v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Universal level-spacing statistics in quasiperiodic tight-binding models,1999,"  We study statistical properties of the energy spectra of two-dimensional quasiperiodic tight-binding models. The multifractal nature of the eigenstates of these models is corroborated by the scaling of the participation numbers with the systems size. Hence one might have expected `critical' or `intermediate' statistics for the level-spacing distributions as observed at the metal-insulator transition in the three-dimensional Anderson model of disorder. However, our numerical results are in perfect agreement with the universal level-spacing distributions of the Gaussian orthogonal random matrix ensemble, including the distribution of spacings between second, third, and forth neighbour energy levels. ",https://doi.org/10.1016/S0921-5093(00)01173-4,cond-mat/9908063v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Dynamical Scaling Properties of Electrons in Quantum Systems with   Multifractal Eigenstates,2000,"  We study the intricate relationships between the dynamical scaling properties of electron wave packets and the multifractality of the eigenstates in quantum systems. Numerical simulations for the Harper model and the Fibonacci chain indicate that the root mean square displacement displays the scaling behavior $r(t)\sim t^\beta$ with $\beta=D_2^\psi$, where $D_2^\psi$ is the correlation dimension of the multifractal eigenstates. The equality can be generalized to $d$-dimensional systems as $\beta=D_2^\psi/d$, as long as the electron motion is ballistic in the effective $D_2^\psi$-dimensional space. This equality should be replaced by $\beta<D_2^\psi/d$ if the motion is non-ballistic, as supported by all known results. ",Kein DOI-Link verfügbar,cond-mat/0011118v1,Yes,intricate(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Fock space localization, return probability, and conductance of   disordered interacting electrons",2001,"  We numerically simulate the low-energy properties of interacting electrons in a random potential using the Hartree-Fock based exact diagonalization method. In particular, we investigate how the transport properties are influenced by the combined effects of disorder and correlations in the presence of the electron spin. To this end we calculate the participation number of many-particle states in Fock space, the return probability of single-particle excitations, and the Kubo-Greenwood conductance. It turns out that in the strongly localized regime interactions increase the conductance whereas for weak disorder interactions decrease the conductance. In contrast, single-particle excitations in general experience a localizing influence of the interactions. ",https://doi.org/10.1016/S0921-4526(00)00778-X,cond-mat/0105481v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The concept of correlated density and its application,2006,"  The correlated density appears in many physical systems ranging from dense interacting gases up to Fermi liquids which develop a coherent state at low temperatures, the superconductivity. One consequence of the correlated density is the Bernoulli potential in superconductors which compensates forces from dielectric currents. This Bernoulli potential allows to access material parameters. Though within the surface potential these contributions are largely canceled, the bulk measurements with NMR can access this potential. Recent experiments are explained and new ones suggested. The underlying quantum statistical theory in nonequilibrium is the nonlocal kinetic theory developed earlier. ",https://doi.org/10.1142/S0217979207043713,cond-mat/0612080v1,Yes,potent(4)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Supersymmetric Embedding of the Quantum Hall Matrix Model,2004,"  We develop a supersymmetric extension of the Susskind-Polychronakos matrix theory for the quantum Hall fluids. This is done by considering a system combining two sets of different particles and using both a component field method as well as world line superfields. Our construction yields a class of models for fractional quantum Hall systems with two phases U and D involving, respectively $N_1$ bosons and $N_2$ fermions. We build the corresponding supersymmetric matrix action, derive and solve the supersymmetric generalization of the Susskind-Polychronakos constraint equations. We show that the general U(N) gauge invariant solution for the ground state involves two configurations parameterized by the bosonic contribution $k_{1}$ (integer) and in addition a new degree of freedom $k_{2}$, which is restricted to 0 and 1. We study in detail the two particular values of $k_{2}$ and show that the classical (Susskind) filling factor $\nu $ receives no quantum correction. We conclude that the Polychronakos effect is exactly compensated by the opposite fermionic contributions. ",https://doi.org/10.1088/1126-6708/2004/11/075,hep-th/0410070v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Single spontaneous photon as a coherent beamsplitter for an atomic   matterwave,2010,"  In spontaneous emission an atom in an excited state undergoes a transition to the ground state and emits a single photon. Associated with the emission is a change of the atomic momentum due to photon recoil. Photon emission can be modified close to surfaces and in cavities. For an ion, localized in front of a mirror, coherence of the emitted resonance fluorescence has been reported. In free space experiments demonstrated that spontaneous emission destroys motional coherence. Here we report on motional coherence created by a single spontaneous emission event close to a mirror surface. The coherence in the free atomic motion is verified by atom interferometry. The photon can be regarded as a beamsplitter for an atomic matterwave and consequently our experiment extends the original recoiling slit Gedanken experiment by Einstein to the case where the slit is in a robust coherent superposition of the two recoils associated with the two paths of the quanta. ",https://doi.org/10.1038/NPHYS1961,1012.4704v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Stirrers and movers actuated by oscillating fields,2016,"  Locomotion via cyclic moves presents a challenge to mesoscopic objects in overdamped environments, where time reversibility may prevent directed motion. Most reported cyclic movers exploit anisotropic drag to push themselves forward. Under an oscillating drive, however, anisotropic drag enables locomotion only if the objects can change their shape. Here, we present a strategy that unexpectedly enables structurally invariant objects to move under oscillating fields. The objects are self-assembled clusters of magnetic particles that exhibit an off-centered dipole moment. By theoretical modeling and in experiments with magnetic Janus particles, we demonstrate that the interaction between such anisotropic particles in the cluster breaks time reversibility. Experimentally, we show that the magnetic configuration of a cluster determines its motion path. We realize stirrers and steerable movers with helical or directed path using the same particle system. The presented strategy based on internal interactions establishes a counterpart to locomotion via anisotropic drag. ",https://doi.org/10.1103/PhysRevResearch.2.023092,1607.04733v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,The igus Humanoid Open Platform: A Child-sized 3D Printed Open-Source   Robot for Research,2018,"  The use of standard robotic platforms can accelerate research and lower the entry barrier for new research groups. There exist many affordable humanoid standard platforms in the lower size ranges of up to 60cm, but larger humanoid robots quickly become less affordable and more difficult to operate, maintain and modify. The igus Humanoid Open Platform is a new and affordable, fully open-source humanoid platform. At 92cm in height, the robot is capable of interacting in an environment meant for humans, and is equipped with enough sensors, actuators and computing power to support researchers in many fields. The structure of the robot is entirely 3D printed, leading to a lightweight and visually appealing design. The main features of the platform are described in this article. ",Kein DOI-Link verfügbar,1809.11110v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,First International HARTING Open Source Prize Winner: The igus Humanoid   Open Platform,2018,"  The use of standard platforms in the field of humanoid robotics can lower the entry barrier for new research groups, and accelerate research by the facilitation of code sharing. Numerous humanoid standard platforms exist in the lower size ranges of up to 60cm, but beyond that humanoid robots scale up quickly in weight and price, becoming less affordable and more difficult to operate, maintain and modify. The igus Humanoid Open Platform is an affordable, fully open-source platform for humanoid research. At 92cm, the robot is capable of acting in an environment meant for humans, and is equipped with enough sensors, actuators and computing power to support researchers in many fields. The structure of the robot is entirely 3D printed, leading to a lightweight and visually appealing design. This paper covers the mechanical and electrical aspects of the robot, as well as the main features of the corresponding open-source ROS software. At RoboCup 2016, the platform was awarded the first International HARTING Open Source Prize. ",Kein DOI-Link verfügbar,1810.00948v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,NimbRo Avatar: Interactive Immersive Telepresence with Force-Feedback   Telemanipulation,2021,"  Robotic avatars promise immersive teleoperation with human-like manipulation and communication capabilities. We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation. Our avatar robot features an anthropomorphic bimanual arm configuration with dexterous hands. The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger. The robot torso is mounted on a holonomic base, providing locomotion capability in typical indoor scenarios, controlled using a 3D rudder device. Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR HMD worn by the operator. Movement latency is hidden using spherical rendering. The head also carries a telepresence screen displaying a synthesized image of the operator with facial animation, which enables direct interaction with remote persons. We evaluate our system successfully both in a user study with untrained operators as well as a longer and more complex integrated mission. We discuss lessons learned from the trials and possible improvements. ",Kein DOI-Link verfügbar,2109.13772v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Audio-based Roughness Sensing and Tactile Feedback for Haptic Perception   in Telepresence,2023,"  Haptic perception is highly important for immersive teleoperation of robots, especially for accomplishing manipulation tasks. We propose a low-cost haptic sensing and rendering system, which is capable of detecting and displaying surface roughness. As the robot fingertip moves across a surface of interest, two microphones capture sound coupled directly through the fingertip and through the air, respectively. A learning-based detector system analyzes the data in real time and gives roughness estimates with both high temporal resolution and low latency. Finally, an audio-based vibrational actuator displays the result to the human operator. We demonstrate the effectiveness of our system through lab experiments and our winning entry in the ANA Avatar XPRIZE competition finals, where briefly trained judges solved a roughness-based selection task even without additional vision feedback. We publish our dataset used for training and evaluation together with our trained models to enable reproducibility of results. ",Kein DOI-Link verfügbar,2303.07186v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Bernoulli potential in type-I and weak type-II superconductors: III.   Electrostatic potential above the vortex lattice,2004,"  The electrostatic potential above the Abrikosov vortex lattice, discussed earlier by Blatter {\em et al.} {[}PRL {\bf 77}, 566 (1996){]}, is evaluated within the Ginzburg-Landau theory. Unlike previous studies we include the surface dipole. Close to the critical temperature, the surface dipole reduces the electrostatic potential to values below a sensitivity of recent sensors. At low temperatures the surface dipole is less effective and the electrostatic potential remains observable as predicted earlier. ",https://doi.org/10.1103/PhysRevB.71.024526,cond-mat/0409397v2,Yes,potent(3)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Negative Absolute Temperature for Motional Degrees of Freedom,2012,"  Absolute temperature, the fundamental temperature scale in thermodynamics, is usually bound to be positive. Under special conditions, however, negative temperatures - where high-energy states are more occupied than low-energy states - are also possible. So far, such states have been demonstrated in localized systems with finite, discrete spectra. Here, we were able to prepare a negative temperature state for motional degrees of freedom. By tailoring the Bose-Hubbard Hamiltonian we created an attractively interacting ensemble of ultracold bosons at negative temperature that is stable against collapse for arbitrary atom numbers. The quasi-momentum distribution develops sharp peaks at the upper band edge, revealing thermal equilibrium and bosonic coherence over several lattice sites. Negative temperatures imply negative pressures and open up new parameter regimes for cold atoms, enabling fundamentally new many-body states and counterintuitive effects such as Carnot engines above unity efficiency. ",https://doi.org/10.1126/science.1227831,1211.0545v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Coupling Identical 1D Many-Body Localized Systems,2015,"  We experimentally study the effects of coupling one-dimensional Many-Body Localized (MBL) systems with identical disorder. Using a gas of ultracold fermions in an optical lattice, we artifically prepare an initial charge density wave in an array of 1D tubes with quasi-random onsite disorder and monitor the subsequent dynamics over several thousand tunneling times. We find a strikingly different behavior between MBL and Anderson Localization. While the non-interacting Anderson case remains localized, in the interacting case any coupling between the tubes leads to a delocalization of the entire system. ",https://doi.org/10.1103/PhysRevLett.116.140401,1509.00478v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Exploring the Single-Particle Mobility Edge in a One-Dimensional   Quasiperiodic Optical Lattice,2017,"  A single-particle mobility edge (SPME) marks a critical energy separating extended from localized states in a quantum system. In one-dimensional systems with uncorrelated disorder, a SPME cannot exist, since all single-particle states localize for arbitrarily weak disorder strengths. However, if correlations are present in the disorder potential, the localization transition can occur at a finite disorder strength and SPMEs become possible. In this work, we find experimental evidence for the existence of such a SPME in a one-dimensional quasi-periodic optical lattice. Specifically, we find a regime where extended and localized single-particle states coexist, in good agreement with theoretical simulations, which predict a SPME in this regime. ",https://doi.org/10.1103/PhysRevLett.120.160404,1709.03478v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Grown-up NimbRo Robots Winning RoboCup 2017 Humanoid AdultSize Soccer   Competitions,2018,"  The ongoing evolution of the RoboCup Humanoid League led in 2017 to the introduction of one vs. one soccer games for the AdultSize robots, which motived our team NimbRo to enter this category. In this paper, we present the mechatronic design of our upgraded robot Copedo and the newly developed NimbRo-OP2, which received the RoboCup Design Award. We also describe improved approaches to visual perception of the game situation, including compassless localization on a soccer field with symmetric appearance, and the generation of soccer behaviors. At RoboCup 2017 in Nagoya, our robots played very well, winning the AdultSize soccer tournament with high scores. Our robots also won the technical challenges and we present the developed solutions. ",Kein DOI-Link verfügbar,1809.04928v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Humanoid TeenSize Open Platform NimbRo-OP,2018,"  In recent years, the introduction of affordable platforms in the KidSize class of the Humanoid League has had a positive impact on the performance of soccer robots. The lack of readily available larger robots, however, severely affects the number of participants in Teen- and AdultSize and consequently the progress of research that focuses on the challenges arising with robots of larger weight and size. This paper presents the first hardware release of a low cost Humanoid TeenSize open platform for research, the first software release, and the current state of ROS-based software development. The NimbRo-OP robot was designed to be easily manufactured, assembled, repaired, and modified. It is equipped with a wide-angle camera, ample computing power, and enough torque to enable full-body motions, such as dynamic bipedal locomotion, kicking, and getting up. ",Kein DOI-Link verfügbar,1809.11024v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Learning to Improve Capture Steps for Disturbance Rejection in Humanoid   Soccer,2018,"  Over the past few years, soccer-playing humanoid robots have advanced significantly. Elementary skills, such as bipedal walking, visual perception, and collision avoidance have matured enough to allow for dynamic and exciting games. When two robots are fighting for the ball, they frequently push each other and balance recovery becomes crucial. In this paper, we report on insights we gained from systematic push experiments performed on a bipedal model and outline an online learning method we used to improve its push-recovery capabilities. In addition, we describe how the localization ambiguity introduced by the uniform goal color was resolved and report on the results of the RoboCup 2013 competition. ",Kein DOI-Link verfügbar,1809.11072v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,RoboCup 2016 Humanoid TeenSize Winner NimbRo: Robust Visual Perception   and Soccer Behaviors,2018,"  The trend in the RoboCup Humanoid League rules over the past few years has been towards a more realistic and challenging game environment. Elementary skills such as visual perception and walking, which had become mature enough for exciting gameplay, are now once again core challenges. The field goals are both white, and the walking surface is artificial grass, which constitutes a much more irregular surface than the carpet used before. In this paper, team NimbRo TeenSize, the winner of the TeenSize class of the RoboCup 2016 Humanoid League, presents its robotic platforms, the adaptations that had to be made to them, and the newest developments in visual perception and soccer behaviour. ",Kein DOI-Link verfügbar,1809.11127v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Autonomous Wall Building with a UGV-UAV Team at MBZIRC 2020,2020,"  Constructing large structures with robots is a challenging task with many potential applications that requires mobile manipulation capabilities. We present two systems for autonomous wall building that we developed for the Mohamed Bin Zayed International Robotics Challenge 2020. Both systems autonomously perceive their environment, find bricks, and build a predefined wall structure. While the UGV uses a 3D LiDAR-based perception system which measures brick poses with high precision, the UAV employs a real-time camera-based system for visual servoing. We report results and insights from our successful participation at the MBZIRC 2020 Finals, additional lab experiments, and discuss the lessons learned from the competition. ",Kein DOI-Link verfügbar,2011.01999v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Fast Object Learning and Dual-arm Coordination for Cluttered Stowing,   Picking, and Packing",2018,"  Robotic picking from cluttered bins is a demanding task, for which Amazon Robotics holds challenges. The 2017 Amazon Robotics Challenge (ARC) required stowing items into a storage system, picking specific items, and packing them into boxes. In this paper, we describe the entry of team NimbRo Picking. Our deep object perception pipeline can be quickly and efficiently adapted to new items using a custom turntable capture system and transfer learning. It produces high-quality item segments, on which grasp poses are found. A planning component coordinates manipulation actions between two robot arms, minimizing execution time. The system has been demonstrated successfully at ARC, where our team reached second places in both the picking task and the final stow-and-pick task. We also evaluate individual components. ",https://doi.org/10.1109/ICRA.2018.8461195,1810.02977v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Robust Immersive Telepresence and Mobile Telemanipulation: NimbRo wins   ANA Avatar XPRIZE Finals,2023,"  Robotic avatar systems promise to bridge distances and reduce the need for travel. We present the updated NimbRo avatar system, winner of the $5M grand prize at the international ANA Avatar XPRIZE competition, which required participants to build intuitive and immersive robotic telepresence systems that could be operated by briefly trained operators. We describe key improvements for the finals, compared to the system used in the semifinals: To operate without a power- and communications tether, we integrated a battery and a robust redundant wireless communication system. Video and audio data are compressed using low-latency HEVC and Opus codecs. We propose a new locomotion control device with tunable resistance force. To increase flexibility, the robot's upper-body height can be adjusted by the operator. We describe essential monitoring and robustness tools which enabled the success at the competition. Finally, we analyze our performance at the competition finals and discuss lessons learned. ",Kein DOI-Link verfügbar,2303.03297v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,RoboCup 2023 Humanoid AdultSize Winner NimbRo: NimbRoNet3 Visual   Perception and Responsive Gait with Waveform In-walk Kicks,2024,"  The RoboCup Humanoid League holds annual soccer robot world championships towards the long-term objective of winning against the FIFA world champions by 2050. The participating teams continuously improve their systems. This paper presents the upgrades to our humanoid soccer system, leading our team NimbRo to win the Soccer Tournament in the Humanoid AdultSize League at RoboCup 2023 in Bordeaux, France. The mentioned upgrades consist of: an updated model architecture for visual perception, extended fused angles feedback mechanisms and an additional COM-ZMP controller for walking robustness, and parametric in-walk kicks through waveforms. ",Kein DOI-Link verfügbar,2401.05909v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet   Peppers,2024,"  Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates. ",Kein DOI-Link verfügbar,2403.15306v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,NimbRo wins ANA Avatar XPRIZE Immersive Telepresence Competition:   Human-Centric Evaluation and Lessons Learned,2023,"  Robotic avatar systems can enable immersive telepresence with locomotion, manipulation, and communication capabilities. We present such an avatar system, based on the key components of immersive 3D visualization and transparent force-feedback telemanipulation. Our avatar robot features an anthropomorphic upper body with dexterous hands. The remote human operator drives the arms and fingers through an exoskeleton-based operator station, which provides force feedback both at the wrist and for each finger. The robot torso is mounted on a holonomic base, providing omnidirectional locomotion on flat floors, controlled using a 3D rudder device. Finally, the robot features a 6D movable head with stereo cameras, which stream images to a VR display worn by the operator. Movement latency is hidden using spherical rendering. The head also carries a telepresence screen displaying an animated image of the operator's face, enabling direct interaction with remote persons. Our system won the \$10M ANA Avatar XPRIZE competition, which challenged teams to develop intuitive and immersive avatar systems that could be operated by briefly trained judges. We analyze our successful participation in the semifinals and finals and provide insight into our operator training and lessons learned. In addition, we evaluate our system in a user study that demonstrates its intuitive and easy usability. ",Kein DOI-Link verfügbar,2308.12238v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Expansion dynamics of interacting bosons in homogeneous lattices in one   and two dimensions,2013,"  We experimentally and numerically investigate the expansion of initially localized ultracold bosons in homogeneous one- and two-dimensional optical lattices. We find that both dimensionality and interaction strength crucially influence these non-equilibrium dynamics. While the atoms expand ballistically in all integrable limits, deviations from these limits dramatically suppress the expansion and lead to the appearance of almost bimodal cloud shapes, indicating diffusive dynamics in the center surrounded by ballistic wings. For strongly interacting bosons, we observe a dimensional crossover of the dynamics from ballistic in the one-dimensional hard-core case to diffusive in two dimensions, as well as a similar crossover when higher occupancies are introduced into the system. ",https://doi.org/10.1103/PhysRevLett.110.205301,1301.5329v3,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Observation of many-body localization of interacting fermions in a   quasi-random optical lattice,2015,"  We experimentally observe many-body localization of interacting fermions in a one-dimensional quasi-random optical lattice. We identify the many-body localization transition through the relaxation dynamics of an initially-prepared charge density wave. For sufficiently weak disorder the time evolution appears ergodic and thermalizing, erasing all remnants of the initial order. In contrast, above a critical disorder strength a significant portion of the initial ordering persists, thereby serving as an effective order parameter for localization. The stationary density wave order and the critical disorder value show a distinctive dependence on the interaction strength, in agreement with numerical simulations. We connect this dependence to the ubiquitous logarithmic growth of entanglement entropy characterizing the generic many-body localized phase. ",https://doi.org/10.1126/science.aaa7432,1501.05661v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,NimbRo Rescue: Solving Disaster-Response Tasks through Mobile   Manipulation Robot Momaro,2018,"  Robots that solve complex tasks in environments too dangerous for humans to enter are desperately needed, e.g. for search and rescue applications. We describe our mobile manipulation robot Momaro, with which we participated successfully in the DARPA Robotics Challenge. It features a unique locomotion design with four legs ending in steerable wheels, which allows it both to drive omnidirectionally and to step over obstacles or climb. Furthermore, we present advanced communication and teleoperation approaches, which include immersive 3D visualization, and 6D tracking of operator head and arm motions. The proposed system is evaluated in the DARPA Robotics Challenge, the DLR SpaceBot Cup Qualification and lab experiments. We also discuss the lessons learned from the competitions. ",https://doi.org/10.1002/rob.21677,1810.01345v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Team NimbRo at MBZIRC 2017: Autonomous Valve Stem Turning using a Wrench,2018,"  The Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2017 has defined ambitious new benchmarks to advance the state-of-the-art in autonomous operation of ground-based and flying robots. In this article, we describe our winning entry to MBZIRC Challenge 2: the mobile manipulation robot Mario. It is capable of autonomously solving a valve manipulation task using a wrench tool detected, grasped, and finally employed to turn a valve stem. Mario's omnidirectional base allows both fast locomotion and precise close approach to the manipulation panel. We describe an efficient detector for medium-sized objects in 3D laser scans and apply it to detect the manipulation panel. An object detection architecture based on deep neural networks is used to find and select the correct tool from grayscale images. Parametrized motion primitives are adapted online to percepts of the tool and valve stem in order to turn the stem. We report in detail on our winning performance at the challenge and discuss lessons learned. ",Kein DOI-Link verfügbar,1810.02997v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"RoboCup 2019 AdultSize Winner NimbRo: Deep Learning Perception, In-Walk   Kick, Push Recovery, and Team Play Capabilities",2019,"  Individual and team capabilities are challenged every year by rule changes and the increasing performance of the soccer teams at RoboCup Humanoid League. For RoboCup 2019 in the AdultSize class, the number of players (2 vs. 2 games) and the field dimensions were increased, which demanded for team coordination and robust visual perception and localization modules. In this paper, we present the latest developments that lead team NimbRo to win the soccer tournament, drop-in games, technical challenges and the Best Humanoid Award of the RoboCup Humanoid League 2019 in Sydney. These developments include a deep learning vision system, in-walk kicks, step-based push-recovery, and team play strategies. ",https://doi.org/10.1007/978-3-030-35699-6,1912.07405v2,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Autonomous Fire Fighting with a UAV-UGV Team at MBZIRC 2020,2021,"  Every day, burning buildings threaten the lives of occupants and first responders trying to save them. Quick action is of essence, but some areas might not be accessible or too dangerous to enter. Robotic systems have become a promising addition to firefighting, but at this stage, they are mostly manually controlled, which is error-prone and requires specially trained personal.   We present two systems for autonomous firefighting from air and ground we developed for the Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020. The systems use LiDAR for reliable localization within narrow, potentially GNSS-restricted environments while maneuvering close to obstacles. Measurements from LiDAR and thermal cameras are fused to track fires, while relative navigation ensures successful extinguishing.   We analyze and discuss our successful participation during the MBZIRC 2020, present further experiments, and provide insights into our lessons learned from the competition. ",Kein DOI-Link verfügbar,2106.06444v1,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Signatures of Many-Body Localization in a Controlled Open Quantum System,2016,"  In the presence of disorder, an interacting closed quantum system can undergo many-body localization (MBL) and fail to thermalize. However, over long times even weak couplings to any thermal environment will necessarily thermalize the system and erase all signatures of MBL. This presents a challenge for experimental investigations of MBL, since no realistic system can ever be fully closed. In this work, we experimentally explore the thermalization dynamics of a localized system in the presence of controlled dissipation. Specifically, we find that photon scattering results in a stretched exponential decay of an initial density pattern with a rate that depends linearly on the scattering rate. We find that the resulting susceptibility increases significantly close to the phase transition point. In this regime, which is inaccessible to current numerical studies, we also find a strong dependence on interactions. Our work provides a basis for systematic studies of MBL in open systems and opens a route towards extrapolation of closed system properties from experiments. ",https://doi.org/10.1103/PhysRevX.7.011034,1610.01613v1,No,
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,Team NimbRo's UGV Solution for Autonomous Wall Building and Fire   Fighting at MBZIRC 2020,2021,"  Autonomous robotic systems for various applications including transport, mobile manipulation, and disaster response are becoming more and more complex. Evaluating and analyzing such systems is challenging. Robotic competitions are designed to benchmark complete robotic systems on complex state-of-the-art tasks. Participants compete in defined scenarios under equal conditions. We present our UGV solution developed for the Mohamed Bin Zayed International Robotics Challenge 2020. Our hard- and software components to address the challenge tasks of wall building and fire fighting are integrated into a fully autonomous system. The robot consists of a wheeled omnidirectional base, a 6 DoF manipulator arm equipped with a magnetic gripper, a highly efficient storage system to transport box-shaped objects, and a water spraying system to fight fires. The robot perceives its environment using 3D LiDAR as well as RGB and thermal camera-based perception modules, is capable of picking box-shaped objects and constructing a pre-defined wall structure, as well as detecting and localizing heat sources in order to extinguish potential fires. A high-level planner solves the challenge tasks using the robot skills. We analyze and discuss our successful participation during the MBZIRC 2020 finals, present further experiments, and provide insights to our lessons learned. ",Kein DOI-Link verfügbar,2105.11979v2,Yes,potent(1)
0000-0002-5443-4176,Michael Schreiber,München Universität der Angewandte Wissenschaften,"Target Chase, Wall Building, and Fire Fighting: Autonomous UAVs of Team   NimbRo at MBZIRC 2020",2022,"  The Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2020 posed diverse challenges for unmanned aerial vehicles (UAVs). We present our four tailored UAVs, specifically developed for individual aerial-robot tasks of MBZIRC, including custom hardware- and software components.   In Challenge 1, a target UAV is pursued using a high-efficiency, onboard object detection pipeline to capture a ball from the target UAV. A second UAV uses a similar detection method to find and pop balloons scattered throughout the arena.   For Challenge 2, we demonstrate a larger UAV capable of autonomous aerial manipulation: Bricks are found and tracked from camera images. Subsequently, they are approached, picked, transported, and placed on a wall.   Finally, in Challenge 3, our UAV autonomously finds fires using LiDAR and thermal cameras. It extinguishes the fires with an onboard fire extinguisher.   While every robot features task-specific subsystems, all UAVs rely on a standard software stack developed for this particular and future competitions. We present our mostly open-source software solutions, including tools for system configuration, monitoring, robust wireless communication, high-level control, and agile trajectory generation. For solving the MBZIRC 2020 tasks, we advanced the state of the art in multiple research areas like machine vision and trajectory generation.   We present our scientific contributions that constitute the foundation for our algorithms and systems and analyze the results from the MBZIRC competition 2020 in Abu Dhabi, where our systems reached second place in the Grand Challenge. Furthermore, we discuss lessons learned from our participation in this complex robotic challenge. ",Kein DOI-Link verfügbar,2201.03844v1,No,
0000-0001-9643-7407,Johannes Lange,München Universität der Angewandte Wissenschaften,"Top quark mass measurement in the $\mathrm{t}\overline{\mathrm{t}}$   all-jets final state with the CMS experiment at $\sqrt{s}=13\,\mathrm{TeV}$",2018,"  The top quark mass is measured using $35.9~\mathrm{fb}^{-1}$ of LHC proton-proton collision data collected with the CMS detector at $\sqrt{s}=13~\mathrm{TeV}$ in 2016. The measurement uses the $\mathrm{t}\overline{\mathrm{t}}$ all-jets final state, which comprises a total of six jets. A kinematic fit is performed to reconstruct the decay of the $\mathrm{t}\overline{\mathrm{t}}$ system and suppress QCD multijet background. By means of the ideogram method, the top quark mass is determined, simultaneously constraining an additional jet energy scale factor ($\text{JSF}$). The result of $172.34\pm0.20\,\text{(stat+JSF)}\pm0.76\,\text{(syst)}~\mathrm{GeV}$ for the top quark mass is in good agreement with previous measurements in the same and different final states. ",Kein DOI-Link verfügbar,1812.05394v1,No,
0000-0001-9643-7407,Johannes Lange,München Universität der Angewandte Wissenschaften,The average GeV-band Emission from Gamma-Ray Bursts,2013,"  We analyze the emission in the 0.3-30 GeV energy range of Gamma-Ray Bursts detected with the Fermi Gamma-ray Space Telescope. We concentrate on bursts that were previously only detected with the Gamma-Ray Burst Monitor in the keV energy range. These bursts will then be compared to the bursts that were individually detected with the Large Area Telescope at higher energies. To estimate the emission of faint GRBs we use non-standard analysis methods and sum over many GRBs to find an average signal which is significantly above background level. We use a subsample of 99 GRBs listed in the Burst Catalog from the first two years of observation. Although mostly not individually detectable, the bursts not detected by the Large Area Telescope on average emit a significant flux in the energy range from 0.3 GeV to 30 GeV, but their cumulative energy fluence is only 8% of that of all GRBs. Likewise, the GeV-to-MeV flux ratio is less and the GeV-band spectra are softer. We confirm that the GeV-band emission lasts much longer than the emission found in the keV energy range. The average allsky energy flux from GRBs in the GeV band is 6.4*10^-4 erg cm^-2 yr^-1 or only 4% of the energy flux of cosmic rays above the ankle at 10^18.6 eV. ",https://doi.org/10.1051/0004-6361/201220652,1301.2914v1,No,
0000-0001-9643-7407,Johannes Lange,München Universität der Angewandte Wissenschaften,Beyond Mass: Detecting Secondary Halo Properties with Galaxy-Galaxy   Lensing,2021,"  Secondary halo properties beyond mass, such as the mass accretion rate (MAR), concentration, and the half mass scale, are essential in understanding the formation of large-scale structure and dark matter halos. In this paper, we study the impact of secondary halo properties on the galaxy-galaxy lensing observable, $\Delta\Sigma$. We build an emulator trained on N-body simulations to model $\Delta\Sigma$ and quantify the impact of different secondary parameters on the $\Delta\Sigma$ profile. We focus on the impact of MAR on $\Delta\Sigma$. We show that a 3$\sigma$ detection of variations in MAR at fixed halo mass could be achieved with the Hyper Suprime Cam survey in combination with a proxy for MAR with scatter $\sigma_{\Gamma_\mathrm{dyn}|\mathrm{obs}}<1.5$. We show that the full radial profile of $\Delta\Sigma$ depends on secondary properties at fixed halo mass. Consequently, an emulator that can perform full shape fitting yields better than 2 times improvement upon the constraints on MAR than only using the outer part of the halo. Finally, we highlight that miscentering and MAR impact the radial profile of $\Delta\Sigma$ in a similar fashion, implying that miscentering and MAR need to be modeled jointly for unbiased estimates of both effects. We show that present-day lensing data sets have the statistical capability to place constraints on halo MAR. Our analysis opens up new possibilities for observationally measuring the assembly history of the dark matter halos that host galaxies and clusters. ",https://doi.org/10.1093/mnras/stac941,2106.06656v1,No,
0000-0001-9643-7407,Johannes Lange,München Universität der Angewandte Wissenschaften,Cluster Cosmology Without Cluster Finding,2023,"  We propose that observations of super-massive galaxies contain cosmological constraining power similar to conventional cluster cosmology, and we provide promising indications that the associated systematic errors are comparably easier to control. We consider a fiducial spectroscopic and stellar mass complete sample of galaxies drawn from the Dark Energy Spectroscopic Survey (DESI) and forecast how constraints on Omega_m-sigma_8 from this sample will compare with those from number counts of clusters based on richness. At fixed number density, we find that massive galaxies offer similar constraints to galaxy clusters. However, a mass-complete galaxy sample from DESI has the potential to probe lower halo masses than standard optical cluster samples (which are typically limited to richness above 20 and halo mass above 10^13.5); additionally, it is straightforward to cleanly measure projected galaxy clustering for such a DESI sample, which we show can substantially improve the constraining power on Omega_m. We also compare the constraining power of stellar mass-limited samples to those from larger but mass-incomplete samples (e.g., the DESI Bright Galaxy Survey, BGS, Sample); relative to a lower number density stellar mass-limited samples, we find that a BGS-like sample improves statistical constraints by 60% for Omega_m and 40% for sigma_8, but this uses small scale information which will be harder to model for BGS. Our initial assessment of the systematics associated with supermassive galaxy cosmology yields promising results. The proposed samples have a 10% satellite fraction, but we show that cosmological constraints may be robust to the impact of satellites. These findings motivate future work to realize the potential of super-massive galaxies to probe lower halo masses than richness-based clusters and to avoid persistent systematics associated with optical cluster finding. ",Kein DOI-Link verfügbar,2306.03777v1,Yes,potent(2)
0000-0001-9643-7407,Johannes Lange,München Universität der Angewandte Wissenschaften,The Outer Stellar Mass of Massive Galaxies: A Simple Tracer of Halo Mass   with Scatter Comparable to Richness and Reduced Projection Effects,2021,"  Using the weak gravitational lensing data from the Hyper Suprime-Cam Subaru Strategic Program (HSC survey), we study the potential of different stellar mass estimates in tracing halo mass. We consider galaxies with $\log {M_{\star}/M_{\odot}}>11.5$ at 0.2 < z < 0.5 with carefully measured light profiles and clusters from the redMaPPer and CAMIRA richness-based algorithms. We devise a method (the ""TopN"" test) to evaluate the scatter in the halo mass-observable relation for different tracers and inter-compare halo mass proxies in four number density bins using stacked galaxy-galaxy lensing profiles. This test reveals three key findings. The stellar mass based on cModel photometry or aperture luminosity within R<30 kpc is a poor proxy of halo mass. In contrast, the stellar mass of the outer envelope is an excellent halo mass proxy. The stellar mass within R=[50,100] kpc, M*[50,100], has performance comparable to the state-of-the-art richness-based cluster finders at $\log{M_{\rm vir}/M_{\odot}}>14.0$ and could be a better halo mass tracer at lower halo masses. Finally, using N-body simulations, we find that the lensing profiles of massive halos selected by M*[50,100] are consistent with the expectation for a sample without projection or mis-centering effects. On the other hand, Richness-selected clusters display an excess at R~1 Mpc in their lensing profiles, which may suggest a more significant impact from selection biases. These results suggest that Mstar-based tracers have distinct advantages in identifying massive halos, which could open up new avenues for cluster cosmology. ",https://doi.org/10.1093/mnras/stac1680,2109.02646v1,Yes,potent(1)
0000-0002-8359-5075,Markus Oswald,München Universität der Angewandte Wissenschaften,JOKARUS - Design of a compact optical iodine frequency reference for a   sounding rocket mission,2017,"  We present the design of a compact absolute optical frequency reference for space applications based on hyperfine transitions in molecular iodine with a targeted fractional frequency instability of better than $3\cdot 10^{-14}$. It is based on a micro-integrated extended cavity diode laser with integrated optical amplifier, fiber pigtailed second harmonic generation wave-guide modules, and a quasi-monolithic spectroscopy setup with operating electronics. The instrument described here is scheduled for launch end of 2017 aboard the TEXUS 54 sounding rocket as an important qualification step towards space application of iodine frequency references and related technologies. The payload will operate autonomously and its optical frequency will be compared to an optical frequency comb during its space flight. ",https://doi.org/10.1140/epjqt/s40507-017-0063-y,1702.08330v1,No,
0000-0002-8359-5075,Markus Oswald,München Universität der Angewandte Wissenschaften,Design of a dual species atom interferometer for space,2014,"  Atom interferometers have a multitude of proposed applications in space including precise measurements of the Earth's gravitational field, in navigation & ranging, and in fundamental physics such as tests of the weak equivalence principle (WEP) and gravitational wave detection. While atom interferometers are realized routinely in ground-based laboratories, current efforts aim at the development of a space compatible design optimized with respect to dimensions, weight, power consumption, mechanical robustness and radiation hardness. In this paper, we present a design of a high-sensitivity differential dual species $^{85}$Rb/$^{87}$Rb atom interferometer for space, including physics package, laser system, electronics and software. The physics package comprises the atom source consisting of dispensers and a 2D magneto-optical trap (MOT), the science chamber with a 3D-MOT, a magnetic trap based on an atom chip and an optical dipole trap (ODT) used for Bose-Einstein condensate (BEC) creation and interferometry, the detection unit, the vacuum system for $10^{-11}$ mbar ultra-high vacuum generation, and the high-suppression factor magnetic shielding as well as the thermal control system. The laser system is based on a hybrid approach using fiber-based telecom components and high-power laser diode technology and includes all laser sources for 2D-MOT, 3D-MOT, ODT, interferometry and detection. Manipulation and switching of the laser beams is carried out on an optical bench using Zerodur bonding technology. The instrument consists of 9 units with an overall mass of 221 kg, an average power consumption of 608 W (819 W peak), and a volume of 470 liters which would well fit on a satellite to be launched with a Soyuz rocket, as system studies have shown. ",https://doi.org/10.1007/s10686-014-9433-y,1412.2713v1,No,
0000-0002-3122-415X,Carsten Franke,München Universität der Angewandte Wissenschaften,Substation Signal Matching with a Bagged Token Classifier,2018,"  Currently, engineers at substation service providers match customer data with the corresponding internally used signal names manually. This paper proposes a machine learning method to automate this process based on substation signal mapping data from a repository of executed projects. To this end, a bagged token classifier is proposed, letting words (tokens) in the customer signal name vote for provider signal names. In our evaluation, the proposed method exhibits better performance in terms of both accuracy and efficiency over standard classifiers. ",Kein DOI-Link verfügbar,1802.04734v1,No,
0000-0002-1572-842X,Marion Gödel,München Universität der Angewandte Wissenschaften,Vadere: An open-source simulation framework to promote interdisciplinary   understanding,2019,"  Pedestrian dynamics is an interdisciplinary field of research. Psychologists, sociologists, traffic engineers, physicists, mathematicians and computer scientists all strive to understand the dynamics of a moving crowd. In principle, computer simulations offer means to further this understanding. Yet, unlike for many classic dynamical systems in physics, there is no universally accepted locomotion model for crowd dynamics. On the contrary, a multitude of approaches, with very different characteristics, compete. Often only the experts in one special model type are able to assess the consequences these characteristics have on a simulation study. Therefore, scientists from all disciplines who wish to use simulations to analyze pedestrian dynamics need a tool to compare competing approaches. Developers, too, would profit from an easy way to get insight into an alternative modeling ansatz. Vadere meets this interdisciplinary demand by offering an open-source simulation framework that is lightweight in its approach and in its user interface while offering pre-implemented versions of the most widely spread models. ",Kein DOI-Link verfügbar,1907.09520v1,No,
0000-0002-1572-842X,Marion Gödel,München Universität der Angewandte Wissenschaften,Can we learn where people come from? Retracing of origins in merging   situations,2020,"  One crucial information for a pedestrian crowd simulation is the number of agents moving from an origin to a certain target. While this setup has a large impact on the simulation, it is in most setups challenging to find the number of agents that should be spawned at a source in the simulation. Often, number are chosen based on surveys and experience of modelers and event organizers. These approaches are important and useful but reach their limits when we want to perform real-time predictions. In this case, a static information about the inflow is not sufficient. Instead, we need a dynamic information that can be retrieved each time the prediction is started. Nowadays, sensor data such as video footage or GPS tracks of a crowd are often available. If we can estimate the number of pedestrians who stem from a certain origin from this sensor data, we can dynamically initialize the simulation. In this study, we use density heatmaps that can be derived from sensor data as input for a random forest regressor to predict the origin distributions. We study three different datasets: A simulated dataset, experimental data, and a hybrid approach with both experimental and simulated data. In the hybrid setup, the model is trained with simulated data and then tested on experimental data. The results demonstrate that the random forest model is able to predict the origin distribution based on a single density heatmap for all three configurations. This is especially promising for applying the approach on real data since there is often only a limited amount of data available. ",Kein DOI-Link verfügbar,2012.11527v1,No,
0000-0002-1572-842X,Marion Gödel,München Universität der Angewandte Wissenschaften,Modelling airborne transmission of SARS-CoV-2 at a local scale,2021,"  The coronavirus disease (COVID-19) pandemic has changed our lives and still poses a challenge to science. Numerous studies have contributed to a better understanding of the pandemic. In particular, inhalation of aerosolised pathogens has been identified as essential for transmission. This information is crucial to slow the spread, but the individual likelihood of becoming infected in everyday situations remains uncertain. Mathematical models help estimate such risks. In this study, we propose how to model airborne transmission of SARS-CoV-2 at a local scale. In this regard, we combine microscopic crowd simulation with a new model for disease transmission. Inspired by compartmental models, we describe agents' health status as susceptible, exposed, infectious or recovered. Infectious agents exhale pathogens bound to persistent aerosols, whereas susceptible agents absorb pathogens when moving through an aerosol cloud left by the infectious agent. The transmission depends on the pathogen load of the aerosol cloud, which changes over time. We propose a 'high risk' benchmark scenario to distinguish critical from non-critical situations. Simulating indoor situations show that the new model is suitable to evaluate the risk of exposure qualitatively and, thus, enables scientists or even decision-makers to better assess the spread of COVID-19 and similar diseases. ",https://doi.org/10.1371/journal.pone.0273820,2111.08547v2,No,
0000-0002-1572-842X,Marion Gödel,München Universität der Angewandte Wissenschaften,Can we learn where people go?,2018,"  In most agent-based simulators, pedestrians navigate from origins to destinations. Consequently, destinations are essential input parameters to the simulation. While many other relevant parameters as positions, speeds and densities can be obtained from sensors, like cameras, destinations cannot be observed directly. Our research question is: Can we obtain this information from video data using machine learning methods? We use density heatmaps, which indicate the pedestrian density within a given camera cutout, as input to predict the destination distributions. For our proof of concept, we train a Random Forest predictor on an exemplary data set generated with the Vadere microscopic simulator. The scenario is a crossroad where pedestrians can head left, straight or right. In addition, we gain first insights on suitable placement of the camera. The results motivate an in-depth analysis of the methodology. ",https://doi.org/10.17815/CD.2020.43,1812.03719v2,No,
0000-0002-6837-0628,Maximilian Balluff,München Universität der Angewandte Wissenschaften,Innovations in Cover Song Detection: A Lyrics-Based Approach,2024,"  Cover songs are alternate versions of a song by a different artist. Long being a vital part of the music industry, cover songs significantly influence music culture and are commonly heard in public venues. The rise of online music platforms has further increased their prevalence, often as background music or video soundtracks. While current automatic identification methods serve adequately for original songs, they are less effective with cover songs, primarily because cover versions often significantly deviate from the original compositions. In this paper, we propose a novel method for cover song detection that utilizes the lyrics of a song. We introduce a new dataset for cover songs and their corresponding originals. The dataset contains 5078 cover songs and 2828 original songs. In contrast to other cover song datasets, it contains the annotated lyrics for the original song and the cover song. We evaluate our method on this dataset and compare it with multiple baseline approaches. Our results show that our method outperforms the baseline approaches. ",Kein DOI-Link verfügbar,2406.04384v1,No,
0000-0002-4287-7535,Andrea Nagy,München Universität der Angewandte Wissenschaften,On the incidence rate of first overtone Blazhko stars in the Large   Magellanic Cloud,2006,"  By using the full span of multicolor data on a representative sample of first overtone RR Lyrae stars in the Large Magellanic Cloud (LMC) we revisit the problem of the incidence rate of the amplitude/phase-modulated (Blazhko) stars. Multicolor data, obtained by the MAssive Compact Halo Objects (MACHO) project, are utilized through a periodogram averaging method. This enables us to increase the number of detected multiperiodic variables by 18% relative to the number obtained by the analysis of the best single color data. We also test the maximum modulation period detectable in the present dataset. We find that variables showing amplitude/phase modulations with periods close to the total time span can still be clearly separated from the class of stars showing period changes. This larger limit on the modulation period, the more efficient data analysis and the longer time span lead to a substantial increase in the incidence rate of the Blazhko stars in comparison with earlier results. We find altogether 99 first overtone Blazhko stars in the full sample of 1332 stars, implying an incidence rate of 7.5%. Although this rate is nearly twice of the one derived earlier, it is still significantly lower than that of the fundamental mode stars in the LMC. The by-products of the analysis (e.g., star-by-star comments, distribution functions of various quantities) are also presented. ",https://doi.org/10.1051/0004-6361:20054538,astro-ph/0602485v1,No,
0000-0002-4287-7535,Andrea Nagy,München Universität der Angewandte Wissenschaften,On the distribution of the modulation frequencies of RR Lyrae stars,2005,"  For the first time connection between the pulsation and modulation properties of RR Lyrae stars has been detected. Based on the available data it is found that the possible range of the modulation frequencies, i.e, the possible maximum value of the modulation frequency depends on the pulsation frequency. Short period variables (P < 0.4 d) can have modulation period as short as some days, while longer period variables (P > 0.6 d) always exhibit modulation with P_mod > 20 d. We interpret this tendency with the equality of the modulation period with the surface rotation period, because similar distribution of the rotational periods is expected if an upper limit of the total angular momentum of stars leaving the RGB exists. The distribution of the projected rotational velocities of red and blue horizontal branch stars at different temperatures shows a similar behaviour as v_rot derived for RR Lyrae stars from their modulation periods. This common behaviour gives reason to identify the modulation period with the rotational period of the modulated RR Lyrae stars. ",Kein DOI-Link verfügbar,astro-ph/0508029v1,No,
0000-0002-4287-7535,Andrea Nagy,München Universität der Angewandte Wissenschaften,Critical Phenomena in DIS,2010,"  Saturation in deep inelastic scattering (DIS) and deeply virtual Compton scattering (DVCS) is associated with a phase transition between the partonic gas, typical of moderate $x$ and $Q^2$, and partonic fluid appearing at increasing $Q^2$ and decreasing Bjorken $x$. In the statistical interpretation of DIS, the large-$x,(1-x)^n$ factor in the SF is associated with a statistical distribution (perfect gas), while the low-$x$, Regge behaved factor $x^{b(Q^2)}$ produces deviations from the perfect gas and ultimately leads to a gas-liquid phase transition. In this paper we do not intend to propose another parametrization of the structure function; instead we suggest a new insight into the internal structure of the nucleon, as seen in DIS, and its connection with that revealed in high-energy nucleons and heavy-ion collisions. ",https://doi.org/10.1142/S0217751X10051104,1009.1632v2,No,
0000-0002-4287-7535,Andrea Nagy,München Universität der Angewandte Wissenschaften,The AKARI FIS catalogue of YSOs and extragalactic objects,2017,"  The point sources in the Bright Source Catalogue of the AKARI Far-Infrared Surveyor (FIS) were classified based on their FIR and mid-IR fluxes and colours into young stellar object (YSO) and extragalactic source types using Quadratic Discriminant Analysis method (QDA) and Support Vector Machines (SVM). The reliability of the selection of YSO candidates is high, and the number of known YSO candidates were increased significantly, that we demonstrate in the case of the nearby open cluster IC348. Our results show that we can separate galactic and extragalactic AKARI point sources in the multidimensional space of FIR fluxes and colours with high reliability, however, differentiating among the extragalactic sub-types needs further information. ",https://doi.org/10.5303/PKAS.2017.32.1.049,1706.01217v1,No,
0000-0002-4287-7535,Andrea Nagy,München Universität der Angewandte Wissenschaften,GraLMatch: Matching Groups of Entities with Graphs and Language Models,2024,"  In this paper, we present an end-to-end multi-source Entity Matching problem, which we call entity group matching, where the goal is to assign to the same group, records originating from multiple data sources but representing the same real-world entity. We focus on the effects of transitively matched records, i.e. the records connected by paths in the graph G = (V,E) whose nodes and edges represent the records and whether they are a match or not. We present a real-world instance of this problem, where the challenge is to match records of companies and financial securities originating from different data providers. We also introduce two new multi-source benchmark datasets that present similar matching challenges as real-world records. A distinctive characteristic of these records is that they are regularly updated following real-world events, but updates are not applied uniformly across data sources. This phenomenon makes the matching of certain groups of records only possible through the use of transitive information.   In our experiments, we illustrate how considering transitively matched records is challenging since a limited amount of false positive pairwise match predictions can throw off the group assignment of large quantities of records. Thus, we propose GraLMatch, a method that can partially detect and remove false positive pairwise predictions through graph-based properties. Finally, we showcase how fine-tuning a Transformer-based model (DistilBERT) on a reduced number of labeled samples yields a better final entity group matching than training on more samples and/or incorporating fine-tuning optimizations, illustrating how precision becomes the deciding factor in the entity group matching of large volumes of records. ",Kein DOI-Link verfügbar,2406.15015v1,No,
0000-0002-1359-8633,Florian Gallwitz,Nuremberg Institute der Technology / Technische Hochschule Nürnberg,Investigating the Validity of Botometer-based Social Bot Studies,2022,"  The idea that social media platforms like Twitter are inhabited by vast numbers of social bots has become widely accepted in recent years. Social bots are assumed to be automated social media accounts operated by malicious actors with the goal of manipulating public opinion. They are credited with the ability to produce content autonomously and to interact with human users. Social bot activity has been reported in many different political contexts, including the U.S. presidential elections, discussions about migration, climate change, and COVID-19. However, the relevant publications either use crude and questionable heuristics to discriminate between supposed social bots and humans or -- in the vast majority of the cases -- fully rely on the output of automatic bot detection tools, most commonly Botometer. In this paper, we point out a fundamental theoretical flaw in the widely-used study design for estimating the prevalence of social bots. Furthermore, we empirically investigate the validity of peer-reviewed Botometer-based studies by closely and systematically inspecting hundreds of accounts that had been counted as social bots. We were unable to find a single social bot. Instead, we found mostly accounts undoubtedly operated by human users, the vast majority of them using Twitter in an inconspicuous and unremarkable fashion without the slightest traces of automation. We conclude that studies claiming to investigate the prevalence, properties, or influence of social bots based on Botometer have, in reality, just investigated false positives and artifacts of this approach. ",Kein DOI-Link verfügbar,2207.11474v1,Yes,undoubtedly(1)
0000-0002-1359-8633,Florian Gallwitz,Nuremberg Institute der Technology / Technische Hochschule Nürnberg,Semantic Processing of Out-Of-Vocabulary Words in a Spoken Dialogue   System,1997,"  One of the most important causes of failure in spoken dialogue systems is usually neglected: the problem of words that are not covered by the system's vocabulary (out-of-vocabulary or OOV words). In this paper a methodology is described for the detection, classification and processing of OOV words in an automatic train timetable information system. The various extensions that had to be effected on the different modules of the system are reported, resulting in the design of appropriate dialogue strategies, as are encouraging evaluation results on the new versions of the word recogniser and the linguistic processor. ",Kein DOI-Link verfügbar,cmp-lg/9709006v1,No,
0000-0002-1359-8633,Florian Gallwitz,Nuremberg Institute der Technology / Technische Hochschule Nürnberg,Segmentation of Photovoltaic Module Cells in Uncalibrated   Electroluminescence Images,2018,"  High resolution electroluminescence (EL) images captured in the infrared spectrum allow to visually and non-destructively inspect the quality of photovoltaic (PV) modules. Currently, however, such a visual inspection requires trained experts to discern different kinds of defects, which is time-consuming and expensive. Automated segmentation of cells is therefore a key step in automating the visual inspection workflow. In this work, we propose a robust automated segmentation method for extraction of individual solar cells from EL images of PV modules. This enables controlled studies on large amounts of data to understanding the effects of module degradation over time-a process not yet fully understood. The proposed method infers in several steps a high-level solar module representation from low-level edge features. An important step in the algorithm is to formulate the segmentation problem in terms of lens calibration by exploiting the plumbline constraint. We evaluate our method on a dataset of various solar modules types containing a total of 408 solar cells with various defects. Our method robustly solves this task with a median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both indicating a very high similarity between automatically segmented and ground truth solar cell masks. ",https://doi.org/10.1007/s00138-021-01191-9,1806.06530v4,No,
0000-0002-1359-8633,Florian Gallwitz,Nuremberg Institute der Technology / Technische Hochschule Nürnberg,Automatic Classification of Defective Photovoltaic Module Cells in   Electroluminescence Images,2018,"  Electroluminescence (EL) imaging is a useful modality for the inspection of photovoltaic (PV) modules. EL images provide high spatial resolution, which makes it possible to detect even finest defects on the surface of PV modules. However, the analysis of EL images is typically a manual process that is expensive, time-consuming, and requires expert knowledge of many different types of defects. In this work, we investigate two approaches for automatic detection of such defects in a single image of a PV cell. The approaches differ in their hardware requirements, which are dictated by their respective application scenarios. The more hardware-efficient approach is based on hand-crafted features that are classified in a Support Vector Machine (SVM). To obtain a strong performance, we investigate and compare various processing variants. The more hardware-demanding approach uses an end-to-end deep Convolutional Neural Network (CNN) that runs on a Graphics Processing Unit (GPU). Both approaches are trained on 1,968 cells extracted from high resolution EL intensity images of mono- and polycrystalline PV modules. The CNN is more accurate, and reaches an average accuracy of 88.42%. The SVM achieves a slightly lower average accuracy of 82.44%, but can run on arbitrary hardware. Both automated approaches make continuous, highly accurate monitoring of PV cells feasible. ",https://doi.org/10.1016/j.solener.2019.02.067,1807.02894v3,No,
0009-0003-5071-7464,Rüdiger Alshut,Esslingen Universität der Angewandte Wissenschaften,Konzept für Bildanalysen in Hochdurchsatz-Systemen am Beispiel des   Zebrabärblings,2017,"  With image-based high-throughput experiments, new challenges arise in both, the design of experiments and the automated analysis. To be able to handle the massive number of single experiments and the corresponding amount of data, a comprehensive concept for the design of experiments and a new evaluation method is needed. This work proposes a new method for an optimized experiment layout that enables the determination of parameters, adapted for the needs of automated image analysis. Furthermore, a catalogue of new image analysis modules, especially developed for zebrafish analysis, is presented. The combination of both parts offers the user, usually a biologist, an approach for high-throughput zebrafish image analysis, which enables the extraction of new signals and optimizes the design of experiments. The result is a reduction of data amount, redundant information and workload as well as classification errors. ",https://doi.org/10.5445/IR/1000061759,1705.02962v1,No,
0000-0003-4855-9586,Dennis Mayer,Esslingen Universität der Angewandte Wissenschaften,Tuning confined states and valley g-factors by quantum dot design in   bilayer graphene,2024,"  Electrostatically confined quantum dots in bilayer graphene have shown potential as building blocks for quantum technologies. To operate the dots, e.g., as qubits, a precise understanding and control of the confined states and their properties is required. Herein, a large-scale numerical characterization of confined quantum states in bilayer graphene dots is performed over an extensive range of gate-tunable parameters such as the dot size, depth, shape, and the bilayer graphene gap. The dot states' orbital degeneracy, wave function distribution, and valley g-factor are established and the parametric dependencies to achieve different regimes are provided. It is found that the dot states are highly susceptible to gate-dependent confinement and material parameters, enabling efficient tuning of confined states and valley g-factor modulation by quantum dot design. ",https://doi.org/10.1002/pssb.202300395,2404.09910v1,Yes,potent(1)
0000-0003-4855-9586,Dennis Mayer,Esslingen Universität der Angewandte Wissenschaften,Unexpected hydrogen dissociation in thymine: predictions from a novel   coupled cluster theory,2024,"  The fate of thymine upon excitation by ultraviolet radiation has been the subject of intense debate over the past three decades. Today, it is widely believed that its ultrafast excited state decay stems from a radiationless transition from the bright ${\pi}{\pi}^*$ state to a dark $n{\pi}^*$ state. However, conflicting theoretical predictions have made the experimental data difficult to interpret. Here we simulate the ultrafast dynamics in thymine at the highest level of theory to date, performing wavepacket dynamics with a new coupled cluster method. Our simulation confirms an ultrafast ${\pi}{\pi}^*$ to $n{\pi}^*$ transition (${\tau} = 41 \pm 14$ fs). Furthermore, the predicted oxygen-edge X-ray absorption spectra agree quantitatively with the experimental results. Our simulation also predicts an as-yet uncharacterized photochemical pathway: a ${\pi}{\sigma}^*$ channel that leads to hydrogen dissociation at one of the two N-H bonds in thymine. Similar behavior has been identified in other heteroaromatic compounds, including adenine, and several authors have speculated that a similar pathway may exist in thymine. However, this was never confirmed theoretically or experimentally. This prediction calls for renewed efforts to experimentally identify or exclude the presence of this channel. ",Kein DOI-Link verfügbar,2403.01045v2,No,
0000-0003-4855-9586,Dennis Mayer,Esslingen Universität der Angewandte Wissenschaften,Following excited-state chemical shifts in molecular ultrafast x-ray   photoelectron spectroscopy,2021,"  The conversion of photon energy into other energetic forms in molecules is accompanied by charge moving on ultrafast timescales. We directly observe the charge motion at a specific site in an electronically excited molecule using time-resolved x-ray photoelectron spectroscopy (TR-XPS). We extend the concept of static chemical shift from conventional XPS by the excited-state chemical shift (ESCS), which is connected to the charge in the framework of a potential model. This allows us to invert TR-XPS spectra to the dynamic charge at a specific atom. We demonstrate the power of TR-XPS by using sulphur 2p-core-electron-emission probing to study the UV-excited dynamics of 2-thiouracil. The new method allows us to discover that a major part of the population relaxes to the molecular ground state within 220-250 fs. In addition, a 250-fs oscillation, visible in the kinetic energy of the TR-XPS, reveals a coherent exchange of population among electronic states. ",https://doi.org/10.1038/s41467-021-27908-y,2102.13431v2,Yes,potent(1)
0000-0003-4855-9586,Dennis Mayer,Esslingen Universität der Angewandte Wissenschaften,X-ray Coulomb explosion imaging reveals role of molecular structure in   internal conversion,2024,"  Molecular photoabsorption results in an electronic excitation/ionization which couples to the rearrangement of the nuclei. The resulting intertwined change of nuclear and electronic degrees of freedom determines the conversion of photoenergy into other molecular energy forms. Nucleobases are excellent candidates for studying such dynamics, and great effort has been taken in the past to observe the electronic changes induced by the initial excitation in a time-resolved manner using ultrafast electron spectroscopy. The linked geometrical changes during nucleobase photorelaxation have so far not been observed directly in time-resolved experiments. Here, we present a study on a thionucleobase, where we extract comprehensive information on the molecular rearrangement using Coulomb explosion imaging. Our measurement links the extracted deplanarization of the molecular geometry to the previously studied temporal evolution of the electronic properties of the system. In particular, the protons of the exploded molecule are well-suited messengers carrying rich information on the molecule's geometry at distinct times after the initial electronic excitation. The combination of ultrashort laser pulses to trigger molecular dynamics, intense X-ray free-electron laser pulses for the explosion of the molecule, and multi-particle coincidence detection opens new avenues for time-resolved studies of complex molecules in the gas phase. ",Kein DOI-Link verfügbar,2405.15367v1,No,
0000-0002-5150-9697,Immanuel Weber,Hochschule Koblenz - RheinAhrCampus,Artificial and beneficial -- Exploiting artificial images for aerial   vehicle detection,2021,"  Object detection in aerial images is an important task in environmental, economic, and infrastructure-related tasks. One of the most prominent applications is the detection of vehicles, for which deep learning approaches are increasingly used. A major challenge in such approaches is the limited amount of data that arises, for example, when more specialized and rarer vehicles such as agricultural machinery or construction vehicles are to be detected. This lack of data contrasts with the enormous data hunger of deep learning methods in general and object recognition in particular. In this article, we address this issue in the context of the detection of road vehicles in aerial images. To overcome the lack of annotated data, we propose a generative approach that generates top-down images by overlaying artificial vehicles created from 2D CAD drawings on artificial or real backgrounds. Our experiments with a modified RetinaNet object detection network show that adding these images to small real-world datasets significantly improves detection performance. In cases of very limited or even no real-world images, we observe an improvement in average precision of up to 0.70 points. We address the remaining performance gap to real-world datasets by analyzing the effect of the image composition of background and objects and give insights into the importance of background. ",https://doi.org/10.1016/j.isprsjprs.2021.02.015,2104.03054v1,No,
0000-0002-5150-9697,Immanuel Weber,Hochschule Koblenz - RheinAhrCampus,Exploring Wilderness Characteristics Using Explainable Machine Learning   in Satellite Imagery,2022,"  Wilderness areas offer important ecological and social benefits and there are urgent reasons to discover where their positive characteristics and ecological functions are present and able to flourish. We apply a novel explainable machine learning technique to satellite images which show wild and anthropogenic areas in Fennoscandia. Occluding certain activations in an interpretable artificial neural network we complete a comprehensive sensitivity analysis regarding wild and anthropogenic characteristics. This enables us to predict detailed and high-resolution sensitivity maps highlighting these characteristics. Our artificial neural network provides an interpretable activation space increasing confidence in our method. Within the activation space, regions are semantically arranged. Our approach advances explainable machine learning for remote sensing, offers opportunities for comprehensive analyses of existing wilderness, and has practical relevance for conservation efforts. ",Kein DOI-Link verfügbar,2203.00379v3,No,
0000-0002-5150-9697,Immanuel Weber,Hochschule Koblenz - RheinAhrCampus,Behind the leaves -- Estimation of occluded grapevine berries with   conditional generative adversarial networks,2021,"  The need for accurate yield estimates for viticulture is becoming more important due to increasing competition in the wine market worldwide. One of the most promising methods to estimate the harvest is berry counting, as it can be approached non-destructively, and its process can be automated. In this article, we present a method that addresses the challenge of occluded berries with leaves to obtain a more accurate estimate of the number of berries that will enable a better estimate of the harvest. We use generative adversarial networks, a deep learning-based approach that generates a likely scenario behind the leaves exploiting learned patterns from images with non-occluded berries. Our experiments show that the estimate of the number of berries after applying our method is closer to the manually counted reference. In contrast to applying a factor to the berry count, our approach better adapts to local conditions by directly involving the appearance of the visible berries. Furthermore, we show that our approach can identify which areas in the image should be changed by adding new berries without explicitly requiring information about hidden areas. ",https://doi.org/10.3389/frai.2022.830026,2105.10325v1,No,
0000-0002-5150-9697,Immanuel Weber,Hochschule Koblenz - RheinAhrCampus,BlessemFlood21: Advancing Flood Analysis with a High-Resolution   Georeferenced Dataset for Humanitarian Aid Support,2024,"  Floods are an increasingly common global threat, causing emergencies and severe damage to infrastructure. During crises, organisations such as the World Food Programme use remotely sensed imagery, typically obtained through drones, for rapid situational analysis to plan life-saving actions. Computer Vision tools are needed to support task force experts on-site in the evaluation of the imagery to improve their efficiency and to allocate resources strategically. We introduce the BlessemFlood21 dataset to stimulate research on efficient flood detection tools. The imagery was acquired during the 2021 Erftstadt-Blessem flooding event and consists of high-resolution and georeferenced RGB-NIR images. In the resulting RGB dataset, the images are supplemented with detailed water masks, obtained via a semi-supervised human-in-the-loop technique, where in particular the NIR information is leveraged to classify pixels as either water or non-water. We evaluate our dataset by training and testing established Deep Learning models for semantic segmentation. With BlessemFlood21 we provide labeled high-resolution RGB data and a baseline for further development of algorithmic solutions tailored to flood detection in RGB imagery. ",Kein DOI-Link verfügbar,2407.05007v1,Yes,strategically(1)
0000-0002-4275-1430,Uwe Jaekel,Koblenz Universität der Angewandte Wissenschaften,Random matrix theory and robust covariance matrix estimation for   financial data,2005,  The traditional class of elliptical distributions is extended to allow for asymmetries. A completely robust dispersion matrix estimator (the `spectral estimator') for the new class of `generalized elliptical distributions' is presented. It is shown that the spectral estimator corresponds to an M-estimator proposed by Tyler (1983) in the context of elliptical distributions. Both the generalization of elliptical distributions and the development of a robust dispersion matrix estimator are motivated by the stylized facts of empirical finance. Random matrix theory is used for analyzing the linear dependence structure of high-dimensional data. It is shown that the Marcenko-Pastur law fails if the sample covariance matrix is considered as a random matrix in the context of elliptically distributed and heavy tailed data. But substituting the sample covariance matrix by the spectral estimator resolves the problem and the Marcenko-Pastur law remains valid. ,Kein DOI-Link verfügbar,physics/0503007v1,No,
0000-0002-4275-1430,Uwe Jaekel,Koblenz Universität der Angewandte Wissenschaften,Singularity strength based characterization of financial networks,2012,"  Financial markets are well known examples of multi-fractal complex systems that have garnered much interest in their characterization through complex network theory. The recent studies have used correlation based distance metrics for defining and analyzing financial networks. In this work the singularity strength is employed to define a distance metric and the existence of hierarchical structure in the Johannesburg Stock Exchange is investigated. The multi-fractal nature of the financial market, which is otherwise hidden in the correlation coefficient based prescriptions, is analyzed through the use of the singularity strength based method. The presence of a super cluster is exhibited in the network which accounts for half of the network size and is homogeneous in the sectoral composition of the South African market. ",Kein DOI-Link verfügbar,1205.1710v1,No,
0000-0002-4275-1430,Uwe Jaekel,Koblenz Universität der Angewandte Wissenschaften,Predicting nuclear masses with product-unit networks,2023,"  Accurate estimation of nuclear masses and their prediction beyond the experimentally explored domains of the nuclear landscape are crucial to an understanding of the fundamental origin of nuclear properties and to many applications of nuclear science, most notably in quantifying the $r$-process of stellar nucleosynthesis. Neural networks have been applied with some success to the prediction of nuclear masses, but they are known to have shortcomings in application to extrapolation tasks. In this work, we propose and explore a novel type of neural network for mass prediction in which the usual neuron-like processing units are replaced by complex-valued product units that permit multiplicative couplings of inputs to be learned from the input data. This generalized network model is tested on both interpolation and extrapolation data sets drawn from the Atomic Mass Evaluation. Its performance is compared with that of several neural-network architectures, substantiating its suitability for nuclear mass prediction. Additionally, a prediction-uncertainty measure for such complex-valued networks is proposed that serves to identify regions of expected low prediction error. ",Kein DOI-Link verfügbar,2305.04675v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Search in Imperfect Information Games,2021,"  From the very dawn of the field, search with value functions was a fundamental concept of computer games research. Turing's chess algorithm from 1950 was able to think two moves ahead, and Shannon's work on chess from $1950$ includes an extensive section on evaluation functions to be used within a search. Samuel's checkers program from 1959 already combines search and value functions that are learned through self-play and bootstrapping. TD-Gammon improves upon those ideas and uses neural networks to learn those complex value functions -- only to be again used within search. The combination of decision-time search and value functions has been present in the remarkable milestones where computers bested their human counterparts in long standing challenging games -- DeepBlue for Chess and AlphaGo for Go. Until recently, this powerful framework of search aided with (learned) value functions has been limited to perfect information games. As many interesting problems do not provide the agent perfect information of the environment, this was an unfortunate limitation. This thesis introduces the reader to sound search for imperfect information games. ",Kein DOI-Link verfügbar,2111.05884v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents,2024,"  While Poker, as a family of games, has been studied extensively in the last decades, collectible card games have seen relatively little attention. Only recently have we seen an agent that can compete with professional human players in Hearthstone, one of the most popular collectible card games. Although artificial agents must be able to work with imperfect information in both of these genres, collectible card games pose another set of distinct challenges. Unlike in many poker variants, agents must deal with state space so vast that even enumerating all states consistent with the agent's beliefs is intractable, rendering the current search methods unusable and requiring the agents to opt for other techniques. In this paper, we investigate the strength of such techniques for this class of games. Namely, we present preliminary analysis results of ByteRL, the state-of-the-art agent in Legends of Code and Magic and Hearthstone. Although ByteRL beat a top-10 Hearthstone player from China, we show that its play in Legends of Code and Magic is highly exploitable. ",Kein DOI-Link verfügbar,2404.16689v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,On the mesonic Lagrangian of order p^6 in chiral SU(2),2007,  We show that the number of operators in the presently known mesonic chiral Lagrangian of order p^6 in the two-flavour sector can be reduced by at least one from 57 to 56 by providing an explicit relation among the operators. We briefly discuss the relevance of this new relation. ,Kein DOI-Link verfügbar,0705.0576v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Electromagnetic low-energy constants in ChPT,2007,"  We investigate three-flavour chiral perturbation theory including virtual photons in a limit where the strange quark mass is much larger than the external momenta and the up and down quark masses, and where the external fields are those of two-flavour chiral perturbation theory. In particular we work out the strange quark mass dependence of the electromagnetic two-flavour low-energy constants C and k_i. We expect that these relations will be useful for a more precise determination of the electromagnetic low-energy constants. ",https://doi.org/10.1140/epjc/s10052-007-0493-2,0710.5432v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Integrating out the heaviest quark in N--flavour ChPT,2011,"  We extend a known method to integrate out the strange quark in three flavour chiral perturbation theory to the context of an arbitrary number of flavours. As an application, we present the explicit formulae to one--loop accuracy for the heavy quark mass dependency of the low energy constants after decreasing the number of flavours by one while integrating out the heaviest quark in N--flavour chiral perturbation theory. ",https://doi.org/10.1140/epjc/s10052-011-1732-0,1107.0598v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Improved Deep Learning Baselines for Ubuntu Corpus Dialogs,2015,"  This paper presents results of our experiments for the next utterance ranking on the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for the next utterance ranking on this dataset. Finally, we discuss our future plans using this corpus. ",Kein DOI-Link verfügbar,1510.03753v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Revisiting CFR+ and Alternating Updates,2018,"  The CFR+ algorithm for solving imperfect information games is a variant of the popular CFR algorithm, with faster empirical performance on a range of problems. It was introduced with a theoretical upper bound on solution error, but subsequent work showed an error in one step of the proof. We provide updated proofs to recover the original bound. ",Kein DOI-Link verfügbar,1810.11542v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Aspects of radiative K^+_e3 decays,2006,"  We re-investigate the radiative charged kaon decay K+- --> pi0 e+- nu_e gamma in chiral perturbation theory, merging the chiral expansion with Low's theorem. We thoroughly analyze the precision of the predicted branching ratio relative to the non-radiative decay channel. Structure dependent terms and their impact on differential decay distributions are investigated in detail, and the possibility to see effects of the chiral anomaly in this decay channel is emphasized. ",https://doi.org/10.1140/epjc/s10052-007-0215-9,hep-ph/0611366v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Variance Reduction in Monte Carlo Counterfactual Regret Minimization   (VR-MCCFR) for Extensive Form Games using Baselines,2018,"  Learning strategies for imperfect information games from samples of interaction is a challenging problem. A common method for this setting, Monte Carlo Counterfactual Regret Minimization (MCCFR), can have slow long-term convergence rates due to high variance. In this paper, we introduce a variance reduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR. Using this technique, per-iteration estimated values and updates are reformulated as a function of sampled values and state-action baselines, similar to their use in policy gradient reinforcement learning. The new formulation allows estimates to be bootstrapped from other estimates within the same episode, propagating the benefits of baselines along the sampled trajectory; the estimates remain unbiased even when bootstrapping from other estimates. Finally, we show that given a perfect baseline, the variance of the value estimates can be reduced to zero. Experimental evaluation shows that VR-MCCFR brings an order of magnitude speedup, while the empirical variance decreases by three orders of magnitude. The decreased variance allows for the first time CFR+ to be used with sampling, increasing the speedup to two orders of magnitude. ",Kein DOI-Link verfügbar,1809.03057v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Low-Variance and Zero-Variance Baselines for Extensive-Form Games,2019,"  Extensive-form games (EFGs) are a common model of multi-agent interactions with imperfect information. State-of-the-art algorithms for solving these games typically perform full walks of the game tree that can prove prohibitively slow in large games. Alternatively, sampling-based methods such as Monte Carlo Counterfactual Regret Minimization walk one or more trajectories through the tree, touching only a fraction of the nodes on each iteration, at the expense of requiring more iterations to converge due to the variance of sampled values. In this paper, we extend recent work that uses baseline estimates to reduce this variance. We introduce a framework of baseline-corrected values in EFGs that generalizes the previous work. Within our framework, we propose new baseline functions that result in significantly reduced variance compared to existing techniques. We show that one particular choice of such a function --- predictive baseline --- is provably optimal under certain sampling schemes. This allows for efficient computation of zero-variance value estimates even along sampled trajectories. ",Kein DOI-Link verfügbar,1907.09633v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,AIVAT: A New Variance Reduction Technique for Agent Evaluation in   Imperfect Information Games,2016,"  Evaluating agent performance when outcomes are stochastic and agents use randomized strategies can be challenging when there is limited data available. The variance of sampled outcomes may make the simple approach of Monte Carlo sampling inadequate. This is the case for agents playing heads-up no-limit Texas hold'em poker, where man-machine competitions have involved multiple days of consistent play and still not resulted in statistically significant conclusions even when the winner's margin is substantial. In this paper, we introduce AIVAT, a low variance, provably unbiased value assessment tool that uses an arbitrary heuristic estimate of state value, as well as the explicit strategy of a subset of the agents. Unlike existing techniques which reduce the variance from chance events, or only consider game ending actions, AIVAT reduces the variance both from choices by nature and by players with a known strategy. The resulting estimator in no-limit poker can reduce the number of hands needed to draw statistical conclusions by more than a factor of 10. ",Kein DOI-Link verfügbar,1612.06915v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Integrating out strange quarks in ChPT,2007,"  We study three flavour chiral perturbation theory in a limit where the strange quark mass is much larger than the external momenta and the up and down quark masses, and where the external fields are those of two-flavour chiral perturbation theory. In this case, the theory reduces to the one of SU(2)_L x SU(2)_R. Through this reduction, one can work out the strange quark mass dependence of the LECs in the two-flavour case. We present the pertinent relations at two-loop order for F,B and l_i. ",https://doi.org/10.1016/j.physletb.2007.06.058,0706.0955v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Integrating out strange quarks in ChPT: terms at order p^6,2009,"  Chiral perturbation theory in the two-flavour sector allows one to analyse Green functions in QCD in a limit where the strange quark mass is considered to be large in comparison to the external momenta and to the light quark masses. In this framework, the pertinent low-energy constants depend on the value of the heavy quark masses. In a recent article, we have worked out, for the coupling constants which occur at order p^4, the dependence on the strange quark mass at two-loop accuracy. Here, we provide analogous relations for some of the couplings which are relevant at order p^6. To keep the calculations somewhat reasonable in size, we consider only those 28 couplings which enter the Green functions built from vector- and axial-vector quark currents in the chiral limit. This provides the matching for 27 linear combinations of the 28 couplings. ",https://doi.org/10.1016/j.physletb.2009.03.056,0903.0801v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Relations between SU(2)- and SU(3)-LECs in chiral perturbation theory,2009,"  Chiral perturbation theory in the two--flavour sector allows one to analyse Green functions in QCD in the limit where the strange quark mass is considered to be large in comparison to the external momenta and to the light quark masses m_u and m_d. In this framework, the low--energy constants of SU{2}_R \times SU{2}_L depend on the value of the heavy quark masses. For the coupling constants which occur at order p^2 and p^4 in the chiral expansion, we worked out the dependence on the strange quark mass at two--loop accuracy, and provided analogous relations for some of the couplings c_i which are relevant at order p^6. This talk comments on the methods used, and illustrates implications of the results obtained. ",https://doi.org/10.1134/S1063779610060249,0910.4880v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Text Understanding with the Attention Sum Reader Network,2016,"  Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets. ",Kein DOI-Link verfügbar,1603.01547v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Direct Characterization of Band Bending in GaP/Si(001) Heterostructures   with Hard X-ray Photoelectron Spectroscopy,2016,  We apply hard X-ray photoelectron spectroscopy (HAXPES) to investigate the electronic structures in ~50-nm thick epitaxial GaP layers grown on Si(001) under different conditions. Depth profiles of the local binding energies for the core levels are obtained by measuring the photoemission spectra at different incident photon energies between 3 and 7 keV and analyzing them with simple numerical models. The obtained depth profiles are in quantitative agreement with the band bending determinations for the same samples in a previous coherent phonon spectroscopic study. Our results demonstrate the applicability of the HAXPES with varying incident photon energy to characterize the electric potential profiles at buried semiconductor heterointerfaces. ,Kein DOI-Link verfügbar,1611.03785v1,Yes,potent(1)
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Learning not to Regret,2023,"  The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel ""learning not to regret"" framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned algorithms outpace their non-meta-learned counterparts, achieving more than tenfold improvements. ",Kein DOI-Link verfügbar,2303.01074v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Rethinking Formal Models of Partially Observable Multiagent Decision   Making,2019,"  Multiagent decision-making in partially observable environments is usually modelled as either an extensive-form game (EFG) in game theory or a partially observable stochastic game (POSG) in multiagent reinforcement learning (MARL). One issue with the current situation is that while most practical problems can be modelled in both formalisms, the relationship of the two models is unclear, which hinders the transfer of ideas between the two communities. A second issue is that while EFGs have recently seen significant algorithmic progress, their classical formalization is unsuitable for efficient presentation of the underlying ideas, such as those around decomposition.   To solve the first issue, we introduce factored-observation stochastic games (FOSGs), a minor modification of the POSG formalism which distinguishes between private and public observation and thereby greatly simplifies decomposition. To remedy the second issue, we show that FOSGs and POSGs are naturally connected to EFGs: by ""unrolling"" a FOSG into its tree form, we obtain an EFG. Conversely, any perfect-recall timeable EFG corresponds to some underlying FOSG in this manner. Moreover, this relationship justifies several minor modifications to the classical EFG formalization that recently appeared as an implicit response to the model's issues with decomposition. Finally, we illustrate the transfer of ideas between EFGs and MARL by presenting three key EFG techniques -- counterfactual regret minimization, sequence form, and decomposition -- in the FOSG framework. ",Kein DOI-Link verfügbar,1906.11110v4,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Sound Algorithms in Imperfect Information Games,2020,"  Search has played a fundamental role in computer game research since the very beginning. And while online search has been commonly used in perfect information games such as Chess and Go, online search methods for imperfect information games have only been introduced relatively recently. This paper addresses the question of what is a sound online algorithm in an imperfect information setting of two-player zero-sum games. We argue that the~fixed-strategy~definitions of exploitability and $\epsilon$-Nash equilibria are ill-suited to measure an online algorithm's worst-case performance. We thus formalize $\epsilon$-soundness, a concept that connects the worst-case performance of an online algorithm to the performance of an $\epsilon$-Nash equilibrium. As $\epsilon$-soundness can be difficult to compute in general, we introduce a consistency framework -- a hierarchy that connects an online algorithm's behavior to a Nash equilibrium. These multiple levels of consistency describe in what sense an online algorithm plays ""just like a fixed Nash equilibrium"". These notions further illustrate the difference between perfect and imperfect information settings, as the same consistency guarantees have different worst-case online performance in perfect and imperfect information games. The definitions of soundness and the consistency hierarchy finally provide appropriate tools to analyze online algorithms in repeated imperfect information games. We thus inspect some of the previous online algorithms in a new light, bringing new insights into their worst-case performance guarantees. ",Kein DOI-Link verfügbar,2006.08740v2,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,2017,"  Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches. ",https://doi.org/10.1126/science.aam6960,1701.01724v3,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Approximate exploitability: Learning a best response in large games,2020,"  Researchers have demonstrated that neural networks are vulnerable to adversarial examples and subtle environment changes, both of which one can view as a form of distribution shift. To humans, the resulting errors can look like blunders, eroding trust in these agents. In prior games research, agent evaluation often focused on the in-practice game outcomes. While valuable, such evaluation typically fails to evaluate robustness to worst-case outcomes. Prior research in computer poker has examined how to assess such worst-case performance, both exactly and approximately. Unfortunately, exact computation is infeasible with larger domains, and existing approximations rely on poker-specific knowledge. We introduce ISMCTS-BR, a scalable search-based deep reinforcement learning algorithm for learning a best response to an agent, thereby approximating worst-case performance. We demonstrate the technique in several two-player zero-sum games against a variety of agents, including several AlphaZero-based agents. ",Kein DOI-Link verfügbar,2004.09677v5,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Printable Nanoscopic Metamaterial Absorbers and Images with   Diffraction-Limited Resolution,2016,"  The fabrication of functional metamaterials with extreme feature resolution finds a host of applications such as the broad area of surface/light interaction. Non-planar features of such structures can significantly enhance their performance and tunability, but their facile generation remains a challenge. Here, we show that carefully designed out-of-plane nanopillars made of metal-dielectric composites integrated in a metal-dielectric-nanocomposite configuration, can absorb broadband light very effectively. We further demonstrate that electrohydrodynamic printing in a rapid nanodripping mode, is able to generate precise out-of-plane forests of such composite nanopillars with deposition resolutions at the diffraction limit on flat and non-flat substrates. The nanocomposite nature of the printed material allows the fine-tuning of the overall visible light absorption from complete absorption to complete reflection by simply tuning the pillar height. Almost perfect absorption (~95%) over the entire visible spectrum is achieved by a nanopillar forest covering only 6% of the printed area. Adjusting the height of individual pillar groups by design, we demonstrate on-demand control of the gray scale of a micrograph with a spatial resolution of 400 nm. These results constitute a significant step forward in ultra-high resolution facile fabrication of out-of-plane nanostructures, important to a broad palette of light design applications. nanostructures, important to a broad palette of light design applications. ",https://doi.org/10.1021/acsami.6b01585,1609.07328v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Solving Common-Payoff Games with Approximate Policy Iteration,2021,"  For artificially intelligent learning systems to have widespread applicability in real-world settings, it is important that they be able to operate decentrally. Unfortunately, decentralized control is difficult -- computing even an epsilon-optimal joint policy is a NEXP complete problem. Nevertheless, a recently rediscovered insight -- that a team of agents can coordinate via common knowledge -- has given rise to algorithms capable of finding optimal joint policies in small common-payoff games. The Bayesian action decoder (BAD) leverages this insight and deep reinforcement learning to scale to games as large as two-player Hanabi. However, the approximations it uses to do so prevent it from discovering optimal joint policies even in games small enough to brute force optimal solutions. This work proposes CAPI, a novel algorithm which, like BAD, combines common knowledge with deep reinforcement learning. However, unlike BAD, CAPI prioritizes the propensity to discover optimal joint policies over scalability. While this choice precludes CAPI from scaling to games as large as Hanabi, empirical results demonstrate that, on the games to which CAPI does scale, it is capable of discovering optimal joint policies even when other modern multi-agent reinforcement learning algorithms are unable to do so. Code is available at https://github.com/ssokota/capi . ",Kein DOI-Link verfügbar,2101.04237v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Color Change Effect in an Organic-Inorganic Hybrid Material Based on a   Porphyrin Diacid,2016,"  Porphyrinic materials show a range of interesting and useful optical and electrical properties. The less well-known sub-class of porphyrin diacids has been used in this work to construct an ionic hybrid organic-inorganic material in combination with a halogenidometalate anion. The resulting compound, $[H_6TPyP][BiCl_6]_2$ (1) (TPyP = tetra(4-pyridyl)porphyrin) has been obtained via a facile solution based synthesis in single crystalline form. The material exhibits a broad photoluminescence emission band between 650 and 850 nm at room temperature. Single crystals of $[H_6TPyP][BiCl_6]_2$ show a photocurrent in the fA and a much higher dark current in the nA range. They also display an unexpected reversible color change upon wetting with different liquids. This phenomenon has been investigated with optical spectroscopy, SEM, XPS and NEXAFS techniques, showing that a surface-based structural coloration effect is the source of the color change. This stands in contrast to other materials where structural coloration typically has to be introduced through elaborate, multi-step processes or the use of natural templates. Additionally, it underscores the potential of self-assembly of porphyrinic hybrid compounds in the fabrication of materials with unusual optical properties. ",Kein DOI-Link verfügbar,1611.04737v1,Yes,potent(1)
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,The Advantage Regret-Matching Actor-Critic,2020,"  Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em. ",Kein DOI-Link verfügbar,2008.12234v1,No,
0000-0002-4658-0501,Martin Schmid,Koblenz Universität der Angewandte Wissenschaften,Student of Games: A unified learning algorithm for both perfect and   imperfect information games,2021,"  Games have a long history as benchmarks for progress in artificial intelligence. Approaches using search and learning produced strong performance across many perfect information games, and approaches using game-theoretic reasoning and learning demonstrated strong performance for specific imperfect information poker variants. We introduce Student of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Student of Games achieves strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Student of Games is sound, converging to perfect play as available computation and approximation capacity increases. Student of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning. ",https://doi.org/10.1126/sciadv.adg3256,2112.03178v2,No,
0000-0001-6798-5092,Marcel Konrad,FOM Hochschule,Knowledge Graph Building Blocks: An easy-to-use Framework for developing   FAIREr Knowledge Graphs,2023,"  Knowledge graphs and ontologies provide promising technical solutions for implementing the FAIR Principles for Findable, Accessible, Interoperable, and Reusable data and metadata. However, they also come with their own challenges. Nine such challenges are discussed and associated with the criterion of cognitive interoperability and specific FAIREr principles (FAIR + Explorability raised) that they fail to meet. We introduce an easy-to-use, open source knowledge graph framework that is based on knowledge graph building blocks (KGBBs). KGBBs are small information modules for knowledge-processing, each based on a specific type of semantic unit. By interrelating several KGBBs, one can specify a KGBB-driven FAIREr knowledge graph. Besides implementing semantic units, the KGBB Framework clearly distinguishes and decouples an internal in-memory data model from data storage, data display, and data access/export models. We argue that this decoupling is essential for solving many problems of knowledge management systems. We discuss the architecture of the KGBB Framework as we envision it, comprising (i) an openly accessible KGBB-Repository for different types of KGBBs, (ii) a KGBB-Engine for managing and operating FAIREr knowledge graphs (including automatic provenance tracking, editing changelog, and versioning of semantic units); (iii) a repository for KGBB-Functions; (iv) a low-code KGBB-Editor with which domain experts can create new KGBBs and specify their own FAIREr knowledge graph without having to think about semantic modelling. We conclude with discussing the nine challenges and how the KGBB Framework provides solutions for the issues they raise. While most of what we discuss here is entirely conceptual, we can point to two prototypes that demonstrate the principle feasibility of using semantic units and KGBBs to manage and structure knowledge graphs. ",Kein DOI-Link verfügbar,2304.09029v1,No,
0000-0001-6798-5092,Marcel Konrad,FOM Hochschule,Towards a Rosetta Stone for (meta)data: Learning from natural language   to improve semantic and cognitive interoperability,2023,"  In order to effectively manage the overwhelming influx of data, it is crucial to ensure that data is findable, accessible, interoperable, and reusable (FAIR). While ontologies and knowledge graphs have been employed to enhance FAIRness, challenges remain regarding semantic and cognitive interoperability. We explore how English facilitates reliable communication of terms and statements, and transfer our findings to a framework of ontologies and knowledge graphs, while treating terms and statements as minimal information units. We categorize statement types based on their predicates, recognizing the limitations of modeling non-binary predicates with multiple triples, which negatively impacts interoperability. Terms are associated with different frames of reference, and different operations require different schemata. Term mappings and schema crosswalks are therefore vital for semantic interoperability. We propose a machine-actionable Rosetta Stone Framework for (meta)data, which uses reference terms and schemata as an interlingua to minimize mappings and crosswalks. Modeling statements rather than a human-independent reality ensures cognitive familiarity and thus better interoperability of data structures. We extend this Rosetta modeling paradigm to reference schemata, resulting in simple schemata with a consistent structure across statement types, empowering domain experts to create their own schemata using the Rosetta Editor, without requiring knowledge of semantics. The Editor also allows specifying textual and graphical display templates for each schema, delivering human-readable data representations alongside machine-actionable data structures. The Rosetta Query Builder derives queries based on completed input forms and the information from corresponding reference schemata. This work sets the conceptual ground for the Rosetta Stone Framework that we plan to develop in the future. ",Kein DOI-Link verfügbar,2307.09605v1,No,
0000-0001-6798-5092,Marcel Konrad,FOM Hochschule,FAIR 2.0: Extending the FAIR Guiding Principles to Address Semantic   Interoperability,2024,"  FAIR data presupposes their successful communication between machines and humans while preserving their meaning and reference, requiring all parties involved to share the same background knowledge. Inspired by English as a natural language, we investigate the linguistic structure that ensures reliable communication of information and draw parallels with data structures, understanding both as models of systems of interest. We conceptualize semantic interoperability as comprising terminological and propositional interoperability. The former includes ontological (i.e., same meaning) and referential (i.e., same referent/extension) interoperability and the latter schema (i.e., same data schema) and logical (i.e., same logical framework) interoperability. Since no best ontology and no best data schema exists, establishing semantic interoperability and FAIRness of data and metadata requires the provision of a comprehensive set of relevant ontological and referential entity mappings and schema crosswalks. We therefore propose appropriate additions to the FAIR Guiding Principles, leading to FAIR 2.0. Furthermore, achieving FAIRness of data requires the provision of FAIR services in addition to organizing data into FAIR Digital Objects. FAIR services include a terminology, a schema, and an operations service. ",Kein DOI-Link verfügbar,2405.03345v1,No,
0000-0001-6798-5092,Marcel Konrad,FOM Hochschule,Rosetta Statements: Lowering the Barrier for Semantic Parsing and   Increasing the Cognitive Interoperability of Knowledge Graphs,2024,"  Machines need data and metadata to be machine-actionable and FAIR (findable, accessible, interoperable, reusable) to manage increasing data volumes. Knowledge graphs and ontologies are key to this, but their use is hampered by high access barriers due to required prior knowledge in semantics and data modelling. The Rosetta Statement approach proposes modeling English natural language statements instead of a mind-independent reality. We propose a metamodel for creating semantic schema patterns for simple statement types. The approach supports versioning of statements and provides a detailed editing history. Each Rosetta Statement pattern has a dynamic label for displaying statements as natural language sentences. Implemented in the Open Research Knowledge Graph (ORKG) as a use case, this approach allows domain experts to define data schema patterns without needing semantic knowledge. Future plans include combining Rosetta Statements with semantic units to organize ORKG into meaningful subgraphs, improving usability. A search interface for querying statements without needing SPARQL or Cypher knowledge is also planned, along with tools for data entry and display using Large Language Models and NLP. The Rosetta Statement metamodel supports a two-step knowledge graph construction procedure. Domain experts can model semantic content without support from ontology engineers, lowering entry barriers and increasing cognitive interoperability. The second level involves developing semantic graph patterns for reasoning, requiring collaboration with ontology engineers. ",Kein DOI-Link verfügbar,2407.20007v1,No,
0000-0002-2746-0029,Guido H. Baltes,Hochschule Konstanz Universität der Angewandte Wissenschaften,Software Startups -- A Research Agenda,2023,"  Software startup companies develop innovative, software-intensive products within limited time frames and with few resources, searching for sustainable and scalable business models. Software startups are quite distinct from traditional mature software companies, but also from micro-, small-, and medium-sized enterprises, introducing new challenges relevant for software engineering research. This paper's research agenda focuses on software engineering in startups, identifying, in particular, 70+ research questions in the areas of supporting startup engineering activities, startup evolution models and patterns, ecosystems and innovation hubs, human aspects in software startups, applying startup concepts in non-startup environments, and methodologies and theories for startup research. We connect and motivate this research agenda with past studies in software startup research, while pointing out possible future directions. While all authors of this research agenda have their main background in Software Engineering or Computer Science, their interest in software startups broadens the perspective to the challenges, but also to the opportunities that emerge from multi-disciplinary research. Our audience is therefore primarily software engineering researchers, even though we aim at stimulating collaborations and research that crosses disciplinary boundaries. We believe that with this research agenda we cover a wide spectrum of the software startup industry current needs. ",https://doi.org/10.5277/e-Inf160105,2308.12816v1,Yes,innovative(1)
0000-0002-4156-5775,Hanno Langweg,Hochschule Konstanz Universität der Angewandte Wissenschaften,Extending the Trusted Path in Client-Server Interaction,2006,"  We present a method to secure the complete path between a server and the local human user at a network node. This is useful for scenarios like internet banking, electronic signatures, or online voting. Protection of input authenticity and output integrity and authenticity is accomplished by a combination of traditional and novel technologies, e.g., SSL, ActiveX, and DirectX. Our approach does not require administrative privileges to deploy and is hence suitable for consumer applications. Results are based on the implementation of a proof-of-concept application for the Windows platform. ",Kein DOI-Link verfügbar,cs/0611102v1,No,
0000-0001-7685-5919,Christopher Knievel,Hochschule Konstanz Universität der Angewandte Wissenschaften,Sensor Object Plausibilization with Boids Flocking Algorithm,2022,"  Driver assistance systems are increasingly becoming part of the standard equipment of vehicles and thus contribute to road safety. However, as they become more widespread, the requirements for cost efficiency are also increasing, and so few and inexpensive sensors are used in these systems. Especially in challenging situations, this leads to the fact that target discrimination cannot be ensured which in turn leads to a false reaction of the driver assistance system. Typically, the interaction between moving traffic participants is not modeled directly in the environmental model so that tracked objects can split, merge or disappear. The Boids flocking algorithm is used to model the interaction between road users on already tracked objects by applying the movement rules (separation, cohesion, alignment) on the boids. This facilitates the creation of semantic neighborhood information between road users. We show in a comprehensive simulation that with only 7 boids per traffic participant, the estimated median separatation between objects can improve from 2.4 m to 3 m for a ground truth of 3.7 m. The bottom percentile improves from 1.85 m to 2.8 m. ",Kein DOI-Link verfügbar,2203.08036v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The Chang-Skjelbred lemma and generalizations,2023,"  We review the Chang-Skjelbred lemma for torus-equivariant cohomology and discuss several generalizations of it: to other coefficients, other groups and also to syzygies in equivariant cohomology and the Atiyah-Bredon sequence. ",Kein DOI-Link verfügbar,2306.05165v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Tensor products of homotopy Gerstenhaber algebras,2010,"  On the tensor product of two homotopy Gerstenhaber algebras we construct a Hirsch algebra structure which extends the canonical dg algebra structure. Our result applies more generally to tensor products of ""level 3 Hirsch algebras"" and also to the Mayer-Vietoris double complex. ",https://doi.org/10.4310/HHA.2011.v13.n2.a15,1009.1116v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Koszul duality and equivariant cohomology for tori,2003,  Let T be a torus. We show that Koszul duality can be used to compute the equivariant cohomology of topological T-spaces as well as the cohomology of pull backs of the universal T-bundle. The new features are that no further assumptions about the spaces are made and that the coefficient ring may be arbitrary. This gives in particular a Cartan-type model for the equivariant cohomology of a T-space with arbitrary coefficients. Our method works for intersection homology as well. ,https://doi.org/10.1155/S1073792803206103,math/0301083v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,On the integral cohomology of smooth toric varieties,2003,"  Let $X_\Sigma$ be a smooth, not necessarily compact toric variety. We show that a certain complex, defined in terms of the fan $\Sigma$, computes the integral cohomology of $X_\Sigma$, including the module structure over the homology of the torus. In some cases we can also give the product. As a corollary we obtain that the cycle map from Chow groups to integral Borel-Moore homology is split injective for smooth toric varieties. Another result is that the differential algebra of singular cochains on the Borel construction of $X_\Sigma$ is formal. ",https://doi.org/10.1134/S008154380601007X,math/0308253v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Symmetric products of equivariantly formal spaces,2016,"  Let X be a CW complex with a continuous action of a topological group G. We show that if X is equivariantly formal for singular cohomology with coefficients in a field, then so are all symmetric products of X and in fact all its Gamma-products. In particular, symmetric products of quasi-projective M-varieties are again M-varieties. This generalizes a result by Biswas and D'Mello about symmetric products of M-curves. We also discuss several related questions. ",https://doi.org/10.4153/CMB-2017-032-0,1604.08273v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Homotopy Gerstenhaber algebras are strongly homotopy commutative,2019,"  We show that any homotopy Gerstenhaber algebra is naturally a strongly homotopy commutative (shc) algebra in the sense of Stasheff-Halperin with a homotopy associative structure map. In the presence of certain additional operations corresponding to a cup-1 product on the bar construction, the structure map becomes homotopy commutative, so that one obtains an shc algebra in the sense of Munkholm. ",https://doi.org/10.1007/s40062-020-00268-y,1907.04778v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Szczarba's twisting cochain is comultiplicative,2020,"  We prove that Szczarba's twisting cochain is comultiplicative. In particular, the induced map from the cobar construction of the chains on a 1-reduced simplicial set X to the chains on the Kan loop group of X is a quasi-isomorphism of dg bialgebras. We also show that Szczarba's twisted shuffle map is a dgc map connecting a twisted Cartesian product with the associated twisted tensor product. This gives a natural dgc model for fibre bundles. We apply our results to finite covering spaces and to the Serre spectral sequence. ",Kein DOI-Link verfügbar,2008.08943v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The cohomology rings of real toric spaces and smooth real toric   varieties,2020,"  We compute the cohomology rings of smooth real toric varieties and of real toric spaces, which are quotients of real moment-angle complexes by freely acting subgroups of the ambient 2-torus. The differential graded algebra we present is in fact an equivariant dga model, valid for arbitrary coefficients. We deduce from our description that smooth toric varieties are M-varieties. ",https://doi.org/10.1017/prm.2021.30,2008.08961v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Dga models for moment-angle complexes,2020,"  A dga model for the integral singular cochains on a moment-angle complex is given by the twisted tensor product of the corresponding Stanley-Reisner ring and an exterior algebra. We present a short proof of this fact and extend it to real moment-angle complexes. We also compare various descriptions of the cohomology rings of these spaces, including one stated without proof by Gitler and L\'opez de Medrano. ",Kein DOI-Link verfügbar,2006.01571v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Szczarba's twisting cochain and the Eilenberg-Zilber maps,2020,"  We show that Szczarba's twisting cochain for a twisted Cartesian product is essentially the same as the one constructed by Shih. More precisely, Szczarba's twisting cochain can be obtained via the basic perturbation lemma if one uses a 'reversed' version of the classical Eilenberg-MacLane homotopy for the Eilenberg-Zilber contraction. Along the way we prove several new identities involving these homotopies. ",https://doi.org/10.1007/s13348-020-00299-x,2006.02819v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Describing toric varieties and their equivariant cohomology,2009,"  Topologically, compact toric varieties can be constructed as identification spaces: they are quotients of the product of a compact torus and the order complex of the fan. We give a detailed proof of this fact, extend it to the non-compact case and draw several, mostly cohomological conclusions.   In particular, we show that the equivariant integral cohomology of a toric variety can be described in terms of piecewise polynomials on the fan if the ordinary integral cohomology is concentrated in even degrees. This generalizes a result of Bahri-Franz-Ray to the non-compact case. We also investigate torsion phenomena in integral cohomology. ",https://doi.org/10.4064/cm121-1-1,0909.0057v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Syzygies in equivariant cohomology for non-abelian Lie groups,2014,"  We extend the work of Allday-Franz-Puppe on syzygies in equivariant cohomology from tori to arbitrary compact connected Lie groups G. In particular, we show that for a compact orientable G-manifold X the analogue of the Chang-Skjelbred sequence is exact if and only if the equivariant cohomology of X is reflexive, if and only if the equivariant Poincare pairing for X is perfect. Along the way we establish that the equivariant cohomology modules arising from the orbit filtration of X are Cohen-Macaulay. We allow singular spaces and introduce a Cartan model for their equivariant cohomology. We also develop a criterion for the finiteness of the number of infinitesimal orbit types of a G-manifold. ",https://doi.org/10.1007/978-3-319-31580-5_14,1409.0681v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Koszul duality and equivariant cohomology,2003,"  Let G be a topological group such that its homology H(G) with coefficients in a principal ideal domain R is an exterior algebra, generated in odd degrees. We show that the singular cochain functor carries the duality between G-spaces and spaces over BG to the Koszul duality between modules up to homotopy over H(G) and H^*(BG). This gives in particular a Cartan-type model for the equivariant cohomology of a G-space. As another corollary, we obtain a multiplicative quasi-isomorphism C^*(BG) -> H^*(BG).   A key step in the proof is to show that a differential Hopf algebra is formal in the category of A-infinity algebras provided that it is free over R and its homology an exterior algebra. ",Kein DOI-Link verfügbar,math/0307115v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,A quotient criterion for syzygies in equivariant cohomology,2012,"  Let X be a manifold with an action of a torus T such that all isotropy groups are connected and satisfying some other mild hypotheses. We provide a necessary and sufficient criterion for the T-equivariant cohomology of X with real coefficients to be a certain syzygy as a module over the cohomology of BT. It turns out that, possibly after blowing up the non-free part of the action, this only depends on the orbit space X/T together with its stratification by orbit type. Our criterion unifies and generalizes results of many authors about the freeness and torsion-freeness of equivariant cohomology for various classes of T-manifolds. ",https://doi.org/10.1007/s00031-016-9408-3 10.1007/s00031-019-09520-z,1205.4462v4,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Big polygon spaces,2014,"  We study a new class of compact orientable manifolds, called big polygon spaces. They are intersections of real quadrics and related to polygon spaces, which appear as their fixed point set under a canonical torus action.   What makes big polygon spaces interesting is that they exhibit remarkable new features in equivariant cohomology: The Chang-Skjelbred sequence can be exact for them and the equivariant Poincare pairing perfect although their equivariant cohomology is never free as a module over the cohomology ring of BT. More generally, big polygon spaces show that a certain bound on the syzygy order of the equivariant cohomology of compact orientable T-manifolds obtained by Allday, Puppe and the author is sharp. ",https://doi.org/10.1093/imrn/rnv090,1403.4485v4,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The cohomology rings of homogeneous spaces,2019,"  Let $G$ be a compact connected Lie group and $K$ a closed connected subgroup. Assume that the order of any torsion element in the integral cohomology of $G$ and $K$ is invertible in a given principal ideal domain $k$. It is known that in this case the cohomology of the homogeneous space $G/K$ with coefficients in $k$ and the torsion product of $H^{*}(BK)$ and $k$ over $H^{*}(BG)$ are isomorphic as $k$-modules. We show that this isomorphism is multiplicative and natural in the pair $(G,K)$ provided that 2 is invertible in $k$. The proof uses homotopy Gerstenhaber algebras in an essential way. In particular, we show that the normalized singular cochains on the classifying space of a torus are formal as a homotopy Gerstenhaber algebra. ",https://doi.org/10.1112/topo.12213,1907.04777v4,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Homotopy Gerstenhaber formality of Davis-Januszkiewicz spaces,2019,"  A homotopy Gerstenhaber structure on a differential graded algebra is essentially a family of operations defining a multiplication on its bar construction. We prove that the normalized singular cochain algebra of a Davis-Januszkiewicz space is formal as a homotopy Gerstenhaber algebra, for any coefficient ring. This generalizes a recent result by the author about classifying spaces of tori and also strengthens the well-known dga formality result for Davis-Januszkiewicz spaces due to the author and Notbohm-Ray. As an application, we determine the cohomology rings of free and based loop spaces of Davis-Januszkiewicz spaces. ",https://doi.org/10.4310/HHA.2021.v23.n2.a17,1907.04782v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The cohomology rings of smooth toric varieties and quotients of   moment-angle complexes,2019,"  Partial quotients of moment-angle complexes are topological analogues of smooth, not necessarily compact toric varieties. In 1998, Buchstaber and Panov proposed a formula for the cohomology ring of such a partial quotient in terms of a torsion product involving the corresponding Stanley-Reisner ring. We show that their formula gives the correct cup product if 2 is invertible in the chosen coefficient ring, but not in general. We rectify this by defining an explicit deformation of the canonical multiplication on the torsion product. ",https://doi.org/10.2140/gt.2021.25.2109,1907.04791v4,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,An $A_\infty$-version of the Eilenberg-Moore theorem,2023,"  We construct an $A_\infty$-structure on the two-sided bar construction involving homotopy Gerstenhaber algebras (hgas). It extends the non-associative product defined by Carlson and the author and generalizes the dga structure on the one-sided bar construction due to Kadeishvili-Saneblidze. As a consequence, the multiplicative cohomology isomorphism from the Eilenberg-Moore theorem is promoted to a quasi-isomorphism of $A_\infty$-algebras.   We also show that the resulting product on the differential torsion product involving cochain algebras agrees with the one defined by Eilenberg-Moore and Smith, for all triples of spaces. This is a consequence of the following result, which is of independent interest: The strongly homotopy commutative (shc) structure on cochains inductively constructed by Gugenheim-Munkholm agrees with the one previously defined by the author for all hgas. ",Kein DOI-Link verfügbar,2311.16947v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Exact sequences for equivariantly formal spaces,2003,"  Let T be a torus. We present an exact sequence relating the relative equivariant cohomologies of the skeletons of an equivariantly formal T-space. This sequence, which goes back to Atiyah and Bredon, generalizes the so-called Chang-Skjelbred lemma. As coefficients, we allow prime fields and subrings of the rationals, including the integers. We extend to the same coefficients a generalization of this ""Atiyah-Bredon sequence"" for actions without fixed points which has recently been obtained by Goertsches and Toeben. ",Kein DOI-Link verfügbar,math/0307112v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Exact cohomology sequences with integral coefficients for torus actions,2005,"  Using methods applied by Atiyah in equivariant K-theory, Bredon obtained exact sequences for the relative cohomologies (with rational coefficients) of the equivariant skeletons of (sufficiently nice) T-spaces, T=(S^1)^n, with free equivariant cohomology over the cohomology of BT. Here we characterise those finite T-CW complexes with connected isotropy groups for which an analogous result holds with integral coefficients. ",https://doi.org/10.1007/s00031-005-1127-0,math/0505607v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Steenrod squares on conjugation spaces,2005,"  We prove that the coefficients of the so-called conjugation equation for conjugation spaces in the sense of Hausmann-Holm-Puppe are completely determined by Steenrod squares. This generalises a result of V.A. Krasnov for certain complex algebraic varieties. It also leads to a generalisation of a formula given by Borel and Haefliger, thereby largely answering an old question of theirs in the affirmative. ",https://doi.org/10.1016/j.crma.2005.12.012,math/0510157v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Freeness of equivariant cohomology and mutants of compactified   representations,2007,"  We survey generalisations of the Chang-Skjelbred Lemma for integral coefficients. Moreover, we construct examples of manifolds with actions of tori of rank > 2 whose equivariant cohomology is torsion-free, but not free. This answers a question of Allday's. The ""mutants"" we construct are obtained from compactified representations and involve Hopf bundles in a crucial way. ",https://doi.org/10.1090/conm/460/09012,0710.2302v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The syzygy order of big polygon spaces,2019,  Big polygon spaces are compact orientable manifolds with a torus action whose equivariant cohomology can be torsion-free or reflexive without being free as a module over $H^*(BT)$. We determine the exact syzygy order of the equivariant cohomology of a big polygon space in terms of the length vector defining it. The proof uses a refined characterization of syzygies in terms of certain linearly independent elements in $H^2(BT)$ adapted to the isotropy groups occurring in a given $T$-space. ,https://doi.org/10.2140/agt.2020.20.2657,1904.01051v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Cohomology of smooth toric varieties: naturality,2021,"  Building on the recent computation of the cohomology rings of smooth toric varieties and partial quotients of moment-angle complexes, we investigate the naturality properties of the resulting isomorphism between the cohomology of such a space and the torsion product involving the Stanley-Reisner ring. If 2 is invertible in the chosen coefficient ring, then the isomorphism is natural with respect to toric morphisms, which for partial quotients are defined in analogy with toric varieties. In general there are deformation terms that we describe explicitly. ",https://doi.org/10.1016/j.jpaa.2023.107590,2104.03825v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Graph equivariant cohomological rigidity for GKM graphs,2017,  We formulate the notion of an isomorphism of GKM graphs. We then show that two GKM graphs have isomorphic graph equivariant cohomology algebras if and only if the graphs are isomorphic. ,https://doi.org/10.3792/pjaa.95.107,1710.08264v5,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Mutants of compactified representations revisited,2015,  We show that the mutants of compactified representations constructed by Franz and Puppe can be written as intersections of real quadrics involving division algebras and as generalizations of polygon spaces. We also show that these manifolds are connected sums of products of spheres. ,https://doi.org/10.1007/s40590-016-0128-4,1511.01115v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The equivariant cohomology of weighted projective space,2007,"  We describe the integral equivariant cohomology ring of a weighted projective space in terms of piecewise polynomials, and thence by generators and relations. We deduce that the ring is a perfect invariant, and prove a Chern class formula for weighted projective bundles. ",https://doi.org/10.1017/S0305004108001965,0708.1581v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,The classification of weighted projective spaces,2011,"  We obtain two classifications of weighted projective spaces; up to homeomorphism and up to homotopy equivalence. We show that the former coincides with Al Amrani's classification up to isomorphism of algebraic varieties, and deduce the latter by proving that the Mislin genus of any weighted projective space is rigid. ",https://doi.org/10.4064/fm220-3-3,1108.1938v4,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Syzygies in equivariant cohomology in positive characteristic,2020,"  We develop a theory of syzygies in equivariant cohomology for tori as well as $p$-tori and coefficients in $\mathbb{F}_p$. A noteworthy feature is a new algebraic approach to the partial exactness of the Atiyah-Bredon sequence, which also covers all instances considered so far. ",https://doi.org/10.1515/forum-2020-0188,2007.00496v2,Yes,noteworthy(1)
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Weights in cohomology and the Eilenberg-Moore spectral sequence,2004,"  We show that in the category of complex algebraic varieties, the Eilenberg--Moore spectral sequence can be endowed with a weight filtration. This implies that it degenerates if all involved spaces have pure cohomology. As application, we compute the rational cohomology of an algebraic $G$-variety $X$ ($G$ being a connected algebraic group) in terms of its equivariant cohomology provided that $H_G(X)$ is pure. This is the case, for example, if $X$ is smooth and has only finitely many orbits. We work in the category of mixed sheaves; therefore our results apply equally to (equivariant) intersection homology. ",Kein DOI-Link verfügbar,math/0405589v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Weighted projective spaces and iterated Thom spaces,2011,"  For any (n+1)-dimensional weight vector {\chi} of positive integers, the weighted projective space P(\chi) is a projective toric variety, and has orbifold singularities in every case other than CP^n. We study the algebraic topology of P(\chi), paying particular attention to its localisation at individual primes p. We identify certain p-primary weight vectors {\pi} for which P(\pi) is homeomorphic to an iterated Thom space over S^2, and discuss how any P(\chi) may be reconstructed from its p-primary factors. We express Kawasaki's computations of the integral cohomology ring H^*(P(\chi);Z) in terms of iterated Thom isomorphisms, and recover Al Amrani's extension to complex K-theory. Our methods generalise to arbitrary complex oriented cohomology algebras E^*(P(\chi)) and their dual homology coalgebras E_*(P(\chi)), as we demonstrate for complex cobordism theory (the universal example). In particular, we describe a fundamental class in \Omega^U_{2n}(P(\chi)), which may be interpreted as a resolution of singularities. ",Kein DOI-Link verfügbar,1109.2359v1,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,"Equivariant cohomology, syzygies and orbit structure",2011,"  Let X be a ""nice"" space with an action of a torus T. We consider the Atiyah-Bredon sequence of equivariant cohomology modules arising from the filtration of X by orbit dimension. We show that a front piece of this sequence is exact if and only if the H^*(BT)-module H_T^*(X) is a certain syzygy. Moreover, we express the cohomology of that sequence as an Ext module involving a suitably defined equivariant homology of X. One consequence is that the GKM method for computing equivariant cohomology applies to a Poincare duality space if and only if the equivariant Poincare pairing is perfect. ",https://doi.org/10.1090/S0002-9947-2014-06165-5,1111.0957v2,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Equivariant Poincaré-Alexander-Lefschetz duality and the   Cohen-Macaulay property,2013,"  We prove a Poincare-Alexander-Lefschetz duality theorem for rational torus-equivariant cohomology and rational homology manifolds. We allow non-compact and non-orientable spaces. We use this to deduce certain short exact sequences in equivariant cohomology, originally due to Duflot in the differentiable case, from similar, but more general short exact sequences in equivariant homology. A crucial role is played by the Cohen-Macaulayness of relative equivariant cohomology modules arising from the orbit filtration. ",https://doi.org/10.2140/agt.2014.14.1339,1303.1146v3,No,
0000-0003-3789-8849,Matthias Franz,Hochschule Konstanz Universität der Angewandte Wissenschaften,Is every toric variety an M-variety?,2005,"  A complex algebraic variety X defined over the real numbers is called an M-variety if the sum of its Betti numbers (for homology with closed supports and coefficients in Z/2) coincides with the corresponding sum for the real part of X. It has been known for a long time that any nonsingular complete toric variety is an M-variety. In this paper we consider whether this remains true for toric varieties that are singular or not complete, and we give a positive answer when the dimension of X is less than or equal to 3. ",https://doi.org/10.1007/s00229-006-0004-z,math/0510228v1,No,
0000-0003-1417-3896,Marcel Arpogaus,Hochschule Konstanz Universität der Angewandte Wissenschaften,Short-Term Density Forecasting of Low-Voltage Load using   Bernstein-Polynomial Normalizing Flows,2022,"  The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures. ",https://doi.org/10.1109/TSG.2023.3254890,2204.13939v3,No,
0000-0003-1417-3896,Marcel Arpogaus,Hochschule Konstanz Universität der Angewandte Wissenschaften,How Inverse Conditional Flows Can Serve as a Substitute for   Distributional Regression,2024,"  Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning. ",Kein DOI-Link verfügbar,2405.05429v3,No,
0000-0003-2271-8630,Oliver Dürr,Hochschule Konstanz Universität der Angewandte Wissenschaften,Using Community Structure for Complex Network Layout,2012,"  We present a new layout algorithm for complex networks that combines a multi-scale approach for community detection with a standard force-directed design. Since community detection is computationally cheap, we can exploit the multi-scale approach to generate network configurations with close-to-minimal energy very fast. As a further asset, we can use the knowledge of the community structure to facilitate the interpretation of large networks, for example the network defined by protein-protein interactions. ",Kein DOI-Link verfügbar,1207.6282v1,No,
0009-0003-7261-0123,Simon Walther,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Noise Regularization for Conditional Density Estimation,2019,"  Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce. ",Kein DOI-Link verfügbar,1907.08982v2,No,
0009-0003-7261-0123,Simon Walther,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Conditional Density Estimation with Neural Networks: Best Practices and   Benchmarks,2019,"  Given a set of empirical observations, conditional density estimation aims to capture the statistical relationship between a conditional variable $\mathbf{x}$ and a dependent variable $\mathbf{y}$ by modeling their conditional probability $p(\mathbf{y}|\mathbf{x})$. The paper develops best practices for conditional density estimation for finance applications with neural networks, grounded on mathematical insights and empirical evaluations. In particular, we introduce a noise regularization and data normalization scheme, alleviating problems with over-fitting, initialization and hyper-parameter sensitivity of such estimators. We compare our proposed methodology with popular semi- and non-parametric density estimators, underpin its effectiveness in various benchmarks on simulated and Euro Stoxx 50 data and show its superior performance. Our methodology allows to obtain high-quality estimators for statistical expectations of higher moments, quantiles and non-linear return transformations, with very little assumptions about the return dynamic. ",Kein DOI-Link verfügbar,1903.00954v2,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,The Virtual Doctor: An Interactive Artificial Intelligence based on Deep   Learning for Non-Invasive Prediction of Diabetes,2019,"  Artificial intelligence (AI) will pave the way to a new era in medicine. However, currently available AI systems do not interact with a patient, e.g., for anamnesis, and thus are only used by the physicians for predictions in diagnosis or prognosis. However, these systems are widely used, e.g., in diabetes or cancer prediction. In the current study, we developed an AI that is able to interact with a patient (virtual doctor) by using a speech recognition and speech synthesis system and thus can autonomously interact with the patient, which is particularly important for, e.g., rural areas, where the availability of primary medical care is strongly limited by low population densities. As a proof-of-concept, the system is able to predict type 2 diabetes mellitus (T2DM) based on non-invasive sensors and deep neural networks. Moreover, the system provides an easy-to-interpret probability estimation for T2DM for a given patient. Besides the development of the AI, we further analyzed the acceptance of young people for AI in healthcare to estimate the impact of such system in the future. ",https://doi.org/10.1016/j.artmed.2019.101706,1903.12069v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Symmetric measurements attaining the accessible information,2005,  A theorem of Davies states that for symmetric quantum states there exists a symmetric POVM maximizing the mutual information. To apply this theorem the representation of the symmetry group has to be irreducible. We obtain a similar yet weaker result for reducible representations. We apply our results to the double trines ensemble and show numerically that for this ensemble the pretty good measurement is optimal. ,Kein DOI-Link verfügbar,quant-ph/0509122v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Quantum circuits for single-qubit measurements corresponding to platonic   solids,2003,  Each platonic solid defines a single-qubit positive operator valued measure (POVM) by interpreting its vertices as points on the Bloch sphere. We construct simple circuits for implementing this kind of measurements and other simple types of symmetric POVMs on one qubit. Each implementation consists of a discrete Fourier transform and some elementary quantum operations followed by an orthogonal measurement in the computational basis. ,Kein DOI-Link verfügbar,quant-ph/0308098v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Performing joint measurements and transformations on several qubits by   operating on a single control qubit,2002,  An n-qubit quantum register can in principle be completely controlled by operating on a single qubit that interacts with the register via an appropriate fixed interaction. We consider a hypothetical system consisting of n spin-1/2 nuclei that interact with an electron spin via a magnetic interaction. We describe algorithms that measure non-trivial joint observables on the register by acting on the control spin only. For large n this is not an efficient model for universal quantum computation but it can be modified to an efficient one if one allows n possible positions of the control particle.   This toy model of measurements illustrates in which way specific interactions between the register and a probe particle support specific types of joint measurements in the sense that some joint observables can be measured by simple sequences of operations on the probe particle. ,https://doi.org/10.1103/PhysRevA.67.042320,quant-ph/0207134v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Measuring 4-local n-qubit observables could probabilistically solve   PSPACE,2003,"  We consider a hypothetical apparatus that implements measurements for arbitrary 4-local quantum observables A on n qubits. The apparatus implements the ``measurement algorithm'' after receiving a classical description of A. We show that a few precise measurements, applied to a basis state would provide a probabilistic solution of PSPACE problems. The error probability decreases exponentially with the number of runs if the measurement accuracy is of the order of the spectral gaps of A.   Moreover, every decision problem which can be solved on a quantum computer in T time steps can be encoded into a 4-local observable such that the solution requires only measurements of accuracy O(1/T).   Provided that BQP<>PSPACE, our result shows that efficient algorithms for precise measurements of general 4-local observables cannot exist. We conjecture that the class of physically existing interactions is large enough to allow the conclusion that precise energy measurements for general many-particle systems require control algorithms with high complexity. ",Kein DOI-Link verfügbar,quant-ph/0308011v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Towards Scenario-based Safety Validation for Autonomous Trains with Deep   Generative Models,2023,"  Modern AI techniques open up ever-increasing possibilities for autonomous vehicles, but how to appropriately verify the reliability of such systems remains unclear. A common approach is to conduct safety validation based on a predefined Operational Design Domain (ODD) describing specific conditions under which a system under test is required to operate properly. However, collecting sufficient realistic test cases to ensure comprehensive ODD coverage is challenging. In this paper, we report our practical experiences regarding the utility of data simulation with deep generative models for scenario-based ODD validation. We consider the specific use case of a camera-based rail-scene segmentation system designed to support autonomous train operation. We demonstrate the capabilities of semantically editing railway scenes with deep generative models to make a limited amount of test data more representative. We also show how our approach helps to analyze the degree to which a system complies with typical ODD requirements. Specifically, we focus on evaluating proper operation under different lighting and weather conditions as well as while transitioning between them. ",https://doi.org/10.1007/978-3-031-40923-3_20,2310.10635v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Explaining Deep Neural Networks for Bearing Fault Detection with   Vibration Concepts,2023,"  Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks. However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types. In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals. Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery. Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first. ",https://doi.org/10.1109/INDIN51400.2023.10218170,2310.11450v1,Yes,potent(1)
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault   Detection with Deep Learning,2023,"  Deep Learning has already been successfully applied to analyze industrial sensor data in a variety of relevant use cases. However, the opaque nature of many well-performing methods poses a major obstacle for real-world deployment. Explainable AI (XAI) and especially feature attribution techniques promise to enable insights about how such models form their decision. But the plain application of such methods often fails to provide truly informative and problem-specific insights to domain experts. In this work, we focus on the specific task of detecting faults in rolling element bearings from vibration signals. We propose a novel and domain-specific feature attribution framework that allows us to evaluate how well the underlying logic of a model corresponds with expert reasoning. Utilizing the framework we are able to validate the trustworthiness and to successfully anticipate the generalization ability of different well-performing deep learning models. Our methodology demonstrates how signal processing tools can effectively be used to enhance Explainable AI techniques and acts as a template for similar problems. ",https://doi.org/10.1109/ICASSP49357.2023.10096396,2310.12967v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,The Optimal Single Copy Measurement for the Hidden Subgroup Problem,2007,"  The optimization of measurements for the state distinction problem has recently been applied to the theory of quantum algorithms with considerable successes, including efficient new quantum algorithms for the non-abelian hidden subgroup problem. Previous work has identified the optimal single copy measurement for the hidden subgroup problem over abelian groups as well as for the non-abelian problem in the setting where the subgroups are restricted to be all conjugate to each other. Here we describe the optimal single copy measurement for the hidden subgroup problem when all of the subgroups of the group are given with equal a priori probability. The optimal measurement is seen to be a hybrid of the two previously discovered single copy optimal measurements for the hidden subgroup problem. ",https://doi.org/10.1103/PhysRevA.77.032335,0706.4478v2,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Hidden Symmetry Subgroup Problems,2011,"  We advocate a new approach of addressing hidden structure problems and finding efficient quantum algorithms. We introduce and investigate the Hidden Symmetry Subgroup Problem (HSSP), which is a generalization of the well-studied Hidden Subgroup Problem (HSP). Given a group acting on a set and an oracle whose level sets define a partition of the set, the task is to recover the subgroup of symmetries of this partition inside the group. The HSSP provides a unifying framework that, besides the HSP, encompasses a wide range of algebraic oracle problems, including quadratic hidden polynomial problems. While the HSSP can have provably exponential quantum query complexity, we obtain efficient quantum algorithms for various interesting cases. To achieve this, we present a general method for reducing the HSSP to the HSP, which works efficiently in several cases related to symmetries of polynomials. The HSSP therefore connects in a rather surprising way certain hidden polynomial problems with the HSP. Using this connection, we obtain the first efficient quantum algorithm for the hidden polynomial problem for multivariate quadratic polynomials over fields of constant characteristic. We also apply the new methods to polynomial function graph problems and present an efficient quantum procedure for constant degree multivariate polynomials over any field. This result improves in several ways the currently known algorithms. ",https://doi.org/10.1137/120864416,1107.2189v2,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Provably Better Explanations with Optimized Aggregation of Feature   Attributions,2024,"  Using feature attributions for post-hoc explanations is a common practice to understand and verify the predictions of opaque machine learning models. Despite the numerous techniques available, individual methods often produce inconsistent and unstable results, putting their overall reliability into question. In this work, we aim to systematically improve the quality of feature attributions by combining multiple explanations across distinct methods or their variations. For this purpose, we propose a novel approach to derive optimal convex combinations of feature attributions that yield provable improvements of desired quality criteria such as robustness or faithfulness to the model behavior. Through extensive experiments involving various model architectures and popular feature attribution techniques, we demonstrate that our combination strategy consistently outperforms individual methods and existing baselines. ",Kein DOI-Link verfügbar,2406.05090v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Explanatory Model Monitoring to Understand the Effects of Feature Shifts   on Performance,2024,"  Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures. ",https://doi.org/10.1145/3637528.3671959,2408.13648v1,Yes,potent(1)
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Minimally-disturbing Heisenberg-Weyl symmetric measurements using   hard-core collisions of Schrödinger particles,2005,"  In a previous paper we have presented a general scheme for the implementation of symmetric generalized measurements (POVMs) on a quantum computer. This scheme is based on representation theory of groups and methods to decompose matrices that intertwine two representations. We extend this scheme in such a way that the measurement is minimally disturbing, i.e., it changes the state vector \ket{\Psi} of the system to \sqrt{\Pi} \ket{\Psi} where \Pi is the positive operator corresponding to the measured result.   Using this method, we construct quantum circuits for measurements with Heisenberg-Weyl symmetry. A continuous generalization leads to a scheme for optimal simultaneous measurements of position and momentum of a Schr""odinger particle moving in one dimension such that the outcomes satisfy \Delta x \Delta p \geq \hbar.   The particle to be measured collides with two probe particles, one for the position and the other for the momentum measurement. The position and momentum resolution can be tuned by the entangled joint state of the probe particles which is also generated by a collision with hard-core potential. The parameters of the POVM can then be controlled by the initial widths of the wave functions of the probe particles. We point out some formal similarities and differences to simultaneous measurements of quadrature amplitudes in quantum optics. ",https://doi.org/10.1063/1.2222080,quant-ph/0507097v2,Yes,potent(1)
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Efficient Quantum Algorithm for Hidden Quadratic and Cubic Polynomial   Function Graphs,2007,"  We introduce the Hidden Polynomial Function Graph Problem as a natural generalization of an abelian Hidden Subgroup Problem (HSP) where the subgroups and their cosets correspond to graphs of linear functions over the finite field F_p. For the Hidden Polynomial Function Graph Problem the functions are not restricted to be linear but can also be multivariate polynomial functions of higher degree.   For a fixed number of indeterminates and bounded total degree the Hidden Polynomial Function Graph Problem is hard on a classical computer as its black box query complexity is polynomial in p. In contrast, this problem can be reduced to a quantum state identification problem so that the resulting quantum query complexity does not depend on p. For univariate polynomials we construct a von Neumann measurement for distinguishing the states. We relate the success probability and the implementation of this measurement to certain classical problems involving polynomial equations. We present an efficient algorithm for hidden quadratic and cubic function graphs by establishing that the success probability of the measurement is lower bounded by a constant and that it can be implemented efficiently. ",Kein DOI-Link verfügbar,quant-ph/0703195v3,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,How much is a quantum controller controlled by the controlled system?,2007,"  We consider unitary transformations on a bipartite system A x B. To what extent entails the ability to transmit information from A to B the ability to transfer information in the converse direction? We prove a dimension-dependent lower bound on the classical channel capacity C(A<--B) in terms of the capacity C(A-->B) for the case that the bipartite unitary operation consists of controlled local unitaries on B conditioned on basis states on A. This can be interpreted as a statement on the strength of the inevitable backaction of a quantum system on its controller.   If the local operations are given by the regular representation of a finite group G we have C(A-->B)=log |G| and C(A<--B)=log N where N is the sum over the degrees of all inequivalent representations. Hence the information deficit C(A-->B)-C(A<--B) between the forward and the backward capacity depends on the ""non-abelianness"" of the control group. For regular representations, the ratio between backward and forward capacities cannot be smaller than 1/2. The symmetric group S_n reaches this bound asymptotically. However, for the general case (without group structure) all bounds must depend on the dimensions since it is known that the ratio can tend to zero. ",Kein DOI-Link verfügbar,0708.1505v2,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Infrared properties of micromachined vanadium oxide thin films,2017,"  This paper discusses questions of synthesizing and pressing vanadium oxides to create film-forming materials that can be used in producing optical coatings. Based on the film-forming materials thus created, technological processes have been developed for fabricating coatings from vanadium dioxide by two methods of producing thin films: vacuum evaporation and magnetron sputtering. Questions of using films made from vanadium oxide in optical instrumentation are considered. ",Kein DOI-Link verfügbar,1703.00127v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Implementation of group-covariant POVMs by orthogonal measurements,2004,"  We consider group-covariant positive operator valued measures (POVMs) on a finite dimensional quantum system. Following Neumark's theorem a POVM can be implemented by an orthogonal measurement on a larger system. Accordingly, our goal is to find an implementation of a given group-covariant POVM by a quantum circuit using its symmetry. Based on representation theory of the symmetry group we develop a general approach for the implementation of group-covariant POVMs which consist of rank-one operators. The construction relies on a method to decompose matrices that intertwine two representations of a finite group. We give several examples for which the resulting quantum circuits are efficient. In particular, we obtain efficient quantum circuits for a class of POVMs generated by Weyl-Heisenberg groups. These circuits allow to implement an approximative simultaneous measurement of the position and crystal momentum of a particle moving on a cyclic chain. ",https://doi.org/10.1063/1.1827924,quant-ph/0407054v1,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Efficient Quantum Algorithm for Identifying Hidden Polynomials,2007,"  We consider a natural generalization of an abelian Hidden Subgroup Problem where the subgroups and their cosets correspond to graphs of linear functions over a finite field F with d elements. The hidden functions of the generalized problem are not restricted to be linear but can also be m-variate polynomial functions of total degree n>=2.   The problem of identifying hidden m-variate polynomials of degree less or equal to n for fixed n and m is hard on a classical computer since Omega(sqrt{d}) black-box queries are required to guarantee a constant success probability. In contrast, we present a quantum algorithm that correctly identifies such hidden polynomials for all but a finite number of values of d with constant probability and that has a running time that is only polylogarithmic in d. ",Kein DOI-Link verfügbar,0706.1219v3,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,Polynomial time quantum algorithms for certain bivariate hidden   polynomial problems,2013,"  We present a new method for solving the hidden polynomial graph problem (HPGP) which is a special case of the hidden polynomial problem (HPP). The new approach yields an efficient quantum algorithm for the bivariate HPGP even when the input consists of several level set superpositions, a more difficult version of the problem than the one where the input is given by an oracle. For constant degree, the algorithm is polylogarithmic in the size of the base field. We also apply the results to give an efficient quantum algorithm for the oracle version of the HPP for an interesting family of bivariate hidden functions. This family includes diagonal quadratic forms and elliptic curves. ",https://doi.org/10.26421/QIC14.9-10-6,1305.1543v2,No,
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,The Thousand Faces of Explainable AI Along the Machine Learning Life   Cycle: Industrial Reality and Current State of Research,2023,"  In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the model evaluation phase and data scientists. Their versatile capabilities for other stages are currently either not sufficiently explored or not popular among practitioners. In line with existing work, our findings also confirm that more efforts are needed to enable also non-expert users' interpretation and understanding of opaque AI models with existing methods and frameworks. ",https://doi.org/10.1007/978-3-031-35891-3_13,2310.07882v1,Yes,versatile(1)
0000-0001-6897-4053,Thomas Decker,Weihenstephan-Triesdorf Universität der Angewandte Wissenschaften,An efficient quantum algorithm for finding hidden parabolic subgroups in   the general linear group,2014,"  In the theory of algebraic groups, parabolic subgroups form a crucial building block in the structural studies. In the case of general linear groups over a finite field $F_q$, given a sequence of positive integers $n_1, ..., n_k$, where $n=n_1+...+n_k$, a parabolic subgroup of parameter $(n_1, ..., n_k)$ in $GL_n(F_q)$ is a conjugate of the subgroup consisting of block lower triangular matrices where the $i$th block is of size $n_i$. Our main result is a quantum algorithm of time polynomial in $\log q$ and $n$ for solving the hidden subgroup problem in $GL_n(F_q)$, when the hidden subgroup is promised to be a parabolic subgroup. Our algorithm works with no prior knowledge of the parameter of the hidden parabolic subgroup. Prior to this work, such an efficient quantum algorithm was only known for the case $n=2$ (A. Denney, C. Moore, and A. Russell (2010), Quantum Inf. Comput., Vol. 10, pp. 282-291), and for minimal parabolic subgroups (Borel subgroups), for the case when $q$ is not much smaller than $n$ (G. Ivanyos: Quantum Inf. Comput., Vol. 12, pp. 661-669). ",Kein DOI-Link verfügbar,1406.6511v2,No,
0000-0003-3567-1173,Jürgen Dunkel,Hochschule Hannover,On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using   Streaming Data,2024,"  In parcel delivery, the ""last mile"" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens (""the crowd"") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach. ",https://doi.org/10.3233/FAIA200075,2401.12108v1,No,
0000-0003-3567-1173,Jürgen Dunkel,Hochschule Hannover,Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported   Coordination of Mobile Crowdsourcing,2024,"  Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing. ",https://doi.org/10.3390/s23020614,2401.12866v1,No,
0000-0003-3567-1173,Jürgen Dunkel,Hochschule Hannover,Stream-based perception for cognitive agents in mobile ecosystems,2024,"  Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception. ",https://doi.org/10.3233/AIC-190614,2401.13604v1,No,
0000-0002-3678-5431,Yongjian Liu,Hochschule Hannover,Supervised Online Hashing via Hadamard Codebook Learning,2019,"  In recent years, binary code learning, a.k.a hashing, has received extensive attention in large-scale multimedia retrieval. It aims to encode high-dimensional data points to binary codes, hence the original high-dimensional metric space can be efficiently approximated via Hamming space. However, most existing hashing methods adopted offline batch learning, which is not suitable to handle incremental datasets with streaming data or new instances. In contrast, the robustness of the existing online hashing remains as an open problem, while the embedding of supervised/semantic information hardly boosts the performance of the online hashing, mainly due to the defect of unknown category numbers in supervised learning. In this paper, we proposed an online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH), which aims to solve the above problems towards robust and supervised online hashing. In particular, we first assign an appropriate high-dimensional binary codes to each class label, which is generated randomly by Hadamard codes to each class label, which is generated randomly by Hadamard codes. Subsequently, LSH is adopted to reduce the length of such Hadamard codes in accordance with the hash bits, which can adapt the predefined binary codes online, and theoretically guarantee the semantic similarity. Finally, we consider the setting of stochastic data acquisition, which facilitates our method to efficiently learn the corresponding hashing functions via stochastic gradient descend (SGD) online. Notably, the proposed HCOH can be embedded with supervised labels and it not limited to a predefined category number. Extensive experiments on three widely-used benchmarks demonstrate the merits of the proposed scheme over the state-of-the-art methods. The code is available at https://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH. ",Kein DOI-Link verfügbar,1905.03694v2,No,
0000-0001-5842-4218,Ralf Bruns,Hochschule Hannover,On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using   Streaming Data,2024,"  In parcel delivery, the ""last mile"" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens (""the crowd"") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach. ",https://doi.org/10.3233/FAIA200075,2401.12108v1,No,
0000-0001-5842-4218,Ralf Bruns,Hochschule Hannover,Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported   Coordination of Mobile Crowdsourcing,2024,"  Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing. ",https://doi.org/10.3390/s23020614,2401.12866v1,No,
0000-0001-5842-4218,Ralf Bruns,Hochschule Hannover,Stream-based perception for cognitive agents in mobile ecosystems,2024,"  Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auctions, which the agents use to reach agreements. Experiments with real smartphone data demonstrate the benefits of stream-based agent perception. ",https://doi.org/10.3233/AIC-190614,2401.13604v1,No,
0009-0000-9772-6868,Hanno Homann,Hochschule Hannover,Automatic Reverse Engineering: Creating computer-aided design (CAD)   models from multi-view images,2023,"  Generation of computer-aided design (CAD) models from multi-view images may be useful in many practical applications. To date, this problem is usually solved with an intermediate point-cloud reconstruction and involves manual work to create the final CAD models. In this contribution, we present a novel network for an automated reverse engineering task. Our network architecture combines three distinct stages: A convolutional neural network as the encoder stage, a multi-view pooling stage and a transformer-based CAD sequence generator.   The model is trained and evaluated on a large number of simulated input images and extensive optimization of model architectures and hyper-parameters is performed. A proof-of-concept is demonstrated by successfully reconstructing a number of valid CAD models from simulated test image data. Various accuracy metrics are calculated and compared to a state-of-the-art point-based network.   Finally, a real world test is conducted supplying the network with actual photographs of two three-dimensional test objects. It is shown that some of the capabilities of our network can be transferred to this domain, even though the training exclusively incorporates purely synthetic training data. However to date, the feasible model complexity is still limited to basic shapes. ",Kein DOI-Link verfügbar,2309.13281v1,No,
0000-0002-6182-1940,Torben Woltjen,Hochschule Bremen,ANALYSE -- Learning to Attack Cyber-Physical Energy Systems With   Intelligent Agents,2023,"  The ongoing penetration of energy systems with information and communications technology (ICT) and the introduction of new markets increase the potential for malicious or profit-driven attacks that endanger system stability. To ensure security-of-supply, it is necessary to analyze such attacks and their underlying vulnerabilities, to develop countermeasures and improve system design. We propose ANALYSE, a machine-learning-based software suite to let learning agents autonomously find attacks in cyber-physical energy systems, consisting of the power system, ICT, and energy markets. ANALYSE is a modular, configurable, and self-documenting framework designed to find yet unknown attack types and to reproduce many known attack strategies in cyber-physical energy systems from the scientific literature. ",https://doi.org/10.1016/j.softx.2023.101484,2305.09476v1,Yes,potent(1)
0000-0002-6182-1940,Torben Woltjen,Hochschule Bremen,"Analyzing Power Grid, ICT, and Market Without Domain Knowledge Using   Distributed Artificial Intelligence",2020,"  Modern cyber-physical systems (CPS), such as our energy infrastructure, are becoming increasingly complex: An ever-higher share of Artificial Intelligence (AI)-based technologies use the Information and Communication Technology (ICT) facet of energy systems for operation optimization, cost efficiency, and to reach CO2 goals worldwide. At the same time, markets with increased flexibility and ever shorter trade horizons enable the multi-stakeholder situation that is emerging in this setting. These systems still form critical infrastructures that need to perform with highest reliability. However, today's CPS are becoming too complex to be analyzed in the traditional monolithic approach, where each domain, e.g., power grid and ICT as well as the energy market, are considered as separate entities while ignoring dependencies and side-effects. To achieve an overall analysis, we introduce the concept for an application of distributed artificial intelligence as a self-adaptive analysis tool that is able to analyze the dependencies between domains in CPS by attacking them. It eschews pre-configured domain knowledge, instead exploring the CPS domains for emergent risk situations and exploitable loopholes in codices, with a focus on rational market actors that exploit the system while still following the market rules. ",Kein DOI-Link verfügbar,2006.06074v1,No,
0000-0002-8489-3197,Philip Maloney,Hochschule Bremen,"Multiple CO Transitions, CI, and HCN from the Cloverleaf Quasar",1997,"  New millimeter-wavelength emission lines are reported for the Cloverleaf (H1413+117), a gravitationally lensed quasar at redshift 2.56. High signal-to-noise ratio spectra of four lines in CO (J=3-2, J=4-3, J=5-4, and J=7-6) have been obtained and are modeled using an escape probability formalism. The line brightness temperatures are flat or rising between CO(3-2) and CO(4-3) and then drop to the CO(5-4) and CO(7-6) lines; this falloff suggests that the optical depths in the CO lines are modest (tau_{4-3} < 3), and that the gas is relatively warm (T ~ 100 K) and dense (n_H2 > 3 X 10^3 cm^{-3}). The neutral carbon fine structure line CI(3P1-3P0) was detected with moderate SNR on two separate occasions, and appears secure. The HCN(4-3) line is somewhat more tentative, as it was observed only once and detected with moderate SNR. However, its brightness ratio of 0.25 relative to the CO(4-3) line is very similar to the ratio seen in luminous infrared galaxies. The detectability of the Cloverleaf in molecular and neutral atomic transitions is largely due to high emissivity of the gas and magnification from gravitational lensing, rather than an extremely large mass of gas. ",https://doi.org/10.1086/304382,astro-ph/9702118v1,No,
0000-0002-8489-3197,Philip Maloney,Hochschule Bremen,"Cygnus A Obscuring Torus: Ionized, Atomic or Molecular?",2023,"  The prototypical powerful FR \Romannum{2} radio galaxy Cygnus A fits extremely well into the quasar/radio galaxy unified model: high polarization with an angle almost perpendicular to the radio jet and polarized flux showing broad permitted lines. It has been claimed that ionized gas in the torus reveals a very clear torus shape via Bremmstrahlung emission. We rule out the later with an energetic argument, and we constrain the molecular and atomic gas properties with existing observations. The atomic absorption against the core has been shown to match the X-ray column only if the spin temperature is an implausible $T_{\rm s} = 1\times 10^6$ K. This points to a molecular medium for the X-ray column $\log(N_{\rm H} ~[\rm{cm^{-2}}]) \sim 23.5$. Yet not low-J CO absorption is detected to sensitive limits. The non-detection is surprising given that this powerful radio galaxy hosts a luminous, dust-obscured active nucleus and copious warm molecular hydrogen. These conditions suggest a detectable level of emission. Furthermore, the torus X-ray column density suggests detectable absorption. We explore various possibilities to explain the lack of a signature from warm CO (200-250K). Specifically, that the radiative excitation by the radio core renders low-J CO absorption below current sensitivities, and that high-J levels are well populated and conducive to producing absorption. We test this hypothesis using archival \textit{Hershel}/SPIRE FTS observations of Cygnus A of high-J CO lines ($14 \geq J \geq 4$ transitions). Still high-J CO lines are not detected. We suggest that ALMA observations near its high frequency limit can be critical to obtain the signature of molecular line of the torus of Cygnus A. ",Kein DOI-Link verfügbar,2310.19401v1,No,
0000-0002-8489-3197,Philip Maloney,Hochschule Bremen,Morphology and Kinematics of Warm Molecular Gas in the Nuclear Region of   Arp 220 as Revealed by ALMA,2015,"  We present Atacama Large Millimeter Array (ALMA) Cycle-0 observations of the CO J = 6-5 line in the advanced galaxy merger Arp 220. This line traces warm molecular gas, which dominates the total CO luminosity. The CO emission from the two nuclei is well resolved by the 0.39"" x 0.22"" beam and the exceptional sensitivity and spatial/spectral resolution reveal new complex features in the morphology and kinematics of the warm gas. The line profiles are asymmetric between the red and blue sides of the nuclear disks and the peak of the line emission is offset from the peak of the continuum emission in both nuclei by about 100 pc in the same direction. CO self-absorption is detected at the centers of both nuclei but it is much deeper in the eastern nucleus. We also clearly detect strong, highly redshifted CO absorption located near the southwest side of each nucleus. For the eastern nucleus, we reproduce the major line profile features with a simple kinematic model of a highly turbulent, rotating disk with a substantial line center optical depth and a large gradient in the excitation temperature. The red/blue asymmetries and line-to-continuum offset are likely produced by absorption of the blue (SW) sides of the two nuclei by blue-shifted, foreground molecular gas; the mass of the absorber is comparable to the nuclear warm gas mass (10^8 M_solar). We measure an unusually high L_CO/L_FIR ratio in the eastern nucleus, suggesting there is an additional energy source, such as mechanical energy from shocks, present in this nucleus. ",https://doi.org/10.1088/0004-637X/806/1/17,1504.01773v1,No,
0000-0003-4572-2819,Rainer Hartmann,Hochschule Bremen Fakultät 3 Gesellschaftswissenschaften,A Sparse Grid Discretization with Variable Coefficient in High   Dimensions,2016,"  We present a Ritz-Galerkin discretization on sparse grids using pre-wavelets, which allows to solve elliptic differential equations with variable coefficients for dimension $d=2,3$ and higher dimensions $d>3$. The method applies multilinear finite elements. We introduce an efficient algorithm for matrix vector multiplication using a Ritz-Galerkin discretization and semi-orthogonality.   This algorithm is based on standard 1-dimensional restrictions and prolongations, a simple pre-wavelet stencil, and the classical operator dependent stencil for multilinear finite elements. Numerical simulation results are presented for a 3-dimensional problem on a curvilinear bounded domain and for a 6-dimensional problem with variable coefficients.   Simulation results show a convergence of the discretization according to the approximation properties of the finite element space. The condition number of the stiffness matrix can be bounded below $10$ using a standard diagonal preconditioner. ",Kein DOI-Link verfügbar,1603.02906v1,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,Robust Analog Function Computation via Wireless Multiple-Access Channels,2012,"  Various wireless sensor network applications involve the computation of a pre-defined function of the measurements without the need for reconstructing each individual sensor reading. Widely-considered examples of such functions include the arithmetic mean and the maximum value. Standard approaches to the computation problem separate computation from communication: quantized sensor readings are transmitted interference-free to a fusion center that reconstructs each sensor reading and subsequently computes the sought function value. Such separation-based computation schemes are generally highly inefficient as a complete reconstruction of individual sensor readings is not necessary for the fusion center to compute a function of them. In particular, if the mathematical structure of the wireless channel is suitably matched (in some sense) to the function, then channel collisions induced by concurrent transmissions of different nodes can be beneficially exploited for computation purposes. Therefore, in this paper a practically relevant analog computation scheme is proposed that allows for an efficient estimate of linear and nonlinear functions over the wireless multiple-access channel. After analyzing the asymptotic properties of the estimation error, numerical simulations are presented to show the potential for huge performance gains when compared with time-division multiple-access based computation schemes. ",Kein DOI-Link verfügbar,1210.2967v1,Yes,potent(1)
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,Energy-Efficient Classification for Anomaly Detection: The Wireless   Channel as a Helper,2015,"  Anomaly detection has various applications including condition monitoring and fault diagnosis. The objective is to sense the environment, learn the normal system state, and then periodically classify whether the instantaneous state deviates from the normal one or not. A flexible and cost-effective way of monitoring a system state is to use a wireless sensor network. In the traditional approach, the sensors encode their observations and transmit them to a fusion center by means of some interference avoiding channel access method. The fusion center then decodes all the data and classifies the corresponding system state. As this approach can be highly inefficient in terms of energy consumption, in this paper we propose a transmission scheme that exploits interference for carrying out the anomaly detection directly in the air. In other words, the wireless channel helps the fusion center to retrieve the sought classification outcome immediately from the channel output. To achieve this, the chosen learning model is linear support vector machines. After discussing the proposed scheme and proving its reliability, we present numerical examples demonstrating that the scheme reduces the energy consumption for anomaly detection by up to 53% compared to a strategy that uses time division multiple-access. ",Kein DOI-Link verfügbar,1512.04857v1,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,On Secure Computation Over the Binary Modulo-2 Adder Multiple-Access   Wiretap Channel,2016,"  In this paper, the problem of securely computing a function over the binary modulo-2 adder multiple-access wiretap channel is considered. The problem involves a legitimate receiver that wishes to reliably and efficiently compute a function of distributed binary sources while an eavesdropper has to be kept ignorant of them. In order to characterize the corresponding fundamental limit, the notion of secrecy computation-capacity is introduced. Although determining the secrecy computation-capacity is challenging for arbitrary functions, it surprisingly turns out that if the function perfectly matches the algebraic structure of the channel and the joint source distribution fulfills certain conditions, the secrecy computation-capacity equals the computation capacity, which is the supremum of all achievable computation rates without secrecy constraints. Unlike the case of securely transmitting messages, no additional randomness is needed at the encoders nor does the legitimate receiver need any advantage over the eavesdropper. The results therefore show that the problem of securely computing a function over a multiple-access wiretap channel may significantly differ from the one of securely communicating messages. ",https://doi.org/10.1109/ITW.2016.7606788,1603.07227v2,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,Upper and Lower Bounds on the Capacity of Amplitude-Constrained MIMO   Channels,2017,"  In this work, novel upper and lower bounds for the capacity of channels with arbitrary constraints on the support of the channel input symbols are derived. As an immediate practical application, the case of multiple-input multiple-output channels with amplitude constraints is considered. The bounds are shown to be within a constant gap if the channel matrix is invertible and are tight in the high amplitude regime for arbitrary channel matrices. Moreover, in the high amplitude regime, it is shown that the capacity scales linearly with the minimum between the number of transmit and receive antennas, similarly to the case of average power-constrained inputs. ",Kein DOI-Link verfügbar,1708.09517v1,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,A Dimensionality Reduction Method for Finding Least Favorable Priors   with a Focus on Bregman Divergence,2022,"  A common way of characterizing minimax estimators in point estimation is by moving the problem into the Bayesian estimation domain and finding a least favorable prior distribution. The Bayesian estimator induced by a least favorable prior, under mild conditions, is then known to be minimax. However, finding least favorable distributions can be challenging due to inherent optimization over the space of probability distributions, which is infinite-dimensional. This paper develops a dimensionality reduction method that allows us to move the optimization to a finite-dimensional setting with an explicit bound on the dimension. The benefit of this dimensionality reduction is that it permits the use of popular algorithms such as projected gradient ascent to find least favorable priors. Throughout the paper, in order to make progress on the problem, we restrict ourselves to Bayesian risks induced by a relatively large class of loss functions, namely Bregman divergences. ",Kein DOI-Link verfügbar,2202.11598v1,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,Nomographic Functions: Efficient Computation in Clustered Gaussian   Sensor Networks,2013,"  In this paper, a clustered wireless sensor network is considered that is modeled as a set of coupled Gaussian multiple-access channels. The objective of the network is not to reconstruct individual sensor readings at designated fusion centers but rather to reliably compute some functions thereof. Our particular attention is on real-valued functions that can be represented as a post-processed sum of pre-processed sensor readings. Such functions are called nomographic functions and their special structure permits the utilization of the interference property of the Gaussian multiple-access channel to reliably compute many linear and nonlinear functions at significantly higher rates than those achievable with standard schemes that combat interference. Motivated by this observation, a computation scheme is proposed that combines a suitable data pre- and post-processing strategy with a nested lattice code designed to protect the sum of pre-processed sensor readings against the channel noise. After analyzing its computation rate performance, it is shown that at the cost of a reduced rate, the scheme can be extended to compute every continuous function of the sensor readings in a finite succession of steps, where in each step a different nomographic function is computed. This demonstrates the fundamental role of nomographic representations. ",https://doi.org/10.1109/TWC.2014.2380317,1310.7123v2,No,
0000-0001-6485-330X,Mario Goldenbaum,Bremen Universität der Angewandte Wissenschaften,Exploiting Interference for Efficient Distributed Computation in   Cluster-based Wireless Sensor Networks,2013,"  This invited paper presents some novel ideas on how to enhance the performance of consensus algorithms in distributed wireless sensor networks, when communication costs are considered. Of particular interest are consensus algorithms that exploit the broadcast property of the wireless channel to boost the performance in terms of convergence speeds. To this end, we propose a novel clustering based consensus algorithm that exploits interference for computation, while reducing the energy consumption in the network. The resulting optimization problem is a semidefinite program, which can be solved offline prior to system startup. ",https://doi.org/10.1109/GlobalSIP.2013.6737045,1309.3139v3,No,
0000-0003-3597-6284,Sebastian Rieger,Hochschule Fulda,A Study of Deep Learning for Network Traffic Data Forecasting,2019,"  We present a study of deep learning applied to the domain of network traffic data forecasting. This is a very important ingredient for network traffic engineering, e.g., intelligent routing, which can optimize network performance, especially in large networks. In a nutshell, we wish to predict, in advance, the bit rate for a transmission, based on low-dimensional connection metadata (""flows"") that is available whenever a communication is initiated. Our study has several genuinely new points: First, it is performed on a large dataset (~50 million flows), which requires a new training scheme that operates on successive blocks of data since the whole dataset is too large for in-memory processing. Additionally, we are the first to propose and perform a more fine-grained prediction that distinguishes between low, medium and high bit rates instead of just ""mice"" and ""elephant"" flows. Lastly, we apply state-of-the-art visualization and clustering techniques to flow data and show that visualizations are insightful despite the heterogeneous and non-metric nature of the data. We developed a processing pipeline to handle the highly non-trivial acquisition process and allow for proper data preprocessing to be able to apply DNNs to network traffic data. We conduct DNN hyper-parameter optimization as well as feature selection experiments, which clearly show that fine-grained network traffic forecasting is feasible, and that domain-dependent data enrichment and augmentation strategies can improve results. An outlook about the fundamental challenges presented by network traffic analysis (high data throughput, unbalanced and dynamic classes, changing statistics, outlier detection) concludes the article. ",https://doi.org/10.1007/978-3-030-30490-4_40,1909.04501v2,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",Continual Learning with Deep Learning Methods in an Application-Oriented   Context,2022,"  Abstract knowledge is deeply grounded in many computer-based applications. An important research area of Artificial Intelligence (AI) deals with the automatic derivation of knowledge from data. Machine learning offers the according algorithms. One area of research focuses on the development of biologically inspired learning algorithms. The respective machine learning methods are based on neurological concepts so that they can systematically derive knowledge from data and store it. One type of machine learning algorithms that can be categorized as ""deep learning"" model is referred to as Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons arranged in layers that are trained by using the backpropagation algorithm. These deep learning methods exhibit amazing capabilities for inferring and storing complex knowledge from high-dimensional data. However, DNNs are affected by a problem that prevents new knowledge from being added to an existing base. The ability to continuously accumulate knowledge is an important factor that contributed to evolution and is therefore a prerequisite for the development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect causes DNNs to immediately loose already derived knowledge after a few training iterations on a new data distribution. Only an energetically expensive retraining with the joint data distribution of past and new data enables the abstraction of the entire new set of knowledge. In order to counteract the effect, various techniques have been and are still being developed with the goal to mitigate or even solve the CF problem. These published CF avoidance studies usually imply the effectiveness of their approaches for various continual learning tasks. This dissertation is set in the context of continual machine learning with deep learning methods. The first part deals with the development of an ... ",Kein DOI-Link verfügbar,2207.06233v1,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",Continual Learning with Fully Probabilistic Models,2021,"  We present an approach for continual learning (CL) that is based on fully probabilistic (or generative) models of machine learning. In contrast to, e.g., GANs that are ""generative"" in the sense that they can generate samples, fully probabilistic models aim at modeling the data distribution directly. Consequently, they provide functionalities that are highly relevant for continual learning, such as density estimation (outlier detection) and sample generation. As a concrete realization of generative continual learning, we propose Gaussian Mixture Replay (GMR). GMR is a pseudo-rehearsal approach using a Gaussian Mixture Model (GMM) instance for both generator and classifier functionalities. Relying on the MNIST, FashionMNIST and Devanagari benchmarks, we first demonstrate unsupervised task boundary detection by GMM density estimation, which we also use to reject untypical generated samples. In addition, we show that GMR is capable of class-conditional sampling in the way of a cGAN. Lastly, we verify that GMR, despite its simple structure, achieves state-of-the-art performance on common class-incremental learning problems at very competitive time and memory complexity. ",Kein DOI-Link verfügbar,2104.09240v1,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",Gradient-based training of Gaussian Mixture Models for High-Dimensional   Streaming Data,2019,"  We present an approach for efficiently training Gaussian Mixture Model (GMM) by Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional streaming data. Our training scheme does not require data-driven parameter initialization (e.g., k-means) and can thus be trained based on a random initialization. Furthermore, the approach allows mini-batch sizes as low as 1, which are typical for streaming-data settings. Major problems in such settings are undesirable local optima during early training phases and numerical instabilities due to high data dimensionalities. We introduce an adaptive annealing procedure to address the first problem, whereas numerical instabilities are eliminated by using an exponential-free approximation to the standard GMM log-likelihood. Experiments on a variety of visual and non-visual benchmarks show that our SGD approach can be trained completely without, for instance, k-means based centroid initialization. It also compares favorably to an online variant of Expectation-Maximization (EM) - stochastic EM (sEM), which it outperforms by a large margin for very high-dimensional data. ",Kein DOI-Link verfügbar,1912.09379v3,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",A Rigorous Link Between Self-Organizing Maps and Gaussian Mixture Models,2020,"  This work presents a mathematical treatment of the relation between Self-Organizing Maps (SOMs) and Gaussian Mixture Models (GMMs). We show that energy-based SOM models can be interpreted as performing gradient descent, minimizing an approximation to the GMM log-likelihood that is particularly valid for high data dimensionalities. The SOM-like decrease of the neighborhood radius can be understood as an annealing procedure ensuring that gradient descent does not get stuck in undesirable local minima. This link allows to treat SOMs as generative probabilistic models, giving a formal justification for using SOMs, e.g., to detect outliers, or for sampling. ",Kein DOI-Link verfügbar,2009.11710v1,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",Overcoming Catastrophic Forgetting with Gaussian Mixture Replay,2021,"  We present Gaussian Mixture Replay (GMR), a rehearsal-based approach for continual learning (CL) based on Gaussian Mixture Models (GMM). CL approaches are intended to tackle the problem of catastrophic forgetting (CF), which occurs for Deep Neural Networks (DNNs) when sequentially training them on successive sub-tasks. GMR mitigates CF by generating samples from previous tasks and merging them with current training data. GMMs serve several purposes here: sample generation, density estimation (e.g., for detecting outliers or recognizing task boundaries) and providing a high-level feature representation for classification. GMR has several conceptual advantages over existing replay-based CL approaches. First of all, GMR achieves sample generation, classification and density estimation in a single network structure with strongly reduced memory requirements. Secondly, it can be trained at constant time complexity w.r.t. the number of sub-tasks, making it particularly suitable for life-long learning. Furthermore, GMR minimizes a differentiable loss function and seems to avoid mode collapse. In addition, task boundaries can be detected by applying GMM density estimation. Lastly, GMR does not require access to sub-tasks lying in the future for hyper-parameter tuning, allowing CL under real-world constraints. We evaluate GMR on multiple image datasets, which are divided into class-disjoint sub-tasks. ",Kein DOI-Link verfügbar,2104.09220v1,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",Image Modeling with Deep Convolutional Gaussian Mixture Models,2021,"  In this conceptual work, we present Deep Convolutional Gaussian Mixture Models (DCGMMs): a new formulation of deep hierarchical Gaussian Mixture Models (GMMs) that is particularly suitable for describing and generating images. Vanilla (i.e., flat) GMMs require a very large number of components to describe images well, leading to long training times and memory issues. DCGMMs avoid this by a stacked architecture of multiple GMM layers, linked by convolution and pooling operations. This allows to exploit the compositionality of images in a similar way as deep CNNs do. DCGMMs can be trained end-to-end by Stochastic Gradient Descent. This sets them apart from vanilla GMMs which are trained by Expectation-Maximization, requiring a prior k-means initialization which is infeasible in a layered structure. For generating sharp images with DCGMMs, we introduce a new gradient-based technique for sampling through non-invertible operations like convolution and pooling. Based on the MNIST and FashionMNIST datasets, we validate the DCGMMs model by demonstrating its superiority over flat GMMs for clustering, sampling and outlier detection. ",Kein DOI-Link verfügbar,2104.12686v1,No,
0000-0002-0108-1936,Benedikt Pfülb,"Fulda Universität der Angewandte Wissenschaften, Hochschule Fulda",A Study of Deep Learning for Network Traffic Data Forecasting,2019,"  We present a study of deep learning applied to the domain of network traffic data forecasting. This is a very important ingredient for network traffic engineering, e.g., intelligent routing, which can optimize network performance, especially in large networks. In a nutshell, we wish to predict, in advance, the bit rate for a transmission, based on low-dimensional connection metadata (""flows"") that is available whenever a communication is initiated. Our study has several genuinely new points: First, it is performed on a large dataset (~50 million flows), which requires a new training scheme that operates on successive blocks of data since the whole dataset is too large for in-memory processing. Additionally, we are the first to propose and perform a more fine-grained prediction that distinguishes between low, medium and high bit rates instead of just ""mice"" and ""elephant"" flows. Lastly, we apply state-of-the-art visualization and clustering techniques to flow data and show that visualizations are insightful despite the heterogeneous and non-metric nature of the data. We developed a processing pipeline to handle the highly non-trivial acquisition process and allow for proper data preprocessing to be able to apply DNNs to network traffic data. We conduct DNN hyper-parameter optimization as well as feature selection experiments, which clearly show that fine-grained network traffic forecasting is feasible, and that domain-dependent data enrichment and augmentation strategies can improve results. An outlook about the fundamental challenges presented by network traffic analysis (high data throughput, unbalanced and dynamic classes, changing statistics, outlier detection) concludes the article. ",https://doi.org/10.1007/978-3-030-30490-4_40,1909.04501v2,No,
0009-0001-7913-5869,Matthias Ochs,Fulda Universität der Angewandte Wissenschaften,Joint Epipolar Tracking (JET): Simultaneous optimization of epipolar   geometry and feature correspondences,2017,"  Traditionally, pose estimation is considered as a two step problem. First, feature correspondences are determined by direct comparison of image patches, or by associating feature descriptors. In a second step, the relative pose and the coordinates of corresponding points are estimated, most often by minimizing the reprojection error (RPE). RPE optimization is based on a loss function that is merely aware of the feature pixel positions but not of the underlying image intensities. In this paper, we propose a sparse direct method which introduces a loss function that allows to simultaneously optimize the unscaled relative pose, as well as the set of feature correspondences directly considering the image intensity values. Furthermore, we show how to integrate statistical prior information on the motion into the optimization process. This constructive inclusion of a Bayesian bias term is particularly efficient in application cases with a strongly predictable (short term) dynamic, e.g. in a driving scenario. In our experiments, we demonstrate that the JET algorithm we propose outperforms the classical reprojection error optimization on two synthetic datasets and on the KITTI dataset. The JET algorithm runs in real-time on a single CPU thread. ",Kein DOI-Link verfügbar,1703.05065v1,No,
0009-0001-7913-5869,Matthias Ochs,Fulda Universität der Angewandte Wissenschaften,DistanceNet: Estimating Traveled Distance from Monocular Images using a   Recurrent Convolutional Neural Network,2019,"  Classical monocular vSLAM/VO methods suffer from the scale ambiguity problem. Hybrid approaches solve this problem by adding deep learning methods, for example by using depth maps which are predicted by a CNN. We suggest that it is better to base scale estimation on estimating the traveled distance for a set of subsequent images. In this paper, we propose a novel end-to-end many-to-one traveled distance estimator. By using a deep recurrent convolutional neural network (RCNN), the traveled distance between the first and last image of a set of consecutive frames is estimated by our DistanceNet. Geometric features are learned in the CNN part of our model, which are subsequently used by the RNN to learn dynamics and temporal information. Moreover, we exploit the natural order of distances by using ordinal regression to predict the distance. The evaluation on the KITTI dataset shows that our approach outperforms current state-of-the-art deep learning pose estimators and classical mono vSLAM/VO methods in terms of distance prediction. Thus, our DistanceNet can be used as a component to solve the scale problem and help improve current and future classical mono vSLAM/VO methods. ",Kein DOI-Link verfügbar,1904.08105v1,No,
0009-0001-7913-5869,Matthias Ochs,Fulda Universität der Angewandte Wissenschaften,SDNet: Semantically Guided Depth Estimation Network,2019,"  Autonomous vehicles and robots require a full scene understanding of the environment to interact with it. Such a perception typically incorporates pixel-wise knowledge of the depths and semantic labels for each image from a video sensor. Recent learning-based methods estimate both types of information independently using two separate CNNs. In this paper, we propose a model that is able to predict both outputs simultaneously, which leads to improved results and even reduced computational costs compared to independent estimation of depth and semantics. We also empirically prove that the CNN is capable of learning more meaningful and semantically richer features. Furthermore, our SDNet estimates the depth based on ordinal classification. On the basis of these two enhancements, our proposed method achieves state-of-the-art results in semantic segmentation and depth estimation from single monocular input images on two challenging datasets. ",Kein DOI-Link verfügbar,1907.10659v1,No,
0009-0001-7913-5869,Matthias Ochs,Fulda Universität der Angewandte Wissenschaften,Learning Rank Reduced Interpolation with Principal Component Analysis,2017,"  In computer vision most iterative optimization algorithms, both sparse and dense, rely on a coarse and reliable dense initialization to bootstrap their optimization procedure. For example, dense optical flow algorithms profit massively in speed and robustness if they are initialized well in the basin of convergence of the used loss function. The same holds true for methods as sparse feature tracking when initial flow or depth information for new features at arbitrary positions is needed. This makes it extremely important to have techniques at hand that allow to obtain from only very few available measurements a dense but still approximative sketch of a desired 2D structure (e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded as sample from a 2D random process. The method presented here exploits the complete information given by the principal component analysis (PCA) of that process, the principal basis and its prior distribution. The method is able to determine a dense reconstruction from sparse measurement. When facing situations with only very sparse measurements, typically the number of principal components is further reduced which results in a loss of expressiveness of the basis. We overcome this problem and inject prior knowledge in a maximum a posterior (MAP) approach. We test our approach on the KITTI and the virtual KITTI datasets and focus on the interpolation of depth maps for driving scenes. The evaluation of the results show good agreement to the ground truth and are clearly better than results of interpolation by the nearest neighbor method which disregards statistical information. ",Kein DOI-Link verfügbar,1703.05061v1,No,
0009-0001-7913-5869,Matthias Ochs,Fulda Universität der Angewandte Wissenschaften,Learning and Sequencing of Object-Centric Manipulation Skills for   Industrial Tasks,2020,"  Enabling robots to quickly learn manipulation skills is an important, yet challenging problem. Such manipulation skills should be flexible, e.g., be able adapt to the current workspace configuration. Furthermore, to accomplish complex manipulation tasks, robots should be able to sequence several skills and adapt them to changing situations. In this work, we propose a rapid robot skill-sequencing algorithm, where the skills are encoded by object-centric hidden semi-Markov models. The learned skill models can encode multimodal (temporal and spatial) trajectory distributions. This approach significantly reduces manual modeling efforts, while ensuring a high degree of flexibility and re-usability of learned skills. Given a task goal and a set of generic skills, our framework computes smooth transitions between skill instances. To compute the corresponding optimal end-effector trajectory in task space we rely on Riemannian optimal controller. We demonstrate this approach on a 7 DoF robot arm for industrial assembly tasks. ",Kein DOI-Link verfügbar,2008.10471v1,No,
0000-0003-4002-6160,Christoph Hardegen,Fulda Universität der Angewandte Wissenschaften,A Study of Deep Learning for Network Traffic Data Forecasting,2019,"  We present a study of deep learning applied to the domain of network traffic data forecasting. This is a very important ingredient for network traffic engineering, e.g., intelligent routing, which can optimize network performance, especially in large networks. In a nutshell, we wish to predict, in advance, the bit rate for a transmission, based on low-dimensional connection metadata (""flows"") that is available whenever a communication is initiated. Our study has several genuinely new points: First, it is performed on a large dataset (~50 million flows), which requires a new training scheme that operates on successive blocks of data since the whole dataset is too large for in-memory processing. Additionally, we are the first to propose and perform a more fine-grained prediction that distinguishes between low, medium and high bit rates instead of just ""mice"" and ""elephant"" flows. Lastly, we apply state-of-the-art visualization and clustering techniques to flow data and show that visualizations are insightful despite the heterogeneous and non-metric nature of the data. We developed a processing pipeline to handle the highly non-trivial acquisition process and allow for proper data preprocessing to be able to apply DNNs to network traffic data. We conduct DNN hyper-parameter optimization as well as feature selection experiments, which clearly show that fine-grained network traffic forecasting is feasible, and that domain-dependent data enrichment and augmentation strategies can improve results. An outlook about the fundamental challenges presented by network traffic analysis (high data throughput, unbalanced and dynamic classes, changing statistics, outlier detection) concludes the article. ",https://doi.org/10.1007/978-3-030-30490-4_40,1909.04501v2,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften",Transformations and singularities of polarized curves,2018,"  We study the limiting behaviour of Darboux and Calapso transforms of polarized curves in the conformal n-dimensional sphere, when the polarization has a pole of first or second order at some point. We prove that for a pole of first order, as the singularity is approached all Darboux transforms converge to the original curve and all Calapso transforms converge. For a pole of second order, a generic Darboux transform converges to the original curve while a Calapso transform has a limit point or a limit circle, depending on the value of the transformation parameter. In particular, our results apply to Darboux and Calapso transforms of isothermic surfaces when a singular umbilic with index 1/2 or 1 is approached along a curvature line. ",Kein DOI-Link verfügbar,1807.00497v1,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften",Darboux and Calapso transforms of meromorphically isothermic surfaces,2019,"  We consider those simply connected isothermic surfaces for which their Hopf differential factorizes into a real function and a meromorphic quadratic differential that has a zero or pole at some point, but is nowhere zero and holomorphic otherwise. Upon restriction to a simply connected patch that does not contain the zero or pole, the Darboux and Calapso transformations yield new isothermic surfaces. We determine the limiting behaviour of these transformed patches as the zero or pole of the meromorphic quadratic differential is approached and investigate whether they are continuous around that point. ",Kein DOI-Link verfügbar,1901.05774v1,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften",Integrable structures and the quantization of free null initial data for   gravity,2017,"  Variables for constraint free null canonical vacuum general relativity are presented which have simple Poisson brackets that facilitate quantization. Free initial data for vacuum general relativity on a pair of intersecting null hypersurfaces has been known since the 1960s. These consist of the ""main"" data which are set on the bulk of the two null hypersurfaces, and additional ""surface"" data set only on their intersection 2-surface. More recently the complete set of Poisson brackets of such data has been obtained. However the complexity of these brackets is an obstacle to their quantization. Part of this difficulty may be overcome using methods from the treatment of cylindrically symmetric gravity. Specializing from general to cylindrically symmetric solutions changes the Poisson algebra of the null initial data surprisingly little, but cylindrically symmetric vacuum general relativity is an integrable system, making powerful tools available. Here a transformation is constructed at the cylindrically symmetric level which maps the main initial data to new data forming a Poisson algebra for which an exact deformation quantization is known. (Although an auxiliary condition on the data has been quantized only in the asymptotically flat case, and a suitable representation of the algebra of quantum data by operators on a Hilbert space has not yet been found.) The definition of the new main data generalizes naturally to arbitrary, symmetryless gravitational fields, with the Poisson brackets retaining their simplicity. The corresponding generalization of the quantization is however ambiguous and requires further analysis. ",https://doi.org/10.1088/1361-6382/aa7d2b,1704.06992v1,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften",Symmetry breaking in geometry,2022,"  A geometric mechanism that may, in analogy to similar notions in physics, be considered as ""symmetry breaking"" in geometry is described, and several instances of this mechanism in differential geometry are discussed: it is shown how spontaneous symmetry breaking may occur, and it is discussed how explicit symmetry breaking may be used to tackle certain geometric problems. A systematic study of symmetry breaking in geometry is proposed, and some preliminary thoughts on further research are formulated. ",Kein DOI-Link verfügbar,2206.13401v1,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften",Detecting Token Systems on Ethereum,2018,"  We propose and compare two approaches to identify smart contracts as token systems by analyzing their public bytecode. The first approach symbolically executes the code in order to detect token systems by their characteristic behavior of updating internal accounts. The second approach serves as a comparison base and exploits the common interface of ERC-20, the most popular token standard. We present quantitative results for the Ethereum blockchain, and validate the effectiveness of both approaches using a set of curated token systems as ground truth. We observe 100% recall for the second approach. Recall rates of 89% (with well explainable missed detections) indicate that the first approach may also be able to identify ""hidden"" or undocumented token systems that intentionally do not implement the standard. One possible application of the proposed methods is to facilitate regulator' tasks of monitoring and policing the use of token systems and their underlying platforms. ",Kein DOI-Link verfügbar,1811.11645v1,No,
0000-0003-1477-9556,Andreas Fuchs,"Darmstadt Universität der Angewandte Wissenschaften, Fulda Universität der Angewandte Wissenschaften","Final Architecture Specification of security, privacy, and incentive   mechanisms",2009,"  In this document, we define the NADA security architecture based on refined use case scenarios, a derived high level model and security analysis. For the architecure design and verification we are applying the well known STRIDE model. ",Kein DOI-Link verfügbar,0911.3343v1,No,
0000-0002-6477-1272,Nicole Schwarz,Hochschule für Technik und Wirtschaft des Saarlandes,Neutron-star merger ejecta as obstacles to neutrino-powered jets of   gamma-ray bursts,2015,"  We present the first special relativistic, axisymmetric hydrodynamic simulations of black hole-torus systems (approximating general relativistic gravity) as remnants of binary-neutron star (NS-NS) and neutron star-black hole (NS-BH) mergers, in which the viscously driven evolution of the accretion torus is followed with self-consistent energy-dependent neutrino transport and the interaction with the cloud of dynamical ejecta expelled during the NS-NS merging is taken into account. The modeled torus masses, BH masses and spins, and the ejecta masses, velocities, and spatial distributions are adopted from relativistic merger simulations. We find that energy deposition by neutrino annihilation can accelerate outflows with initially high Lorentz factors along polar low-density funnels, but only in mergers with extremely low baryon pollution in the polar regions. NS-BH mergers, where polar mass ejection during the merging phase is absent, provide sufficiently baryon-poor environments to enable neutrino-powered, ultrarelativistic jets with terminal Lorentz factors above 100 and considerable dynamical collimation, favoring short gamma-ray bursts (sGRBs), although their typical energies and durations might be too small to explain the majority of events. In the case of NS-NS mergers, however, neutrino emission of the accreting and viscously spreading torus is too short and too weak to yield enough energy for the outflows to break out from the surrounding ejecta shell as highly relativistic jets. We conclude that neutrino annihilation alone cannot power sGRBs from NS-NS mergers. ",https://doi.org/10.3847/2041-8205/816/2/L30,1510.04288v3,No,
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,Zur Integration von Post-Quantum Verfahren in bestehende   Softwareprodukte,2021,"  Currently, PQC algorithms are being standardized to address the emerging threat to conventional asymmetric algorithms from quantum computing. These new algorithms must then be integrated into existing protocols, applications and infrastructures. Integration problems are to be expected, due to incompatibilities with existing standards and implementations on the one hand, but also due to a lack of knowledge among software developers about how to handle PQC algorithms. To illustrate incompatibilities, we integrate two different PQC algorithms into two different existing software products (the InboxPager email client for the Android OS and the TLS implementation of the Bouncy Castle crypto library). Here, we rely on the highly-abstract crypto library eUCRITE, which hides technical details about the correct usage of classical and PCQ algorithms and thus prevents some potential implementation errors. ",Kein DOI-Link verfügbar,2102.00157v1,Yes,potent(1)
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,Zur Benutzbarkeit und Verwendung von API-Dokumentationen,2020,"  A good documentation is essential for a good usability of (security) APIs, i.e. especially for the correct use of the APIs. Requirements for good documentation of APIs have been described in several papers, but there is no technical implementation (hereinafter referred to as a documentation system) that implements these requirements. The requirements can be divided into requirements for the documentation system and requirements for the documentation content. Out of 13 identified requirements for a documentation system itself, 9 were implemented in a prototype and evaluated in a user study with 22 test persons using a cryptographic API. It turned out that the implementation of the requirement 'Enable quick use of the API' depends on the one hand on the quality of the content entered, but on the other hand also includes 5 other requirements or their implementation. The two other implemented requirements ('classic reference' and 'question and answer function') were hardly or not at all used by the test persons. Their usefulness and relevance should be investigated in a long-term study. ",https://doi.org/10.18420/muc2020-ws119-002,2007.04983v1,No,
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,Design and Implementation Aspects of Mobile Derived Identities,2017,"  With the ongoing digitalisation of our everyday tasks, more and more eGovernment services make it possible for citizens to take care of their administrative obligations online. This type of services requires a certain assurance level for user authentication. To meet these requirements, a digital identity issued to the citizen is essential. Nowadays, due to the widespread use of smartphones, mobile user authentication is often favoured. This naturally supports two-factor authentication schemes (2FA). We use the term mobile derived identity to stress two aspects: a) the identity is enabled for mobile usage and b) the identity is somehow derived from a physical or digital proof of identity. This work reviews 21 systems that support mobile derived identities. One subset of the considered systems is already in place (public or private sector in Europe), another subset is subject to research. Our goal is to identify prevalent design and implementation aspects for these systems in order to gain a better understanding on best practises and common views on mobile derived identities. We found, that research prefers storing identity data on the mobile device itself whereas real world systems usually rely on cloud storage. 2FA is common in both worlds, however biometrics as second factor is the exception. ",Kein DOI-Link verfügbar,1707.06505v1,No,
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,On PQC Migration and Crypto-Agility,2021,"  Besides the development of PQC algorithms, the actual migration of IT systems to such new schemes has to be considered, best by utilizing or establishing crypto-agility. Much work in this respect is currently conducted all over the world, making it hard to keep track of the many individual challenges and respective solutions that have been identified. In consequence, it is difficult to judge for both individual application scenarios and on a global scale, whether all (known) challenges have been addressed respectively or what their current state is. We provide a literature survey and a snapshot of the discovered challenges and solutions categorized in different areas. We use this as starting point for a community project to keep track of the ongoing efforts and the state of the art in this field. Thereby we offer a single entry-point into the subject reflecting the current state in a timely manner. ",Kein DOI-Link verfügbar,2106.09599v1,No,
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,Fast Updates on Read-Optimized Databases Using Multi-Core CPUs,2011,"  Read-optimized columnar databases use differential updates to handle writes by maintaining a separate write-optimized delta partition which is periodically merged with the read-optimized and compressed main partition. This merge process introduces significant overheads and unacceptable downtimes in update intensive systems, aspiring to combine transactional and analytical workloads into one system. In the first part of the paper, we report data analyses of 12 SAP Business Suite customer systems. In the second half, we present an optimized merge process reducing the merge overhead of current systems by a factor of 30. Our linear-time merge algorithm exploits the underlying high compute and bandwidth resources of modern multi-core CPUs with architecture-aware optimizations and efficient parallelization. This enables compressed in-memory column stores to handle the transactional update rate required by enterprise applications, while keeping properties of read-optimized databases for analytic-style queries. ",Kein DOI-Link verfügbar,1109.6885v1,No,
0000-0003-1717-5029,Alexander Zeier,Hochschule Darmstadt,A Comparative Study of Data Storage and Processing Architectures for the   Smart Grid,2020,"  A number of governments and organizations around the world agree that the first step to address national and international problems such as energy independence, global warming or emergency resilience, is the redesign of electricity networks, known as Smart Grids. Typically, power grids have broadcast power from generation plants to large population of consumers on a sub-optimal way. Nevertheless, the fusion of energy delivery networks and digital information networks, along with the introduction of intelligent monitoring systems (Smart Meters) and renewable energies, would enable two-way electricity trading relationships between electricity suppliers and electricity consumers. The availability of real-time information on electricity demand and pricing, would enable suppliers optimizing their delivery systems, while consumers would have the means to minimize their bill by turning on appliances at off-peak hours. The construction of the Smart Grid entails the design and deployment of information networks and systems of unprecedented requirements on storage, real-time event processing and availability. In this paper, a series of system architectures to store and process Smart Meter reading data are explored and compared aiming to establish a solid foundation in which future intelligent systems could be supported. ",https://doi.org/10.1109/SMARTGRID.2010.5622058,2006.02515v1,No,
0009-0006-8822-9089,Vladyslav Gapyak,Hochschule Darmstadt,Reconstruction Formulae for 3D Field-Free Line Magnetic Particle Imaging,2023,"  Magnetic Particle Imaging (MPI) is a promising noninvasive in vivo imaging modality that makes it possible to map the spatial distribution of superparamagnetic nanoparticles by exposing them to dynamic magnetic fields. In the Field-Free Line (FFL) scanner topology, the spatial encoding of the particle distribution is performed by applying magnetic fields vanishing on straight lines. The voltage induced in the receiving coils by the particles when exposed to the magnetic fields constitute the signal from which the particle distribution is to be reconstructed. To avoid lengthy calibration, model-based reconstruction formulae have been developed for the 2D FFL scanning topology. In this work we develop reconstruction formulae for 3D FFL. Moreover, we provide a model-based reconstruction algorithm for 3D FFL and we validate it with a numerical experiment. ",Kein DOI-Link verfügbar,2309.06254v1,No,
0009-0006-8822-9089,Vladyslav Gapyak,Hochschule Darmstadt,Variational Model-Based Reconstruction Techniques for Multi-Patch Data   in Magnetic Particle Imaging,2023,"  Magnetic Particle Imaging is an emerging imaging modality through which it is possible to detect tracers containing superparamagnetic nanoparticles. The exposure of the particles to dynamic magnetic fields generates a non-linear response that is used to locate the particles and produce an image of their distribution. The bounding box that can be covered by a single scan curve depends on the strength of the gradients of the magnetic fields applied, which is limited due to the risk of causing peripheral nerve stimulation (PNS) in the patients. To address this issue, multiple scans are performed. The scan data must be merged together to produce reconstructions of larger regions of interest. In this paper we propose a mathematical framework which can deal with rather general multi-patching scenarios including rigid transformations of the field of view (FoV), the specimen and of the scanner. We show the flexibility of this framework in a variety of different scanning scenarios. Moreover, we describe an iterative reconstruction algorithm that yields a reconstruction of the target distribution by minimizing a convex functional which includes positivity constraints and sparsity enforcing priors. We show its convergence to a minimizer and perform numerical experiments on simulated data. ",https://doi.org/10.1016/j.cam.2024.116046,2306.14454v2,No,
0009-0006-8822-9089,Vladyslav Gapyak,Hochschule Darmstadt,An $\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser   with Evaluation on the 3D Open MPI Dataset,2023,"  Objective: Magnetic particle imaging (MPI) is an emerging medical imaging modality which has gained increasing interest in recent years. Among the benefits of MPI are its high temporal resolution, and that the technique does not expose the specimen to any kind of ionizing radiation. It is based on the non-linear response of magnetic nanoparticles to an applied magnetic field. From the electric signal measured in receive coils, the particle concentration has to be reconstructed. Due to the ill-posedness of the reconstruction problem, various regularization methods have been proposed for reconstruction ranging from early stopping methods, via classical Tikhonov regularization and iterative methods to modern machine learning approaches. In this work, we contribute to the latter class: we propose a plug-and-play approach based on a generic zero-shot denoiser with an $\ell^1$-prior.   Approach: We validate the reconstruction parameters of the method on a hybrid dataset and compare it with the baseline Tikhonov, DIP and the previous PP-MPI, which is a plug-and-play method with denoiser trained on MPI-friendly data.   Main results: We offer a quantitative and qualitative evaluation of the zero-shot plug-and-play approach on the 3D Open MPI dataset. Moreover, we show the quality of the approach with different levels of preprocessing of the data.   Significance: The proposed method employs a zero-shot denoiser which has not been trained for the MPI task and therefore saves the cost for training. Moreover, it offers a method that can be potentially applied in future MPI contexts. ",Kein DOI-Link verfügbar,2401.00275v2,Yes,potent(1)
0000-0003-0798-2919,Rolf Huesmann,Hochschule Darmstadt,Zur Benutzbarkeit und Verwendung von API-Dokumentationen,2020,"  A good documentation is essential for a good usability of (security) APIs, i.e. especially for the correct use of the APIs. Requirements for good documentation of APIs have been described in several papers, but there is no technical implementation (hereinafter referred to as a documentation system) that implements these requirements. The requirements can be divided into requirements for the documentation system and requirements for the documentation content. Out of 13 identified requirements for a documentation system itself, 9 were implemented in a prototype and evaluated in a user study with 22 test persons using a cryptographic API. It turned out that the implementation of the requirement 'Enable quick use of the API' depends on the one hand on the quality of the content entered, but on the other hand also includes 5 other requirements or their implementation. The two other implemented requirements ('classic reference' and 'question and answer function') were hardly or not at all used by the test persons. Their usefulness and relevance should be investigated in a long-term study. ",https://doi.org/10.18420/muc2020-ws119-002,2007.04983v1,No,
0000-0003-0798-2919,Rolf Huesmann,Hochschule Darmstadt,$crypto_{lib}$: Comparing and selecting cryptography libraries (long   version of EICC 2022 publication),2022,"  Selecting a library out of numerous candidates can be a laborious and resource-intensive task. We present the $crypto_{lib}$ index, a tool for decision-makers to choose the best fitting cryptography library for a given context. To define our index, 15 library attributes were synthesized from findings based on a literature review and interviews with decision-makers. These attributes were afterwards validated and weighted via an online survey. In order to create the index value for a given library, the individual attributes are assessed using given evaluation criteria associated with the respective attribute. As a proof of concept and to give a practical usage example, the derivation of the $crypto_{lib}$ values for the libraries Bouncy Castle and Tink are shown in detail. Overall, by tailoring the weighting of the $crypto_{lib}$ attributes to their current use case, decision-makers are enabled to systematically select a cryptography library fitting best to their software project at hand in a guided, repeatable and reliable way. ",Kein DOI-Link verfügbar,2203.16370v1,No,
0000-0003-0798-2919,Rolf Huesmann,Hochschule Darmstadt,"Mobile Touchless Fingerprint Recognition: Implementation, Performance   and Usability Aspects",2021,"  This work presents an automated touchless fingerprint recognition system for smartphones. We provide a comprehensive description of the entire recognition pipeline and discuss important requirements for a fully automated capturing system. Also, our implementation is made publicly available for research purposes. During a database acquisition, a total number of 1,360 touchless and touch-based samples of 29 subjects are captured in two different environmental situations. Experiments on the acquired database show a comparable performance of our touchless scheme and the touch-based baseline scheme under constrained environmental influences. A comparative usability study on both capturing device types indicates that the majority of subjects prefer the touchless capturing method. Based on our experimental results we analyze the impact of the current COVID-19 pandemic on fingerprint recognition systems. Finally, implementation aspects of touchless fingerprint recognition are summarized. ",Kein DOI-Link verfügbar,2103.03038v1,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Echo-chambers and Idea Labs: Communication Styles on Twitter,2024,"  This paper investigates the communication styles and structures of Twitter (X) communities within the vaccination context. While mainstream research primarily focuses on the echo-chamber phenomenon, wherein certain ideas are reinforced and participants are isolated from opposing opinions, this study reveals the presence of diverse communication styles across various communities. In addition to the communities exhibiting echo-chamber behavior, this research uncovers communities with distinct communication patterns. By shedding light on the nuanced nature of communication within social networks, this study emphasizes the significance of understanding the diversity of perspectives within online communities. ",Kein DOI-Link verfügbar,2403.19423v1,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Real-space renormalization group flow in quantum impurity systems: local   moment formation and the Kondo screening cloud,2011,"  The existence of a length-scale $\xi_K\sim 1/T_K$ (with $T_K$ the Kondo temperature) has long been predicted in quantum impurity systems. At low temperatures $T\ll T_K$, the standard interpretation is that a spin-$\tfrac{1}{2}$ impurity is screened by a surrounding `Kondo cloud' of spatial extent $\xi_K$. We argue that renormalization group (RG) flow between any two fixed points (FPs) results in a characteristic length-scale, observed in real-space as a crossover between physical behaviour typical of each FP. In the simplest example of the Anderson impurity model, three FPs arise; and we show that `free orbital', `local moment' and `strong coupling' regions of space can be identified at zero temperature. These regions are separated by two crossover length-scales $\xi_{\text{LM}}$ and $\xi_K$, with the latter diverging as the Kondo effect is destroyed on increasing temperature through $T_K$. One implication is that moment formation occurs inside the `Kondo cloud', while the screening process itself occurs on flowing to the strong coupling FP at distances $\sim \xi_K$. Generic aspects of the real-space physics are exemplified by the two-channel Kondo model, where $\xi_K$ now separates `local moment' and `overscreening' clouds. ",https://doi.org/10.1103/PhysRevB.84.115120,1105.0560v2,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Interplay between Kondo and Majorana interactions in quantum dots,2013,"  We study the properties of a quantum dot coupled to a one-dimensional topological superconductor and a normal lead and discuss the interplay between Kondo and Majorana-induced couplings in quantum dot. The latter appears due to the presence of Majorana zero-energy modes localized at the ends of the one-dimensional superconductor. We investigate the phase diagram of the system as a function of Kondo and Majorana interactions using a renormalization-group analysis, a slave-boson mean-field theory and numerical simulations using the density-matrix renormalization group method. We show that, in addition to the well-known Kondo fixed point, the system may flow to a new fixed point controlled by the Majorana-induced coupling which is characterized by non-trivial correlations between a localized spin on the dot and the fermion parity of the topological superconductor and normal lead. We compute several measurable quantities such as differential tunneling conductance and impurity spin susceptibility which highlight some peculiar features characteristic to the Majorana fixed point. ",https://doi.org/10.1103/PhysRevX.4.031051,1308.4156v4,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Spin-orbit physics of j=1/2 Mott insulators on the triangular lattice,2014,"  The Heisenberg-Kitaev (HK) model on the triangular lattice is conceptually interesting for its interplay of geometric and exchange frustration. HK models are also thought to capture the essential physics of the spin-orbital entanglement in effective $j=1/2$ Mott insulators studied in the context of various 5d transition metal oxides. Here we argue that the recently synthesized Ba$_3$IrTi$_2$O$_9$ is a prime candidate for a microscopic realization of the triangular HK model. We establish that an infinitesimal Kitaev exchange destabilizes the 120$^\circ$ order of the quantum Heisenberg model and results in the formation of an extended $\mathbb{Z}_2$-vortex crystal phase in the parameter regime most likely relevant to the real material. Using a combination of analytical and numerical techniques we map out the entire phase diagram of the model, which further includes various ordered phases as well as an extended nematic phase around the antiferromagnetic Kitaev point. ",https://doi.org/10.1103/PhysRevB.91.155135,1409.6972v2,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Revisiting Effects of Nitrogen Incorporation and Graphitization on   Conductivity of Ultra-nano-crystalline Diamond Films,2019,"  Detailed structural and electrical properties of ultra-nano-crystalline diamond (UNCD) films grown in H$_\text{2}$/CH$_\text{4}$/N$_\text{2}$ plasma were systematically studied as a function of deposition temperature ($T_d$) and nitrogen content ($\%$ N$_2$) to thoroughly evaluate their effects on conductivity. $T_d$ was scanned from 1000 to 1300 K for N$_2$ fixed at 0, 5, 10 and 20 $\%$. It was found that even the films grown in the synthetic gas mixture with no nitrogen could be made as conductive as 1$-$10$^{-2}$ $\Omega$ cm with overall resistivity of all the films tuned over 4 orders of magnitude through varying growth parameters. On a set of 27 samples, Raman spectroscopy and scanning electron microscopy show a progressive and highly reproducible film material phase transformation, from ultra-nano-crystalline diamond to nano-crystalline graphite as deposition temperature increases. The rate of this transformation is heavily dependent on N$_2$ content. Addition of nitrogen greatly increases the amount of $sp^2$ bonded carbon in the films thus enhancing the physical connectivity in the GB network that have high electronic density of states. However, addition of nitrogen greatly slows down crystallization of $sp^2$ phase in the GBs. Therefore, proper balance between GB connectivity and crystallinity is the key in conductivity engineering of (N)UNCD. ",Kein DOI-Link verfügbar,1910.09595v1,No,
0000-0001-5502-7444,Michael Becker,Hochschule Darmstadt,Photoluminescence from Silicon nanoparticles embedded in ammonium   silicon hexafluoride,2015,"  Silicon (Si) nanoparticles (NPs) were synthesized by transforming Si wafer surface to ammonium silicon hexafluoride (ASH) or (NH4)2SiF6 under acid vapor treatment. Si-NPs are embedded within the polycrystalline (ASH) layer formed on the Si surface exhibit a strong green-orange photoluminescence (PL). Difference measurements revealed a major double component spectra consisting of a broad band associated with the ASH-Si wafer interfacial porous oxide layer and a high energy band attributable to Si-NPs embedded in the ASH. The origin of the latter emission can be explained in terms of quantum/spatial confinement effects probably mediated by oxygen related defects in or around Si-NPs. Although Si-NPs are derived from the interface they are much smaller in size than those embedded within the interfacial porous oxide layer (SiOx, 1 < x < 2). Transmission electron microscopy (TEM) combined with Raman scattering and Fourier transformed infrared (FTIR) analysis confirmed the presence of Si-NPs and Si-O bondings pointing to the role of oxygen related defects. The presence of oxygen of up to 4.5 at.% in the (NH4)2SiF6 layer was confirmed by energy dispersive spectroscopy (EDS) analysis. ",https://doi.org/10.1088/0957-4484/21/43/435701,1512.07991v1,No,
0000-0001-6470-2966,Lazaro Janier Gonzalez-Soler,"Darmstadt Universität der Angewandte Wissenschaften, Hochschule Darmstadt",TattTRN: Template Reconstruction Network for Tattoo Retrieval,2024,"  Tattoos have been used effectively as soft biometrics to assist law enforcement in the identification of offenders and victims, as they contain discriminative information, and are a useful indicator to locate members of a criminal gang or organisation. Due to various privacy issues in the acquisition of images containing tattoos, only a limited number of databases exists. This lack of databases has delayed the development of new methods to effectively retrieve a potential suspect's tattoo images from a candidate gallery. To mitigate this issue, in our work, we use an unsupervised generative approach to create a balanced database consisting of 28,550 semi-synthetic images with tattooed subjects from 571 tattoo categories. Further, we introduce a novel Tattoo Template Reconstruction Network (TattTRN), which learns to map the input tattoo sample to its respective tattoo template to enhance the distinguishing attributes of the final feature embedding. Experimental results with real data, i.e., WebTattoo and BIVTatt databases, demonstrate the soundness of the presented approach: an accuracy of up to 99% is achieved for checking at most the first 20 entries of the candidate list. ",Kein DOI-Link verfügbar,2405.07571v1,Yes,potent(1)
0000-0001-8740-3547,Patrick Berger,Darmstadt Universität der Angewandte Wissenschaften,CT evaluation of 2D and 3D holistic deep learning methods for the   volumetric segmentation of airway lesions,2024,"  This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application. ",Kein DOI-Link verfügbar,2403.08042v2,No,
0009-0006-3397-4938,Elke Hergenroether,Darmstadt Universität der Angewandte Wissenschaften,Challenges of the Creation of a Dataset for Vision Based Human Hand   Action Recognition in Industrial Assembly,2023,"  This work presents the Industrial Hand Action Dataset V1, an industrial assembly dataset consisting of 12 classes with 459,180 images in the basic version and 2,295,900 images after spatial augmentation. Compared to other freely available datasets tested, it has an above-average duration and, in addition, meets the technical and legal requirements for industrial assembly lines. Furthermore, the dataset contains occlusions, hand-object interaction, and various fine-grained human hand actions for industrial assembly tasks that were not found in combination in examined datasets. The recorded ground truth assembly classes were selected after extensive observation of real-world use cases. A Gated Transformer Network, a state-of-the-art model from the transformer domain was adapted, and proved with a test accuracy of 86.25% before hyperparameter tuning by 18,269,959 trainable parameters, that it is possible to train sequential deep learning models with this dataset. ",Kein DOI-Link verfügbar,2303.03716v1,No,
0009-0006-3397-4938,Elke Hergenroether,Darmstadt Universität der Angewandte Wissenschaften,CNN Based Flank Predictor for Quadruped Animal Species,2024,"  The bilateral asymmetry of flanks of animals with visual body marks that uniquely identify an individual, complicates tasks like population estimations. Automatically generated additional information on the visible side of the animal would improve the accuracy for individual identification. In this study we used transfer learning on popular CNN image classification architectures to train a flank predictor that predicts the visible flank of quadruped mammalian species in images. We automatically derived the data labels from existing datasets originally labeled for animal pose estimation. We trained the models in two phases with different degrees of retraining. The developed models were evaluated in different scenarios of different unknown quadruped species in known and unknown environments. As a real-world scenario, we used a dataset of manually labeled Eurasian lynx (Lynx lynx) from camera traps in the Bavarian Forest National Park to evaluate the model. The best model, trained on an EfficientNetV2 backbone, achieved an accuracy of 88.70 % for the unknown species lynx in a complex habitat. ",Kein DOI-Link verfügbar,2406.13588v1,No,
0009-0006-3397-4938,Elke Hergenroether,Darmstadt Universität der Angewandte Wissenschaften,Automated Bioacoustic Monitoring for South African Bird Species on   Unlabeled Data,2024,"  Analyses for biodiversity monitoring based on passive acoustic monitoring (PAM) recordings is time-consuming and challenged by the presence of background noise in recordings. Existing models for sound event detection (SED) worked only on certain avian species and the development of further models required labeled data. The developed framework automatically extracted labeled data from available platforms for selected avian species. The labeled data were embedded into recordings, including environmental sounds and noise, and were used to train convolutional recurrent neural network (CRNN) models. The models were evaluated on unprocessed real world data recorded in urban KwaZulu-Natal habitats. The Adapted SED-CRNN model reached a F1 score of 0.73, demonstrating its efficiency under noisy, real-world conditions. The proposed approach to automatically extract labeled data for chosen avian species enables an easy adaption of PAM to other species and habitats for future conservation projects. ",Kein DOI-Link verfügbar,2406.13579v1,No,
0009-0006-3397-4938,Elke Hergenroether,Darmstadt Universität der Angewandte Wissenschaften,Automatic Individual Identification of Patterned Solitary Species Based   on Unlabeled Video Data,2023,"  The manual processing and analysis of videos from camera traps is time-consuming and includes several steps, ranging from the filtering of falsely triggered footage to identifying and re-identifying individuals. In this study, we developed a pipeline to automatically analyze videos from camera traps to identify individuals without requiring manual interaction. This pipeline applies to animal species with uniquely identifiable fur patterns and solitary behavior, such as leopards (Panthera pardus). We assumed that the same individual was seen throughout one triggered video sequence. With this assumption, multiple images could be assigned to an individual for the initial database filling without pre-labeling. The pipeline was based on well-established components from computer vision and deep learning, particularly convolutional neural networks (CNNs) and scale-invariant feature transform (SIFT) features. We augmented this basis by implementing additional components to substitute otherwise required human interactions. Based on the similarity between frames from the video material, clusters were formed that represented individuals bypassing the open set problem of the unknown total population. The pipeline was tested on a dataset of leopard videos collected by the Pan African Programme: The Cultured Chimpanzee (PanAf) and achieved a success rate of over 83% for correct matches between previously unknown individuals. The proposed pipeline can become a valuable tool for future conservation projects based on camera trap data, reducing the work of manual analysis for individual identification, when labeled data is unavailable. ",Kein DOI-Link verfügbar,2304.09657v1,No,
0000-0003-4365-1762,Dustin Kern,Darmstadt Universität der Angewandte Wissenschaften,Self-Sovereign Identity for Electric Vehicle Charging,2024,"  Electric Vehicles (EVs) are more and more charged at public Charge Points (CPs) using Plug-and-Charge (PnC) protocols such as the ISO 15118 standard which eliminates user interaction for authentication and authorization. Currently, this requires a rather complex Public Key Infrastructure (PKI) and enables driver tracking via the included unique identifiers. In this paper, we propose an approach for using Self-Sovereign Identities (SSIs) as trusted credentials for EV charging authentication and authorization which overcomes the privacy problems and the issues of a complex centralized PKI. Our implementation shows the feasibility of our approach with ISO 15118. The security and privacy of the proposed approach is shown in a formal analysis using the Tamarin prover. ",https://doi.org/10.1007/978-3-031-54776-8_6,2403.06632v1,No,
0009-0005-7258-1363,Dominik Kuehn,Darmstadt Universität der Angewandte Wissenschaften,Automated Bioacoustic Monitoring for South African Bird Species on   Unlabeled Data,2024,"  Analyses for biodiversity monitoring based on passive acoustic monitoring (PAM) recordings is time-consuming and challenged by the presence of background noise in recordings. Existing models for sound event detection (SED) worked only on certain avian species and the development of further models required labeled data. The developed framework automatically extracted labeled data from available platforms for selected avian species. The labeled data were embedded into recordings, including environmental sounds and noise, and were used to train convolutional recurrent neural network (CRNN) models. The models were evaluated on unprocessed real world data recorded in urban KwaZulu-Natal habitats. The Adapted SED-CRNN model reached a F1 score of 0.73, demonstrating its efficiency under noisy, real-world conditions. The proposed approach to automatically extract labeled data for chosen avian species enables an easy adaption of PAM to other species and habitats for future conservation projects. ",Kein DOI-Link verfügbar,2406.13579v1,No,
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,Autism Detection in Speech -- A Survey,2024,"  There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this context. Additionally, we were unable to find research combining both features from audio and transcripts. ",Kein DOI-Link verfügbar,2402.12880v1,No,
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,Investigating Bias in Multilingual Language Models: Cross-Lingual   Transfer of Debiasing Techniques,2023,"  This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using translations of the CrowS-Pairs dataset, our analysis identifies SentenceDebias as the best technique across different languages, reducing bias in mBERT by an average of 13%. We also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. These novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts. ",Kein DOI-Link verfügbar,2310.10310v1,No,
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,BigScience: A Case Study in the Social Construction of a Multilingual   Large Language Model,2022,"  The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception. ",Kein DOI-Link verfügbar,2212.04960v1,No,
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,What Can Natural Language Processing Do for Peer Review?,2024,"  The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond. ",Kein DOI-Link verfügbar,2405.06563v1,Yes,potent(1)
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,"Missing Information, Unresponsive Authors, Experimental Flaws: The   Impossibility of Assessing the Reproducibility of Previous Human Evaluations   in NLP",2023,"  We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP. ",Kein DOI-Link verfügbar,2305.01633v2,No,
0009-0006-2268-4804,Margot Mieskes,Darmstadt Universität der Angewandte Wissenschaften,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,2022,"  Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License. ",Kein DOI-Link verfügbar,2211.05100v4,No,
0000-0002-5064-5750,Melanie Siegel,Darmstadt Universität der Angewandte Wissenschaften,The syntactic processing of particles in Japanese spoken language,1999,"  Particles fullfill several distinct central roles in the Japanese language. They can mark arguments as well as adjuncts, can be functional or have semantic funtions. There is, however, no straightforward matching from particles to functions, as, e.g., GA can mark the subject, the object or an adjunct of a sentence. Particles can cooccur. Verbal arguments that could be identified by particles can be eliminated in the Japanese sentence. And finally, in spoken language particles are often omitted. A proper treatment of particles is thus necessary to make an analysis of Japanese sentences possible. Our treatment is based on an empirical investigation of 800 dialogues. We set up a type hierarchy of particles motivated by their subcategorizational and modificational behaviour. This type hierarchy is part of the Japanese syntax in VERBMOBIL. ",Kein DOI-Link verfügbar,cs/9906003v1,No,
0000-0002-5064-5750,Melanie Siegel,Darmstadt Universität der Angewandte Wissenschaften,Efficient Deep Processing of Japanese,2002,"  We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages. ",Kein DOI-Link verfügbar,cs/0207005v1,No,
0000-0002-5064-5750,Melanie Siegel,Darmstadt Universität der Angewandte Wissenschaften,Easy-to-Read in Germany: A Survey on its Current State and Available   Resources,2023,"  Easy-to-Read Language (E2R) is a controlled language variant that makes any written text more accessible through the use of clear, direct and simple language. It is mainly aimed at people with cognitive or intellectual disabilities, among other target users. Plain Language (PL), on the other hand, is a variant of a given language, which aims to promote the use of simple language to communicate information. German counts with Leichte Sprache (LS), its version of E2R, and Einfache Sprache (ES), its version of PL. In recent years, important developments have been conducted in the field of LS. This paper offers an updated overview of the existing Natural Language Processing (NLP) tools and resources for LS. Besides, it also aims to set out the situation with regard to LS and ES in Germany. ",Kein DOI-Link verfügbar,2306.03189v1,No,
0000-0002-5064-5750,Melanie Siegel,Darmstadt Universität der Angewandte Wissenschaften,Challenges of the Creation of a Dataset for Vision Based Human Hand   Action Recognition in Industrial Assembly,2023,"  This work presents the Industrial Hand Action Dataset V1, an industrial assembly dataset consisting of 12 classes with 459,180 images in the basic version and 2,295,900 images after spatial augmentation. Compared to other freely available datasets tested, it has an above-average duration and, in addition, meets the technical and legal requirements for industrial assembly lines. Furthermore, the dataset contains occlusions, hand-object interaction, and various fine-grained human hand actions for industrial assembly tasks that were not found in combination in examined datasets. The recorded ground truth assembly classes were selected after extensive observation of real-world use cases. A Gated Transformer Network, a state-of-the-art model from the transformer domain was adapted, and proved with a test accuracy of 86.25% before hyperparameter tuning by 18,269,959 trainable parameters, that it is possible to train sequential deep learning models with this dataset. ",Kein DOI-Link verfügbar,2303.03716v1,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt Universität der Angewandte Wissenschaften,Comparing autonomous vehicle acceptance of German residents with and   without visual impairments,2023,"  Connected and autonomous vehicles (CAVs) will greatly impact the lives of individuals with visual impairments, but how they differ in expectations compared to sighted individuals is not clear. The present research reports results based on survey responses from 114 visually impaired participants and 117 panel recruited participants without visual impairments, from Germany. Their attitudes towards autonomous vehicles and their expectations for consequences of wide-spread adoption of CAVs are assessed. Results indicate significantly more positive CAV attitudes in participants with visual impairments compared to those without visual impairments. Mediation analyses indicate that visually impaired individuals' more positive CAV attitudes (compared to sighted individuals') are largely explained by higher hopes for independence, and more optimistic expectations regarding safety and sustainability. Policy makers should ensure accessibility without sacrificing goals for higher safety and lower ecological impact to make CAVs an acceptable inclusive mobility solution. ",https://doi.org/10.1080/17483107.2024.2317930,2311.12900v2,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt Universität der Angewandte Wissenschaften,Ambivalence in stakeholders' views on connected and autonomous vehicles,2024,"  Connected and autonomous vehicles (CAVs) are often discussed as a solution to pressing issues of the current transport systems, including congestion, safety, social inclusion and ecological sustainability. Scientifically, there is agreement that CAVs may solve, but can also aggravate these issues, depending on the specific CAV solution. In the current paper, we investigate the visions and worst-case scenarios of various stakeholders, including representatives of public administrations, automotive original equipment manufacturers, insurance companies, public transportation service providers, mobility experts and politicians. A qualitative analysis of 17 semi-structured interviews is presented. It reveals experts' ambivalence towards the introduction of CAVs, reflecting high levels of uncertainty about CAV consequences, including issues of efficiency, comfort and sustainability, and concerns about road co-users such as pedestrians and cyclists. Implications of the sluggishness of policymakers to set boundary conditions and for the labor market are discussed. An open debate between policymakers, citizens and other stakeholders on how to introduce CAVs seems timely. ",https://doi.org/10.1007/978-3-030-50523-3_4,2403.12070v1,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt Universität der Angewandte Wissenschaften,Creating inclusive mobility systems: towards age and education sensitive   interventions for enhancing autonomous vehicle acceptance,2023,"  The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N=799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels. ",Kein DOI-Link verfügbar,2311.16780v1,No,
0009-0005-2485-5120,Martin Weber,"Darmstadt Universität der Angewandte Wissenschaften, Frankfurt Universität der Angewandte Wissenschaften",Measurement of Boson Self Couplings at LEP and Search for Anomalies,2002,"  With center of mass energies up to 209 GeV of LEP II, massive W and Z bosons can be produced in pairs and jointly with photons. This allows to study boson-boson couplings. Since the W and Z bosons are unstable and decay into fermions, two- and four-fermion final states, accompanied possibly by photons, play an important role for these measurements. The couplings of the W to other bosons have been measured to be g1Z = 0.990+0.023-0.024, kappa_gamma = 0.896+0.058-0.056, and lambda_gamma = -0.023 +0.025 -0.023. They are in agreement with the Standard Model expectation of g1Z = 1, kappa_gamma = 1, and lambda_gamma = 0. No sign for couplings of three neutral bosons, parametrized by the couplings f_i^V and h_i^V, and for anomalous couplings of four gauge bosons, parametrized by a_0, a_n and a_c has been found. ",Kein DOI-Link verfügbar,hep-ex/0205024v3,No,
0000-0003-4489-845X,Andreas Fischbach,TH Köln – Universität der Angewandte Wissenschaften,CAAI -- A Cognitive Architecture to Introduce Artificial Intelligence in   Cyber-Physical Production Systems,2020,"  This paper introduces CAAI, a novel cognitive architecture for artificial intelligence in cyber-physical production systems. The goal of the architecture is to reduce the implementation effort for the usage of artificial intelligence algorithms. The core of the CAAI is a cognitive module that processes declarative goals of the user, selects suitable models and algorithms, and creates a configuration for the execution of a processing pipeline on a big data platform. Constant observation and evaluation against performance criteria assess the performance of pipelines for many and varying use cases. Based on these evaluations, the pipelines are automatically adapted if necessary. The modular design with well-defined interfaces enables the reusability and extensibility of pipeline components. A big data platform implements this modular design supported by technologies such as Docker, Kubernetes, and Kafka for virtualization and orchestration of the individual components and their communication. The implementation of the architecture is evaluated using a real-world use case. ",https://doi.org/10.1007/s00170-020-06094-z,2003.00925v1,No,
0000-0003-4489-845X,Andreas Fischbach,TH Köln – Universität der Angewandte Wissenschaften,Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems,2020,"  This paper presents the cognitive module of the cognitive architecture for artificial intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to test algorithms from different classes. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging-technology for module communication is used to evaluate a real-world use case. ",https://doi.org/10.1007/s00170-021-07248-3,2012.01823v1,No,
0000-0003-4489-845X,Andreas Fischbach,TH Köln – Universität der Angewandte Wissenschaften,Evaluation of Cognitive Architectures for Cyber-Physical Production   Systems,2019,"  Cyber-physical production systems (CPPS) integrate physical and computational resources due to increasingly available sensors and processing power. This enables the usage of data, to create additional benefit, such as condition monitoring or optimization. These capabilities can lead to cognition, such that the system is able to adapt independently to changing circumstances by learning from additional sensors information. Developing a reference architecture for the design of CPPS and standardization of machines and software interfaces is crucial to enable compatibility of data usage between different machine models and vendors. This paper analysis existing reference architecture regarding their cognitive abilities, based on requirements that are derived from three different use cases. The results from the evaluation of the reference architectures, which include two instances that stem from the field of cognitive science, reveal a gap in the applicability of the architectures regarding the generalizability and the level of abstraction. While reference architectures from the field of automation are suitable to address use case specific requirements, and do not address the general requirements, especially w.r.t. adaptability, the examples from the field of cognitive science are well usable to reach a high level of adaption and cognition. It is desirable to merge advantages of both classes of architectures to address challenges in the field of CPPS in Industrie 4.0. ",Kein DOI-Link verfügbar,1902.08448v3,No,
0000-0003-4489-845X,Andreas Fischbach,TH Köln – Universität der Angewandte Wissenschaften,Benchmarking in Optimization: Best Practice and Open Issues,2020,"  This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, well-specified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates. ",Kein DOI-Link verfügbar,2007.03488v2,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Distance-based Kernels for Surrogate Model-based Neuroevolution,2018,"  The topology optimization of artificial neural networks can be particularly difficult if the fitness evaluations require expensive experiments or simulations. For that reason, the optimization methods may need to be supported by surrogate models. We propose different distances for a suitable surrogate model, and compare them in a simple numerical test scenario. ",Kein DOI-Link verfügbar,1807.07839v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Improving NeuroEvolution Efficiency by Surrogate Model-based   Optimization with Phenotypic Distance Kernels,2019,"  In NeuroEvolution, the topologies of artificial neural networks are optimized with evolutionary algorithms to solve tasks in data regression, data classification, or reinforcement learning. One downside of NeuroEvolution is the large amount of necessary fitness evaluations, which might render it inefficient for tasks with expensive evaluations, such as real-time learning. For these expensive optimization tasks, surrogate model-based optimization is frequently applied as it features a good evaluation efficiency. While a combination of both procedures appears as a valuable solution, the definition of adequate distance measures for the surrogate modeling process is difficult. In this study, we will extend cartesian genetic programming of artificial neural networks by the use of surrogate model-based optimization. We propose different distance measures and test our algorithm on a replicable benchmark task. The results indicate that we can significantly increase the evaluation efficiency and that a phenotypic distance, which is based on the behavior of the associated neural networks, is most promising. ",Kein DOI-Link verfügbar,1902.03419v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,A new Taxonomy of Continuous Global Optimization Algorithms,2018,"  Surrogate-based optimization, nature-inspired metaheuristics, and hybrid combinations have become state of the art in algorithm design for solving real-world optimization problems. Still, it is difficult for practitioners to get an overview that explains their advantages in comparison to a large number of available methods in the scope of optimization. Available taxonomies lack the embedding of current approaches in the larger context of this broad field. This article presents a taxonomy of the field, which explores and matches algorithm strategies by extracting similarities and differences in their search strategies. A particular focus lies on algorithms using surrogates, nature-inspired designs, and those created by design optimization. The extracted features of components or operators allow us to create a set of classification indicators to distinguish between a small number of classes. The features allow a deeper understanding of components of the search strategies and further indicate the close connections between the different algorithm designs. We present intuitive analogies to explain the basic principles of the search algorithms, particularly useful for novices in this research field. Furthermore, this taxonomy allows recommendations for the applicability of the corresponding algorithms. ",https://doi.org/10.1007/s11047-020-09820-4,1808.08818v2,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Surrogate Models for Enhancing the Efficiency of Neuroevolution in   Reinforcement Learning,2019,"  In the last years, reinforcement learning received a lot of attention. One method to solve reinforcement learning tasks is Neuroevolution, where neural networks are optimized by evolutionary algorithms. A disadvantage of Neuroevolution is that it can require numerous function evaluations, while not fully utilizing the available information from each fitness evaluation. This is especially problematic when fitness evaluations become expensive. To reduce the cost of fitness evaluations, surrogate models can be employed to partially replace the fitness function. The difficulty of surrogate modeling for Neuroevolution is the complex search space and how to compare different networks. To that end, recent studies showed that a kernel based approach, particular with phenotypic distance measures, works well. These kernels compare different networks via their behavior (phenotype) rather than their topology or encoding (genotype). In this work, we discuss the use of surrogate model-based Neuroevolution (SMB-NE) using a phenotypic distance for reinforcement learning. In detail, we investigate a) the potential of SMB-NE with respect to evaluation efficiency and b) how to select adequate input sets for the phenotypic distance measure in a reinforcement learning problem. The results indicate that we are able to considerably increase the evaluation efficiency using dynamic input sets. ",https://doi.org/10.1145/3321707.3321829,1907.09300v1,Yes,potent(1)
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Comparison of different Methods for Univariate Time Series Imputation in   R,2015,"  Missing values in datasets are a well-known problem and there are quite a lot of R packages offering imputation functions. But while imputation in general is well covered within R, it is hard to find functions for imputation of univariate time series. The problem is, most standard imputation techniques can not be applied directly. Most algorithms rely on inter-attribute correlations, while univariate time series imputation needs to employ time dependencies. This paper provides an overview of univariate time series imputation in general and an in-detail insight into the respective implementations within R packages. Furthermore, we experimentally compare the R functions on different time series using four different ratios of missing data. Our results show that either an interpolation with seasonal kalman filter from the zoo package or a linear interpolation on seasonal loess decomposed data from the forecast package were the most effective methods for dealing with missing data in most of the scenarios assessed in this paper. ",Kein DOI-Link verfügbar,1510.03924v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Linear Combination of Distance Measures for Surrogate Models in Genetic   Programming,2018,"  Surrogate models are a well established approach to reduce the number of expensive function evaluations in continuous optimization. In the context of genetic programming, surrogate modeling still poses a challenge, due to the complex genotype-phenotype relationships. We investigate how different genotypic and phenotypic distance measures can be used to learn Kriging models as surrogates. We compare the measures and suggest to use their linear combination in a kernel.   We test the resulting model in an optimization framework, using symbolic regression problem instances as a benchmark. Our experiments show that the model provides valuable information. Firstly, the model enables an improved optimization performance compared to a model-free algorithm. Furthermore, the model provides information on the contribution of different distance measures. The data indicates that a phenotypic distance measure is important during the early stages of an optimization run when less data is available. In contrast, genotypic measures, such as the tree edit distance, contribute more during the later stages. ",Kein DOI-Link verfügbar,1807.01019v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,CAAI -- A Cognitive Architecture to Introduce Artificial Intelligence in   Cyber-Physical Production Systems,2020,"  This paper introduces CAAI, a novel cognitive architecture for artificial intelligence in cyber-physical production systems. The goal of the architecture is to reduce the implementation effort for the usage of artificial intelligence algorithms. The core of the CAAI is a cognitive module that processes declarative goals of the user, selects suitable models and algorithms, and creates a configuration for the execution of a processing pipeline on a big data platform. Constant observation and evaluation against performance criteria assess the performance of pipelines for many and varying use cases. Based on these evaluations, the pipelines are automatically adapted if necessary. The modular design with well-defined interfaces enables the reusability and extensibility of pipeline components. A big data platform implements this modular design supported by technologies such as Docker, Kubernetes, and Kafka for virtualization and orchestration of the individual components and their communication. The implementation of the architecture is evaluated using a real-world use case. ",https://doi.org/10.1007/s00170-020-06094-z,2003.00925v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Behavior-based Neuroevolutionary Training in Reinforcement Learning,2021,"  In addition to their undisputed success in solving classical optimization problems, neuroevolutionary and population-based algorithms have become an alternative to standard reinforcement learning methods. However, evolutionary methods often lack the sample efficiency of standard value-based methods that leverage gathered state and value experience. If reinforcement learning for real-world problems with significant resource cost is considered, sample efficiency is essential. The enhancement of evolutionary algorithms with experience exploiting methods is thus desired and promises valuable insights. This work presents a hybrid algorithm that combines topology-changing neuroevolutionary optimization with value-based reinforcement learning. We illustrate how the behavior of policies can be used to create distance and loss functions, which benefit from stored experiences and calculated state values. They allow us to model behavior and perform a directed search in the behavior space by gradient-free evolutionary algorithms and surrogate-based optimization. For this purpose, we consolidate different methods to generate and optimize agent policies, creating a diverse population. We exemplify the performance of our algorithm on standard benchmarks and a purpose-built real-world problem. Our results indicate that combining methods can enhance the sample efficiency and learning speed for evolutionary approaches. ",https://doi.org/10.1145/3449726.3463171,2105.07960v1,No,
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Underwater Acoustic Networks for Security Risk Assessment in Public   Drinking Water Reservoirs,2021,"  We have built a novel system for the surveillance of drinking water reservoirs using underwater sensor networks. We implement an innovative AI-based approach to detect, classify and localize underwater events. In this paper, we describe the technology and cognitive AI architecture of the system based on one of the sensor networks, the hydrophone network. We discuss the challenges of installing and using the hydrophone network in a water reservoir where traffic, visitors, and variable water conditions create a complex, varying environment. Our AI solution uses an autoencoder for unsupervised learning of latent encodings for classification and anomaly detection, and time delay estimates for sound localization. Finally, we present the results of experiments carried out in a laboratory pool and the water reservoir and discuss the system's potential. ",Kein DOI-Link verfügbar,2107.13977v1,Yes,"innovative(1), potent(1)"
0000-0002-7471-3498,Jörg Stork,TH Köln – Universität der Angewandte Wissenschaften,Prediction of neural network performance by phenotypic modeling,2019,"  Surrogate models are used to reduce the burden of expensive-to-evaluate objective functions in optimization. By creating models which map genomes to objective values, these models can estimate the performance of unknown inputs, and so be used in place of expensive objective functions. Evolutionary techniques such as genetic programming or neuroevolution commonly alter the structure of the genome itself. A lack of consistency in the genotype is a fatal blow to data-driven modeling techniques: interpolation between points is impossible without a common input space. However, while the dimensionality of genotypes may differ across individuals, in many domains, such as controllers or classifiers, the dimensionality of the input and output remains constant. In this work we leverage this insight to embed differing neural networks into the same input space. To judge the difference between the behavior of two neural networks, we give them both the same input sequence, and examine the difference in output. This difference, the phenotypic distance, can then be used to situate these networks into a common input space, allowing us to produce surrogate models which can predict the performance of neural networks regardless of topology. In a robotic navigation task, we show that models trained using this phenotypic embedding perform as well or better as those trained on the weight values of a fixed topology neural network. We establish such phenotypic surrogate models as a promising and flexible approach which enables surrogate modeling even for representations that undergo structural changes. ",https://doi.org/10.1145/3319619.3326815,1907.07075v1,No,
0000-0002-5340-3001,Arjuna Nebel,TH Köln - Universität der Angewandte Wissenschaften,Opening the black box of energy modelling: Strategies and lessons   learned,2017,"  The global energy system is undergoing a major transition, and in energy planning and decision-making across governments, industry and academia, models play a crucial role. Because of their policy relevance and contested nature, the transparency and open availability of energy models and data are of particular importance. Here we provide a practical how-to guide based on the collective experience of members of the Open Energy Modelling Initiative (Openmod). We discuss key steps to consider when opening code and data, including determining intellectual property ownership, choosing a licence and appropriate modelling languages, distributing code and data, and providing support and building communities. After illustrating these decisions with examples and lessons learned from the community, we conclude that even though individual researchers' choices are important, institutional changes are still also necessary for more openness and transparency in energy research. ",https://doi.org/10.1016/j.esr.2017.12.002,1707.08164v2,No,
0009-0003-9509-0140,Johanna Fink,TH Köln - Universität der Angewandte Wissenschaften,Can the creation of separate bidding zones within countries create   imbalances in PV uptake? Evidence from Sweden,2023,"  This paper estimates how electricity price divergence within Sweden has affected incentives to invest in photovoltaic (PV) generation between 2016 and 2022 based on a synthetic control approach. Sweden is chosen as the research subject since it is together with Italy the only EU country with multiple bidding zones and is facing dramatic divergence in electricity prices between low-tariff bidding zones in Northern and high-tariff bidding zones in Southern Sweden since 2020. The results indicate that PV uptake in municipalities located north of the bidding zone border is reduced by 40.9-48% compared to their Southern counterparts. Based on these results, the creation of separate bidding zones within countries poses a threat to the expansion of PV generation and other renewables since it disincentivizes investment in areas with low electricity prices. ",Kein DOI-Link verfügbar,2312.16161v1,No,
0000-0003-4592-1711,Caterina Neef,TH Köln – Universität der Angewandte Wissenschaften,Towards Intelligent Pick and Place Assembly of Individualized Products   Using Reinforcement Learning,2020,"  Individualized manufacturing is becoming an important approach as a means to fulfill increasingly diverse and specific consumer requirements and expectations. While there are various solutions to the implementation of the manufacturing process, such as additive manufacturing, the subsequent automated assembly remains a challenging task. As an approach to this problem, we aim to teach a collaborative robot to successfully perform pick and place tasks by implementing reinforcement learning. For the assembly of an individualized product in a constantly changing manufacturing environment, the simulated geometric and dynamic parameters will be varied. Using reinforcement learning algorithms capable of meta-learning, the tasks will first be trained in simulation. They will then be performed in a real-world environment where new factors are introduced that were not simulated in training to confirm the robustness of the algorithms. The robot will gain its input data from tactile sensors, area scan cameras, and 3D cameras used to generate heightmaps of the environment and the objects. The selection of machine learning algorithms and hardware components as well as further research questions to realize the outlined production scenario are the results of the presented work. ",Kein DOI-Link verfügbar,2002.08333v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Towards Game-Playing AI Benchmarks via Performance Reporting Standards,2020,"  While games have been used extensively as milestones to evaluate game-playing AI, there exists no standardised framework for reporting the obtained observations. As a result, it remains difficult to draw general conclusions about the strengths and weaknesses of different game-playing AI algorithms. In this paper, we propose reporting guidelines for AI game-playing performance that, if followed, provide information suitable for unbiased comparisons between different AI approaches. The vision we describe is to build benchmarks and competitions based on such guidelines in order to be able to draw more general conclusions about the behaviour of different AI algorithms, as well as the types of challenges different games pose. ",Kein DOI-Link verfügbar,2007.02742v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Automating Speedrun Routing: Overview and Vision,2021,"  Speedrunning in general means to play a video game fast, i.e. using all means at one's disposal to achieve a given goal in the least amount of time possible. To do so, a speedrun must be planned in advance, or routed, as referred to by the community. This paper focuses on discovering challenges and defining models needed when trying to approach the problem of routing algorithmically. To do so, this paper is split in two parts. The first part provides an overview of relevant speedrunning literature, extracting vital information and formulating criticism. Important categorizations are pointed out and a nomenclature is built to support professional discussion. The second part of this paper then refers to the actual speedrun routing optimization problem. Different concepts of graph representations are presented and their potential is discussed. Visions both for problem modeling as well as solving are presented and assessed regarding suitability and expected challenges. Finally, a first assessment of the applicability of existing optimization methods to the defined problem is made, including metaheuristics/EA and Deep Learning methods. ",https://doi.org/10.1007/978-3-031-02462-7_30,2106.01182v3,Yes,potent(1)
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Demonstrating the Feasibility of Automatic Game Balancing,2016,"  Game balancing is an important part of the (computer) game design process, in which designers adapt a game prototype so that the resulting gameplay is as entertaining as possible. In industry, the evaluation of a game is often based on costly playtests with human players. It suggests itself to automate this process using surrogate models for the prediction of gameplay and outcome. In this paper, the feasibility of automatic balancing using simulation- and deck-based objectives is investigated for the card game top trumps. Additionally, the necessity of a multi-objective approach is asserted by a comparison with the only known (single-objective) method. We apply a multi-objective evolutionary algorithm to obtain decks that optimise objectives, e.g. win rate and average number of tricks, developed to express the fairness and the excitement of a game of top trumps. The results are compared with decks from published top trumps decks using simulation-based objectives. The possibility to generate decks better or at least as good as decks from published top trumps decks in terms of these objectives is demonstrated. Our results indicate that automatic balancing with the presented approach is feasible even for more complex games such as real-time strategy games. ",Kein DOI-Link verfügbar,1603.03795v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Surrogate-Assisted Partial Order-based Evolutionary Optimisation,2016,"  In this paper, we propose a novel approach (SAPEO) to support the survival selection process in multi-objective evolutionary algorithms with surrogate models - it dynamically chooses individuals to evaluate exactly based on the model uncertainty and the distinctness of the population. We introduce variants that differ in terms of the risk they allow when doing survival selection. Here, the anytime performance of different SAPEO variants is evaluated in conjunction with an SMS-EMOA using the BBOB bi-objective benchmark. We compare the obtained results with the performance of the regular SMS-EMOA, as well as another surrogate-assisted approach. The results open up general questions about the applicability and required conditions for surrogate-assisted multi-objective evolutionary algorithms to be tackled in the future. ",Kein DOI-Link verfügbar,1611.00260v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,The Influence of Color Stimuli on Adolescents' Emotion Playing Mobile   Games,2024,"  Video games elicit emotions which can be influenced by color stimuli as shown by previous studies. However, little research has been conducted on whether this applies to mobile games played by adolescents. Therefore, we examined the influence of color stimuli hue and saturation on mobile game play. Adolescents (n=21) played a mobile platformer game with varying hue and saturation per level for about 25 minutes. We gathered data on emotional states after each level using the Self-Assessment Manikin questionnaire, recorded time spent in each level, and collected participant self-reports on their video game experience. We performed statistical tests, such as ANOVA, which depict no significant influence of hue and/or saturation on the emotional state of our players. We conclude that it is possible that color alone is not an effective measure for eliciting emotion in mobile games, and further research is needed to consider measures such as time spent in the game and screen size, as these are unique to mobile games. There was a noticeable variance in emotional response between male and female players, with a significant interaction of hue and saturation among male players for valence ratings. This may be an indication that color preference influences perceived pleasantness. ",Kein DOI-Link verfügbar,2402.19064v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Expected Improvement versus Predicted Value in Surrogate-Based   Optimization,2020,"  Surrogate-based optimization relies on so-called infill criteria (acquisition functions) to decide which point to evaluate next. When Kriging is used as the surrogate model of choice (also called Bayesian optimization), one of the most frequently chosen criteria is expected improvement. We argue that the popularity of expected improvement largely relies on its theoretical properties rather than empirically validated performance. Few results from the literature show evidence, that under certain conditions, expected improvement may perform worse than something as simple as the predicted value of the surrogate model. We benchmark both infill criteria in an extensive empirical study on the `BBOB' function set. This investigation includes a detailed study of the impact of problem dimensionality on algorithm performance. The results support the hypothesis that exploration loses importance with increasing problem dimensionality. A statistical analysis reveals that the purely exploitative search with the predicted value criterion performs better on most problems of five or higher dimensions. Possible reasons for these results are discussed. In addition, we give an in-depth guide for choosing the infill criteria based on prior knowledge about the problem at hand, its dimensionality, and the available budget. ",Kein DOI-Link verfügbar,2001.02957v2,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Tools for Landscape Analysis of Optimisation Problems in Procedural   Content Generation for Games,2023,"  The term Procedural Content Generation (PCG) refers to the (semi-)automatic generation of game content by algorithmic means, and its methods are becoming increasingly popular in game-oriented research and industry. A special class of these methods, which is commonly known as search-based PCG, treats the given task as an optimisation problem. Such problems are predominantly tackled by evolutionary algorithms.   We will demonstrate in this paper that obtaining more information about the defined optimisation problem can substantially improve our understanding of how to approach the generation of content. To do so, we present and discuss three efficient analysis tools, namely diagonal walks, the estimation of high-level properties, as well as problem similarity measures. We discuss the purpose of each of the considered methods in the context of PCG and provide guidelines for the interpretation of the results received. This way we aim to provide methods for the comparison of PCG approaches and eventually, increase the quality and practicality of generated content in industry. ",https://doi.org/10.1016/j.asoc.2023.110121,2302.08479v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Optimally Weighted Ensembles of Regression Models: Exact Weight   Optimization and Applications,2022,"  Automated model selection is often proposed to users to choose which machine learning model (or method) to apply to a given regression task. In this paper, we show that combining different regression models can yield better results than selecting a single ('best') regression model, and outline an efficient method that obtains optimally weighted convex linear combination from a heterogeneous set of regression models. More specifically, in this paper, a heuristic weight optimization, used in a preceding conference paper, is replaced by an exact optimization algorithm using convex quadratic programming. We prove convexity of the quadratic programming formulation for the straightforward formulation and for a formulation with weighted data points. The novel weight optimization is not only (more) exact but also more efficient. The methods we develop in this paper are implemented and made available via github-open source. They can be executed on commonly available hardware and offer a transparent and easy to interpret interface. The results indicate that the approach outperforms model selection methods on a range of data sets, including data sets with mixed variable type from drug discovery applications. ",Kein DOI-Link verfügbar,2206.11263v1,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Towards Realistic Optimization Benchmarks: A Questionnaire on the   Properties of Real-World Problems,2020,"  Benchmarks are a useful tool for empirical performance comparisons. However, one of the main shortcomings of existing benchmarks is that it remains largely unclear how they relate to real-world problems. What does an algorithm's performance on a benchmark say about its potential on a specific real-world problem? This work aims to identify properties of real-world problems through a questionnaire on real-world single-, multi-, and many-objective optimization problems. Based on initial responses, a few challenges that have to be considered in the design of realistic benchmarks can already be identified. A key point for future work is to gather more responses to the questionnaire to allow an analysis of common combinations of properties. In turn, such common combinations can then be included in improved benchmark suites. To gather more data, the reader is invited to participate in the questionnaire at: https://tinyurl.com/opt-survey ",https://doi.org/10.1145/3377929.3389974,2004.06395v1,Yes,potent(1)
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Identifying Properties of Real-World Optimisation Problems through a   Questionnaire,2020,"  Optimisation algorithms are commonly compared on benchmarks to get insight into performance differences. However, it is not clear how closely benchmarks match the properties of real-world problems because these properties are largely unknown. This work investigates the properties of real-world problems through a questionnaire to enable the design of future benchmark problems that more closely resemble those found in the real world. The results, while not representative as they are based on only 45 responses, indicate that many problems possess at least one of the following properties: they are constrained, deterministic, have only continuous variables, require substantial computation times for both the objectives and the constraints, or allow a limited number of evaluations. Properties like known optimal solutions and analytical gradients are rarely available, limiting the options in guiding the optimisation process. These are all important aspects to consider when designing realistic benchmark problems. At the same time, the design of realistic benchmarks is difficult, because objective functions are often reported to be black-box and many problem properties are unknown. To further improve the understanding of real-world problems, readers working on a real-world optimisation problem are encouraged to fill out the questionnaire: https://tinyurl.com/opt-survey ",Kein DOI-Link verfügbar,2011.05547v2,No,
0000-0002-8969-4795,Boris Naujoks,TH Köln – Universität der Angewandte Wissenschaften,Benchmarking in Optimization: Best Practice and Open Issues,2020,"  This survey compiles ideas and recommendations from more than a dozen researchers with different backgrounds and from different institutes around the world. Promoting best practice in benchmarking is its main goal. The article discusses eight essential topics in benchmarking: clearly stated goals, well-specified problems, suitable algorithms, adequate performance measures, thoughtful analysis, effective and efficient designs, comprehensible presentations, and guaranteed reproducibility. The final goal is to provide well-accepted guidelines (rules) that might be useful for authors and reviewers. As benchmarking in optimization is an active and evolving field of research this manuscript is meant to co-evolve over time by means of periodic updates. ",Kein DOI-Link verfügbar,2007.03488v2,No,
0000-0002-8348-8958,Alexandra Kapp,"Hochschule für Technik und Wirtschaft Berlin, Universität der Angewandte Wissenschaften","Collection, usage and privacy of mobility data in the enterprise and   public administrations",2024,"  Human mobility data is a crucial resource for urban mobility management, but it does not come without personal reference. The implementation of security measures such as anonymization is thus needed to protect individuals' privacy. Often, a trade-off arises as such techniques potentially decrease the utility of the data and limit its use. While much research on anonymization techniques exists, there is little information on the actual implementations by practitioners, especially outside the big tech context. Within our study, we conducted expert interviews to gain insights into practices in the field. We categorize purposes, data sources, analysis, and modeling tasks to provide a profound understanding of the context such data is used in. We survey privacy-enhancing methods in use, which generally do not comply with state-of-the-art standards of differential privacy. We provide groundwork for further research on practice-oriented research by identifying privacy needs of practitioners and extracting relevant mobility characteristics for future standardized evaluations of privacy-enhancing methods. ",https://doi.org/10.56553/popets-2022-0117,2407.03732v1,Yes,potent(1)
0000-0002-8348-8958,Alexandra Kapp,"Hochschule für Technik und Wirtschaft Berlin, Universität der Angewandte Wissenschaften",Reconsidering utility: unveiling the limitations of synthetic mobility   data generation algorithms in real-life scenarios,2024,"  In recent years, there has been a surge in the development of models for the generation of synthetic mobility data. These models aim to facilitate the sharing of data while safeguarding privacy, all while ensuring high utility and flexibility regarding potential applications. However, current utility evaluation methods fail to fully account for real-life requirements. We evaluate the utility of five state-of-the-art synthesis approaches, each with and without the incorporation of differential privacy (DP) guarantees, in terms of real-world applicability. Specifically, we focus on so-called trip data that encode fine granular urban movements such as GPS-tracked taxi rides. Such data prove particularly valuable for downstream tasks at the road network level. Thus, our initial step involves appropriately map matching the synthetic data and subsequently comparing the resulting trips with those generated by the routing algorithm implemented in OpenStreetMap, which serves as an efficient and privacy-friendly baseline. Out of the five evaluated models, one fails to produce data within reasonable computation time and another generates too many jumps to meet the requirements for map matching. The remaining three models succeed to a certain degree in maintaining spatial distribution, one even with DP guarantees. However, all models struggle to produce meaningful sequences of geo-locations with reasonable trip lengths and to model traffic flow at intersections accurately. It is important to note that trip data encompasses various relevant characteristics beyond spatial distribution, such as temporal information, all of which are discarded by these models. Consequently, our results imply that current synthesis models fall short in their promise of high utility and flexibility. ",https://doi.org/10.1145/3589132.36256,2407.03237v1,Yes,potent(1)
0000-0002-8348-8958,Alexandra Kapp,"Hochschule für Technik und Wirtschaft Berlin, Universität der Angewandte Wissenschaften",Generative Models for Synthetic Urban Mobility Data: A Systematic   Literature Review,2024,"  Although highly valuable for a variety of applications, urban mobility data is rarely made openly available as it contains sensitive personal information. Synthetic data aims to solve this issue by generating artificial data that resembles an original dataset in structural and statistical characteristics, but omits sensitive information. For mobility data, a large number of corresponding models have been proposed in the last decade. This systematic review provides a structured comparative overview of the current state of this heterogeneous, active field of research. A special focus is put on the applicability of the reviewed models in practice. ",https://doi.org/10.1145/3610224,2407.09198v1,No,
0000-0002-8348-8958,Alexandra Kapp,"Hochschule für Technik und Wirtschaft Berlin, Universität der Angewandte Wissenschaften",StreetSurfaceVis: a dataset of crowdsourced street-level imagery with   semi-automated annotations of road surface type and quality,2024,"  Road unevenness significantly impacts the safety and comfort of various traffic participants, especially vulnerable road users such as cyclists and wheelchair users. This paper introduces StreetSurfaceVis, a novel dataset comprising 9,122 street-level images collected from a crowdsourcing platform and manually annotated by road surface type and quality. The dataset is intended to train models for comprehensive surface assessments of road networks. Existing open datasets are constrained by limited geospatial coverage and camera setups, typically excluding cycleways and footways. By crafting a heterogeneous dataset, we aim to fill this gap and enable robust models that maintain high accuracy across diverse image sources. However, the frequency distribution of road surface types and qualities is highly imbalanced. We address the challenge of ensuring sufficient images per class while reducing manual annotation by proposing a sampling strategy that incorporates various external label prediction resources. More precisely, we estimate the impact of (1) enriching the image data with OpenStreetMap tags, (2) iterative training and application of a custom surface type classification model, (3) amplifying underrepresented classes through prompt-based classification with GPT-4o or similarity search using image embeddings. We show that utilizing a combination of these strategies effectively reduces manual annotation workload while ensuring sufficient class representation. ",Kein DOI-Link verfügbar,2407.21454v2,No,
0000-0002-8348-8958,Alexandra Kapp,"Hochschule für Technik und Wirtschaft Berlin, Universität der Angewandte Wissenschaften",Towards Standardized Mobility Reports with User-Level Privacy,2022,"  The importance of human mobility analyses is growing in both research and practice, especially as applications for urban planning and mobility rely on them. Aggregate statistics and visualizations play an essential role as building blocks of data explorations and summary reports, the latter being increasingly released to third parties such as municipal administrations or in the context of citizen participation. However, such explorations already pose a threat to privacy as they reveal potentially sensitive location information, and thus should not be shared without further privacy measures.   There is a substantial gap between state-of-the-art research on privacy methods and their utilization in practice. We thus conceptualize a standardized mobility report with differential privacy guarantees and implement it as open-source software to enable a privacy-preserving exploration of key aspects of mobility data in an easily accessible way. Moreover, we evaluate the benefits of limiting user contributions using three data sets relevant to research and practice. Our results show that even a strong limit on user contribution alters the original geospatial distribution only within a comparatively small range, while significantly reducing the error introduced by adding noise to achieve privacy guarantees. ",https://doi.org/10.1080/17489725.2022.2148008,2209.08921v1,Yes,potent(1)
0000-0002-2750-0502,Denis Hock,Frankfurt Universität der Angewandte Wissenschaften,Robust Consensus-Based Network Intrusion Detection in Presence of   Byzantine Attacks,2016,"  Consensus algorithms provide strategies to solve problems in a distributed system with the added constraint that data can only be shared between adjacent computing nodes. We find these algorithms in applications for wireless and sensor networks, spectrum sensing for cognitive radio, even for some IoT services. However, consensus-based applications are not resilient to compromised nodes sending falsified data to their neighbors, i.e. they can be the target of Byzantine attacks. Several solutions have been proposed in the literature inspired from reputation based systems, outlier detection or model-based fault detection techniques in process control. We have reviewed some of these solutions, and propose two mitigation techniques to protect the consensus-based Network Intrusion Detection System in \cite{toulouse2015consensus}. We analyze several implementation issues such as computational overhead, fine tuning of the solution parameters, impacts on the convergence of the consensus phase, accuracy of the intrusion detection system. ",Kein DOI-Link verfügbar,1611.04227v1,No,
0000-0002-2059-8084,Eric Guiffo Kaigom,Frankfurt Universität der Angewandte Wissenschaften,Potentials of the Metaverse for Robotized Applications in Industry 4.0   and Industry 5.0,2024,"  As a digital environment of interconnected virtual ecosystems driven by measured and synthesized data, the Metaverse has so far been mostly considered from its gaming perspective that closely aligns with online edutainment. Although it is still in its infancy and more research as well as standardization efforts remain to be done, the Metaverse could provide considerable advantages for smart robotized applications in the industry.Workflow efficiency, collective decision enrichment even for executives, as well as a natural, resilient, and sustainable robotized assistance for the workforce are potential advantages. Hence, the Metaverse could consolidate the connection between Industry 4.0 and Industry 5.0. This paper identifies and puts forward potential advantages of the Metaverse for robotized applications and highlights how these advantages support goals pursued by the Industry 4.0 and Industry 5.0 visions.   Keywords: Robotics, Metaverse, Digital Twin, VR/AR, AI/ML, Foundation Model; ",https://doi.org/10.1016/j.procs.2024.02.005,2404.00783v2,Yes,potent(2)
0000-0002-2059-8084,Eric Guiffo Kaigom,Frankfurt Universität der Angewandte Wissenschaften,"Metarobotics for Industry and Society: Vision, Technologies, and   Opportunities",2024,"  Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determination, self-efficacy, and work-life-flexibility in robotics-related applications in Society 5.0, Industry 4.0, and Industry 5.0 are outlined. ",https://doi.org/10.1109/TII.2023.3337380,2404.00797v2,Yes,potent(1)
0000-0002-8939-1858,Besfort Shala,Frankfurt Universität der Angewandte Wissenschaften,The Probabilistic Zeta Function of a Finite Lattice,2021,"  We study Brown's definition of the probabilistic zeta function of a finite lattice as a generalization of that of a finite group. We propose a natural alternative or extension that may be better suited for non-atomistic lattices. The probabilistic zeta function admits a general Dirichlet series expression, which unlike for groups, need not be ordinary. We compute the function for several examples of finite lattices, establishing a connection with the Stirling numbers of the second kind in the case of the divisibility lattice. Furthermore, in the context of moving from groups to lattices, we are interested in lattices with probabilistic zeta function given by ordinary Dirichlet series. In this regard, we focus on partition lattices and $d$-divisible partition lattices. Using the prime number theorem, we show that the probabilistic zeta functions of the latter typically fail to be ordinary Dirichlet series. ",Kein DOI-Link verfügbar,2112.13766v6,No,
0009-0004-1939-0026,Rohan Tiwari,Frankfurt Universität der Angewandte Wissenschaften,"Mission-level Robustness with Rapidly-deployed, Autonomous Aerial   Vehicles by Carnegie Mellon Team Tartan at MBZIRC 2020",2021,"  For robotic systems to succeed in high risk, real-world situations, they have to be quickly deployable and robust to environmental changes, under-performing hardware, and mission subtask failures. These robots are often designed to consider a single sequence of mission events, with complex algorithms lowering individual subtask failure rates under some critical constraints. Our approach utilizes common techniques in vision and control, and encodes robustness into mission structure through outcome monitoring and recovery strategies. In addition, our system infrastructure enables rapid deployment and requires no central communication. This report also includes lessons in rapid field robotic development and testing. We developed and evaluated our systems through real-robot experiments at an outdoor test site in Pittsburgh, Pennsylvania, USA, as well as in the 2020 Mohamed Bin Zayed International Robotics Challenge. All competition trials were completed in fully autonomous mode without RTK-GPS. Our system placed fourth in Challenge 2 and seventh in the Grand Challenge, with notable achievements such as popping five balloons (Challenge 1), successfully picking and placing a block (Challenge 2), and dispensing the most water onto an outdoor, real fire with an autonomous UAV (Challenge 3). ",https://doi.org/10.55417/fr.2022007,2107.01507v2,Yes,notable(1)
0000-0003-4812-0928,Karsten Wendland,Hochschule Aalen,Dürfen Maschinen denken (können)? Warum Künstliche Intelligenz   eine Ethik braucht. (Are Machines Allowed to (be able to) Think? Why   Artificial Intelligence Needs Ethics),2022,"  Speech manuscript (German + English) of the impulse lecture for the panel discussion ""May machines (be able to) think?"" at the 102nd Katholikentag on May 28, 2022 in Stuttgart. Panel: Winfried Kretschmann (MdL, Prime Minister Baden-W\""urttemberg, Stuttgart), Ursula Nothelle-Wildfeuer (Freiburg), Michael Resch (Stuttgart),Karsten Wendland (Aalen). Moderation: Stefanie Rentsch (Fulda). Advocate of the audience: Verena Neuhausen (Stuttgart). ",https://doi.org/10.48800/opus-2121,2208.07402v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of Cool Stars,2014,"  The measurement of oscillations excited by surface convection is a powerful method to study the structure and evolution of cool stars. CoRoT and Kepler have initiated a revolution in asteroseismology by detecting oscillations in thousands of stars from the main sequence to the red-giant branch, including a large number of exoplanet host stars. In this contribution I will review recent asteroseismic results, focusing in particular on the internal rotation of red giant stars and the impact of asteroseismology on the characterization of exoplanets. ",Kein DOI-Link verfügbar,1411.1063v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Self-Gravitating N-Body Systems out of Equilibrium,2000,"  Real physical systems are often maintained off equilibrium by energy or matter flows. If these systems are far from equilibrium then the thermodynamical branch become unstable and fluctuations can lead them to other more stable states. These new states are often endowed with higher degrees of organization. In order to explore whether an energy-flow in combination with self-gravity can lead to complex, inhomogeneous structures, like observed in the interstellar medium (ISM), we perform N-body simulations of self-gravitating systems subjected to an energy-flow. Moreover we perform some simple gravo-thermal N-body experiments and compare them with theoretical results. We find negative specific heat in an energy range as predicted by Follana & Laliena (1999). ",Kein DOI-Link verfügbar,astro-ph/0010383v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"N-Body Simulations of Open, Self-Gravitating Systems",2001,"  Astrophysical systems differ often in two points from classical thermodynamical systems: 1.) They are open and 2.) gravity is a dominant factor. Both modifies the homogeneous equilibrium structure, known from classical thermodynamics. In order to study the consequence for structure formation in astrophysical systems, we carry out N-body simulations of self-gravitating systems, subjected to an energy-flow. The simulations show that physically realistic, time-dependent boundary conditions can maintain a molecular cloud in a statistically steady state, out of thermodynamic equilibrium.   Moreover we perform some simple ""gravo-thermal"" N-body experiments and compare them with theoretical results. We find negative specific heat in an energy range predicted by Follana and Laliena (1999). ",Kein DOI-Link verfügbar,astro-ph/0101265v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of Eclipsing Binary Stars in the Kepler Era,2014,"  Eclipsing binary stars have long served as benchmark systems to measure fundamental stellar properties. In the past few decades, asteroseismology - the study of stellar pulsations - has emerged as a new powerful tool to study the structure and evolution of stars across the HR diagram. Pulsating stars in eclipsing binary systems are particularly valuable since fundamental properties (such as radii and masses) can determined using two independent techniques. Furthermore, independently measured properties from binary orbits can be used to improve asteroseismic modeling for pulsating stars in which mode identifications are not straightforward. This contribution provides a review of asteroseismic detections in eclipsing binary stars, with a focus on space-based missions such as CoRoT and Kepler, and empirical tests of asteroseismic scaling relations for stochastic (""solar-like"") oscillations. ",https://doi.org/10.1007/978-3-319-09198-3_7,1404.7501v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of Exoplanet Host Stars,2015,"  Asteroseismology is among the most powerful observational tools to determine fundamental properties of stars. Space-based photometry has recently enabled the systematic detection of oscillations in exoplanet host stars, allowing a combination of asteroseismology with transit and radial-velocity measurements to characterize planetary systems. In this contribution I will review the key synergies between asteroseismology and exoplanet science such as the precise determination of radii and ages of exoplanet host stars, as well as applications of asteroseismology to measure spin-orbit inclinations in multiplanet systems and orbital eccentricities of small planets. Finally I will give a brief outlook on asteroseismic studies of exoplanet hosts with current and future space-based missions such as K2 and TESS. ",Kein DOI-Link verfügbar,1511.07441v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Precision Stellar Astrophysics in the Kepler Era,2016,"  The study of fundamental properties (such as temperatures, radii, masses, and ages) and interior processes (such as convection and angular momentum transport) of stars has implications on various topics in astrophysics, ranging from the evolution of galaxies to understanding exoplanets. In this contribution I will review the basic principles of two key observational methods for constraining fundamental and interior properties of single field stars: the study stellar oscillations (asteroseismology) and optical long-baseline interferometry. I will highlight recent breakthrough discoveries in asteroseismology such as the measurement of core rotation rates in red giants and the characterization of exoplanet systems. I will furthermore comment on the reliability of interferometry as a tool to calibrate indirect methods to estimate fundamental properties, and present a new angular diameter measurement for the exoplanet host star HD219134 which demonstrates that diameters for stars which are relatively well resolved (~> 1 mas for the K band) are consistent across different instruments. Finally I will discuss the synergy between asteroseismology and interferometry to test asteroseismic scaling relations, and give a brief outlook on the expected impact of space-based missions such as K2, TESS and Gaia. ",Kein DOI-Link verfügbar,1604.07442v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Scaling Laws in Self-Gravitating Disks,1999,"  The interstellar medium (ISM) reveals strongly inhomogeneous structures at every scale. These structures do not seem completely random since they obey certain power laws. Larson's law (\citeyear{Larson81}) $\sigma \propto R^{\delta}$ and the plausible assumption of virial equilibrium justify to consider fractals as a possible description of the ISM. In the following we investigate how self-gravitation, differential rotation and dissipation affect the matter distribution in galaxies. To this end we have performed 3D-simulations for self-gravitating local boxes embedded in a larger disk, extending the 2D-method of Toomre & Kalnajs (\citeyear{Toomre91}) and Wisdom & Tremaine (\citeyear{Wisdom88}). Our simulations lead to the conclusion that gravitation, shearing and dissipation can be dominantly responsible for maintaining an inhomogeneous and eventually a fractal distribution of the matter. ",https://doi.org/10.1023/A:1017532808950,astro-ph/9904209v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Fragmentation in Kinematically Cold Disks,2001,"  Gravity is scale free. Thus gravity may form similar structures in self-gravitating systems on different scales. Indeed, observations of the interstellar medium, spiral disks and cosmic structures, reveal similar characteristics. The structures in these systems are very lumpy and inhomogeneous. Moreover some of these structures do not seem to be of random nature, but obey certain power laws.   Models of slightly dissipative self-gravitating disks show how such inhomogeneous structures can be maintained on the kpc-scale. The basic physical processes in these models are self-gravity, dissipation and differential rotation. In order to explore the structures resulting from these processes, local simulations of self-gravitating disks are performed in 2D and 3D. We observe persistent patterns, formed by transient structures, whose intensity and morphological characteristic depend on the dissipation rate. ",Kein DOI-Link verfügbar,astro-ph/0101264v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Lumpy Structures in Self-Gravitating Disks,2001,"  Following Toomre & Kalnajs (1991), local models of slightly dissipative self-gravitating disks show how inhomogeneous structures can be maintained over several galaxy rotations. Their basic physical ingredients are self-gravity, dissipation and differential rotation. In order to explore the structures resulting from these processes on the kpc scale, local simulation of self-gravitating disks are performed in this paper in 2D as well as in 3D. The third dimension becomes a priori important as soon as matter clumping causes a tight coupling of the 3D equations of motion. The physically simple and general framework of the model permits to make conclusions beyond the here considered scales. A time dependent affine coordinate system is used, allowing to calculate the gravitational forces via a particle-mesh FFT-method, increasing the performance with respect to previous direct force calculations. Persistent patterns, formed by transient structures, whose intensity and morphological characteristic depend on the dissipation rate are obtained and described. Some of our simulations reveal first signs of mass-size and velocity dispersion-size power-law relations, but a clear scale invariant behavior will require more powerful computer techniques. ",https://doi.org/10.1051/0004-6361:20010703,astro-ph/0105501v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Long-Range Correlations in Self-Gravitating N-Body Systems,2002,"  Observed self-gravitating systems reveal often fragmented non-equilibrium structures that feature characteristic long-range correlations. However, models accounting for non-linear structure growth are not always consistent with observations and a better understanding of self-gravitating $N$-body systems appears necessary. Because unstable gravitating systems are sensitive to non-gravitational perturbations we study the effect of different dissipative factors as well as different small and large scale boundary conditions on idealized $N$-body systems. We find, in the interval of negative specific heat, equilibrium properties differing from theoretical predictions made for gravo-thermal systems, substantiating the importance of microscopic physics and the lack of consistent theoretical tools to describe self-gravitating gas. Also, in the interval of negative specific heat, yet outside of equilibrium, unforced systems fragment and establish transient long-range correlations. The strength of these correlations depends on the degree of granularity, suggesting to make the resolution of mass and force coherent. Finally, persistent correlations appear in model systems subject to an energy flow. ",https://doi.org/10.1051/0004-6361:20020232,astro-ph/0201470v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Synergies between Asteroseismology and Exoplanetary Science,2017,"  Over the past decade asteroseismology has become a powerful method to systematically characterize host stars and dynamical architectures of exoplanet systems. In this contribution I review current key synergies between asteroseismology and exoplanetary science such as the precise determination of planet radii and ages, the measurement of orbital eccentricities, stellar obliquities and their impact on hot Jupiter formation theories, and the importance of asteroseismology on spectroscopic analyses of exoplanet hosts. I also give an outlook on future synergies such as the characterization of sub-Neptune-size planets orbiting solar-type stars, the study of planet populations orbiting evolved stars, and the determination of ages of intermediate-mass stars hosting directly imaged planets. ",https://doi.org/10.1007/978-3-319-59315-9_6,1711.01281v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,A Mean-Field Model for Extended Stochastic Systems with Distributed Time   Delays,2005,"  A network of noisy bistable elements with global time-delayed couplings is considered. A dichotomous mean field model has recently been developed describing the collective dynamics in such systems with uniform time delays near the bifurcation points. Here the theory is extended and applied to systems with nonuniform time delays. For strong enough couplings the systems exhibit delay-independent stationary states and delay-dependent oscillatory states. We find that the regions of oscillatory states in the parameter space are reduced with increasing width of the time delay distribution function; that is, nonuniformity of the time delays increases the stability of the trivial equilibrium. However, for symmetric distribution functions the properties of the oscillatory states depend only on the mean time delay. ",Kein DOI-Link verfügbar,cond-mat/0501650v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Cooperative Dynamics in a Network of Stochastic Elements with Delayed   Feedback,2005,"  Networks of globally coupled, noise activated, bistable elements with connection time delays are considered. The dynamics of these systems is studied numerically using a Langevin description and analytically using (1) a Gaussian approximation as well as (2) a dichotomous model. The system demonstrates ordering phase transitions and multi-stability. That is, for a strong enough feedback it exhibits nontrivial stationary states and oscillatory states whose frequencies depend only on the mean of the time delay distribution function. Other observed dynamical phenomena include coherence resonance and, in the case of non-uniform coupling strengths, amplitude death and chaos. Furthermore, an increase of the stability of the trivial equilibrium with increasing non-uniformity of the time delays is observed. ",https://doi.org/10.1103/PhysRevE.71.036150,cond-mat/0503287v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Precise Time-Domain Asteroseismology and a Revised Target List for TESS   Solar-Like Oscillators,2024,"  The TESS mission has provided a wealth of asteroseismic data for solar-like oscillators. However, these data are subject to varying cadences, large gaps, and unequal sampling, which complicates analysis in the frequency domain. One solution is to model the oscillations in the time domain by treating them as stochastically damped simple harmonic oscillators through a linear combination of Gaussian Process kernels. We demonstrate this method on the well-studied subgiant star nu Indi and a sample of Kepler red giant stars observed by TESS, finding that the time domain model achieves an almost two-fold increase in accuracy for measuring {\nu}max compared to typical frequency domain methods. To apply the method to new detections, we use stellar parameters from Gaia DR3 and the TESS input catalog to calculate revised asteroseismic detection probabilities for all TESS input catalog targets with T<12 mag and a predicted {\nu}max>240{\mu}Hz. We also provide a software tool to calculate detection probabilities for any target of interest. Using the updated detection probabilities we show that time-domain asteroseismology is sensitive enough to recover marginal detections, which may explain the current small number of frequency-based detections of TESS oscillations compared to pre-flight expectations. ",Kein DOI-Link verfügbar,2403.02489v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Guaranteed Parameter Estimation for Discrete Energy Minimization,2017,"  Structural learning, a method to estimate the parameters for discrete energy minimization, has been proven to be effective in solving computer vision problems, especially in 3D scene parsing. As the complexity of the models increases, structural learning algorithms turn to approximate inference to retain tractability. Unfortunately, such methods often fail because the approximation can be arbitrarily poor. In this work, we propose a method to overcome this limitation through exploiting the properties of the joint problem of training time inference and learning. With the help of the learning framework, we transform the inapproximable inference problem into a polynomial time solvable one, thereby enabling tractable exact inference while still allowing an arbitrary graph structure and full potential interactions. Our learning algorithm is guaranteed to return a solution with a bounded error to the global optimal within the feasible parameter space. We demonstrate the effectiveness of this method on two point cloud scene parsing datasets. Our approach runs much faster and solves a problem that is intractable for previous, well-known approaches. ",Kein DOI-Link verfügbar,1701.03151v1,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Dynamics in a Bistable-Element-Network with Delayed Coupling and Local   Noise,2003,"  The dynamics of an ensemble of bistable elements under the influence of noise and with global time-delayed coupling is studied numerically by using a Langevin description and analytically by using 1) a Gaussian approximation and 2) a dichotomous model. We find that for a strong enough positive feedback the system undergoes a phase transition and adopts a non-zero stationary mean field. A variety of coexisting oscillatory mean field states are found for positive and negative couplings. The magnitude of the oscillatory states is maximal for a certain noise temperature, i.e., the system demonstrates the phenomenon of coherence resonance. While away form the transition points the system dynamics is well described by the Gaussian approximation, near the bifurcations it is more adequately described by the dichotomous model. ",https://doi.org/10.1063/1.1764201,cond-mat/0310314v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Solar-Like Oscillations: Lessons Learned & First Results from TESS,2020,"  Solar-like oscillations are excited in cool stars with convective envelopes and provide a powerful tool to constrain fundamental stellar properties and interior physics. We provide a brief history of the detection of solar-like oscillations, focusing in particular on the space-based photometry revolution started by the CoRoT and Kepler Missions. We then discuss some of the lessons learned from these missions, and highlight the continued importance of smaller space telescopes such as BRITE constellation to characterize very bright stars with independent observational constraints. As an example, we use BRITE observations to measure a tentative surface rotation period of 28.3+/-0.5 days for alpha Cen A, which has so far been poorly constrained. We also discuss the expected yields of solar-like oscillators from the TESS Mission, demonstrating that TESS will complement Kepler by discovering oscillations in a large number of nearby subgiants, and present first detections of oscillations in TESS exoplanet host stars. ",Kein DOI-Link verfügbar,2007.02170v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Complexity of Discrete Energy Minimization Problems,2016,"  Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models. The problem, in general, is notoriously intractable, and finding the global optimal solution is known to be NP-hard. However, is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time? We show in this paper that the answer is no. Specifically, we show that general energy minimization, even in the 2-label pairwise case, and planar energy minimization with three or more labels are exp-APX-complete. This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems, including constant factor approximations. Moreover, we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes -- PO, APX, and exp-APX, corresponding to problems that are solvable, approximable, and inapproximable in polynomial time. Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones. This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms. ",Kein DOI-Link verfügbar,1607.08905v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Mixed-mode Ensemble Asteroseismology of Low-Luminosity Kepler Red Giants,2023,"  We present measurements of the dipole mode asymptotic period spacing ($\Delta\Pi_1$), the coupling factor between p- and g- modes ($q$), the g-mode phase offset ($\epsilon_g$), and the mixed-mode frequency rotational splitting ($\delta\nu_{\mathrm{rot}}$) for 1,074 low-luminosity red giants from the Kepler mission. Using oscillation mode frequencies extracted from each star, we apply Bayesian optimization to estimate $\Delta\Pi_1$ from the power spectrum of the stretched period spectrum and to perform the subsequent forward modelling of the mixed-mode frequencies. With our measurements, we show that the mode coupling factor $q$ shows significant anti-correlation with both stellar mass and metallicity, and can reveal highly metal-poor stars. We present the evolution of $\epsilon_g$ up the lower giant branch up to before the luminosity bump, and find no significant trends in $\epsilon_g$ or $\delta\nu_{\mathrm{rot}}$ with stellar mass and metallicity in our sample. Additionally, we identify six new red giants showing anomalous distortions in their g-mode pattern. Our data products, code, and results are provided in a public repository. ",Kein DOI-Link verfügbar,2307.06482v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Semiconductor quantum dots as an ideal source of polarization entangled   photon pairs on-demand: a review,2018,"  More than 80 years passed since the first publication on entangled quantum states. In this period of time the concept of spookily interacting quantum states became an emerging field of science. After various experiments proving the existence of such non-classical states, visionary ideas were put forward to exploit entanglement in quantum information science and technology. These novel concepts have not yet come out of the experimental stage, mostly because of the lack of suitable, deterministic sources of entangled quantum states. Among many systems under investigation, semiconductor quantum dots are particularly appealing emitters of on-demand, single polarization-entangled photon-pairs. Although, it was originally believed that quantum dots must exhibit a limited degree of entanglement related to numerous decoherence effects present in the solid-state. Recent studies invalidated the premise of unavoidable entanglement degrading effects. We review the relevant experiments which have led to these important discoveries and discuss the remaining challenges for the anticipated quantum technologies. ",https://doi.org/10.1088/2040-8986/aac4c4,1804.10472v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Evidence for spatially-correlated ${\it Gaia}$ parallax errors in the   ${\it Kepler}$ field,2017,"  We present evidence for a spatially-dependent systematic error in the first data release of ${\it Gaia}$ parallaxes based on comparisons to asteroseismic parallaxes in the ${\it Kepler}$ field, and present a parametrized model of the angular dependence of these systematics. We report an error of $0.059^{+0.004}_{-0.004}$mas on scales of 0.3deg, which decreases for larger scales to become $0.011^{+0.006}_{-0.004}$mas at 8deg. This is consistent with the $\sim2\%$ zeropoint offset for the whole sample discussed by Huber et al., and is compatible with the effect predicted by the ${\it Gaia}$ team. Our results are robust to dust prescriptions and choices in temperature scales used to calculate asteroseismic parallaxes. We also do not find evidence for significant differences in the signal when using red clump versus red giant stars. Our approach allows us to quantify and map the correlations in an astrophysically interesting field, resulting in a parametrized model of the spatial systematics that can be used to construct a covariance matrix for any work that relies upon TGAS parallaxes. ",https://doi.org/10.3847/1538-4357/aa7c1c,1706.09416v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,$\texttt{pySYD}$: Automated measurements of global asteroseismic   parameters,2021,"  Asteroseismology is well-established in astronomy as the gold standard for determining precise and accurate fundamental stellar properties like masses, radii, and ages. Several tools have been developed for asteroseismic analyses but many of them are closed-source and therefore not accessible to the general astronomy community. Here we present $\texttt{pySYD}$, a Python package for detecting solar-like oscillations and measuring global asteroseismic parameters. $\texttt{pySYD}$ was adapted from the IDL-based $\texttt{SYD}$ pipeline, which was extensively used to measure asteroseismic parameters for $\textit{Kepler}$ stars. $\texttt{pySYD}$ was developed using the same well-tested methodology and comes with several new improvements to provide accessible and reproducible results. Well-documented, open-source asteroseismology software that has been benchmarked against closed-source tools are critical to ensure the reproducibility of legacy results from the $\textit{Kepler}$ mission. Moreover, $\texttt{pySYD}$ will also be a promising tool for the broader astronomy community to analyze current and forthcoming data from the NASA TESS mission. ",https://doi.org/10.5281/zenodo.5140574,2108.00582v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Investigating APOKASC Red Giant Stars with Abnormal Carbon to Nitrogen   Ratios,2023,"  The success of galactic archaeology and the reconstruction of the formation history of our galaxy critically relies on precise ages for large populations of stars. For evolved stars in the red clump and red giant branch, the carbon to nitrogen ratio ([C/N]) has recently been identified as a powerful diagnostic of mass and age that can be applied to stellar samples from spectroscopic surveys such as SDSS/APOGEE. Here, we show that at least 10\% of red clump stars and %$\approx 10\%$ of red giant branch stars deviate from the standard relationship between [C/N] and mass. {We use the APOGEE-\kepler\ (APOKASC) overlap sample to show that binary interactions are %the majority contributors to these responsible for the majority of these outliers and that stars with %any indicators of current or previous binarity should be excluded from galactic archaeology analyses that rely on [C/N] abundances to infer stellar masses. We also show that the %standard DR14 APOGEE analysis overestimates the surface gravities for even moderately rotating giants (vsini$>2$ km/s)} ",Kein DOI-Link verfügbar,2310.19872v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Six new rapidly oscillating Ap stars in the Kepler long-cadence data   using super-Nyquist asteroseismology,2019,"  We perform a search for rapidly oscillating Ap stars in the Kepler long-cadence data, where true oscillations above the Nyquist limit of 283.21 {\mu}Hz can be reliably distinguished from aliases as a consequence of the barycentric time corrections applied to the Kepler data. We find evidence for rapid oscillations in six stars: KIC 6631188, KIC 7018170, KIC 10685175, KIC 11031749, KIC 11296437 and KIC 11409673, and identify each star as chemically peculiar through either pre-existing classifications or spectroscopic measurements. For each star, we identify the principal pulsation mode, and are able to observe several additional pulsation modes in KIC 7018170. We find that KIC 7018170 and KIC 11409673 both oscillate above their theoretical acoustic cutoff frequency, whilst KIC 11031749 oscillates at the cutoff frequency within uncertainty. All but KIC 11031749 exhibit strong amplitude modulation consistent with the oblique pulsator model, confirming their mode geometry and periods of rotation. ",https://doi.org/10.1093/mnras/stz1633,1906.04353v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Automated extraction of oscillation parameters for Kepler observations   of solar-type stars,2009,"  The recent launch of the Kepler space telescope brings the opportunity to study oscillations systematically in large numbers of solar-like stars. In the framework of the asteroFLAG project, we have developed an automated pipeline to estimate global oscillation parameters, such as the frequency of maximum power (nu_max) and the large frequency spacing (Delta_nu), for a large number of time series. We present an effective method based on the autocorrelation function to find excess power and use a scaling relation to estimate granulation timescales as initial conditions for background modelling. We derive reliable uncertainties for nu_max and Delta_nu through extensive simulations. We have tested the pipeline on about 2000 simulated Kepler stars with magnitudes of V~7-12 and were able to correctly determine nu_max and Delta_nu for about half of the sample. For about 20%, the returned large frequency spacing is accurate enough to determine stellar radii to a 1% precision. We conclude that the methods presented here are a promising approach to process the large amount of data expected from Kepler. ",Kein DOI-Link verfügbar,0910.2764v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Swan: Data-Driven Inference of Stellar Surface Gravities for Cool   Stars from Photometric Light Curves,2020,"  Stellar light curves are well known to encode physical stellar properties. Precise, automated and computationally inexpensive methods to derive physical parameters from light curves are needed to cope with the large influx of these data from space-based missions such as Kepler and TESS. Here we present a new methodology which we call The Swan, a fast, generalizable and effective approach for deriving stellar surface gravity ($\log g$) for main sequence, subgiant and red giant stars from Kepler light curves using local linear regression on the full frequency content of Kepler long cadence power spectra. With this inexpensive data-driven approach, we recover $\log g$ to a precision of $\sim$0.02 dex for 13,822 stars with seismic $\log g$ values between 0.2-4.4 dex, and $\sim$0.11 dex for 4,646 stars with Gaia derived $\log g$ values between 2.3-4.6 dex. We further develop a signal-to-noise metric and find that granulation is difficult to detect in many cool main sequence stars ($T_{\text{eff}}$ $\lesssim$ 5500 K), in particular K dwarfs. By combining our $\log g$ measurements with Gaia radii, we derive empirical masses for 4,646 subgiant and main sequence stars with a median precision of $\sim$7%. Finally, we demonstrate that our method can be used to recover $\log g$ to a similar mean absolute deviation precision for TESS-baseline of 27 days. Our methodology can be readily applied to photometric time-series observations to infer stellar surface gravities to high precision across evolutionary states. ",https://doi.org/10.3847/1538-3881/abdf4c,2011.10062v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Eclipse timing the Milky Way's gravitational potential,2021,"  We show that a small, but \textit{measurable} shift in the eclipse mid-point time of eclipsing binary (EBs) stars of $\sim$ 0.1 seconds over a decade baseline can be used to directly measure the Galactic acceleration of stars in the Milky Way at $\sim$ kpc distances from the Sun. We consider contributions to the period drift rate from dynamical mechanisms other than the Galaxy's gravitational field, and show that the Galactic acceleration can be reliably measured using a sample of $\textit{Kepler}$ EBs with orbital and stellar parameters from the literature. Given the uncertainties on the formulation of tidal decay, our approach here is necessarily approximate, and the contribution from tidal decay is an upper limit assuming the stars are not tidally synchronized. We also use simple analytic relations to search for well-timed sources in the \textit{Kepler} field, and find $\sim$ 70 additional detached EBs with low eccentricities that have estimated timing precision better than 1 second. We illustrate the method with a prototypical, precisely timed EB using an archival \textit{Kepler} light curve and a modern synthetic \textit{HST} light curve (which provides a decade baseline). This novel method establishes a realistic possibility for obtaining fundamental Galactic parameters using eclipse timing to measure Galactic accelerations, along with other emerging new methods, including pulsar timing and extreme precision radial velocity observations. This acceleration signal grows quadratically with time. Therefore, given baselines established in the near-future for distant EBs, we can expect to measure the period drift in the future with space missions like \textit{JWST} and the \textit{Roman Space Telescope}. ",https://doi.org/10.3847/2041-8213/ac5c43,2112.08231v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Slow Cooling and Fast Reinflation for Hot Jupiters,2021,"  The unexpectedly large radii of hot Jupiters are a longstanding mystery whose solution will provide important insights into their interior physics. Many potential solutions have been suggested, which make diverse predictions about the details of inflation. In particular, although any valid model must allow for maintaining large planetary radii, only some allow for radii to increase with time. This reinflation process would potentially occur when the incident flux on the planet is increased. In this work, we examine the observed population of hot Jupiters to see if they grow as their parent stars brighten along the main sequence. We consider the relation between radius and other observables, including mass, incident flux, age, and fractional age (age over main sequence lifetime), and show that main sequence brightening is often sufficient to produce detectable reinflation. We further argue that these provide strong evidence for the relatively rapid reinflation of giant planets, and discuss the implications for proposed heating mechanisms. In our population analysis we also find evidence for a ""delayed-cooling effect"", wherein planets cool and contract far more slowly than expected. While not capable of explaining the observed radii alone, it may represent an important component of the effect. Finally, we identify a weak negative relationship between stellar metallicity and planet radius which is presumably the result of enhanced planetary bulk metallicity around metal-rich stars and has important implications for planet formation theory. ",https://doi.org/10.3847/2041-8213/abe86d,2101.05285v2,Yes,potent(2)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Predicting radial-velocity jitter induced by stellar oscillations based   on Kepler data,2018,"  Radial-velocity (RV) jitter due to intrinsic stellar variability introduces challenges when characterizing exoplanet systems, particularly when studying small (sub-Neptune-sized) planets orbiting solar-type stars. In this Letter we predicted for dwarfs and giants the jitter due to stellar oscillations, which in velocity have much larger amplitudes than noise introduced by granulation. We then fitted the jitter in terms of the following sets of stellar parameters: (1) Luminosity, mass, and effective temperature: the fit returns precisions (i.e., standard deviations of fractional residuals) of 17.9% and 27.1% for dwarfs and giants, respectively. (2) Luminosity, effective temperature, and surface gravity: The precisions are the same as using the previous parameter set. (3) Surface gravity and effective temperature: we obtain a precision of 22.6% for dwarfs and 27.1% for giants. (4): Luminosity and effective temperature: the precision is 47.8% for dwarfs and 27.5% for giants. Our method will be valuable for anticipating the radial-velocity stellar noise level of exoplanet host stars to be found by the TESS and PLATO space missions, and thus can be useful for their follow-up spectroscopic observations. We provide publicly available code (https://github.com/Jieyu126/Jitter) to set a prior for the jitter term as a component when modeling the Keplerian orbits of the exoplanets. ",https://doi.org/10.1093/mnrasl/sly123,1807.00096v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"Surface gravities for 15,000 Kepler stars measured from stellar   granulation and validated with Gaia DR2 parallaxes",2018,"  We have developed a method to estimate surface gravity (log g) from light curves by measuring the granulation background, similar to the ""flicker"" method by Bastien et al. (2016) but working in the Fourier power spectrum. We calibrated the method using Kepler stars for which asteroseismology has been possible with short-cadence data, demonstrating a precision in log g of about 0.05 dex. We also derived a correction for white noise as a function of Kepler magnitude by measuring white noise directly from observations. We then applied the method to the same sample of long-cadence stars as Bastien et al. We found that about half the stars are too faint for the granulation background to be reliably detected above the white noise. We provide a catalogue of log g values for about 15,000 stars having uncertainties better than 0.5 dex. We used Gaia DR2 parallaxes to validate that granulation is a powerful method to measure log g from light curves. Our method can also be applied to the large number of light curves collected by K2 and TESS. ",https://doi.org/10.1093/mnras/sty1869,1807.03624v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Bayesian Asteroseismology data Modeling pipeline and its application   to $\it K2$ data,2019,"  We present the Bayesian Asteroseismology data Modeling (BAM) pipeline, an automated asteroseismology pipeline that returns global oscillation parameters and granulation parameters from the analysis of photometric time-series. BAM also determines if a star is likely to be a solar-like oscillator. We have designed BAM to specially process ${\it K2}$ light curves, which suffer from unique noise signatures that can confuse asteroseismic analysis, though it may be used on any photometric time series --- including those from ${\it Kepler}$ and ${\it TESS}$. We demonstrate the BAM oscillation parameters are consistent within $\sim 1.53\%\ (\mathrm{random}) \pm 0.2\%\ (\mathrm{systematic})$ and $1.51\%\ (\mathrm{random}) \pm 0.6\%\ (\mathrm{systematic})$ for $\nu_{\mathrm{max}}$ and $\Delta \nu$ with benchmark results for typical ${\it K2}$ red giant stars in the ${\it K2}$ Galactic Archaeology Program's (GAP) Campaign 1 sample. Application of BAM to $13016$ ${\it K2}$ Campaign 1 targets not in the GAP sample yields $104$ red giant solar-like oscillators. Based on the number of serendipitous giants we find, we estimate an upper limit on the average purity in dwarf selection among C1 proposals is $\approx 99\%$, which could be lower when considering incompleteness in BAM detection efficiency, and proper motion cuts specific to C1 Guest Observer proposals. ",https://doi.org/10.3847/1538-4357/ab43c0,1909.11927v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Gaia-Kepler-TESS-Host Stellar Properties Catalog: Uniform Physical   Parameters for 7993 Host Stars and 9324 Planets,2023,"  We present the first homogeneous catalog of $Kepler$, $K2$, and $TESS$ host stars and the corresponding catalog of exoplanet properties, which contain 7993 stars and 9324 planets, respectively. We used isochrone fitting and $Gaia$ DR3 photometry, parallaxes, and spectrophotometric metallicities to compute precise, homogeneous $T_{\mathrm{eff}}$, $\log g$, masses, radii, mean stellar densities, luminosities, ages, distances, and V-band extinctions for 3248, 565, and 4180 $Kepler$, $K2$, and $TESS$ stars, respectively. We compared our stellar properties to studies using fundamental and precise constraints, such as interferometry and asteroseismology, and find residual scatters of 2.8%, 5.6%, 5.0%, and 31%, with offsets of 0.2%, 1.0%, 1.2%, and 0.7% between our $T_{\mathrm{eff}}$, radii, masses, and ages and those in the literature, respectively. In addition, we compute planet radii, semimajor axes, and incident fluxes for 4281, 676, and 4367 $Kepler$, $K2$, and $TESS$ planets, respectively, and find that the exoplanet radius gap is less prominent in the $K2$, $TESS$, and combined samples than it is in the $Kepler$ sample alone. We suspect this difference is largely due to heterogeneous planet-to-star radius ratios, shorter time baselines of $K2$ and $TESS$, and smaller sample sizes. Finally, we identify a clear radius inflation trend in our large sample of hot Jupiters and find 150 hot sub-Neptunian desert planets, in addition to a population of over 400 young host stars as potential opportunities for testing theories of planet formation and evolution. ",Kein DOI-Link verfügbar,2301.11338v1,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Hybrid Debris Disk Host Star HD 21997 is a High-Frequency Delta   Scuti Pulsator,2024,"  HD 21997 is host to a prototypical ""hybrid"" debris disk characterized by debris disk-like dust properties and a CO gas mass comparable to a protoplanetary disk. We use Transiting Exoplanet Survey Satellite time series photometry to demonstrate that HD 21997 is a high-frequency delta Scuti pulsator. If the mode identification can be unambiguously determined in future works, an asteroseismic age of HD 21997 may become feasible. ",Kein DOI-Link verfügbar,2403.19741v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Using asteroseismology to characterise exoplanet host stars,2018,"  The last decade has seen a revolution in the field of asteroseismology - the study of stellar pulsations. It has become a powerful method to precisely characterise exoplanet host stars, and as a consequence also the exoplanets themselves. This synergy between asteroseismology and exoplanet science has flourished in large part due to space missions such as $\textit{Kepler}$, which have provided high-quality data that can be used for both types of studies. Perhaps the primary contribution from asteroseismology to the research on transiting exoplanets is the determination of very precise stellar radii that translate into precise planetary radii, but asteroseismology has also proven useful in constraining eccentricities of exoplanets as well as the dynamical architecture of planetary systems. In this contribution, we introduce some basic principles of asteroseismology and review current synergies between the two fields. ",Kein DOI-Link verfügbar,1804.02214v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"A New Catalog of 100,000 Variable \emph{TESS} A-F Stars Reveals a   Correlation Between $δ$ Scuti Pulsator Fraction and Stellar Rotation",2024,"  {\delta} Scuti variables are found at the intersection of the classical instability strip and the main sequence on the Hertzsprung-Russell diagram. With space-based photometry providing millions of light-curves of A-F type stars, we can now probe the occurrence rate of {\delta} Scuti pulsations in detail. Using 30-min cadence light-curves from NASA's Transiting Exoplanet Survey Satellite's (TESS) first 26 sectors, we identify variability in 103,810 stars within 5-24 cycles per day down to a magnitude of $T=11.25$. We fit the period-luminosity relation of the fundamental radial mode for {\delta} Scuti stars in the Gaia $G$-band, allowing us to distinguish classical pulsators from contaminants for a subset of 39,367 stars. Out of this subset, over 15,918 are found on or above the expected period-luminosity relation. We derive an empirical red edge to the classical instability strip using Gaia photometry. The center where pulsator fraction peaks at 50-70%, combined with the red edge, agree well with previous work in the Kepler field. While many variable sources are found below the period-luminosity relation, over 85% of sources inside of the classical instability strip derived in this work are consistent with being {\delta} Scuti stars. The remaining 15% of variables within the instability strip are likely hybrid or {\gamma} Doradus pulsators. Finally, we discover strong evidence for a correlation between pulsator fraction and spectral line broadening from the Radial Velocity Spectrometer (RVS) aboard the Gaia spacecraft, confirming that rotation has a role in driving pulsations in {\delta} Scuti stars. ",Kein DOI-Link verfügbar,2405.19388v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Single-Particle-Picture Breakdown in laterally weakly confining GaAs   Quantum Dots,2019,"  We present a detailed investigation of different excitonic states weakly confined in single GaAs/AlGaAs quantum dots obtained by the Al droplet-etching method. For our analysis we make use of temperature-, polarization- and magnetic field-dependent $\mu$-photoluminescence measurements, which allow us to identify different excited states of the quantum dot system. Besides that, we present a comprehensive analysis of g-factors and diamagnetic coefficients of charged and neutral excitonic states in Voigt and Faraday configuration. Supported by theoretical calculations by the Configuration interaction method, we show that the widely used single-particle Zeeman Hamiltonian cannot be used to extract reliable values of the g-factors of the constituent particles from excitonic transition measurements. ",https://doi.org/10.1103/PhysRevB.100.235425,1909.04906v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Stars Kepler Missed: Investigating the Kepler Target Selection   Function Using Gaia DR2,2021,"  The Kepler Mission revolutionized exoplanet science and stellar astrophysics by obtaining highly precise photometry of over 200,000 stars over 4 years. A critical piece of information to exploit Kepler data is its selection function, since all targets had to be selected from a sample of half a million stars on the Kepler CCDs using limited information. Here we use Gaia DR2 to reconstruct the Kepler selection function and explore possible biases with respect to evolutionary state, stellar multiplicity, and kinematics. We find that the Kepler target selection is nearly complete for stars brighter than $Kp < 14$ mag and was effective at selecting main-sequence stars, with the fraction of observed stars decreasing from 95% to 60% between $14 < Kp < 16$ mag. We find that the observed fraction for subgiant stars is only 10% lower, confirming that a significant number of subgiants selected for observation were believed to be main-sequence stars. Conversely we find a strong selection bias against low-luminosity red giant stars ($R \approx 3-5 R_\odot$, $T_{eff} \approx 5500$K), dropping from 90% at $Kp = 14$ mag to below 30% at $Kp = 16$ mag, confirming that the target selection was efficient at distinguishing dwarfs from giants. We compare the Gaia Re-normalized Unit Weight Error (RUWE) values of the observed and non-observed main sequence stars and find a difference in elevated ($>$ 1.2) RUWE values at $\sim\,5\,\sigma$ significance, suggesting that the Kepler target selection shows some bias against either close or wide binaries. We furthermore use the Gaia proper motions to show that the Kepler selection function was unbiased with respect to kinematics. ",https://doi.org/10.3847/1538-3881/abee1d,2101.03190v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Tidally Induced Oscillations and Orbital Decay in Compact Triple-Star   Systems,2012,"  We investigate the nature of tidal effects in compact triple-star systems. The hierarchical structure of a triple system produces tidal forcing at high frequencies unobtainable in binary systems, allowing for the tidal excitation of high frequency p-modes in the stellar components. The tidal forcing exists even for circular, aligned, and synchronized systems. We calculate the magnitude and frequencies of three-body tidal forcing on the central primary star for circular and coplanar orbits, and we estimate the amplitude of the tidally excited oscillation modes. We also calculate the secular orbital changes induced by the tidally excited modes, and show that they can cause significant orbital decay. During certain phases of stellar evolution, the tidal dissipation may be greatly enhanced by resonance locking. We then compare our theory to observations of HD 181068, which is a hierarchical triply eclipsing star system in the Kepler field of view. The observed oscillation frequencies in HD 181068 can be naturally explained by three-body tidal effects. We then compare the observed oscillation amplitudes and phases in HD 181068 to our predictions, finding mostly good agreement. Finally, we discuss the past and future evolution of compact triple systems like HD 181068. ",https://doi.org/10.1093/mnras/sts511,1211.6814v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Stellar population synthesis based modelling of the Milky Way using   asteroseismology of dwarfs and subgiants from Kepler,2017,"  Early attempts to apply asteroseismology to study the Galaxy have already shown unexpected discrepancies for the mass distribution of stars between the Galactic models and the data; a result that is still unexplained. Here, we revisit the analysis of the asteroseismic sample of dwarf and subgiant stars observed by Kepler and investigate in detail the possible causes for the reported discrepancy. We investigate two models of the Milky Way based on stellar population synthesis, Galaxia and TRILEGAL. In agreement with previous results, we find that TRILEGAL predicts more massive stars compared to Galaxia, and that TRILEGAL predicts too many blue stars compared to 2MASS observations. Both models fail to match the distribution of the stellar sample in $(\log g,T_{\rm eff})$ space, pointing to inaccuracies in the models and/or the assumed selection function. When corrected for this mismatch in $(\log g,T_{\rm eff})$ space, the mass distribution calculated by Galaxia is broader and the mean is shifted toward lower masses compared to that of the observed stars. This behaviour is similar to what has been reported for the Kepler red giant sample. The shift between the mass distributions is equivalent to a change of 2\% in $\nu_{\rm max}$, which is within the current uncertainty in the $\nu_{\rm max}$ scaling relation. Applying corrections to the $\Delta \nu$ scaling relation predicted by the stellar models makes the observed mass distribution significantly narrower, but there is no change to the mean. ",https://doi.org/10.3847/1538-4357/835/2/163,1701.02464v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"The Kepler IRIS Catalog: Image subtraction light curves for 9,150 stars   in and around the open clusters NGC 6791 and NGC 6819",2021,"  The four-year Kepler mission collected long cadence images of the open clusters NGC 6791 and NGC 6819, known as ""superstamps."" Each superstamp region is a 200-pixel square that captures thousands of cluster members, plus foreground and background stars, of which only the brightest were targeted for long or short cadence photometry during the Kepler mission. Using image subtraction photometry, we have produced light curves for every object in the Kepler Input Catalog that falls on the superstamps. The IRIS catalog includes light curves for 9,150 stars, and contains a wealth of new data: 8,427 of these stars were not targeted at all by Kepler, and we have increased the number of available quarters of long cadence data for 382 stars. The catalog is available as a high-level science product on MAST, with both raw photometric data for each quarter and corrected light curves for all available quarters for each star. We also present an introduction to our implementation of image subtraction photometry and the open source IRIS pipeline, alongside an overview of the data products, systematics, and catalog statistics. ",https://doi.org/10.3847/1538-4365/ac3a11,2112.05174v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The more the merrier: grid based modelling of Kepler dwarfs with   5-dimensional stellar grids,2017,"  We present preliminary results of our grid based modelling (GBM) of the dwarf/subgiant sample of stars observed with Kepler including global asteroseismic parameters. GBM analysis in this work is based on a large grid of stellar models that is characterized by five independent parameters: model mass and age, initial metallicity ($\zini$), initial helium ($\yini$), and mixing length parameter ($\alpha_{mlt}$). Using this grid relaxes assumptions used in all previous GBM work where the initial composition is determined by a single parameter and that $\alpha_{mlt}$ is fixed to a solar-calibrated value. The new grid allows us to study, for example, the impact of different galactic chemical enrichment models on the determination of stellar parameters such as mass radius and age. Also, it allows to include new results from stellar atmosphere models on $\alpha_{mlt}$ in the GBM analysis in a simple manner. Alternatively, it can be tested if global asteroseismology is a useful tool to constraint our ignorance on quantities such as $\yini$ and $\alpha_{mlt}$. Initial findings show that mass determination is robust with respect to freedom in the latter quantities, with a 4.4\% maximum deviation for extreme assumptions regarding prior information on $\yini-\zini$ relations and $\alpha_{mlt}$. On the other hand, tests carried out so far seem to indicate that global seismology does not have much power to constrain $\yini-\zini$ relations of $\alpha_{mlt}$ values without resourcing to additional information. ",https://doi.org/10.1051/epjconf/201716003011,1706.00503v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Data-driven derivation of stellar properties from photometric time   series data using convolutional neural networks,2020,"  Stellar variability is driven by a multitude of internal physical processes that depend on fundamental stellar properties. These properties are our bridge to reconciling stellar observations with stellar physics, and for understanding the distribution of stellar populations within the context of galaxy formation. Numerous ongoing and upcoming missions are charting brightness fluctuations of stars over time, which encode information about physical processes such as rotation period, evolutionary state (such as effective temperature and surface gravity), and mass (via asteroseismic parameters). Here, we explore how well we can predict these stellar properties, across different evolutionary states, using only photometric time series data. To do this, we implement a convolutional neural network, and with data-driven modeling we predict stellar properties from light curves of various baselines and cadences. Based on a single quarter of \textit{Kepler} data, we recover stellar properties, including surface gravity for red giant stars (with an uncertainty of $\lesssim$ 0.06 dex), and rotation period for main sequence stars (with an uncertainty of $\lesssim$ 5.2 days, and unbiased from $\approx$5 to 40 days). Shortening the \textit{Kepler} data to a 27-day TESS-like baseline, we recover stellar properties with a small decrease in precision, $\sim$0.07 dex for log $g$ and $\sim$5.5 days for $P_{\rm rot}$, unbiased from $\approx$5 to 35 days. Our flexible data-driven approach leverages the full information content of the data, requires minimal feature engineering, and can be generalized to other surveys and datasets. This has the potential to provide stellar property estimates for many millions of stars in current and future surveys. ",Kein DOI-Link verfügbar,2005.09682v1,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,A Guide to Realistic Uncertainties on Fundamental Properties of   Solar-Type Exoplanet Host Stars,2020,"  Our understanding of the properties and demographics of exoplanets critically relies on our ability to determine fundamental properties of their host stars. The advent of Gaia and large spectroscopic surveys has now made it in principle possible to infer properties of individual stars, including most exoplanet hosts, to very high precision. However, we show that in practice, such analyses are limited both by uncertainties in the fundamental scale, and by uncertainties in our models of stellar evolution, even for stars similar to the Sun. For example, we show that current uncertainties on measured interferometric angular diameters and bolometric fluxes set a systematic uncertainty floor of $\sim$2% in temperature, $\sim$2% in luminosity, and $\sim$4% in radius. Comparisons between widely available model grids suggest uncertainties of order $\sim$5% in mass and $\sim$20% in age for main sequence and subgiant stars. While the radius uncertainties are roughly constant over this range of stars, the model dependent uncertainties are a complex function of luminosity, temperature, and metallicity. We provide open-source software for approximating these uncertainties for individual targets, and discuss strategies for reducing these uncertainties in the future. ",https://doi.org/10.3847/1538-4357/ac4bbc,2012.07957v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,HD-TESS: An Asteroseismic Catalog of Bright Red Giants within TESS   Continuous Viewing Zones,2022,"  We present HD-TESS, a catalog of 1,709 bright ($V\sim3-10$) red giants from the Henry Draper (HD) Catalog with asteroseismic measurements based on photometry from NASA's Transiting Exoplanet Survey Satellite (TESS). Using light curves spanning at least six months across a single TESS observing cycle, we provide measurements of global asteroseismic parameters ($\nu_{\mathrm{max}}$ and $\Delta\nu$) and evolutionary state for each star in the catalog. We adopt literature values of atmospheric stellar parameters to estimate the masses and radii of the giants in our catalog using asteroseismic scaling relations, and observe that HD-TESS giants on average have larger masses compared to Kepler red giants. Additionally, we present the discovery of oscillations in 99 red giants in astrometric binary systems, including those with subdwarf or white dwarf companions. Finally, we benchmark radii from asteroseismic scaling relations against those measured using long-baseline interferometry for 18 red giants and find that correction factors to the scaling relations improve the agreement between asteroseismic and interferometric radii to approximately 3%. ",https://doi.org/10.3847/1538-3881/ac8931,2208.06478v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Evidence that Core-Powered Mass-Loss Dominates Over Photoevaporation in   Shaping the Kepler Radius Valley,2023,"  The dearth of planets with sizes around 1.8 $\mathrm{R_\oplus}$ is a key demographic feature discovered by the $Kepler$ mission. Two theories have emerged as potential explanations for this valley: photoevaporation and core-powered mass-loss. However, Rogers et al. (2021) shows that differentiating between the two theories is possible using the three-dimensional parameter space of planet radius, incident flux, and stellar mass. We use homogeneously-derived stellar and planetary parameters to measure the $Kepler$ exoplanet radius gap in this three-dimensional space. We compute the slope of the gap as a function of incident flux at constant stellar mass ($\alpha$ $\equiv$ $\left(\partial \log R_{\mathrm{gap}} / \partial \log S \right)_{M_\star}$) and the slope of the gap as a function of stellar mass at constant incident flux ($\beta$ $\equiv$ $\left(\partial \log R_{\mathrm{gap}} / \partial \log M_\star \right)_{S}$) and find $\alpha$ = 0.069$^{+0.019}_{-0.023}$ and $\beta$ = $-$0.046$^{+0.125}_{-0.117}$. Given that Rogers et al. (2021) shows that core-powered mass-loss predicts $\alpha$ $\approx$ 0.08 and $\beta$ $\approx$ 0.00 while photoevaporation predicts $\alpha$ $\approx$ 0.12 and $\beta$ $\approx$ --0.17, our measurements are more consistent with core-powered mass-loss than photoevaporation. However, we caution that different gap-determination methods can produce systematic offsets in both $\alpha$ and $\beta$; therefore, we motivate a comprehensive re-analysis of $Kepler$ light curves with modern, updated priors on eccentricity and mean stellar density to improve both the accuracy and precision of planet radii and subsequent measurements of the gap. ",Kein DOI-Link verfügbar,2302.00009v1,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Radial Velocity Observations and Light Curve Noise Modeling Confirm That   Kepler-91b is a Giant Planet Orbiting a Giant Star,2014,"  Kepler-91b is a rare example of a transiting hot Jupiter around a red giant star, providing the possibility to study the formation and composition of hot Jupiters under different conditions compared to main-sequence stars. However, the planetary nature of Kepler-91b, which was confirmed using phase-curve variations by Lillo-Box et al., was recently called into question based on a re-analysis of Kepler data. We have obtained ground-based radial velocity observations from the Hobby-Eberly Telescope and unambiguously confirm the planetary nature of Kepler-91b by simultaneously modeling the Kepler and radial velocity data. The star exhibits temporally correlated noise due to stellar granulation which we model as a Gaussian Process. We hypothesize that it is this noise component that led previous studies to suspect Kepler-91b to be a false positive. Our work confirms the conclusions presented by Lillo-Box et al. that Kepler-91b is a 0.73+/-0.13 Mjup planet orbiting a red giant star. ",https://doi.org/10.1088/0004-637X/800/1/46,1408.3149v4,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Five Planets in the Kepler-296 Binary System All Orbit the Primary:   A Statistical and Analytical Analysis,2015,"  Kepler-296 is a binary star system with two M-dwarf components separated by 0.2 arcsec. Five transiting planets have been confirmed to be associated with the Kepler-296 system; given the evidence to date, however, the planets could in principle orbit either star. This ambiguity has made it difficult to constrain both the orbital and physical properties of the planets. Using both statistical and analytical arguments, this paper shows that all five planets are highly likely to orbit the primary star in this system. We performed a Markov-Chain Monte Carlo simulation using a five transiting planet model, leaving the stellar density and dilution with uniform priors. Using importance sampling, we compared the model probabilities under the priors of the planets orbiting either the brighter or the fainter component of the binary. A model where the planets orbit the brighter component, Kepler-296A, is strongly preferred by the data. Combined with our assertion that all five planets orbit the same star, the two outer planets in the system, Kepler-296 Ae and Kepler-296 Af, have radii of 1.53 +/- 0.26 and 1.80 +/- 0.31 R_earth, respectively, and receive incident stellar fluxes of 1.40 +/- 0.23 and 0.62 +/- 0.10 times the incident flux the Earth receives from the Sun. This level of irradiation places both planets within or close to the circumstellar habitable zone of their parent star. ",https://doi.org/10.1088/0004-637X/809/1/7,1505.01845v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,A binary with a $δ$~Scuti star and an oscillating red giant: orbit   and asteroseismology of KIC9773821,2021,"  We study the $\delta$ Scuti -- red giant binary KIC9773821, the first double-pulsator binary of its kind. It was observed by \textit{Kepler} during its four-year mission. Our aims are to ascertain whether the system is bound, rather than a chance alignment, and to identify the evolutionary state of the red giant via asteroseismology. An extension of these aims is to determine a dynamical mass and an age prior for a $\delta$ Sct star, which may permit mode identification via further asteroseismic modelling. We determine spectroscopic parameters and radial velocities (RVs) for the red giant component using HERMES@Mercator spectroscopy. Light arrival-time delays from the $\delta$ Sct pulsations are used with the red-giant RVs to determine that the system is bound and to infer its orbital parameters, including the binary mass ratio. We use asteroseismology to model the individual frequencies of the red giant to give a mass of $2.10^{+0.20}_{-0.10}$ M$_{\odot}$ and an age of $1.08^{+0.06}_{-0.24}$ Gyr. We find that it is a helium-burning secondary clump star, confirm that it follows the standard $\nu_{\rm max}$ scaling relation, and confirm its observed period spacings match their theoretical counterparts in the modelling code MESA. Our results also constrain the mass and age of the $\delta$ Sct star. We leverage these constraints to construct $\delta$ Sct models in a reduced parameter space and identify four of its five pulsation modes. ",https://doi.org/10.1093/mnras/stab1436,2105.13577v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,TESS observations of the Pleiades cluster: a nursery for delta Scuti   stars,2022,"  We studied 89 A- and F-type members of the Pleiades open cluster, including five escaped members. We measured projected rotational velocities (v sin i) for 49 stars and confirmed that stellar rotation causes a broadening of the main sequence in the color-magnitude diagram. Using time-series photometry from NASA's TESS Mission (plus one star observed by Kepler/K2), we detected delta Scuti pulsations in 36 stars. The fraction of Pleiades stars in the middle of the instability strip that pulsate is unusually high (over 80%), and their range of effective temperatures agrees well with theoretical models. On the other hand, the characteristics of the pulsation spectra are varied and do not correlate with stellar temperature, calling into question the existence of a useful nu_max relation for delta Scutis, at least for young stars. By including delta Scuti stars observed in the Kepler field, we show that the instability strip is shifted to the red with increasing distance by interstellar reddening. Overall, this work demonstrates the power of combining observations with Gaia and TESS for studying pulsating stars in open clusters. ",https://doi.org/10.3847/2041-8213/acc17a,2212.12087v5,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,HIP 65426 is a High-Frequency Delta Scuti Pulsator in Plausible   Spin-Orbit Alignment with its Directly Imaged Exoplanet,2023,"  HIP 65426 hosts a young giant planet that has become the first exoplanet directly imaged with JWST. Using time-series photometry from the Transiting Exoplanet Survey Satellite (TESS), we classify HIP 65426 as a high-frequency $\delta$ Scuti pulsator with a possible large frequency separation of $\Delta \nu =$7.23$\pm$0.02 cycles day$^{-1}$. We check the TESS data for pulsation timing variations and use the nondetection to estimate a 95% dynamical mass upper limit of 12.8 Mjup for HIP 65426 b. We also identify a low-frequency region of signal that we interpret as stellar latitudinal differential rotation with two rapid periods of 7.85$\pm$0.08 hr and 6.67$\pm$0.04 hr. We use our TESS rotation periods together with published values of radius and $v \sin{i}$ to jointly measure the inclination of HIP 65426 to $i_{\star}=107_{-11}^{+12}$$^\circ$. Our stellar inclination is consistent with the orbital inclination of HIP 65426 b ($108_{-3}^{+6}$$^{\circ}$) at the $68\%$ percent level based on our orbit fit using published relative astrometry. The lack of significant evidence for spin-orbit misalignment in the HIP 65426 system supports an emerging trend consistent with preferential alignment between imaged long-period giant planets and their host stars. ",Kein DOI-Link verfügbar,2312.05310v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Highly indistinguishable and strongly entangled photons from symmetric   GaAs quantum dots,2016,"  The development of scalable sources of non-classical light is fundamental to unlock the technological potential of quantum photonics\cite{Kimble:Nat2008}. Among the systems under investigation, semiconductor quantum dots are currently emerging as near-optimal sources of indistinguishable single photons. However, their performances as sources of entangled-photon pairs are in comparison still modest. Experiments on conventional Stranski-Krastanow InGaAs quantum dots have reported non-optimal levels of entanglement and indistinguishability of the emitted photons. For applications such as entanglement teleportation and quantum repeaters, both criteria have to be met simultaneously. In this work, we show that this is possible focusing on a system that has received limited attention so far: GaAs quantum dots grown via droplet etching. Using a two-photon resonant excitation scheme, we demonstrate that these quantum dots can emit triggered polarization-entangled photons with high purity (g^(2)(0)=0.002 +/-0.002), high indistinguishability (0.93 +/-0.07) and high entanglement fidelity (0.94 +/-0.01). Such unprecedented degree of entanglement, which in contrast to InGaAs can theoretically reach near-unity values, allows Bell's inequality (2.64 +/-0.01) to be violated without the aid of temporal or spectral filtering. Our results show that if quantum-dot entanglement resources are to be used for future quantum technologies, GaAs might be the system of choice. ",https://doi.org/10.1038/ncomms15506,1610.06889v1,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Strain-Tunable GaAs Quantum dot: A Nearly Dephasing-Free Source of   Entangled Photon Pairs on Demand,2018,"  Entangled photon generation from semiconductor quantum dots via the biexciton-exciton cascade underlies various decoherence mechanisms related to the solid-state nature of the quantum emitters. So far, this has prevented the demonstration of nearly-maximally entangled photons without the aid of inefficient and complex post-selection techniques that are hardly suitable for quantum communication technologies. Here, we tackle this challenge using strain-tunable GaAs quantum dots driven under two-photon resonant excitation and with strictly-degenerate exciton states. We demonstrate experimentally that our on-demand source generates polarization-entangled photons with fidelity of 0.978(5) and concurrence of 0.97(1) without resorting to post-selection techniques. Moreover, we show that the remaining decoherence mechanisms can be overcome using a modest Purcell enhancement so as to achieve a degree of entanglement >0.99. Our results highlight that GaAs quantum dots can be readily used in advanced communication protocols relying on the non-local properties of quantum entanglement. ",https://doi.org/10.1103/PhysRevLett.121.033902,1801.06655v3,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Mapping out the time-evolution of exoplanet processes,2019,"  There are many competing theories and models describing the formation, migration and evolution of exoplanet systems. As both the precision with which we can characterize exoplanets and their host stars, and the number of systems for which we can make such a characterization increase, we begin to see pathways forward for validating these theories. In this white paper we identify predicted, observable correlations that are accessible in the near future, particularly trends in exoplanet populations, radii, orbits and atmospheres with host star age. By compiling a statistically significant sample of well-characterized exoplanets with precisely measured ages, we should be able to begin identifying the dominant processes governing the time-evolution of exoplanet systems. ",Kein DOI-Link verfügbar,1903.09110v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Kepler's Unparalleled Exploration of the Time Dimension,2013,"  We show that the Kepler spacecraft in two-reaction wheel mode of operation is very well suited for the study of eclipsing binary star systems. Continued observations of the Kepler field will provide the most enduring and long-term valuable science. It will enable the discovery and characterization of eclipsing binaries with periods greater than 1 year - these are the most important, yet least understood binaries for habitable-zone planet background considerations. The continued mission will also enable the investigation of hierarchical multiple systems (discovered through eclipse timing variations), and provide drastically improved orbital parameters for circumbinary planetary systems. ",Kein DOI-Link verfügbar,1309.1176v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Stellar population synthesis based modelling of the Milky Way using   asteroseismology of 13000 Kepler red giants,2016,"  With current space-based missions it is now possible to obtain age-sensitive asteroseismic information for tens of thousands of red giants. This provides a promising opportunity to study the Galactic structure and evolution. We use asteroseismic data of red giants, observed by Kepler, to test the current theoretical framework of modelling the Galaxy based on population synthesis modeling and the use of asteroseismic scaling relations for giants. We use the open source code Galaxia to model the Milky Way and find the distribution of the masses predicted by Galaxia to be systematically offset with respect to the seismically-inferred observed masses. The Galactic model overestimates the number of low mass stars, and these stars are predominantly old and of low metallicity. Using corrections to the $\Delta \nu$ scaling relation suggested by stellar models (available for download) significantly reduces the disagreement between predicted and observed masses. For a few cases where non-seismic mass estimates are available, the corrections to $\Delta \nu$ also improve the agreement between seismic and non-seismic mass estimates. The disagreement between predictions of the Galactic model and the observations is most pronounced for stars with ${\rm [Fe/H]}<-0.5$ and ${\rm [Fe/H]}>0$ or for $T_{\rm eff}>4700$ K. Altering the star formation rate in order to suppress stars older than 10 Gyr improves the agreement for mass but leads to inconsistent color distributions. We also tested the predictions of the TRILEGAL Galactic model. However, unlike {\sl Galaxia}, it had difficulties in reproducing the photometric properties of the Kepler Input Catalog because it overestimates the number of blue stars. We conclude that either the scaling relations and/or the Galactic models need to be revised to reconcile predictions of theory with asteroseismic observations. ",https://doi.org/10.3847/0004-637X/822/1/15,1603.05661v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Revised Radii of Kepler Stars and Planets Using Gaia Data Release 2,2018,"  One bottleneck for the exploitation of data from the $Kepler$ mission for stellar astrophysics and exoplanet research has been the lack of precise radii and evolutionary states for most of the observed stars. We report revised radii of 177,911 $Kepler$ stars derived by combining parallaxes from $Gaia$ Data Release 2 with the DR25 $Kepler$ Stellar Properties Catalog. The median radius precision is $\approx$ 8%, a typical improvement by a factor of 4-5 over previous estimates for typical $Kepler$ stars. We find that $\approx$ 67% ($\approx$ 120,000) of all $Kepler$ targets are main-sequence stars, $\approx$ 21% ($\approx$ 37,000) are subgiants, and $\approx$ 12% ($\approx$ 21,000) are red giants, demonstrating that subgiant contamination is less severe than some previous estimates and that Kepler targets are mostly main-sequence stars. Using the revised stellar radii, we recalculate the radii for 2123 confirmed and 1922 candidate exoplanets. We confirm the presence of a gap in the radius distribution of small, close-in planets, but find that the gap is mostly limited to incident fluxes $>$ 200$F_\oplus$ and its location may be at a slightly larger radius (closer to $\approx$ 2$R_\oplus$) when compared to previous results. Further, we find several confirmed exoplanets occupying a previously-described ""hot super-Earth desert"" at high irradiance, show the relation between gas-giant planet radius and incident flux, and establish a bona-fide sample of eight confirmed planets and 30 planet candidates with $R_{\mathrm{p}}$ $<$ 2$R_\oplus$ in circumstellar ""habitable zones"" (incident fluxes between 0.25--1.50 $F_\oplus$). The results presented here demonstrate the potential for transformative characterization of stellar and exoplanet populations using $Gaia$ data. ",https://doi.org/10.3847/1538-4357/aada83,1805.00231v4,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Confirmation of the ${\rm \it Gaia}$ DR2 parallax zero-point offset   using asteroseismology and spectroscopy in the ${\rm \it Kepler}$ field,2018,"  We present an independent confirmation of the zero-point offset of ${\rm \it Gaia}$ Data Release 2 (DR2) parallaxes using asteroseismic data of evolved stars in the ${\rm \it Kepler}$ field. Using well-characterized red giant branch (RGB) stars from the APOKASC-2 catalogue we identify a ${\rm \it Gaia}$ astrometric pseudo-color ($\nu_{\rm eff}$)- and ${\rm \it Gaia}$ $G$-band magnitude-dependent zero-point offset of $\varpi_{\rm seis} - \varpi_{Gaia} = 52.8 \pm 2.4 {\rm\ (rand.)} \pm 8.6 {\rm\ (syst.)} - (150.7 \pm 22.7)(\nu_{\rm eff} - 1.5) - (4.21 \pm 0.77)(G - 12.2) \mu{\rm as}$, in the sense that ${\rm \it Gaia}$ parallaxes are too small. The offset is found in high and low-extinction samples, as well as among both shell H-burning red giant stars and core He-burning red clump stars. We show that errors in the asteroseismic radius and temperature scales may be distinguished from errors in the ${\rm \it Gaia}$ parallax scale. We estimate systematic effects on the inferred global ${\rm \it Gaia}$ parallax offset, $c$, due to radius and temperature systematics, as well as choices in bolometric correction and the adopted form for ${\rm \it Gaia}$ parallax spatial correlations. Because of possible spatially-correlated parallax errors, as discussed by the ${\rm \it Gaia}$ team, our ${\rm \it Gaia}$ parallax offset model is specific to the ${\rm \it Kepler}$ field, but broadly compatible with the magnitude- and color-dependent offset inferred by the ${\rm \it Gaia}$ team and several subsequent investigations using independent methods. ",https://doi.org/10.3847/1538-4357/ab1f66,1805.02650v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,TESS Data for Asteroseismology: Light Curve Systematics Correction,2021,"  Data from the Transiting Exoplanet Survey Satellite (TESS) has produced of order one million light curves at cadences of 120 s and especially 1800 s for every ~27-day observing sector during its two-year nominal mission. These data constitute a treasure trove for the study of stellar variability and exoplanets. However, to fully utilize the data in such studies a proper removal of systematic noise sources must be performed before any analysis. The TESS Data for Asteroseismology (T'DA) group is tasked with providing analysis-ready data for the TESS Asteroseismic Science Consortium, which covers the full spectrum of stellar variability types, including stellar oscillations and pulsations, spanning a wide range of variability timescales and amplitudes. We present here the two current implementations for co-trending of raw photometric light curves from TESS, which cover different regimes of variability to serve the entire seismic community. We find performance in terms of commonly used noise statistics to meet expectations and to be applicable to a wide range of different intrinsic variability types. Further, we find that the correction of light curves from a full sector of data can be completed well within a few days, meaning that when running in steady-state our routines are able to process one sector before data from the next arrives. Our pipeline is open-source and all processed data will be made available on TASOC and MAST. ",https://doi.org/10.3847/1538-4365/ac214a,2108.11780v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The far side of the Galactic bar/bulge revealed through semi-regular   variables,2023,"  The Galactic bulge and bar are critical to our understanding of the Milky Way. However, due to the lack of reliable stellar distances, the structure and kinematics of the bulge/bar beyond the Galactic center have remained largely unexplored. Here, we present a method to measure distances of luminous red giants using a period-amplitude-luminosity relation anchored to the Large Magellanic Cloud, with random uncertainties of 10-15% and systematic errors below 1-2%. We apply this method to data from the Optical Gravitational Lensing Experiment (OGLE) to measure distances to $190,302$ stars in the Galactic bulge and beyond out to 20 kpc. Using this sample we measure a distance to the Galactic center of $R_0$ = $8108\pm106_{\rm stat}\pm93_{\rm sys}$ pc, consistent with astrometric monitoring of stars orbiting Sgr A*. We cross-match our distance catalog with Gaia DR3 and use the subset of $39,566$ overlapping stars to provide the first constraints on the Milky Way's velocity field ($V_R,V_\phi,V_z$) beyond the Galactic center. We show that the $V_R$ quadrupole from the bar's near side is reflected with respect to the Galactic center, indicating that the bar is both bi-symmetric and aligned with the inner disk, and therefore dynamically settled along its full extent. We also find that the vertical height $V_Z$ map has no major structure in the region of the Galactic bulge, which is inconsistent with a current episode of bar buckling. Finally, we demonstrate with N-body simulations that distance uncertainty plays a major factor in the alignment of the major and kinematic axes of the bar and distribution of velocities, necessitating caution when interpreting results for distant stars. ",https://doi.org/10.3847/1538-3881/ad01bf,2305.19319v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Calculating asteroseismic diagrams for solar-like oscillations,2011,"  With the success of the Kepler and CoRoT missions, the number of stars with detected solar-like oscillations has increased by several orders of magnitude, for the first time we are able to perform large-scale ensemble asteroseismology of these stars. In preparation for this golden age of asteroseismology we have computed expected values of various asteroseismic observables from models of varying mass and metallicity. The relationships between these asteroseismic observables, such as the separations between mode frequencies, are able to significantly constrain estimates of the ages and masses of these stars. We investigate the scaling relation between the large frequency separation, Delta nu, and mean stellar density. Furthermore we present model evolutionary tracks for several asteroseismic diagrams. We have extended the so-called C-D diagram beyond the main sequence to the subgiants and the red-giant branch. We also consider another asteroseismic diagram, the epsilon diagram, which is more sensitive to variations in stellar properties at the subgiant stages and can aid in determining the correct mode identification. The recent discovery of gravity-mode period spacings in red giants forms the basis for a third asteroseismic diagram. We compare the evolutionary model tracks in these asteroseismic diagrams with results from pre-Kepler studies of solar-like oscillations, and early results from Kepler. ",https://doi.org/10.1088/0004-637X/743/2/161,1109.3455v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Flicker as a tool for characterizing planets through Asterodensity   Profiling,2014,"  Variability in the time series brightness of a star on a timescale of 8 hours, known as 'flicker', has been previously demonstrated to serve as a proxy for the surface gravity of a star by Bastien et al. (2013). Although surface gravity is crucial for stellar classification, it is the mean stellar density which is most useful when studying transiting exoplanets, due to its direct impact on the transit light curve shape. Indeed, an accurate and independent measure of the stellar density can be leveraged to infer subtle properties of a transiting system, such as the companion's orbital eccentricity via asterodensity profiling. We here calibrate flicker to the mean stellar density of 439 Kepler targets with asteroseismology, allowing us to derive a new empirical relation given by $\log_{10}(\rho_{\star}\,[\mathrm{kg}\,\mathrm{m}^{-3}]) = 5.413 - 1.850 \log_{10}(F_8\,[\mathrm{ppm}])$. The calibration is valid for stars with $4500$K$<T_{\mathrm{eff}}<6500$K, $K_P<14$ and flicker estimates corresponding to stars with $3.25<\log g_{\star}<4.43$. Our relation has a model error in the stellar density of 31.7% and so has $\sim8$ times lower precision than that from asteroseismology but is applicable to a sample $\sim40$ times greater. Flicker therefore provides an empirical method to enable asterodensity profiling on hundreds of planetary candidates from present and future missions. ",https://doi.org/10.1088/2041-8205/785/2/L32,1403.5264v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Photometry of Very Bright Stars with Kepler and K2 Smear Data,2015,"  High-precision time series photometry with the Kepler satellite has been crucial to our understanding both of exoplanets, and via asteroseismology, of stellar physics. After the failure of two reaction wheels, the Kepler satellite has been repurposed as Kepler-2 (K2), observing fields close to the ecliptic plane. As these fields contain many more bright stars than the original Kepler field, K2 provides an unprecedented opportunity to study nearby objects amenable to detailed follow-up with ground-based instruments. Due to bandwidth constraints, only a small fraction of pixels can be downloaded, with the result that most bright stars which saturate the detector are not observed. We show that engineering data acquired for photometric calibration, consisting of collateral `smear' measurements, can be used to reconstruct light curves for bright targets not otherwise observable with Kepler/K2. Here we present some examples from Kepler Quarter 6 and K2 Campaign 3, including the delta Scuti variables HD 178875 and 70 Aqr, and the red giant HR 8500 displaying solar-like oscillations. We compare aperture and smear photometry where possible, and also study targets not previously observed. These encouraging results suggest this new method can be applied to most Kepler and K2 fields. ",https://doi.org/10.1093/mnrasl/slv143,1510.00008v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of 1523 misclassified red giants using   $\textit{Kepler}$ data,2016,"  We analysed solar-like oscillations in 1523 $\textit{Kepler}$ red giants which have previously been misclassified as subgiants, with predicted $\nu_{\rm max}$ values (based on the Kepler Input Catalogue) between 280$\mu$Hz to 700$\mu$Hz. We report the discovery of 626 new oscillating red giants in our sample, in addition to 897 oscillators that were previously characterized by Hekker et al. (2011) from one quarter of $\textit{Kepler}$ data. Our sample increases the known number of oscillating low-luminosity red giants by $26\%$ (up to $\sim$ 1900 stars). About three quarters of our sample are classified as ascending red-giant-branch stars, while the remainder are red-clump stars. A novel scheme was applied to determine $\Delta \nu$ for 108 stars with $\nu_{\rm max}$ close to the Nyquist frequency (240$\mu$Hz < $\nu_{\rm max}$ < 320$\mu$Hz). Additionally, we identified 47 stars oscillating in the super-Nyquist frequency regime, up to 387$\mu$Hz, using long-cadence light curves. We show that the misclassifications are most likely due to large uncertainties in KIC surface gravities, and do not result from the absence of broadband colors or from different physical properties such as reddening, spatial distribution, mass or metallicity. The sample will be valuable to study oscillations in low-luminosity red giants and to characterize planet candidates around those stars. ",https://doi.org/10.1093/mnras/stw2074,1608.05803v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Probing the Deep End of the Milky Way with New Oscillating Kepler Giants,2016,"  The Kepler mission has been a success in both exoplanet search and stellar physics studies. Red giants have actually been quite a highlight in the Kepler scene. The Kepler long and almost continuous four-year observations allowed us to detect oscillations in more than 15,000 red giants targeted by the mission. However by looking at the power spectra of 45,000 stars classified as dwarfs according to the Q1-16 Kepler star properties catalog, we detected red-giant like oscillations in 850 stars. Even though this is a small addition to the known red-giant sample, these misclassified stars represent a goldmine for galactic archeology studies. Indeed they happen to be fainter (down to Kp~16) and more distant (d>10kpc) than the known red giants, opening the possibility to probe unknown regions of our Galaxy. The faintness of these red giants with detected oscillations is very promising for detecting acoustic modes in red giants observed with K2 and TESS. In this talk, I will present this new sample of red giants with their revised stellar parameters derived from asteroseismology. Then I will discuss about the distribution of their masses, distances, and evolutionary states compared to the previously known sample of red giants. ",https://doi.org/10.1051/epjconf/201716005001,1611.04237v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Phonon-assisted two-photon interference from remote quantum emitters,2017,"  Photonic quantum technologies are on the verge of finding applications in everyday life with quantum cryptography and the quantum internet on the horizon. Extensive research has been carried out to determine suitable quantum emitters and single epitaxial quantum dots are emerging as near-optimal sources of bright, on-demand, highly indistinguishable single photons and entangled photon pairs. In order to build up quantum networks, it is now essential to interface remote quantum emitters. However, this is still an outstanding challenge, as the quantum states of dissimilar 'artificial atoms' have to be prepared on-demand with high fidelity, and the generated photons have to be made indistinguishable in all possible degrees of freedom. Here, we overcome this major obstacle and show an unprecedented two-photon interference (visibility of 51+/-5%) from remote strain-tunable GaAs quantum dots, emitting on-demand photon-pairs. We achieve this result by exploiting for the first time the full potential of the novel phonon-assisted two-photon excitation scheme, which allows for the generation of highly indistinguishable (visibility of 71+/-9%) entangled photon-pairs (fidelity of 90+/-2%), it enables push-to button biexciton state preparation (fidelity of 80+/-2%) and it outperforms conventional resonant two-photon excitation schemes in terms of robustness against environmental decoherence. Our results mark an important milestone for the practical realization of quantum repeaters and complex multi-photon entanglement experiments involving dissimilar artificial atoms. ",https://doi.org/10.1021/acs.nanolett.7b00777,1701.07812v3,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Modelling Kepler Red Giants in Eclipsing Binaries:Calibrating the Mixing   Length Parameter with Asteroseismology,2017,"  Stellar models rely on a number of free parameters. High-quality observations of eclipsing binary stars observed by Kepler offer a great opportunity to calibrate model parameters for evolved stars. Our study focuses on six Kepler red giants with the goal of calibrating the mixing-length parameter of convection as well as the asteroseismic surface term in models. We introduce a new method to improve the identification of oscillation modes which exploits theoretical frequencies to guide the mode identification ('peak-bagging') stage of the data analysis. Our results indicate that the convective mixing-length parameter (alpha) is about 14% larger for red giants than for the Sun, in agreement with recent results from modelling the APOGEE stars. We found that the asteroseismic surface term (i.e. the frequency offset between the observed and predicted modes) correlates with stellar parameters (Teff, log g) and the mixing-length parameter. This frequency offset generally decreases as giants evolve. The two coefficients a_-1 and a_3 for the inverse and cubic terms that have been used to describe the surface term correction are found to correlate linearly. The effect of the surface term is also seen in the p-g mixed modes, however, established methods for correcting the effect are not able to properly correct the g-dominated modes in late evolved stars. ",https://doi.org/10.1093/mnras/stx3079,1712.01424v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Angular Sizes and Effective Temperatures of O-type Stars from Optical   Interferometry with the CHARA Array,2018,"  We present interferometric observations of six O-type stars that were made with the Precision Astronomical Visible Observations (PAVO) beam combiner at the Center for High Angular Resolution Astronomy (CHARA) Array. The observations include multiple brackets for three targets, $\lambda$~Ori~A, $\zeta$~Oph, and 10~Lac, but there are only preliminary, single observations of the other three stars, $\xi$~Per, $\alpha$~Cam, and $\zeta$~Ori~A. The stellar angular diameters range from 0.55 milliarcsec for $\zeta$~Ori~A down to 0.11 mas for 10~Lac, the smallest star yet resolved with the CHARA Array. The rotational oblateness of the rapidly rotating star $\zeta$ Oph is directly measured for the first time. We assembled ultraviolet to infrared flux measurements for these stars, and then derived angular diameters and reddening estimates using model atmospheres and an effective temperature set by published results from analysis of the line spectrum. The model-based angular diameters are in good agreement with observed angular diameters. We also present estimates for the effective temperatures of these stars derived by setting the interferometric angular size and fitting the spectrophotometry. ",https://doi.org/10.3847/1538-4357/aaec04,1812.05511v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Giant planet occurrence within 0.2 AU of low-luminosity red giant branch   stars with K2,2019,"  Every Sun-like star will eventually evolve into a red giant, a transition which can profoundly affect the evolution of a surrounding planetary system. The timescale of dynamical planet evolution and orbital decay has important implications for planetary habitability, as well as post-main sequence star and planet interaction, evolution and internal structure. Here, we investigate these effects by estimating planet occurrence around 2476 low-luminosity red giant branch (LLRGB) stars observed by the NASA K2 mission. We measure stellar masses and radii using asteroseismology, with median random uncertainties of 3.7% in mass and 2.2% in radius. We compare this planet population to the known population of planets around dwarf Sun-like stars, accounting for detection efficiency differences between the stellar populations. We find that 0.51% +/- 0.29% of LLRGB stars host planets larger than Jupiter with orbital periods less than 10 days, tentatively higher than main sequence stars hosting similar planets (0.15% +/- 0.06%). Our results suggest that the effects of stellar evolution on the occurrence of close-in planets larger than Jupiter is not significant until stars have begun ascending substantially up the red giant branch (>~ 5-6 Rsun). ",https://doi.org/10.3847/1538-3881/ab4c35,1910.05346v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"The Gaia-Kepler Stellar Properties Catalog. I. Homogeneous Fundamental   Properties for 186,301 Kepler Stars",2020,"  An accurate and precise Kepler Stellar Properties Catalog is essential for the interpretation of the Kepler exoplanet survey results. Previous Kepler Stellar Properties Catalogs have focused on reporting the best-available parameters for each star, but this has required combining data from a variety of heterogeneous sources. We present the Gaia-Kepler Stellar Properties Catalog, a set of stellar properties of 186,301 Kepler stars, homogeneously derived from isochrones and broadband photometry, Gaia Data Release 2 parallaxes, and spectroscopic metallicities, where available. Our photometric effective temperatures, derived from $g-K_s$ colors, are calibrated on stars with interferometric angular diameters. Median catalog uncertainties are 112 K for $T_{\mathrm{eff}}$, 0.05 dex for $\log g$, 4% for $R_\star$, 7% for $M_\star$, 13% for $\rho_\star$, 10% for $L_\star$, and 56% for stellar age. These precise constraints on stellar properties for this sample of stars will allow unprecedented investigations into trends in stellar and exoplanet properties as a function of stellar mass and age. In addition, our homogeneous parameter determinations will permit more accurate calculations of planet occurrence and trends with stellar properties. ",Kein DOI-Link verfügbar,2001.07737v3,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of luminous red giants with Kepler I: Long Period   Variables with radial and non-radial modes,2020,"  While long period variables (LPVs) have been extensively investigated, especially with MACHO and OGLE data for the Magellanic Clouds, there still exist open questions in their pulsations regarding the excitation mechanisms, radial order and angular degree assignment. Here, we perform asteroseismic analyses on LPVs observed by the 4-year Kepler mission. Using a cross-correlation method, we detect unambiguous pulsation ridges associated with radial fundamental modes ($n=1$) and overtones ($n\geqslant2$), where the radial order assignment is made by using theoretical frequencies and observed frequencies. Our results confirm that the amplitude variability seen in semiregulars is consistent with oscillations being solar-like. We identify that the dipole modes, $l=1$, are dominant in the radial orders of $3\leq n \leq6$, and that quadrupole modes, $l=2$, are dominant in the first overtone $n=2$. A test of seismic scaling relations using Gaia DR2 parallaxes reveals the possibility that the relations break down when $\nu_{\rm max}$ $\lesssim$ 3 $\mu$Hz (R $\gtrsim$ 40 R$_{\odot}$, or log $\rm L/L_{\odot}$ $\gtrsim$ 2.6). Our homogeneous measurements of pulsation amplitude and period for 3213 LPVs will be very valuable for probing effects of pulsation on mass loss, in particular in those stars with periods around 60 days, which has been argued as a threshold of substantial pulsation-triggered mass loss. ",https://doi.org/10.1093/mnras/staa300,2001.10878v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Testing the intrinsic scatter of the asteroseismic scaling relations   with Kepler red giants,2020,"  Asteroseismic scaling relations are often used to derive stellar masses and radii, particulaly for stellar, exoplanet, and Galactic studies. It is therefore important that their precisions are known. Here we measure the intrinsic scatter of the underlying seismic scaling relations for $\Delta\nu$ and $\nu_{\rm max}$, using two sharp features that are formed in the H--R diagram (or related diagrams) by the red giant populations. These features are the edge near the zero-age core-helium-burning phase, and the strong clustering of stars at the so-called red giant branch bump. The broadening of those features is determined by factors including the intrinsic scatter of the scaling relations themselves, and therefore it is capable of imposing constraints on them. We modelled Kepler stars with a Galaxia synthetic population, upon which we applied the intrinsic scatter of the scaling relations to match the degree of sharpness seen in the observation. We found that the random errors from measuring $\Delta\nu$ and $\nu_{\rm max}$ provide the dominating scatter that blurs the features. As a consequence, we conclude that the scaling relations have intrinsic scatter of $\sim0.5\%$ ($\Delta\nu$), $\sim1.1\%$ ($\nu_{\rm max}$), $\sim1.7\%$ ($M$) and $\sim0.4\%$ ($R$), for the SYD pipeline measured $\Delta\nu$ and $\nu_{\rm max}$. This confirms that the scaling relations are very powerful tools. In addition, we show that standard evolution models fail to predict some of the structures in the observed population of both the HeB and RGB stars. Further stellar model improvements are needed to reproduce the exact distributions. ",https://doi.org/10.1093/mnras/staa3932,2012.10038v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Boyajian's Star B: The co-moving stellar companion to KIC 8462852,2021,"  The light curve of KIC 8462852, a.k.a Boyajian's Star, undergoes deep dips the origin of which remains unclear. A faint star $\approx$2\arcsec to the east was discovered in Keck/NIRC2 imaging in Boyajian et al. (2016), but its status as a binary, and possible contribution to the observed variability, was unclear. Here, we use three epochs of Keck/NIRC2 imaging, spanning five years, in JHK near-infrared bands to obtain 1-mas precision astrometry. We show that the two objects exhibit common proper motion, measure a relative velocity of $\mu=0.14\pm0.44$ mas yr$^{-1}$ ($\mu=0.30\pm0.93$ km s$^{-1}$) and conclude that they are a binary pair at $880\pm10$ AU projected separation. There is marginal detection of possible orbital motion, but our astrometry is insufficient to characterize the orbit. We show that two other point sources are not associated with KIC 8462852. We recommend that attempts to model KIC 8462852 A's light curve should revisit the possibility that the bound stellar companion may play a role in causing the irregular brightness variations, for example through disruption of the orbits of bodies around the primary due to long-term orbital evolution of the binary orbit. ",https://doi.org/10.3847/1538-4357/abdd33,2101.06313v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"A simple method to measure numax for asteroseismology: application to   16,000 oscillating Kepler red giants",2024,"  The importance of numax (the frequency of maximum oscillation power) for asteroseismology has been demonstrated widely in the previous decade, especially for red giants. With the large amount of photometric data from CoRoT, Kepler and TESS, several automated algorithms to retrieve numax values have been introduced. Most of these algorithms correct the granulation background in the power spectrum by fitting a model and subtracting it before measuring numax. We have developed a method that does not require fitting to the granulation background. Instead, we simply divide the power spectrum by a function of the form nu^-2, to remove the slope due to granulation background, and then smooth to measure numax. This method is fast, simple and avoids degeneracies associated with fitting. The method is able to measure oscillations in 99.9% of previously-studied Kepler red giants, with a systematic offset of 1.5 % in numax values that that we are able to calibrate. On comparing the seismic radii from this work with Gaia, we see similar trends to those observed in previous studies. Additionally, our values of width of the power envelope can clearly identify the dipole mode suppressed stars as a distinct population, hence as a way to detect them. We also applied our method to stars with low (0.19--18.35 muHz) and found it works well to correctly identify the oscillations. ",https://doi.org/10.1093/mnras/stae991,2401.17557v4,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,An unlikely survivor: a low-density hot Neptune orbiting a red giant   star,2023,"  Hot Neptunes, gaseous planets smaller than Saturn ($\sim$ 3-8 R$_\oplus$) with orbital periods less than 10 days, are rare. Models predict this is due to high-energy stellar irradiation stripping planetary atmospheres over time, often leaving behind only rocky planetary cores. We present the discovery of a 6.2 R$_\oplus$(0.55 R$_\mathrm{J}$), 19.2 M$_\oplus$(0.060 M$_\mathrm{J}$) planet transiting a red giant star every 4.21285 days. The old age and high equilibrium temperature yet remarkably low density of this planet suggests that its gaseous envelope should have been stripped by high-energy stellar irradiation billions of years ago. The present day planet mass and radius suggest atmospheric stripping was slower than predicted. Unexpectedly low stellar activity and/or late-stage planet inflation could be responsible for the observed properties of this system. ",Kein DOI-Link verfügbar,2303.06728v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Solar-like oscillations and activity in Procyon: A comparison of the   2007 MOST and ground-based radial velocity campaigns,2011,"  We compare the simultaneous 2007 space-based MOST photometry and ground-based radial velocity observations of the F5 star Procyon. We identify slow variations in the MOST data that are similar to those reported in the radial velocity (RV) time series, and confirm by comparison with the Sun that these variations are likely the signature of stellar activity. The MOST power spectrum yields clear evidence for individual oscillation frequencies that match those found in the radial velocity data by Bedding et al. (2010). We identify the same ridges due to modes of different spherical degree in both datasets, but are not able to confirm a definite ridge identification using the MOST data. We measure the luminosity amplitude per radial mode A_{l=0, phot} = 9.1 +/- 0.5 ppm. Combined with the estimate for the RV data by Arentoft et al. (2008) this gives a mean amplitude ratio of A_{l=0, phot}/A_{l=0, RV} = 0.24 +/- 0.02 ppm cm^{-1} s, considerably higher than expected from scaling relations but in reasonable agreement with theoretical models by Houdek (2010). We also compare the amplitude ratio as a function of frequency, and find that the maximum of the oscillation envelope is shifted to higher frequencies in photometry than in velocity. ",https://doi.org/10.1088/0004-637X/731/2/94,1102.2894v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"The K2 Ecliptic Plane Input Catalog (EPIC) and Stellar Classifications   of 138,600 Targets in Campaigns 1-8",2015,"  The K2 Mission uses the Kepler spacecraft to obtain high-precision photometry over ~80 day campaigns in the ecliptic plane. The Ecliptic Plane Input Catalog (EPIC) provides coordinates, photometry and kinematics based on a federation of all-sky catalogs to support target selection and target management for the K2 mission. We describe the construction of the EPIC, as well as modifications and shortcomings of the catalog. Kepler magnitudes (Kp) are shown to be accurate to ~0.1 mag for the Kepler field, and the EPIC is typically complete to Kp~17 (Kp~19 for campaigns covered by SDSS). We furthermore classify 138,600 targets in Campaigns 1-8 (~88% of the full target sample) using colors, proper motions, spectroscopy, parallaxes, and galactic population synthesis models, with typical uncertainties for G-type stars of ~3% in Teff, ~0.3 dex in log(g), ~40% in radius, ~10% in mass, and ~40% in distance. Our results show that stars targeted by K2 are dominated by K-M dwarfs (~41% of all selected targets), F-G dwarfs (~36%) and K giants (~21%), consistent with key K2 science programs to search for transiting exoplanets and galactic archeology studies using oscillating red giants. However, we find a significant variation of the fraction of cool dwarfs with galactic latitude, indicating a target selection bias due to interstellar reddening and the increased contamination by giant stars near the galactic plane. We discuss possible systematic errors in the derived stellar properties, and differences to published classifications for K2 exoplanet host stars. The EPIC is hosted at the Mikulski Archive for Space Telescopes (MAST): http://archive.stsci.edu/k2/epic/search.php. ",https://doi.org/10.3847/0067-0049/224/1/2,1512.02643v4,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Validation of the Exoplanet Kepler-21b using PAVO/CHARA Long-Baseline   Interferometry,2012,"  We present long-baseline interferometry of the Kepler exoplanet host star HD179070 (Kepler-21) using the PAVO beam combiner at the CHARA Array. The visibility data are consistent with a single star and exclude stellar companions at separations ~1-1000 mas (~ 0.1-113 AU) and contrasts < 3.5 magnitudes. This result supports the validation of the 1.6 R_{earth} exoplanet Kepler-21b by Howell et al. (2012) and complements the constraints set by adaptive optics imaging, speckle interferometry, and radial velocity observations to rule out false-positives due to stellar companions. We conclude that long-baseline interferometry has strong potential to validate transiting extrasolar planets, particularly for future projects aimed at brighter stars and for host stars where radial velocity follow-up is not available. ",https://doi.org/10.1111/j.1745-3933.2012.01242.x,1202.5307v2,Yes,potent(1)
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"Large eccentricity, low mutual inclination: the three-dimensional   architecture of a hierarchical system of giant planets",2014,"  We establish the three-dimensional architecture of the Kepler-419 (previously KOI-1474) system to be eccentric yet with a low mutual inclination. Kepler-419b is a warm Jupiter at semi-major axis a = 0.370 +0.007/-0.006 AU with a large eccentricity e=0.85 +0.08/-0.07 measured via the ""photoeccentric effect."" It exhibits transit timing variations induced by the non-transiting Kepler-419c, which we uniquely constrain to be a moderately eccentric (e=0.184 +/- 0.002), hierarchically-separated (a=1.68 +/- 0.03 AU) giant planet (7.3 +/- 0.4 MJup). We combine sixteen quarters of Kepler photometry, radial-velocity (RV) measurements from the HIgh Resolution Echelle Spectrometer (HIRES) on Keck, and improved stellar parameters that we derive from spectroscopy and asteroseismology. From the RVs, we measure the mass of inner planet to be 2.5+/-0.3MJup and confirm its photometrically-measured eccentricity, refining the value to e=0.83+/-0.01. The RV acceleration is consistent with the properties of the outer planet derived from TTVs. We find that, despite their sizable eccentricities, the planets are coplanar to within 9 +8/-6 degrees, and therefore the inner planet's large eccentricity and close-in orbit are unlikely to be the result of Kozai migration. Moreover, even over many secular cycles, the inner planet's periapse is most likely never small enough for tidal circularization. Finally, we present and measure a transit time and impact parameter from four simultaneous ground-based light curves from 1m-class telescopes, demonstrating the feasibility of ground-based follow-up of Kepler giant planets exhibiting large TTVs. ",https://doi.org/10.1088/0004-637X/791/2/89,1405.5229v3,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,On the Stellar Companion to the Exoplanet Hosting Star 30 Arietis B,2015,"  A crucial aspect of understanding planet formation is determining the binarity of the host stars. Results from radial velocity surveys and the follow-up of Kepler exoplanet candidates have demonstrated that stellar binarity certainly does not exclude the presence of planets in stable orbits and the configuration may in fact be relatively common. Here we present new results for the 30 Arietis system which confirms that the B component hosts both planetary and stellar companions. Keck AO imaging provides direct detection of the stellar companion and additional radial velocity data are consistent with an orbiting star. We present a revised orbit of the known planet along with photometry during predicted transit times. Finally, we provide constraints on the properties of the stellar companion based on orbital stability considerations. ",https://doi.org/10.1088/0004-637X/815/1/32,1511.01533v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Electrically-Pumped Wavelength-Tunable GaAs Quantum Dots Interfaced with   Rubidium Atoms,2016,"  We demonstrate the first wavelength-tunable electrically-pumped source of non-classical light that can emit photons with wavelength in resonance with the D2 transitions of 87Rb atoms. The device is fabricated by integrating a novel GaAs single-quantum-dot light-emitting-diode (LED) onto a piezoelectric actuator. By feeding the emitted photons into a 75-mm-long cell containing warm 87Rb atom vapor, we observe slow-light with a temporal delay of up to 3.4 ns. In view of the possibility of using 87Rb atomic vapors as quantum memories, this work makes an important step towards the realization of hybrid-quantum systems for future quantum networks. ",Kein DOI-Link verfügbar,1602.02122v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,A simple model to describe intrinsic stellar noise for exoplanet   detection around red giants,2016,"  In spite of the huge advances in exoplanet research provided by the NASA Kepler Mission, there remain only a small number of transit detections around evolved stars. Here we present a reformulation of the noise properties of red-giant stars, where the intrinsic stellar granulation, and the stellar oscillations described by asteroseismology play a key role. The new noise model is a significant improvement on the current Kepler results for evolved stars. Our noise model may be used to help understand planet detection thresholds for the ongoing K2 and upcoming TESS missions, and serve as a predictor of stellar noise for these missions. As an application of our noise model, we explore the minimum detectable planet radii for red giant stars, and find that Neptune sized planets should be detectable around low luminosity red giant branch stars. ",https://doi.org/10.1093/mnras/stw2782,1610.08688v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Synergy between asteroseismology and exoplanet science: an outlook,2018,"  Space-based asteroseismology has been playing an important role in the characterization of exoplanet-host stars and their planetary systems. The future looks even brighter, with space missions such as NASA's TESS and ESA's PLATO ready to take on this legacy. In this contribution, we provide an outlook on the synergy between asteroseismology and exoplanet science, namely, on the prospect of conducting a populational study of giant planets around oscillating evolved stars with the TESS mission. ",https://doi.org/10.5281/zenodo.2463210,1812.06150v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"The Impact of Stellar Multiplicity on Planetary Systems, I.: The Ruinous   Influence of Close Binary Companions",2016,"  The dynamical influence of binary companions is expected to profoundly influence planetary systems. However, the difficulty of identifying planets in binary systems has left the magnitude of this effect uncertain; despite numerous theoretical hurdles to their formation and survival, at least some binary systems clearly host planets. We present high-resolution imaging of 382 Kepler Objects of Interest (KOIs) obtained using adaptive-optics imaging and nonredundant aperture-mask interferometry (NRM) on the Keck-II telescope. Among the full sample of 506 candidate binary companions to KOIs, we super-resolve some binary systems to projected separations of <5 AU, showing that planets might form in these dynamically active environments. However, the full distribution of projected separations for our planet-host sample more broadly reveals a deep paucity of binary companions at solar-system scales. For a field binary population, we should have found 58 binary companions with projected separation \rho < 50 AU and mass ratio q > 0.4; we instead only found 23 companions (a 4.6 sigma deficit), many of which must be wider pairs that are only close in projection. When the binary population is parametrized with a semimajor axis cutoff a_cut and a suppression factor inside that cutoff S_bin, we find with correlated uncertainties that inside a_cut = 47 +59/-23 AU, the planet occurrence rate in binary systems is only S_bin = 0.34 +0.14/-0.15 times that of wider binaries or single stars. Our results demonstrate that a fifth of all solar-type stars in the Milky Way are disallowed from hosting planetary systems due to the influence of a binary companion. ",https://doi.org/10.3847/0004-6256/152/1/8,1604.05744v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,"Asteroseismology of 16000 Kepler Red Giants: Global Oscillation   Parameters, Masses, and Radii",2018,"  The Kepler mission has provided exquisite data to perform an ensemble asteroseismic analysis on evolved stars. In this work we systematically characterize solar-like oscillations and granulation for 16,094 oscillating red giants, using end-of-mission long-cadence data. We produced a homogeneous catalog of the frequency of maximum power (typical uncertainty $\sigma_{\nu_{\rm max}}$=1.6\%), the mean large frequency separation ($\sigma_{\Delta\nu}$=0.6\%), oscillation amplitude ($\sigma_{\rm A}$=4.7\%), granulation power ($\sigma_{\rm gran}$=8.6\%), power excess width ($\sigma_{\rm width}$=8.8\%), seismically-derived stellar mass ($\sigma_{\rm M}$=7.8\%), radius ($\sigma_{\rm R}$=2.9\%), and thus surface gravity ($\sigma_{\log g}$=0.01 dex). Thanks to the large red giant sample, we confirm that red-giant-branch (RGB) and helium-core-burning (HeB) stars collectively differ in the distribution of oscillation amplitude, granulation power, and width of power excess, which is mainly due to the mass difference. The distribution of oscillation amplitudes shows an extremely sharp upper edge at fixed $\nu_{\rm max}$, which might hold clues to understand the excitation and damping mechanisms of the oscillation modes. We find both oscillation amplitude and granulation power depend on metallicity, causing a spread of 15\% in oscillation amplitudes and a spread of 25\% in granulation power from [Fe/H]=-0.7 to 0.5 dex. Our asteroseismic stellar properties can be used as reliable distance indicators and age proxies for mapping and dating galactic stellar populations observed by Kepler. They will also provide an excellent opportunity to test asteroseismology using Gaia parallaxes, and lift degeneracies in deriving atmospheric parameters in large spectroscopic surveys such as APOGEE and LAMOST. ",https://doi.org/10.3847/1538-4365/aaaf74,1802.04455v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Testing the radius scaling relation with ${\it Gaia}$ DR2 in the ${\it   Kepler}$ field,2019,"  We compare radii based on ${\it Gaia}$ parallaxes to asteroseismic scaling relation-based radii of $\sim 300$ dwarfs $\&$ subgiants and $\sim 3600$ first-ascent giants from the ${\it Kepler}$ mission. Systematics due to temperature, bolometric correction, extinction, asteroseismic radius, and the spatially-correlated ${\it Gaia}$ parallax zero-point, contribute to a $2\%$ systematic uncertainty on the ${\it Gaia}$-asteroseismic radius agreement. We find that dwarf and giant scaling radii are on a parallactic scale at the $-2.1 \% \pm 0.5 \% {\rm \ (rand.)} \pm 2.0\% {\rm \ (syst.)}$ level (dwarfs) and $+1.7\% \pm 0.3\% {\rm \ (rand.)} \pm 2.0\% {\rm \ (syst.)}$ level (giants), supporting the accuracy and precision of scaling relations in this domain. In total, the $2\%$ agreement that we find holds for stars spanning radii between $0.8R_{\odot}$ and $30 R_{\odot}$. We do, however, see evidence for $\textit{relative}$ errors in scaling radii between dwarfs and giants at the $4\% \pm 0.6\%$ level, and find evidence of departures from simple scaling relations for radii above $30 R_{\odot}$. Asteroseismic masses for very metal-poor stars are still overestimated relative to astrophysical priors, but at a reduced level. We see no trend with metallicity in radius agreement for stars with $-0.5 <$ [Fe/H] $< +0.5$. We quantify the spatially-correlated parallax errors in the ${\it Kepler}$ field, which globally agree with the ${\it Gaia}$ team's published covariance model. We provide ${\it Gaia}$ radii, corrected for extinction and the ${\it Gaia}$ parallax zero-point for our full sample of $\sim 3900$ stars, including dwarfs, subgiants, and first-ascent giants. ",https://doi.org/10.3847/1538-4357/ab44a9,1910.00719v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Gaia-Kepler Stellar Properties Catalog. II. Planet Radius   Demographics as a Function of Stellar Mass and Age,2020,"  Studies of exoplanet demographics require large samples and precise constraints on exoplanet host stars. Using the homogeneous Kepler stellar properties derived using Gaia Data Release 2 by Berger et al. (2020), we re-compute Kepler planet radii and incident fluxes and investigate their distributions with stellar mass and age. We measure the stellar mass dependence of the planet radius valley to be $d \log R_{\mathrm{p}}$/$d \log M_\star = 0.26^{+0.21}_{-0.16}$, consistent with the slope predicted by a planet mass dependence on stellar mass ($0.24-0.35$) and core-powered mass-loss (0.33). We also find first evidence of a stellar age dependence of the planet populations straddling the radius valley. Specifically, we determine that the fraction of super-Earths ($1-1.8 \mathrm{R_\oplus}$) to sub-Neptunes ($1.8-3.5 \mathrm{R_\oplus}$) increases from $0.61 \pm 0.09$ at young ages (< 1 Gyr) to $1.00 \pm 0.10$ at old ages (> 1 Gyr), consistent with the prediction by core-powered mass-loss that the mechanism shaping the radius valley operates over Gyr timescales. Additionally, we find a tentative decrease in the radii of relatively cool ($F_{\mathrm{p}} < 150 \mathrm{F_\oplus}$) sub-Neptunes over Gyr timescales, which suggests that these planets may possess H/He envelopes instead of higher mean molecular weight atmospheres. We confirm the existence of planets within the hot sub-Neptunian ""desert"" ($2.2 < R_{\mathrm{p}} < 3.8 \mathrm{R_\oplus}$, $F_{\mathrm{p}} > 650 \mathrm{F_\oplus}$) and show that these planets are preferentially orbiting more evolved stars compared to other planets at similar incident fluxes. In addition, we identify candidates for cool ($F_{\mathrm{p}} < 20 \mathrm{F_\oplus}$) inflated Jupiters, present a revised list of habitable zone candidates, and find that the ages of single- and multiple-transiting planet systems are statistically indistinguishable. ",https://doi.org/10.3847/1538-3881/aba18a,2005.14671v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of luminous red giants with Kepler. II. Dependence of   mass loss on pulsations and radiation,2020,"  Mass loss by red giants is an important process to understand the final stages of stellar evolution and the chemical enrichment of the interstellar medium. Mass-loss rates are thought to be controlled by pulsation-enhanced dust-driven outflows. Here we investigate the relationships between mass loss, pulsations, and radiation, using 3213 luminous Kepler red giants and 135000 ASAS-SN semiregulars and Miras. Mass-loss rates are traced by infrared colours using 2MASS and WISE and by observed-to-model WISE fluxes, and are also estimated using dust mass-loss rates from literature assuming a typical gas-to-dust mass ratio of 400. To specify the pulsations, we extract the period and height of the highest peak in the power spectrum of oscillation. Absolute magnitudes are obtained from the 2MASS Ks band and the Gaia DR2 parallaxes. Our results follow. (i) Substantial mass loss sets in at pulsation periods above ~60 and ~100 days, corresponding to Asymptotic-Giant-Branch stars at the base of the period-luminosity sequences C' and C. (ii) The mass-loss rate starts to rapidly increase in semiregulars for which the luminosity is just above the Red-Giant-Branch tip and gradually plateaus to a level similar to that of Miras. (iii) The mass-loss rates in Miras do not depend on luminosity, consistent with pulsation-enhanced dust-driven winds. (iv) The accumulated mass loss on the Red Giant Branch consistent with asteroseismic predictions reduces the masses of red-clump stars by 6.3%, less than the typical uncertainty on their asteroseismic masses. Thus mass loss is currently not a limitation of stellar age estimates for galactic archaeology studies. ",https://doi.org/10.1093/mnras/staa3970,2012.12414v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,The Directly-Imaged Exoplanet Host Star 51 Eridani is a Gamma Doradus   Pulsator,2022,"  51 Eri is well known for hosting a directly-imaged giant planet and for its membership to the $\beta$ Pictoris moving group. Using two-minute cadence photometry from the Transiting Exoplanet Survey Satellite (TESS), we detect multi-periodic variability in 51 Eri that is consistent with pulsations of Gamma Doradus ($\gamma$ Dor) stars. We identify the most significant pulsation modes (with frequencies between $\sim$0.5-3.9 cycles/day and amplitudes ranging between $\sim$1-2 mmag) as dipole and quadrupole gravity-modes, as well as Rossby modes, as previously observed in Kepler $\gamma$ Dor stars. Our results demonstrate that previously reported variability attributed to stellar rotation is instead likely due to $\gamma$ Dor pulsations. Using the mean frequency of the $\ell =1$ gravity-modes, together with empirical trends of the Kepler $\gamma$ Dor population, we estimate a plausible stellar core rotation period of 0.9$^{+0.3}_{-0.1}$ days for 51 Eri. We find no significant evidence for transiting companions around 51 Eri in the residual light curve. The detection of $\gamma$ Dor pulsations presented here, together with follow-up observations and modeling, may enable the determination of an asteroseismic age for this benchmark system. Future TESS observations would allow a constraint on the stellar core rotation rate, which in turn traces the surface rotation rate, and thus would help clarify whether or not the stellar equatorial plane and orbit of 51 Eri b are coplanar. ",https://doi.org/10.3847/1538-4357/ac9229,2205.01103v2,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismic masses of four evolved planet-hosting stars using SONG and   TESS: resolving the retired A-star mass controversy,2020,"  The study of planet occurrence as a function of stellar mass is important for a better understanding of planet formation. Estimating stellar mass, especially in the red giant regime, is difficult. In particular, stellar masses of a sample of evolved planet-hosting stars based on spectroscopy and grid-based modelling have been put to question over the past decade with claims they were overestimated. Although efforts have been made in the past to reconcile this dispute using asteroseismology, results were inconclusive. In an attempt to resolve this controversy, we study four more evolved planet-hosting stars in this paper using asteroseismology, and we revisit previous results to make an informed study of the whole ensemble in a self-consistent way. For the four new stars, we measure their masses by locating their characteristic oscillation frequency, $\mathrm{\nu}_{\mathrm{max}}$, from their radial velocity time series observed by SONG. For two stars, we are also able to measure the large frequency separation, $\mathrm{\Delta\nu}$, helped by extended SONG single-site and dual-site observations and new TESS observations. We establish the robustness of the $\mathrm{\nu}_{\mathrm{max}}$-only-based results by determining the stellar mass from $\mathrm{\Delta\nu}$, and from both $\mathrm{\Delta\nu}$ and $\mathrm{\nu}_{\mathrm{max}}$. We then compare the seismic masses of the full ensemble of 16 stars with the spectroscopic masses from three different literature sources. We find an offset between the seismic and spectroscopic mass scales that is mass-dependent, suggesting that the previously claimed overestimation of spectroscopic masses only affects stars more massive than about 1.6 M$_\mathrm{\odot}$. ",https://doi.org/10.1093/mnras/staa1793,2006.07649v3,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Projected Rotational Velocities and Fundamental Properties of Low-Mass   Pre-Main Sequence Stars in the Taurus-Auriga Star Forming Region,2021,"  The projected stellar rotational velocity ($v \sin i$) is critical for our understanding of processes related to the evolution of angular momentum in pre-main sequence stars. We present $v \sin i$ measurements of high-resolution infrared and optical spectroscopy for 70 pre-main sequence stars in the Taurus-Auriga star-forming region, in addition to effective temperatures measured from line-depth ratios, and stellar rotation periods determined from optical photometry. From the literature, we identified the stars in our sample that show evidence of residing in circumstellar disks or multiple systems. The comparison of infrared $v \sin i$ measurements calculated using two techniques shows a residual scatter of $\sim$ 1.8 km s$^{-1}$, defining a typical error floor for the $v \sin i$ of pre-main sequence stars from infrared spectra. A comparison of the $v \sin i$ distributions of stars with and without companions shows that binaries/multiples typically have a higher measured $v \sin i$, which may be caused by contamination by companion lines, shorter disk lifetimes in binary systems, or tidal interactions in hierarchical triples. A comparison of optical and infrared $v \sin i$ values shows no significant difference regardless of whether the star has a disk or not, indicating that CO contamination from the disk does not impact $v \sin i$ measurements above the typical $\sim$ 1.8 km s$^{-1}$ error floor of our measurements. Finally, we observe a lack of a correlation between the $v \sin i$, presence of a disk, and H-R diagram position, which indicates a complex interplay between stellar rotation and evolution of pre-main sequence stars. ",https://doi.org/10.3847/1538-4357/abeab3,2103.01246v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,A prescription for the asteroseismic surface correction,2022,"  In asteroseismology, the surface effect refers to a disparity between the observed and the modelled frequencies in stars with solar-like oscillations. It originates from improper modelling of the surface layers. Correcting the surface effect usually requires using functions with free parameters, which are conventionally fitted to the observed frequencies. On the basis that the correction should vary smoothly across the H--R diagram, we parameterize it as a simple function of surface gravity, effective temperature, and metallicity. We determine this function by fitting a wide range of stars. The absolute amount of the surface correction decreases with luminosity, but the ratio between it and $\nu_{\rm max}$ increases, suggesting the surface effect is more important for red giants than dwarfs. Applying the prescription can eliminate unrealistic surface correction, which improves parameter estimations with stellar modelling. Using two open clusters, we found a reduction of scatter in the model-derived ages for each star in the same cluster. As an important application, we provide a new revision for the $\Delta\nu$ scaling relation that, for the first time, accounts for the surface correction. The values of the correction factor, $f_{\Delta\nu}$, are up to 2\% smaller than those determined without the surface effect considered, suggesting decreases of up to 4\% in radii and up to 8\% in masses when using the asteroseismic scaling relations. This revision brings the asteroseismic properties into an agreement with those determined from eclipsing binaries. The new correction factor and the stellar models with the corrected frequencies are available at {https://www.github.com/parallelpro/surface}. ",https://doi.org/10.1093/mnras/stad1445,2208.01176v3,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Roman CCS White Paper: Adding Fields Hosting Globular Clusters To The   Galactic Bulge Time Domain Survey,2023,"  Despite multiple previous searches, no transiting planets have yet been identified within a globular cluster. This is believed to be due to a combination of factors: the low metallicities of most globular clusters suggests that there is significantly less planet-forming material per star in most globular clusters relative to the solar neighborhood, the high likelihood of dynamical interactions can also disrupt planetary orbits, and the data available for globular clusters is limited. However, transiting planets have been identified in open clusters, indicating that there may be planets in more massive clusters that have simply gone undetected, or that more massive clusters inhibit planet formation. Less than two degrees away from the nominal Galactic Bulge Time Domain Survey footprint, two globular clusters, NGC 6522 and NGC 6528, can be simultaneously observed by the Roman telescope during the Galactic Bulge Time Domain Survey. These clusters are comparable in mass (1-2 x 10$^5$ solar masses) and age (12 Gyr), but feature drastically different average metallicities: NGC 6522 has an average [Fe/H] $\sim$ -1.3, while NGC 6528 has an average [Fe/H] $\sim$ -0.1. If no transiting planets are detected in one season of time domain observations of these clusters, this would indicate a difference in planet occurrence among field stars and globular clusters at >3-$\sigma$ significance even after accounting for metallicity, which could be enhanced to >5-$\sigma$ significance with similar observations of another nearby field hosting a metal-rich globular cluster. Additionally, time domain observations of NGC 6522 and NGC 6528 will detect variable stars in both clusters, testing the connection between stellar variability and binary fraction to metallicity and cluster environment, as well as testing the dependence of exoplanet yields on stellar density and distance from the Galactic midplane. ",Kein DOI-Link verfügbar,2306.10647v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Asteroseismology of Solar-type stars with Kepler III. Ground-based Data,2010,"  We report on the ground-based follow-up program of spectroscopic and photometric observations of solar-like asteroseismic targets for the Kepler space mission. These stars constitute a large group of more than thousand objects which are the subject of an intensive study of the Kepler Asteroseismic Science Consortium Working Group 1 (KASC WG-1). The main goal of this coordinated research is the determination of the fundamental stellar atmospheric parameters, which are used for the computing of their asteroseismic models, as well as for the verification of the Kepler Input Catalogue (KIC). ",https://doi.org/10.1002/asna.201011440,1005.0986v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Solving the mode identification problem in asteroseismology of F stars   observed with Kepler,2012,"  Asteroseismology of F-type stars has been hindered by an ambiguity in identification of their oscillation modes. The regular mode pattern that makes this task trivial in cooler stars is masked by increased linewidths. The absolute mode frequencies, encapsulated in the asteroseismic variable epsilon, can help solve this impasse because the values of epsilon implied by the two possible mode identifications are distinct. We find that the correct epsilon can be deduced from the effective temperature and the linewidths and we apply these methods to a sample of solar-like oscillators observed with Kepler. ",https://doi.org/10.1088/2041-8205/751/2/L36,1205.0544v1,No,
0009-0008-8481-711X,Daniel Huber,Hochschule Aalen,Hyperfine-interaction limits polarization entanglement of photons from   semiconductor quantum dots,2023,"  Excitons in quantum dots are excellent sources of polarization-entangled photon pairs, but a quantitative understanding of their interaction with the nuclear spin bath is still missing. Here we investigate the role of hyperfine energy shifts using experimentally accessible parameters and derive an upper limit to the achievable entanglement fidelity. Our results are consistent with all available literature, indicate that spin-noise is often the dominant process limiting the entanglement in InGaAs quantum dots, and suggest routes to alleviate its effect. ",https://doi.org/10.1103/PhysRevB.108.L081405,2302.05983v3,No,
0009-0003-0291-3911,Marcus Gelderie,Hochschule Aalen,Weak $ω$-Regular Trace Languages,2014,"  Mazurkiewicz traces describe concurrent behaviors of distributed systems. Trace-closed word languages, which are ""linearizations"" of trace languages, constitute a weaker notion of concurrency but still give us tools to investigate the latter. In this vein, our contribution is twofold. Firstly, we develop definitions that allow classification of $\omega$-regular trace languages in terms of the corresponding trace-closed $\omega$-regular word languages, capturing E-recognizable (reachability) and (deterministically) B\""uchi recognizable languages. Secondly, we demonstrate the first automata-theoretic result that shows the equivalence of $\omega$-regular trace-closed word languages and Boolean combinations of deterministically $I$-diamond B\""uchi recognizable trace-closed languages. ",Kein DOI-Link verfügbar,1402.3199v1,No,
0009-0003-0291-3911,Marcus Gelderie,Hochschule Aalen,Memory Reduction via Delayed Simulation,2011,"  We address a central (and classical) issue in the theory of infinite games: the reduction of the memory size that is needed to implement winning strategies in regular infinite games (i.e., controllers that ensure correct behavior against actions of the environment, when the specification is a regular omega-language). We propose an approach which attacks this problem before the construction of a strategy, by first reducing the game graph that is obtained from the specification. For the cases of specifications represented by ""request-response""-requirements and general ""fairness"" conditions, we show that an exponential gain in the size of memory is possible. ",https://doi.org/10.4204/EPTCS.50.4,1102.4120v1,No,
0000-0001-9828-1039,Daria Kern,Hochschule Aalen,3D Bounding Box Detection in Volumetric Medical Image Data: A Systematic   Literature Review,2020,"  This paper discusses current methods and trends for 3D bounding box detection in volumetric medical image data. For this purpose, an overview of relevant papers from recent years is given. 2D and 3D implementations are discussed and compared. Multiple identified approaches for localizing anatomical structures are presented. The results show that most research recently focuses on Deep Learning methods, such as Convolutional Neural Networks vs. methods with manual feature engineering, e.g. Random-Regression-Forests. An overview of bounding box detection options is presented and helps researchers to select the most promising approach for their target objects. ",Kein DOI-Link verfügbar,2012.05745v1,No,
0000-0003-3649-739X,Till Steinbach,Hamburg Universität der Angewandte Wissenschaften,"Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,   September 2, 2014",2014,"  This is the Proceedings of the 1st OMNeT++ Community Summit, which was held in Hamburg, Germany, September 2, 2014. ",Kein DOI-Link verfügbar,1409.0093v2,No,
0000-0003-3649-739X,Till Steinbach,Hamburg Universität der Angewandte Wissenschaften,Extending OMNeT++ Towards a Platform for the Design of Future In-Vehicle   Network Architectures,2016,"  In-vehicle communication technologies are evolving. While today's cars are equipped with fieldbusses to interconnect the various electronic control units, next generation vehicles have timing and bandwidth requirements that exceed the capacities. In particular Advanced Driver Assistance Systems (ADAS) and automated driving using high bandwidth sensors such as cameras, LIDAR or radar will challenge the in-car network. Automotive Ethernet is the most promising candidate to solve the upcoming challenges. But to design and evaluate new protocols, concepts, and architectures suitable analysis tools are required. Especially in the interim period with architectures using automotive Ethernet and legacy fieldbusses together, careful planning and design is of vital importance. Simulation can provide a good understanding of the expectable network metrics in an early development phase. This paper contributes a workflow as well as the required toolchain to evaluate new real-time Ethernet communication architectures using event based simulation in OMNeT++. We introduce a domain specific language (DSL) - the Abstract Network Description Language (ANDL) - to describe and configure the simulation and present the required simulation models for real-time Ethernet and fieldbus technologies such as CAN and FlexRay. We further introduce new analysis tools for special in-vehicle network use-cases and the interaction of the simulation with third-party applications established in the automotive domain. ",Kein DOI-Link verfügbar,1609.05179v1,No,
0000-0003-3649-739X,Till Steinbach,Hamburg Universität der Angewandte Wissenschaften,Simulation of Mixed Critical In-vehicular Networks,2018,"  Future automotive applications ranging from advanced driver assistance to autonomous driving will largely increase demands on in-vehicular networks. Data flows of high bandwidth or low latency requirements, but in particular many additional communication relations will introduce a new level of complexity to the in-car communication system. It is expected that future communication backbones which interconnect sensors and actuators with ECU in cars will be built on Ethernet technologies. However, signalling from different application domains demands for network services of tailored attributes, including real-time transmission protocols as defined in the TSN Ethernet extensions. These QoS constraints will increase network complexity even further. Event-based simulation is a key technology to master the challenges of an in-car network design. This chapter introduces the domain-specific aspects and simulation models for in-vehicular networks and presents an overview of the car-centric network design process. Starting from a domain specific description language, we cover the corresponding simulation models with their workflows and apply our approach to a related case study for an in-car network of a premium car. ",Kein DOI-Link verfügbar,1808.03081v1,No,
0000-0003-1623-5309,Marina Tropmann-Frick,Hamburg Universität der Angewandte Wissenschaften,Responsible Artificial Intelligence: A Structured Literature Review,2024,"  Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an approach for developing a future framework centered around this concept. Our findings advocate for a human-centric approach to Responsible AI. This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust. ",Kein DOI-Link verfügbar,2403.06910v1,Yes,potent(1)
0000-0003-1623-5309,Marina Tropmann-Frick,Hamburg Universität der Angewandte Wissenschaften,Exploring Automatic Text Simplification of German Narrative Documents,2023,"  In this paper, we apply transformer-based Natural Language Generation (NLG) techniques to the problem of text simplification. Currently, there are only a few German datasets available for text simplification, even fewer with larger and aligned documents, and not a single one with narrative texts. In this paper, we explore to which degree modern NLG techniques can be applied to German narrative text simplifications. We use Longformer attention and a pre-trained mBART model. Our findings indicate that the existing approaches for German are not able to solve the task properly. We conclude on a few directions for future research to address this problem. ",Kein DOI-Link verfügbar,2312.09907v1,No,
0000-0002-7808-3825,Rinie Akkermans,Hamburg Universität der Angewandte Wissenschaften,Coordinating Cooperative Perception in Urban Air Mobility for Enhanced   Environmental Awareness,2024,"  The trend for Urban Air Mobility (UAM) is growing with prospective air taxis, parcel deliverers, and medical and industrial services. Safe and efficient UAM operation relies on timely communication and reliable data exchange. In this paper, we explore Cooperative Perception (CP) for Unmanned Aircraft Systems (UAS), considering the unique communication needs involving high dynamics and a large number of UAS. We propose a hybrid approach combining local broadcast with a central CP service, inspired by centrally managed U-space and broadcast mechanisms from automotive and aviation domains. In a simulation study, we show that our approach significantly enhances the environmental awareness for UAS compared to fully distributed approaches, with an increased communication channel load, which we also evaluate. These findings prompt a discussion on communication strategies for CP in UAM and the potential of a centralized CP service in future research. ",Kein DOI-Link verfügbar,2405.03290v2,Yes,potent(1)
0000-0001-8079-2797,Peer Stelldinger,Hamburg Universität der Angewandte Wissenschaften,On de Bruijn Rings and Families of Almost Perfect Maps,2024,"  De Bruijn tori, also called perfect maps, are two-dimensional periodic arrays of letters drawn from a given finite alphabet, such that each possible pattern of a given shape $(m,n)$ appears exactly once within one period of the torus. It is still unknown if de Bruijn tori of some certain size exist, like e.g. square shaped de Bruijn Tori with odd $m=n\in\{3,5,7\}$ and an even alphabet size $k$. However, in certain applications like positional coding, sub-perfect maps are sufficient, i.e. one does not need every possible $(m,n)$-pattern to appear, as long as a sufficient large number of such patterns is captured and every pattern occurs at most once. We show, that given any $m=n$ and a square alphabet size $k^2$, one can efficiently construct a sub-perfect map which is almost perfect, i.e. of almost maximal size. We do this by introducing de Bruijn rings, i.e. sub-perfect maps of minimal height, and providing an efficient construction method for them. We extend our results to non-square torus shapes and arbitrary non-prime alphabet sizes. ",Kein DOI-Link verfügbar,2405.03309v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Selected inversion as key to a stable Langevin evolution across the QCD   phase boundary,2017,"  We present new results of full QCD at nonzero chemical potential. In PRD 92, 094516 (2015) the complex Langevin method was shown to break down when the inverse coupling decreases and enters the transition region from the deconfined to the confined phase. We found that the stochastic technique used to estimate the drift term can be very unstable for indefinite matrices. This may be avoided by using the full inverse of the Dirac operator, which is, however, too costly for four-dimensional lattices. The major breakthrough in this work was achieved by realizing that the inverse elements necessary for the drift term can be computed efficiently using the selected inversion technique provided by the parallel sparse direct solver package PARDISO. In our new study we show that no breakdown of the complex Langevin method is encountered and that simulations can be performed across the phase boundary. ",https://doi.org/10.1051/epjconf/201817507003,1707.08874v1,Yes,potent(1)
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,On Large Scale Diagonalization Techniques for the Anderson Model of   Localization,2005,"  We propose efficient preconditioning algorithms for an eigenvalue problem arising in quantum physics, namely the computation of a few interior eigenvalues and their associated eigenvectors for the largest sparse real and symmetric indefinite matrices of the Anderson model of localization. We compare the Lanczos algorithm in the 1987 implementation by Cullum and Willoughby with the shift-and-invert techniques in the implicitly restarted Lanczos method and in the Jacobi-Davidson method. Our preconditioning approaches for the shift-and-invert symmetric indefinite linear system are based on maximum weighted matchings and algebraic multilevel incomplete $LDL^T$ factorizations. These techniques can be seen as a complement to the alternative idea of using more complete pivoting techniques for the highly ill-conditioned symmetric indefinite Anderson matrices. We demonstrate the effectiveness and the numerical accuracy of these algorithms. Our numerical examples reveal that recent algebraic multilevel preconditioning solvers can accelerative the computation of a large-scale eigenvalue problem corresponding to the Anderson model of localization by several orders of magnitude. ",https://doi.org/10.1137/050637649,math/0508111v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Complete results for a numerical evaluation of interior point solvers   for large-scale optimal power flow problems,2018,"  Recent advances in open source interior-point optimization methods and power system related software have provided researchers and educators with the necessary platform for simulating and optimizing power networks with unprecedented convenience. Within the Matpower software platform a combination of several different interior point optimization methods are provided and four different optimal power flow (OPF) formulations are recently available: the Polar-Power, Polar-Current, Cartesian-Power, and Cartesian-Current. The robustness and reliability of interior-point methods for different OPF formulations for minimizing the generation cost starting from different initial guesses, for a wide range of networks provided in the Matpower library ranging from 1951 buses to 193000 buses, will be investigated. Performance profiles are presented for iteration counts, overall time, and memory consumption, revealing the most reliable optimization method for the particular metric. ",Kein DOI-Link verfügbar,1807.03964v4,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Structure Exploiting Interior Point Methods,2019,"  Interior point methods are among the most popular techniques for large scale nonlinear optimization, owing to their intrinsic ability of scaling to arbitrary large problem sizes. Their efficiency has attracted in recent years a lot of attention due to increasing demand for large scale optimization in industry and engineering. A parallel interior point method is discussed that exploits the intrinsic structure of large-scale nonlinear optimization problems so that the solution process can employ massively parallel high-performance computing infastructures. Since the overall performance of interior point methods relies heavily on scalable sparse linear algebra solvers, particular emphasis is given to the underlying algorithms for the distributed solution of the associated sparse linear systems obtained at each iteration from the linearization of the optimality conditions. The interior point algorithm is implemented in a object-oriented parallel IPM solver and applied for the solution of large scale optimal control problems solved in a daily basis for the secure transmission and distribution of electricity in modern power grids. ",Kein DOI-Link verfügbar,1907.05420v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,High Performance Block Incomplete LU Factorization,2019,"  Many application problems that lead to solving linear systems make use of preconditioned Krylov subspace solvers to compute their solution. Among the most popular preconditioning approaches are incomplete factorization methods either as single-level approaches or within a multilevel framework. We will present a block incomplete factorization that is based on skillfully blocking the system initially and throughout the factorization. This approach allows for the use of cache-optimized dense matrix kernels such as level-3 BLAS or LAPACK. We will demonstrate how this block approach outperforms the scalar method often by orders of magnitude on modern architectures, paving the way for its prospective use inside various multilevel incomplete factorization approaches or other applications where the core part relies on an incomplete factorization. ",Kein DOI-Link verfügbar,1908.10169v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Reduced-Space Interior Point Methods in Power Grid Problems,2020,"  Due to critical environmental issues, the power systems have to accommodate a significant level of penetration of renewable generation which requires smart approaches to the power grid control. Associated optimal control problems are large-scale nonlinear optimization problems with up to hundreds of millions of variables and constraints. The interior point methods become computationally intractable, mainly due to the solution of large linear systems.   This document addresses the computational bottlenecks of the interior point method during the solution of the security constrained optimal power flow problems by applying reduced space quasi-Newton IPM, which could utilize high-performance computers due to the inherent parallelism in the adjoint method. Reduced space IPM approach and the adjoint method is a novel approach when it comes to solving the (security constrained) optimal power flow problems. These were previously used in the PDE-constrained optimization. The presented methodology is suitable for high-performance architectures due to inherent parallelism in the adjoint method during the gradient evaluation, since the individual contingency scenarios are modeled by independent set of the constraints. Preliminary evaluation of the performance and convergence is performed to study the reduced space approach. ",Kein DOI-Link verfügbar,2001.10815v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Application of deep and reinforcement learning to boundary control   problems,2023,"  The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.   This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments indicate that the proposed method can rival the speed and accuracy of existing solvers. In our preliminary results, the network attains costs lower than IPOPT, a state-of-the-art non-linear IPM, in 51\% cases. The overall number of floating point operations in the proposed method is similar to that of IPOPT. Additionally, the informed initial guess method and the learned momentum-like behaviour in the optimizer method are incorporated to avoid convergence to local minima. ",Kein DOI-Link verfügbar,2310.15191v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Optimizing gas networks using adjoint gradients,2018,"  An increasing amount of gas-fired power plants are currently being installed in modern power grids worldwide. This is due to their low cost and the inherent flexibility offered to the electrical network, particularly in the face of increasing renewable generation. However, the integration and operation of gas generators poses additional challenges to gas network operators, mainly because they can induce rapid changes in the demand. This paper presents an efficient minimization scheme of gas compression costs under dynamic conditions where deliveries to customers are described by time-dependent mass flows. The optimization scheme is comprised of a set of transient nonlinear partial differential equations that model the isothermal gas flow in pipes, an adjoint problem for efficient calculation of the objective gradients and constraint Jacobians, and state-of-the-art optimal control methods for solving nonlinear programs. As the evaluation of constraint Jacobians can become computationally costly as the number of constraints increases, efficient constraint lumping schemes are proposed and investigated with respect to accuracy and performance. The resulting optimal control problems are solved using both interior-point and sequential quadratic programming methods. The proposed optimization framework is validated through several benchmark cases of increasing complexity. ",Kein DOI-Link verfügbar,1804.09601v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Sensitivity Analysis of High-Dimensional Models with Correlated Inputs,2023,"  Sensitivity analysis is an important tool used in many domains of computational science to either gain insight into the mathematical model and interaction of its parameters or study the uncertainty propagation through the input-output interactions. In many applications, the inputs are stochastically dependent, which violates one of the essential assumptions in the state-of-the-art sensitivity analysis methods. Consequently, the results obtained ignoring the correlations provide values which do not reflect the true contributions of the input parameters. This study proposes an approach to address the parameter correlations using a polynomial chaos expansion method and Rosenblatt and Cholesky transformations to reflect the parameter dependencies. Treatment of the correlated variables is discussed in context of variance and derivative-based sensitivity analysis. We demonstrate that the sensitivity of the correlated parameters can not only differ in magnitude, but even the sign of the derivative-based index can be inverted, thus significantly altering the model behavior compared to the prediction of the analysis disregarding the correlations. Numerous experiments are conducted using workflow automation tools within the VECMA toolkit. ",Kein DOI-Link verfügbar,2306.00555v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Multiway $p$-spectral graph cuts on Grassmann manifolds,2020,"  Nonlinear reformulations of the spectral clustering method have gained a lot of recent attention due to their increased numerical benefits and their solid mathematical background. We present a novel direct multiway spectral clustering algorithm in the $p$-norm, for $p \in (1, 2]$. The problem of computing multiple eigenvectors of the graph $p$-Laplacian, a nonlinear generalization of the standard graph Laplacian, is recasted as an unconstrained minimization problem on a Grassmann manifold. The value of $p$ is reduced in a pseudocontinuous manner, promoting sparser solution vectors that correspond to optimal graph cuts as $p$ approaches one. Monitoring the monotonic decrease of the balanced graph cuts guarantees that we obtain the best available solution from the $p$-levels considered. We demonstrate the effectiveness and accuracy of our algorithm in various artificial test-cases. Our numerical examples and comparative results with various state-of-the-art clustering methods indicate that the proposed method obtains high quality clusters both in terms of balanced graph cut metrics and in terms of the accuracy of the labelling assignment. Furthermore, we conduct studies for the classification of facial images and handwritten characters to demonstrate the applicability in real-world datasets. ",https://doi.org/10.1007/s10994-021-06108-1,2008.13210v2,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Level-based Blocking for Sparse Matrices: Sparse Matrix-Power-Vector   Multiplication,2022,"  The multiplication of a sparse matrix with a dense vector (SpMV) is a key component in many numerical schemes and its performance is known to be severely limited by main memory access. Several numerical schemes require the multiplication of a sparse matrix polynomial with a dense vector, which is typically implemented as a sequence of SpMVs. This results in low performance and ignores the potential to increase the arithmetic intensity by reusing the matrix data from cache. In this work we use the recursive algebraic coloring engine (RACE) to enable blocking of sparse matrix data across the polynomial computations. In the graph representing the sparse matrix we form levels using a breadth-first search. Locality relations of these levels are then used to improve spatial and temporal locality when accessing the matrix data and to implement an efficient multithreaded parallelization. Our approach is independent of the matrix structure and avoids shortcomings of existing ""blocking"" strategies in terms of hardware efficiency and parallelization overhead. We quantify the quality of our implementation using performance modelling and demonstrate speedups of up to 3$\times$ and 5$\times$ compared to an optimal SpMV-based baseline on a single multicore chip of recent Intel and AMD architectures. As a potential application, we demonstrate the benefit of our implementation for a Chebyshev time propagation scheme, representing the class of polynomial approximations to exponential integrators. Further numerical schemes which may benefit from our developments include $s$-step Krylov solvers and power clustering algorithms. ",https://doi.org/10.1109/TPDS.2022.3223512,2205.01598v1,Yes,potent(2)
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Integrated Nested Laplace Approximations for Large-Scale   Spatial-Temporal Bayesian Modeling,2023,"  Bayesian inference tasks continue to pose a computational challenge. This especially holds for spatial-temporal modeling where high-dimensional latent parameter spaces are ubiquitous. The methodology of integrated nested Laplace approximations (INLA) provides a framework for performing Bayesian inference applicable to a large subclass of additive Bayesian hierarchical models. In combination with the stochastic partial differential equations (SPDE) approach it gives rise to an efficient method for spatial-temporal modeling. In this work we build on the INLA-SPDE approach, by putting forward a performant distributed memory variant, INLA-DIST, for large-scale applications. To perform the arising computational kernel operations, consisting of Cholesky factorizations, solving linear systems, and selected matrix inversions, we present two numerical solver options, a sparse CPU-based library and a novel blocked GPU-accelerated approach which we propose. We leverage the recurring nonzero block structure in the arising precision (inverse covariance) matrices, which allows us to employ dense subroutines within a sparse setting. Both versions of INLA-DIST are highly scalable, capable of performing inference on models with millions of latent parameters. We demonstrate their accuracy and performance on synthetic as well as real-world climate dataset applications. ",Kein DOI-Link verfügbar,2303.15254v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Compile-Time Symbolic Differentiation Using C++ Expression Templates,2017,"  Template metaprogramming is a popular technique for implementing compile time mechanisms for numerical computing. We demonstrate how expression templates can be used for compile time symbolic differentiation of algebraic expressions in C++ computer programs. Given a positive integer $N$ and an algebraic function of multiple variables, the compiler generates executable code for the $N$th partial derivatives of the function. Compile-time simplification of the derivative expressions is achieved using recursive templates. A detailed analysis indicates that current C++ compiler technology is already sufficient for practical use of our results, and highlights a number of issues where further improvements may be desirable. ",Kein DOI-Link verfügbar,1705.01729v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,State-of-The-Art Sparse Direct Solvers,2019,"  In this chapter we will give an insight into modern sparse elimination methods. These are driven by a preprocessing phase based on combinatorial algorithms which improve diagonal dominance, reduce fill-in, and improve concurrency to allow for parallel treatment. Moreover, these methods detect dense submatrices which can be handled by dense matrix kernels based on multithreaded level-3 BLAS. We will demonstrate for problems arising from circuit simulation, how the improvements in recent years have advanced direct solution methods significantly. ",Kein DOI-Link verfügbar,1907.05309v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,New frontiers in Bayesian modeling using the INLA package in R,2019,"  The INLA package provides a tool for computationally efficient Bayesian modeling and inference for various widely used models, more formally the class of latent Gaussian models. It is a non-sampling based framework which provides approximate results for Bayesian inference, using sparse matrices. The swift uptake of this framework for Bayesian modeling is rooted in the computational efficiency of the approach and catalyzed by the demand presented by the big data era. In this paper, we present new developments within the INLA package with the aim to provide a computationally efficient mechanism for the Bayesian inference of relevant challenging situations. ",Kein DOI-Link verfügbar,1907.10426v2,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,Parallelized integrated nested Laplace approximations for fast Bayesian   inference,2022,"  There is a growing demand for performing larger-scale Bayesian inference tasks, arising from greater data availability and higher-dimensional model parameter spaces. In this work we present parallelization strategies for the methodology of integrated nested Laplace approximations (INLA), a popular framework for performing approximate Bayesian inference on the class of Latent Gaussian models. Our approach makes use of nested OpenMP parallelism, a parallel line search procedure using robust regression in INLA's optimization phase and the state-of-the-art sparse linear solver PARDISO. We leverage mutually independent function evaluations in the algorithm as well as advanced sparse linear algebra techniques. This way we can flexibly utilize the power of today's multi-core architectures. We demonstrate the performance of our new parallelization scheme on a number of different real-world applications. The introduction of parallelism leads to speedups of a factor 10 and more for all larger models. Our work is already integrated in the current version of the open-source R-INLA package, making its improved performance conveniently available to all users. ",Kein DOI-Link verfügbar,2204.04678v1,No,
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,AI Driven Near Real-time Locational Marginal Pricing Method: A   Feasibility and Robustness Study,2023,"  Accurate price predictions are essential for market participants in order to optimize their operational schedules and bidding strategies, especially in the current context where electricity prices become more volatile and less predictable using classical approaches. The Locational Marginal Pricing (LMP) pricing mechanism is used in many modern power markets, where the traditional approach utilizes optimal power flow (OPF) solvers. However, for large electricity grids this process becomes prohibitively time-consuming and computationally intensive. Machine learning (ML) based predictions could provide an efficient tool for LMP prediction, especially in energy markets with intermittent sources like renewable energy. This study evaluates the performance of popular machine learning and deep learning models in predicting LMP on multiple electricity grids. The accuracy and robustness of these models in predicting LMP is assessed considering multiple scenarios. The results show that ML models can predict LMP 4-5 orders of magnitude faster than traditional OPF solvers with 5-6\% error rate, highlighting the potential of ML models in LMP prediction for large-scale power models with the assistance of hardware infrastructure like multi-core CPUs and GPUs in modern HPC clusters. ",Kein DOI-Link verfügbar,2306.10080v2,Yes,potent(1)
0000-0003-4239-8945,Olaf Schenk,MSH Medical School Hamburg Universität der Angewandte Wissenschaften and Medical Universität,A Recursive Algebraic Coloring Technique for Hardware-Efficient   Symmetric Sparse Matrix-Vector Multiplication,2019,"  The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important building block for many numerical linear algebra kernel operations or graph traversal applications. Parallelizing SymmSpMV on today's multicore platforms with up to 100 cores is difficult due to the need to manage conflicting updates on the result vector. Coloring approaches can be used to solve this problem without data duplication, but existing coloring algorithms do not take load balancing and deep memory hierarchies into account, hampering scalability and full-chip performance. In this work, we propose the recursive algebraic coloring engine (RACE), a novel coloring algorithm and open-source library implementation, which eliminates the shortcomings of previous coloring methods in terms of hardware efficiency and parallelization overhead. We describe the level construction, distance-k coloring, and load balancing steps in RACE, use it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices with other state-of-the-art coloring techniques and Intel MKL on two modern multicore processors. RACE outperforms all other approaches substantially and behaves in accordance with the Roofline model. Outliers are discussed and analyzed in detail. While we focus on SymmSpMV in this paper, our algorithm and software is applicable to any sparse matrix operation with data dependencies that can be resolved by distance-k coloring. ",https://doi.org/10.1145/3399732,1907.06487v1,No,
0000-0003-3176-3361,Paul Libbrecht,IU Internationale Hochschule,Notations Around the World: Census and Exploitation,2010,"  Mathematical notations around the world are diverse. Not as much as requiring computing machines' makers to adapt to each culture, but as much as to disorient a person landing on a web-page with a text in mathematics. In order to understand better this diversity, we are building a census of notations: it should allow any content creator or mathematician to grasp which mathematical notation is used in which language and culture. The census is built collaboratively, collected in pages with a given semantic and presenting observations of the widespread notations being used in existing materials by a graphical extract. We contend that our approach should dissipate the fallacies found here and there about the notations in ""other cultures"" so that a better understanding of the cultures can be realized. The exploitation of the census in the math-bridge project is also presented: this project aims at taking learners ""where they are in their math-knowledge"" and bring them to a level ready to start engineering studies. The census serves as definitive reference for the transformation elements that generate the rendering of formul{\ae} in web-browsers. ",Kein DOI-Link verfügbar,1004.5165v1,No,
0000-0003-3176-3361,Paul Libbrecht,IU Internationale Hochschule,Escaping the Trap of too Precise Topic Queries,2013,"  At the very center of digital mathematics libraries lie controlled vocabularies which qualify the {\it topic} of the documents. These topics are used when submitting a document to a digital mathematics library and to perform searches in a library. The latter are refined by the use of these topics as they allow a precise classification of the mathematics area this document addresses. However, there is a major risk that users employ too precise topics to specify their queries: they may be employing a topic that is only ""close-by"" but missing to match the right resource. We call this the {\it topic trap}. Indeed, since 2009, this issue has appeared frequently on the i2geo.net platform. Other mathematics portals experience the same phenomenon. An approach to solve this issue is to introduce tolerance in the way queries are understood by the user. In particular, the approach of including fuzzy matches but this introduces noise which may prevent the user of understanding the function of the search engine.   In this paper, we propose a way to escape the topic trap by employing the navigation between related topics and the count of search results for each topic. This supports the user in that search for close-by topics is a click away from a previous search. This approach was realized with the i2geo search engine and is described in detail where the relation of being {\it related} is computed by employing textual analysis of the definitions of the concepts fetched from the Wikipedia encyclopedia. ",Kein DOI-Link verfügbar,1305.5724v1,No,
0000-0003-3176-3361,Paul Libbrecht,IU Internationale Hochschule,Understanding the Learners' Actions when using Mathematics Learning   Tools,2012,"  The use of computer-based mathematics tools is widespread in learning. Depending on the way that these tools assess the learner's solution paths, one can distinguish between automatic assessment tools and semi-automatic assessment tools. Automatic assessment tools directly provide all feedback necessary to the learners, while semi-automatic assessment tools involve the teachers as part the assessment process. They are provided with as much information as possible on the learners' interactions with the tool.   How can the teachers know how the learning tools were used and which intermediate steps led to a solution? How can the teachers respond to a learner's question that arises while using a computer tool? Little is available to answer this beyond interacting directly with the computer and performing a few manipulations to understand the tools' state.   This paper presents SMALA, a web-based logging architecture that addresses these problems by recording, analyzing and representing user actions. While respecting the learner's privacy, the SMALA architecture supports the teachers by offering fine-grained representations of the learners' activities as well as overviews of the progress of a classroom. ",https://doi.org/10.1007/978-3-642-31374-5,1207.2280v1,No,
0000-0003-3176-3361,Paul Libbrecht,IU Internationale Hochschule,Author Intent: Eliminating Ambiguity in MathML,2024,"  MathML has been successful in improving the accessibility of mathematical notation on the web. All major screen readers support MathML to generate speech, allow navigation of the math, and generate braille. A troublesome area remains: handling ambiguous notations such as \( \vert x\vert\). While it is possible to speak this syntactically, anecdotal evidence indicates most people prefer semantic speech such as ``absolute value of x'' or ``determinant of x'' instead of ``vertical bar x vertical bar'' when first hearing an expression. Several heuristics to infer semantics have improved speech, but ultimately, the author is the one who definitively knows how an expression is meant to be spoken. The W3C Math Working Group is in the process of allowing authors to convey their intent in MathML markup via an intent attribute. This paper describes that work. ",Kein DOI-Link verfügbar,2407.06720v1,No,
0000-0002-2362-417X,Andreas Heinz,IU Internationale Hochschule GmbH,Collinear cluster tri-partition: Kinematics constraints and stability of   collinearity,2016,"  A new mode of nuclear fission has been proposed by the FOBOS collaboration, called Collinear Cluster Tri-partition (CCT), suggesting that three heavy fission fragments can be emitted perfectly collinearly in low-energy fission. It is surprising that CCT escaped observation for so long given the relatively high reported yield, of roughly 0.5% relative to binary fission. These claims call for an independent verification with a different experimental technique. Verification experiments based on direct observation of CCT fragments with fission fragment spectrometers require guidance with respect to the allowed kinetic energy range, which we present in this paper. We discuss corresponding model calculations which, if CCT is found in such verification experiments, could indicate how the breakups proceed. We also study the intrinsic stability of collinearity. Three different decay models are used, which span together the timescales of three-body fission. These models are used to calculate the possible kinetic energy ranges of CCT fragments in 235U(n,f) and 252Cf(sf). We use semi-classical trajectory calculations with a Monte-Carlo method to study the intrinsic stability of collinearity. CCT has a high net Q-value, but in a sequential decay, the intermediate steps are energetically and geometrically unfavorable or even forbidden. Moreover, perfect collinearity is extremely unstable, and broken by the slightest perturbation. According to our results, the central fragment would be very difficult to detect due to its low kinetic energy, raising the question of why previous experiments could not detect a missing-mass signature corresponding to CCT. We find that a realization of CCT would require an unphysical fine-tuning of the initial conditions. Our results enable independent experimental verification and encourage further critical theoretical studies of CCT. ",https://doi.org/10.1103/PhysRevC.95.014602,1612.06583v2,No,
0000-0002-2362-417X,Andreas Heinz,IU Internationale Hochschule GmbH,Search for stable Strange Quark Matter in lunar soil,2009,"  We report results from a search for strangelets (small chunks of Strange Quark Matter) in lunar soil using the Yale WNSL accelerator as a mass spectrometer. We have searched over a range in mass from A=42 to A=70 amu for nuclear charges 5, 6, 8, 9, and 11. No strangelets were found in the experiment. For strangelets with nuclear charge 8, a concentration in lunar soil higher than $10^{-16}$ is excluded at the 95% confidence level. The implied limit on the strangelet flux in cosmic rays is the most sensitive to date for the covered range and is relevant to both recent theoretical flux predictions and a strangelet candidate event found by the AMS-01 experiment. ",https://doi.org/10.1103/PhysRevLett.103.092302,0903.5055v1,No,
0000-0002-2362-417X,Andreas Heinz,IU Internationale Hochschule GmbH,Development of a GEM-TPC prototype,2009,"  The use of GEM foils for the amplification stage of a TPC instead of a con- ventional MWPC allows one to bypass the necessity of gating, as the backdrift is suppressed thanks to the asymmetric field configuration. This way, a novel continuously running TPC, which represents one option for the PANDA central tracker, can be realized. A medium sized prototype with a diameter of 300 mm and a length of 600 mm will be tested inside the FOPI spectrometer at GSI using a carbon or lithium beam at intermediate energies (E = 1-3AGeV). This detector test under realistic experimental conditions should allow us to verify the spatial resolution for single tracks and the reconstruction capability for displaced vertexes. A series of physics measurement implying pion beams is scheduled with the FOPI spectrometer together with the GEM-TPC as well. ",https://doi.org/10.1142/9789814307529_0129,0911.0759v1,No,
0000-0002-4889-7043,Ralf Müller,IU Internationale Hochschule GmbH,Soft Interference Cancellation for Random Coding in Massive Gaussian   Multiple-Access,2020,"  In 2017, Polyanskiy [1] showed that the trade-off between power and bandwidth efficiency for massive Gaussian random access is governed by two fundamentally different regimes: low power and high power. For both regimes, tight performance bounds were found by Zadik et al. [2], in 2019. This work utilizes recent results on the exact block error probability of Gaussian random codes in additive white Gaussian noise to propose practical methods based on iterative soft decoding to closely approach the bounds in [2]. In the low power regime, this work finds that orthogonal random codes can be applied directly. In the high power regime, a more sophisticated effort is needed. This work shows that power-profile optimization by means of linear programming as pioneered by Caire et al. [3], in 2001, is a promising strategy to apply. The proposed combination of orthogonal random coding and iterative soft decoding even outperforms the existence bounds of Zadik et al. [2] in the low power regime and is very close to the non-existence bounds for message lengths around 100 and above. Finally, the approach of power optimization by linear programming proposed for the high power regime is found to benefit from power imbalances due to fading which makes is even more attractive for typical mobile radio channels. ",https://doi.org/10.3390/e23050539,2005.03364v3,No,
0000-0002-4889-7043,Ralf Müller,IU Internationale Hochschule GmbH,Benchmarking for Metaheuristic Black-Box Optimization: Perspectives and   Open Challenges,2020,"  Research on new optimization algorithms is often funded based on the motivation that such algorithms might improve the capabilities to deal with real-world and industrially relevant optimization challenges. Besides a huge variety of different evolutionary and metaheuristic optimization algorithms, also a large number of test problems and benchmark suites have been developed and used for comparative assessments of algorithms, in the context of global, continuous, and black-box optimization. For many of the commonly used synthetic benchmark problems or artificial fitness landscapes, there are however, no methods available, to relate the resulting algorithm performance assessments to technologically relevant real-world optimization problems, or vice versa. Also, from a theoretical perspective, many of the commonly used benchmark problems and approaches have little to no generalization value. Based on a mini-review of publications with critical comments, advice, and new approaches, this communication aims to give a constructive perspective on several open challenges and prospective research directions related to systematic and generalizable benchmarking for black-box optimization. ",Kein DOI-Link verfügbar,2007.00541v1,No,
0000-0002-4889-7043,Ralf Müller,IU Internationale Hochschule GmbH,Blind pilot decontamination,2013,"  A subspace projection to improve channel estimation in massive multi-antenna systems is proposed and analyzed. Together with power-controlled hand-off, it can mitigate the pilot contamination problem without the need for coordination among cells. The proposed method is blind in the sense that it does not require pilot data to find the appropriate subspace. It is based on the theory of large random matrices that predicts that the eigenvalue spectra of large sample covariance matrices can asymptotically decompose into disjoint bulks as the matrix size grows large. Random matrix and free probability theory are utilized to predict under which system parameters such a bulk decomposition takes place. Simulation results are provided to confirm that the proposed method outperforms conventional linear channel estimation if bulk separation occurs. ",https://doi.org/10.1109/JSTSP.2014.2310053,1309.6806v2,No,
0009-0005-5286-6548,Kristina Schaaff,IU Internationale Hochschule GmbH,"Classification of Human- and AI-Generated Texts for English, French,   German, and Spanish",2023,"  In this paper we analyze features to classify human- and AI-generated text for English, French, German and Spanish and compare them across languages. We investigate two scenarios: (1) The detection of text generated by AI from scratch, and (2) the detection of text rephrased by AI. For training and testing the classifiers in this multilingual setting, we created a new text corpus covering 10 topics for each language. For the detection of AI-generated text, the combination of all proposed features performs best, indicating that our features are portable to other related languages: The F1-scores are close with 99% for Spanish, 98% for English, 97% for German and 95% for French. For the detection of AI-rephrased text, the systems with all features outperform systems with other features in many cases, but using only document features performs best for German (72%) and Spanish (86%) and only text vector features leads to best results for English (78%). ",Kein DOI-Link verfügbar,2312.04882v1,No,
0009-0005-5286-6548,Kristina Schaaff,IU Internationale Hochschule GmbH,Impacts of Anthropomorphizing Large Language Models in Learning   Environments,2024,"  Large Language Models (LLMs) are increasingly being used in learning environments to support teaching-be it as learning companions or as tutors. With our contribution, we aim to discuss the implications of the anthropomorphization of LLMs in learning environments on educational theory to build a foundation for more effective learning outcomes and understand their emotional impact on learners. According to the media equation, people tend to respond to media in the same way as they would respond to another person. A study conducted by the Georgia Institute of Technology showed that chatbots can be successfully implemented in learning environments. In this study, learners in selected online courses were unable to distinguish the chatbot from a ""real"" teacher. As LLM-based chatbots such as OpenAI's GPT series are increasingly used in educational tools, it is important to understand how the attribution processes to LLM-based chatbots in terms of anthropomorphization affect learners' emotions. ",Kein DOI-Link verfügbar,2408.03945v1,No,
0009-0005-5286-6548,Kristina Schaaff,IU Internationale Hochschule GmbH,Exploring ChatGPT's Empathic Abilities,2023,"  Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In conversations, ChatGPT reacted with a parallel emotion in 70.7% of cases. The empathic capabilities of ChatGPT were evaluated using a set of five questionnaires covering different aspects of empathy. Even though the results show, that the scores of ChatGPT are still worse than the average of healthy humans, it scores better than people who have been diagnosed with Asperger syndrome / high-functioning autism. ",Kein DOI-Link verfügbar,2308.03527v3,No,
0009-0005-5286-6548,Kristina Schaaff,IU Internationale Hochschule GmbH,Classification of Human- and AI-Generated Texts: Investigating Features   for ChatGPT,2023,"  Recently, generative AIs like ChatGPT have become available to the wide public. These tools can for instance be used by students to generate essays or whole theses. But how does a teacher know whether a text is written by a student or an AI? In our work, we explore traditional and new features to (1) detect text generated by AI from scratch and (2) text rephrased by AI. Since we found that classification is more difficult when the AI has been instructed to create the text in a way that a human would not recognize that it was generated by an AI, we also investigate this more advanced case. For our experiments, we produced a new text corpus covering 10 school topics. Our best systems to classify basic and advanced human-generated/AI-generated texts have F1-scores of over 96%. Our best systems for classifying basic and advanced human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems use a combination of perplexity, semantic, list lookup, error-based, readability, AI feedback, and text vector features. Our results show that the new features substantially help to improve the performance of many classifiers. Our best basic text rephrasing detection system even outperforms GPTZero by 183.8% relative in F1-score. ",https://doi.org/10.1007/978-981-99-7947-9_12,2308.05341v1,No,
0009-0003-4545-8728,Marc-André Heidelmann,IU International Universität der Angewandte Wissenschaften,Impacts of Anthropomorphizing Large Language Models in Learning   Environments,2024,"  Large Language Models (LLMs) are increasingly being used in learning environments to support teaching-be it as learning companions or as tutors. With our contribution, we aim to discuss the implications of the anthropomorphization of LLMs in learning environments on educational theory to build a foundation for more effective learning outcomes and understand their emotional impact on learners. According to the media equation, people tend to respond to media in the same way as they would respond to another person. A study conducted by the Georgia Institute of Technology showed that chatbots can be successfully implemented in learning environments. In this study, learners in selected online courses were unable to distinguish the chatbot from a ""real"" teacher. As LLM-based chatbots such as OpenAI's GPT series are increasingly used in educational tools, it is important to understand how the attribution processes to LLM-based chatbots in terms of anthropomorphization affect learners' emotions. ",Kein DOI-Link verfügbar,2408.03945v1,No,
0000-0002-4218-4595,Michaela Zagler,Hof University of Applied Sciences,Creating Hydrophobic Foils From Biopolymer Blends Using Mechanical Microimprinting.,1970,The front cover artwork is provided by Hang Liu from Prof. Dr. Joachim Albrecht's and Prof. Dr. Katharina Weber's group at Aalen University. The image depicts the wetting properties of a biopolymer foil. Surface microstructuring allows the tailoring of physical chemical properties that can lead to biodegradable packaging foils. Read the full text of the Research Article at 10.1002/cphc.202300301.,https://doi.org/10.1002/cphc.202300670,,No,
0000-0002-7229-316X,Weronika Schary,Hochschule Furtwangen,Label-free identification and differentiation of different microplastics using phasor analysis of fluorescence lifetime imaging microscopy (FLIM)-generated data.,1970,"As plastic pollution is becoming an increasing worldwide problem, a variety of different techniques for the detection and in-depth characterization of plastics, including spectroscopy and chromatography methods, were introduced to the public. Recently we presented fluorescence lifetime imaging microscopy (FLIM) a new approach for the identification and characterization of microplastics using their fluorescence lifetime (τ) for differentiation. A very powerful extension of the recently established FLIM could be phasor analysis, which allows data representation in an interactive 2D graphical phasor plot thereby enabling a global view of the fluorescence decay in each pixel of the measured image. Microplastic particles generated from six different types of plastics were subjected to excitation wavelengths of 440 nm, upon which specific fluorescence lifetimes as well as the photon yield were determined using FLIM and phasor analysis. We could show that phasor analysis for FLIM with a laser pulse repetition frequency of 40 MHz was able to generate specific locations in the phasor plot for the plastics for fast differentiation, e.g. resulting in well-defined phasor plot positions for ABS at 3.019 ns, PPE at 6.239 ns, PET bottle from Germany at 2.703 ns and PET bottle from USA at 2.711 ns. Phasor analysis for FLIM proves to be a fast, label-free, and sensitive method for the identification and differentiation of plastics also with the aid of visualization variation enabling techniques such as heat treatment of plastics.",https://doi.org/10.1016/j.cbi.2021.109466,,No,
0000-0002-7229-316X,Weronika Schary,Hochschule Furtwangen,Using autofluorescence for microplastic detection - Heat treatment increases the autofluorescence of microplastics1.,1970,"More and more researchers are studying the effects of microplastics on the environment and the organisms living in it. Existing detection methods still require a heavy workload, complex sample preparation and high costs. In this study, autofluorescence of plastic was used as a new method for microplastic detection.",https://doi.org/10.3233/CH-209223,,No,
0000-0002-7229-316X,Weronika Schary,Hochschule Furtwangen,Validation of an extraction method for microplastics from human materials.,1970,"Since the beginning of industrial production in 1950, plastic production has continued to grow strongly worldwide and is now at 322 million tonnes in the year 2015. From these very high production volumes ever larger quantities are found in the environment. There the plastics degradate to microplasticity and spread ubiquitously in the world. The present work deals with the possible uptake of microplastic particles in human organisms. For the detection of these plastic particles, an extraction method was developed and validated.",https://doi.org/10.3233/CH-199209,,No,
0000-0002-6233-129X,Rongqing Chen,Hochschule Furtwangen,Effect of a Patient-Specific Structural Prior Mask on Electrical Impedance Tomography Image Reconstructions.,1970,"Electrical Impedance Tomography (EIT) is a low-cost imaging method which reconstructs two-dimensional cross-sectional images, visualising the impedance change within the thorax. However, the reconstruction of an EIT image is an ill-posed inverse problem. In addition, blurring, anatomical alignment, and reconstruction artefacts can hinder the interpretation of EIT images. In this contribution, we introduce a patient-specific structural prior mask into the EIT reconstruction process, with the aim of improving image interpretability. Such a prior mask ensures that only conductivity changes within the lung regions are reconstructed. To evaluate the influence of the introduced structural prior mask, we conducted numerical simulations with two scopes in terms of their different ventilation statuses and varying atelectasis scales. Quantitative analysis, including the reconstruction error and figures of merit, was applied in the evaluation procedure. The results show that the morphological structures of the lungs introduced by the mask are preserved in the EIT reconstructions and the reconstruction artefacts are decreased, reducing the reconstruction error by 25.9% and 17.7%, respectively, in the two EIT algorithms included in this contribution. The use of the structural prior mask conclusively improves the interpretability of the EIT images, which could facilitate better diagnosis and decision-making in clinical settings.",https://doi.org/10.3390/s23094551,,No,
0000-0002-1492-1121,Herag Arabian,Furtwangen,Harnessing Wearable Devices for Emotional Intelligence: Therapeutic Applications in Digital Health.,1970,"Emotional intelligence strives to bridge the gap between human and machine interactions. The application of such systems varies and is becoming more prominent as healthcare services seek to provide more efficient care by utilizing smart digital health apps. One application in digital health is the incorporation of emotion recognition systems as a tool for therapeutic interventions. To this end, a system is designed to collect and analyze physiological signal data, such as electrodermal activity (EDA) and electrocardiogram (ECG), from smart wearable devices. The data are collected from different subjects of varying ages taking part in a study on emotion induction methods. The obtained signals are processed to identify stimulus trigger instances and classify the different reaction stages, as well as arousal strength, using signal processing and machine learning techniques. The reaction stages are identified using a support vector machine algorithm, while the arousal strength is classified using the ResNet50 network architecture. The findings indicate that the EDA signal effectively identifies the emotional trigger, registering a root mean squared error (RMSE) of 0.9871. The features collected from the ECG signal show efficient emotion detection with 94.19% accuracy. However, arousal strength classification is only able to reach 60.37% accuracy on the given dataset. The proposed system effectively detects emotional reactions and can categorize their arousal strength in response to specific stimuli. Such a system could be integrated into therapeutic settings to monitor patients' emotional responses during therapy sessions. This real-time feedback can guide therapists in adjusting their strategies or interventions.",https://doi.org/10.3390/s23198092,,No,
0000-0002-1492-1121,Herag Arabian,Furtwangen,P-CSEM: An Attention Module for Improved Laparoscopic Surgical Tool Detection.,1970,"Minimal invasive surgery, more specifically laparoscopic surgery, is an active topic in the field of research. The collaboration between surgeons and new technologies aims to improve operation procedures as well as to ensure the safety of patients. An integral part of operating rooms modernization is the real-time communication between the surgeon and the data gathered using the numerous devices during surgery. A fundamental tool that can aid surgeons during laparoscopic surgery is the recognition of the different phases during an operation. Current research has shown a correlation between the surgical tools utilized and the present phase of surgery. To this end, a robust surgical tool classifier is desired for optimal performance. In this paper, a deep learning framework embedded with a custom attention module, the P-CSEM, has been proposed to refine the spatial features for surgical tool classification in laparoscopic surgery videos. This approach utilizes convolutional neural networks (CNNs) integrated with P-CSEM attention modules at different levels of the architecture for improved feature refinement. The model was trained and tested on the popular, publicly available Cholec80 database. Results showed that the attention integrated model achieved a mean average precision of 93.14%, and visualizations revealed the ability of the model to adhere more towards features of tool relevance. The proposed approach displays the benefits of integrating attention modules into surgical tool classification models for a more robust and precise detection.",https://doi.org/10.3390/s23167257,,No,
0000-0002-1492-1121,Herag Arabian,Furtwangen,"Laparoscopic Video Analysis Using Temporal, Attention, and Multi-Feature Fusion Based-Approaches.",1970,"Adapting intelligent context-aware systems (CAS) to future operating rooms (OR) aims to improve situational awareness and provide surgical decision support systems to medical teams. CAS analyzes data streams from available devices during surgery and communicates real-time knowledge to clinicians. Indeed, recent advances in computer vision and machine learning, particularly deep learning, paved the way for extensive research to develop CAS. In this work, a deep learning approach for analyzing laparoscopic videos for surgical phase recognition, tool classification, and weakly-supervised tool localization in laparoscopic videos was proposed. The ResNet-50 convolutional neural network (CNN) architecture was adapted by adding attention modules and fusing features from multiple stages to generate better-focused, generalized, and well-representative features. Then, a multi-map convolutional layer followed by tool-wise and spatial pooling operations was utilized to perform tool localization and generate tool presence confidences. Finally, the long short-term memory (LSTM) network was employed to model temporal information and perform tool classification and phase recognition. The proposed approach was evaluated on the Cholec80 dataset. The experimental results (i.e., 88.5% and 89.0% mean precision and recall for phase recognition, respectively, 95.6% mean average precision for tool presence detection, and a 70.1% F1-score for tool localization) demonstrated the ability of the model to learn discriminative features for all tasks. The performances revealed the importance of integrating attention modules and multi-stage feature fusion for more robust and precise detection of surgical phases and tools.",https://doi.org/10.3390/s23041958,,No,
0009-0002-9374-0720,Martin Kohlhase,Bielefeld University of Applied Sciences,Using Artificial Intelligence for Assistance Systems to Bring Motor Learning Principles into Real World Motor Tasks.,1970,"Humans learn movements naturally, but it takes a lot of time and training to achieve expert performance in motor skills. In this review, we show how modern technologies can support people in learning new motor skills. First, we introduce important concepts in motor control, motor learning and motor skill learning. We also give an overview about the rapid expansion of machine learning algorithms and sensor technologies for human motion analysis. The integration between motor learning principles, machine learning algorithms and recent sensor technologies has the potential to develop AI-guided assistance systems for motor skill training. We give our perspective on this integration of different fields to transition from motor learning research in laboratory settings to real world environments and real world motor tasks and propose a stepwise approach to facilitate this transition.",https://doi.org/10.3390/s22072481,,Yes,potent(1)
0000-0002-8452-3890,Annika Gröndahl,Bielefeld University of Applied Sciences,Case management and care expertise as a prevention approach for adults with intellectual disabilities (FaPP-MgB): study protocol for a randomized-controlled trial.,1970,"Adults with intellectual disabilities have a higher prevalence of unhealthy eating habits, stress, low levels of mobility, and comparable drug consumption as the general population. Consequently, they suffer from several chronic diseases earlier and more often, but there are fewer prevention and health promotion services including this population. The goal of this study is to determine if an advanced practice nursing approach in the community with home visits is an effective way to improve the health status of adults with intellectual disabilities.",https://doi.org/10.1186/s13063-023-07155-w,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Algae-Based Biopolymers for Batteries and Biofuel Applications in Comparison with Bacterial Biopolymers-A Review.,1970,"Algae-based biopolymers can be used in diverse energy-related applications, such as separators and polymer electrolytes in batteries and fuel cells and also as microalgal biofuel, which is regarded as a highly renewable energy source. For these purposes, different physical, thermochemical, and biochemical properties are necessary, which are discussed within this review, such as porosity, high temperature resistance, or good mechanical properties for batteries and high energy density and abundance of the base materials in case of biofuel, along with the environmental aspects of using algae-based biopolymers in these applications. On the other hand, bacterial biopolymers are also often used in batteries as bacterial cellulose separators or as biopolymer network binders, besides their potential use as polymer electrolytes. In addition, they are also regarded as potential sustainable biofuel producers and converters. This review aims at comparing biopolymers from both aforementioned sources for energy conversion and storage. Challenges regarding the production of algal biopolymers include low scalability and low cost-effectiveness, and for bacterial polymers, slow growth rates and non-optimal fermentation processes often cause challenges. On the other hand, environmental benefits in comparison with conventional polymers and the better biodegradability are large advantages of these biopolymers, which suggest further research to make their production more economical.",https://doi.org/10.3390/polym16050610,,Yes,potent(2)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Electrospinning of Magnetite-Polyacrylonitrile Composites for the Production of Oxygen Reduction Reaction Catalysts.,1970,"In this study, electrospun carbon fiber electrodes were prepared by the carbonization of PAN-Fe",https://doi.org/10.3390/polym15204064,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,A Recent Review of Electrospun Porous Carbon Nanofiber Mats for Energy Storage and Generation Applications.,1970,"Electrospun porous carbon nanofiber mats have excellent properties, such as a large surface area, tunable porosity, and excellent electrical conductivity, and have attracted great attention in energy storage and power generation applications. Moreover, due to their exceptional properties, they can be used in dye-sensitized solar cells (DSSCs), membrane electrodes for fuel cells, catalytic applications such as oxygen reduction reactions (ORRs), hydrogen evolution reactions (HERs), and oxygen evolution reactions (OERs), and sensing applications such as biosensors, electrochemical sensors, and chemical sensors, providing a comprehensive insight into energy storage development and applications. This study focuses on the role of electrospun porous carbon nanofiber mats in improving energy storage and generation and contributes to a better understanding of the fabrication process of electrospun porous carbon nanofiber mats. In addition, a comprehensive review of various alternative preparation methods covering a wide range from natural polymers to synthetic carbon-rich materials is provided, along with insights into the current literature.",https://doi.org/10.3390/membranes13100830,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Suitability of Mycelium-Reinforced Nanofiber Mats for Filtration of Different Dyes.,1970,"Electrospun nanofiber mats have a high specific surface area and very small pores which can be tailored by the spinning process. They are thus highly suitable as filters for small particles and molecules, such as organic dyes. On the other hand, they are usually very thin and thus have low mechanical properties. As a potential reinforcement, mycelium of ",https://doi.org/10.3390/polym15193951,,Yes,potent(1)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Measuring Physical Properties of Electrospun Nanofiber Mats for Different Biomedical Applications.,1970,"Electrospun nanofiber mats are nowadays often used for biotechnological and biomedical applications, such as wound healing or tissue engineering. While most studies concentrate on their chemical and biochemical properties, the physical properties are often measured without long explanations regarding the chosen methods. Here, we give an overview of typical measurements of topological features such as porosity, pore size, fiber diameter and orientation, hydrophobic/hydrophilic properties and water uptake, mechanical and electrical properties as well as water vapor and air permeability. Besides describing typically used methods with potential modifications, we suggest some low-cost methods as alternatives in cases where special equipment is not available.",https://doi.org/10.3390/membranes13050488,,Yes,potent(1)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,"Electrospun Magnetic Nanofiber Mats for Magnetic Hyperthermia in Cancer Treatment Applications-Technology, Mechanism, and Materials.",1970,"The number of cancer patients is rapidly increasing worldwide. Among the leading causes of human death, cancer can be regarded as one of the major threats to humans. Although many new cancer treatment procedures such as chemotherapy, radiotherapy, and surgical methods are nowadays being developed and used for testing purposes, results show limited efficiency and high toxicity, even if they have the potential to damage cancer cells in the process. In contrast, magnetic hyperthermia is a field that originated from the use of magnetic nanomaterials, which, due to their magnetic properties and other characteristics, are used in many clinical trials as one of the solutions for cancer treatment. Magnetic nanomaterials can increase the temperature of nanoparticles located in tumor tissue by applying an alternating magnetic field. A very simple, inexpensive, and environmentally friendly method is the fabrication of various types of functional nanostructures by adding magnetic additives to the spinning solution in the electrospinning process, which can overcome the limitations of this challenging treatment process. Here, we review recently developed electrospun magnetic nanofiber mats and magnetic nanomaterials that support magnetic hyperthermia therapy, targeted drug delivery, diagnostic and therapeutic tools, and techniques for cancer treatment.",https://doi.org/10.3390/polym15081902,,Yes,potent(1)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Facile Synthesis and Electrochemical Characterization of Polyaniline@TiO,1970,"This research reports the facile, controlled, low-cost fabrication, and evaluation of properties of polyaniline matrix deposited on titanium dioxide and copper(II) oxide ternary-composite (PANI@TiO",https://doi.org/10.3390/polym14214562,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Electrospinning Nanofiber Mats with Magnetite Nanoparticles Using Various Needle-Based Techniques.,1970,"Electrospinning can be used to produce nanofiber mats containing diverse nanoparticles for various purposes. Magnetic nanoparticles, such as magnetite (Fe",https://doi.org/10.3390/polym14030533,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Magnetic Carbon Nanofiber Mats for Prospective Single Photon Avalanche Diode (SPAD) Sensing Applications.,1970,"Electrospinning enables simple and cost-effective production of magnetic nanofibers by adding nanoparticles to a polymer solution. In order to increase the electrical conductivity of such nanofibers, the carbonization process is crucial. In this study, the chemical and morphological properties of magnetic nanofiber mats prepared from polyacrylonitrile (PAN)/magnetite were investigated. In our previous studies, PAN/magnetite nanofiber mats were carbonized at 500 °C, 600 °C, and 800 °C. Here, PAN/magnetite nanofiber mats were carbonized at 1000 °C. The surface morphology of these PAN/magnetite nanofiber mats is not significantly different from nanofiber mats thermally treated at 800 °C and have remained relatively flexible at 1000 °C, which can be advantageous for various application fields. The addition of nanoparticles increased the average fiber diameter compared to pure PAN nanofiber mats and improved the dimensional stability during thermal processes. The high conductivity, the high magnetization properties, as well as shielding against electromagnetic interference of such carbonized nanofibers can be proposed for use in single photon avalanche diode (SPAD), where these properties are advantageous.",https://doi.org/10.3390/s21237873,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,"Development, Investigation, and Comparative Study of the Effects of Various Metal Oxides on Optical Electrochemical Properties Using a Doped PANI Matrix.",1970,"A comparative study was performed in order to analyze the effect of metal oxide (MO) on the properties of a polymeric matrix. In this study, polyaniline (PANI)@Al",https://doi.org/10.3390/polym13193344,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,The Possibility of Reuse of Nanofiber Mats by Machine Washing at Different Temperatures.,1970,"The worldwide spread of coronavirus COVID-19 infections demonstrates the great need for personal protective equipment and, in particular, hygiene masks. These masks are essential for the primary protection of the respiratory tract against pathogens such as viruses and bacteria that are infectious and transmitted through the air as large droplets or via small airborne particles. The use of protective masks will continue to accompany humans for an indefinite period of time, and therefore there is an urgent need for a safe method to extend their usability by reusing them under perspective with minimal loss of protective properties. Nanofiber mats are widely used in masks and in this study the reusability of nanofiber mats is investigated by washing them at different temperatures. This paper shows the first measurements of the washability of nanofiber mats. Furthermore, the air permeability is measured, and the evaporation resistance is evaluated. According to the results of this study, the air permeability performance of nanofiber mats does not change significantly after washing, confirming the possibility of reuse.",https://doi.org/10.3390/ma14174788,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Nonclassical Attack on a Quantum Key Distribution System.,1970,"The article is focused on research of an attack on the quantum key distribution system and proposes a countermeasure method. Particularly noteworthy is that this is not a classic attack on a quantum protocol. We describe an attack on the process of calibration. Results of the research show that quantum key distribution systems have vulnerabilities not only in the protocols, but also in other vital system components. The described type of attack does not affect the cryptographic strength of the received keys and does not point to the vulnerability of the quantum key distribution protocol. We also propose a method for autocompensating optical communication system development, which protects synchronization from unauthorized access. The proposed method is based on the use of sync pulses attenuated to a photon level in the process of detecting a time interval with a signal. The paper presents the results of experimental studies that show the discrepancies between the theoretical and real parameters of the system. The obtained data allow the length of the quantum channel to be calculated with high accuracy.",https://doi.org/10.3390/e23050509,,Yes,noteworthy(1)
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,"Electrospun Nanofiber Mats for Filtering Applications-Technology, Structure and Materials.",1970,"Air pollution is one of the biggest health and environmental problems in the world and a huge threat to human health on a global scale. Due to the great impact of respiratory viral infections, chronic obstructive pulmonary disease, lung cancer, asthma, bronchitis, emphysema, lung disease, and heart disease, respiratory allergies are increasing significantly every year. Because of the special properties of electrospun nanofiber mats, e.g., large surface-to-volume ratio and low basis weight, uniform size, and nanoporous structure, nanofiber mats are the preferred choice for use in large-scale air filtration applications. In this review, we summarize the significant studies on electrospun nanofiber mats for filtration applications, present the electrospinning technology, show the structure and mechanism of air filtration. In addition, an overview of current air filtration materials derived from bio- and synthetic polymers and blends is provided. Apart from this, the use of biopolymers in filtration applications is still relatively new and this field is still under-researched. The application areas of air filtration materials are discussed here and future prospects are summarized in conclusion. In order to develop new effective filtration materials, it is necessary to understand the interaction between technology, materials, and filtration mechanisms, and this study was intended to contribute to this effort.",https://doi.org/10.3390/polym13091368,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Electrospun Carbon Nanofibers from Biomass and Biomass Blends-Current Trends.,1970,"In recent years, ecological issues have led to the search for new green materials from biomass as precursors for producing carbon materials (CNFs). Such green materials are more attractive than traditional petroleum-based materials, which are environmentally harmful and non-biodegradable. Biomass could be ideal precursors for nanofibers since they stem from renewable sources and are low-cost. Recently, many authors have focused intensively on nanofibers' production from biomass using microwave-assisted pyrolysis, hydrothermal treatment, ultrasonication method, but only a few on electrospinning methods. Moreover, still few studies deal with the production of electrospun carbon nanofibers from biomass. This review focuses on the new developments and trends of electrospun carbon nanofibers from biomass and aims to fill this research gap. The review is focusing on recollecting the most recent investigations about the preparation of carbon nanofiber from biomass and biopolymers as precursors using electrospinning as the manufacturing method, and the most important applications, such as energy storage that include fuel cells, electrochemical batteries and supercapacitors, as well as wastewater treatment, CO",https://doi.org/10.3390/polym13071071,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Chemical and Morphological Transition of Poly(acrylonitrile)/Poly(vinylidene Fluoride) Blend Nanofibers during Oxidative Stabilization and Incipient Carbonization.,1970,"Thermally stabilized and subsequently carbonized nanofibers are a promising material for many technical applications in fields such as tissue engineering or energy storage. They can be obtained from a variety of different polymer precursors via electrospinning. While some methods have been tested for post-carbonization doping of nanofibers with the desired ingredients, very little is known about carbonization of blend nanofibers from two or more polymeric precursors. In this paper, we report on the preparation, thermal treatment and resulting properties of poly(acrylonitrile) (PAN)/poly(vinylidene fluoride) (PVDF) blend nanofibers produced by wire-based electrospinning of binary polymer solutions. Using a wide variety of spectroscopic, microscopic and thermal characterization methods, the chemical and morphological transition during oxidative stabilization (280 °C) and incipient carbonization (500 °C) was thoroughly investigated. Both PAN and PVDF precursor polymers were detected and analyzed qualitatively and quantitatively during all stages of thermal treatment. Compared to pure PAN nanofibers, the blend nanofibers showed increased fiber diameters, strong reduction of undesired morphological changes during oxidative stabilization and increased conductivity after carbonization.",https://doi.org/10.3390/nano10061210,,No,
0000-0002-1822-7954,Lilia Sabantina,Bielefeld University of Applied Sciences,Magnetic Properties of Electrospun Magnetic Nanofiber Mats after Stabilization and Carbonization.,1970,"Magnetic nanofibers are of great interest in basic research, as well as for possible applications in spintronics and neuromorphic computing. Here we report on the preparation of magnetic nanofiber mats by electrospinning polyacrylonitrile (PAN)/nanoparticle solutions, creating a network of arbitrarily oriented nanofibers with a high aspect ratio. Since PAN is a typical precursor for carbon, the magnetic nanofiber mats were stabilized and carbonized after electrospinning. The magnetic properties of nanofiber mats containing magnetite or nickel ferrite nanoparticles were found to depend on the nanoparticle diameters and the potential after-treatment, as compared with raw nanofiber mats. Micromagnetic simulations underlined the different properties of both magnetic materials. Atomic force microscopy and scanning electron microscopy images revealed nearly unchanged morphologies after stabilization without mechanical fixation, which is in strong contrast to pure PAN nanofiber mats. While carbonization at 500 °C left the morphology unaltered, as compared with the stabilized samples, stronger connections between adjacent fibers were formed during carbonization at 800 °C, which may be supportive of magnetic data transmission.",https://doi.org/10.3390/ma13071552,,Yes,potent(1)
0000-0003-1771-407X,Anant Patel,Bielefeld University of Applied Sciences and Arts,Unraveling the interaction of co-encapsulated Saccharomyces cerevisiae and Metarhizium brunneum in calcium alginate-based attract-and-kill beads.,1970,"Attract-and-kill (AK) beads are biological, microbial insecticides developed as an alternative to synthetic soil insecticides. For wireworm control, beads are based on calcium alginate/starch co-encapsulating the carbon dioxide (CO",https://doi.org/10.1002/ps.8238,,No,
0000-0003-0419-8192,Andreas Schmid,Albstadt-Sigmaringen University,Regulatory Considerations for Producing mRNA Vaccines for Clinical Trials.,1970,"The approval of clinical trials by the competent authorities requires comprehensive quality documentation on the new drug to be used on the clinical trial participant. In the EU, quality data is summarized as investigational medicinal product dossier (IMPD), in the United States, as investigational new drug (IND) application. For that, several preconditions concerning production, quality control, and assurance have to be fulfilled. Here, specific requirements related to mRNA vaccines are addressed on the basis of European standards.",https://doi.org/10.1007/978-1-0716-3770-8_15,,No,
0000-0003-2119-9461,Klaus Morawetz,Münster University of Applied Sciences,Formation of brine channels in sea ice.,1970,Liquid salty micro-channels (brine) between growing ice platelets in sea ice are an important habitat for CO,https://doi.org/10.1140/epje/i2017-11512-x,,No,
0009-0003-1986-6234,Ulrich Wittrock,Münster University of Applied Sciences,Laryngeally echolocating bats.,1970,Echolocation of bats is a fascinating topic with an ongoing controversy regarding the signal processing that bats perform on the echo. Veselka et al. found that bats that use the larynx for producing the echolocating ultrasound have a stylohyal bone that connects the larynx to the auditory bulla. I propose that the stylohyal bone is used for heterodyne detection of Doppler-shifted echoes. This would allow very precise frequency resolution and phase-sensitive analysis of the returning echoes for determining the velocity of echolocated objects like insects.,https://doi.org/10.1038/nature09156,,No,
0000-0002-9179-4525,Tim Pier,Münster University of Applied Sciences,On the sensitization of Eu,1970,"This work deals with the photoluminescence of various composite structured Ca2LuHf2Al3O12 garnet type LED phosphors. It is well known that sensitization of Eu3+ with Ce3+ suffers from metal-to-metal charge transfer (MMCT) quenching. Spatial separation of the sensitizer and activator results in a reduced quenching mechanism and thus higher luminescence intensities, when Ce3+ is excited in the blue spectral range and transfers its energy to Eu3+. The phosphor particles were prepared via different synthesis techniques. The phase purity of the synthesized particles was determined by X-ray powder diffraction. Scanning electron microscopy images were obtained to study the particle morphology and composite formation. Photoluminescence properties were determined by recording the emission spectra, excitation spectra and diffuse reflectance spectra. Furthermore, the temperature dependent emission spectra and fluorescence lifetimes were recorded to compare thermal quenching and decay behavior of the samples. External quantum efficiencies (EQEs) were calculated to examine the MMCT quenching behavior. Since the EQE of Ca2LuHf2Al3O12:Ce3+,Tb3+,Eu3+ is lower than 1%, it could be demonstrated that the composite approach significantly increases the EQE due to the spatial separation of Ce3+ and Eu3+.",https://doi.org/10.1039/c8dt04125c,,No,
0000-0001-7962-8690,Carola Strassner,FH Münster University of Applied Sciences,Identifying Future Study Designs and Indicators for Somatic Health Associated with Diets of Cohorts Living in Eco-Regions: Findings from the INSUM Expert Workshop.,1970,"Diets, but also overall food environments, comprise a variety of significant factors with direct and indirect impacts on human health. Eco-Regions are geographical areas with a territorial approach to rural development, utilizing organic food and farming practices, and principles and promoting sustainable communities and food systems. However, so far, little attention has been given to quantifying aspects of the health of citizens living in these sustainable transition territories. The project ""Indicators for Assessment of Health Effects of Consumption of Sustainable, Organic School Meals in Eco-Regions"" (INSUM) aims to identify and discuss research approaches and indicators that could be applied to effectively measure the somatic, mental, and social health dimensions of citizens in Eco-Regions, linked to the intake of organic foods in their diets. In this paper, we focus on the somatic (physical) health dimension. A two-day workshop was held to discuss suitable methodology with an interdisciplinary, international group of experts. The results showed the limitations of commonly used tools for measuring dietary intake (e.g., relying on the memory of participants), and nutritional biomarkers (e.g., variations in correlations with specific intakes) for research understanding dietary intake and the health effects of diets. To investigate the complexity of this issue, the most suitable approach seems to be the combination of traditional markers of physical and mental health alongside emerging indicators such as the microbiome, nutrigenomics, metabolomics, or inflammatory biomarkers. Using new, digital, non-invasive, and wearable technologies to monitor indicators could complement future research. We conclude that future studies should adopt systemic, multidisciplinary approaches by combining not only indicators of somatic and mental health and social wellbeing (MHSW) but also considering the potential benefits of organic diets for health as well as aspects of sustainability connected to food environments.",https://doi.org/10.3390/nu16152528,,Yes,potent(1)
0000-0001-7962-8690,Carola Strassner,FH Münster University of Applied Sciences,"Exploring food consumption patterns in the province of Kenitra, Northwest of Morocco.",1970,"Morocco is currently undergoing rapid changes in diets and lifestyles, influenced by globalization and urbanization, leading to a shift away from the Mediterranean diet (MedDiet) toward Western diets.",https://doi.org/10.1186/s12889-024-19335-7,,No,
0000-0001-7962-8690,Carola Strassner,FH Münster University of Applied Sciences,Organic Juice Processing Quality from the Processors' Perspective: A Qualitative Study.,1970,"Organic food quality is based on processing. While the EU organic production regulation focuses on agricultural production, private standards provide more detailed information about further processing. For the development of organic processing, practitioner perspectives can provide valuable input. To get insight into practitioner perspectives, we conducted semi-structured expert interviews with nine employees of seven partly organic juice processing companies from Germany and Austria. Interview topics were (i) quality of organic juice processing in general, (ii) assessment of specific processing techniques, (iii) product quality of organic juice and (iv) flow of information between producer and consumer. We conducted a thematic analysis. We found that the experts' understanding of process quality mostly includes more aspects than the EU organic production regulation. It covers the whole food chain plus aspects of social and environmental sustainability. The experts prefer directly bottled juice of local raw materials but chiefly accept juice made from concentrate of exotic raw materials because of environmental concerns. Organic juice is preferred when it is cloudy and natural fluctuations are interpreted as an indicator of natural quality. The experts report that consumer information is challenging because of low food literacy. Raising this might help reduce the number of processed juices on the market.",https://doi.org/10.3390/foods12020377,,No,
0000-0001-7962-8690,Carola Strassner,FH Münster University of Applied Sciences,Identifying Future Study Designs for Mental Health and Social Wellbeing Associated with Diets of a Cohort Living in Eco-Regions: Findings from the INSUM Expert Workshop.,1970,"Diets influence our mental health and social wellbeing (MHSW) in multiple ways. A rising community concept, Eco-Regions, has gained interest. The research project ""Indicators for assessment of health effects of consumption of sustainable, organic school meals in Ecoregions"" (INSUM) aims to develop future-oriented research approaches to measure the potential health effects of more sustainable and healthy diets. This first part of the project focuses on MHSW with the goal to identify suitable study designs and indicators. The methodology is based on a 2-day workshop with an interdisciplinary group of experts. This paper describes commonly applied research methods on the nexus between diet and MHSW as presented by the experts and summarises key points from the discussions. The results show that the dominating tool to investigate MSHW is questionnaires. Questionnaires vary largely depending on the research design, such as participants or distribution channels. Cohort studies addressing families and including in-depth interventional and/or experimental studies may be suitable for an Eco-Region investigation. Those MHSW studies can be conducted and combined with measurements of somatic health effects. We conclude that indicators should be seen as complementary rather than independent. Explorative research designs are required to investigate complex Eco-Regions.",https://doi.org/10.3390/ijerph20010669,,Yes,potent(1)
0000-0002-2739-8042,Niels Hinricher,Münster University of Applied Sciences,"Hand and wrist complaints in dialysis nurses in Germany: a survey of prevalence, severity, and occupational associations.",1970,"Occupations involving repetitive movements of the wrists, activities that require a lot of force, and hand-arm swinging are particularly likely to contribute to the development of hand and wrist complaints. The daily setup and dismantling of dialysis machines as part of the dialysis treatment process can strain the wrists and fingers of nurses. However, evidence regarding the relationship between the work activities of dialysis nurses and the incidence of hand and wrist complaints is limited. This study aimed to investigate the prevalence and severity of hand and wrist complaints among dialysis nurses in Germany and to relate these to their work activities.",https://doi.org/10.1093/annweh/wxad075,,No,
0000-0003-1357-8453,Barbara Boldrini,Hochschule Reutlingen,Titanium(IV) Surface Complexes Bearing Chelating Catecholato Ligands for Enhanced Band-Gap Reduction.,1970,Protonolysis reactions between dimethylamido titanium(IV) catecholate [Ti(CAT)(NMe,https://doi.org/10.1021/acs.inorgchem.2c02838,,No,
0000-0002-3692-9175,Steffen Ulitzsch,Reutlingen,"Synthesis of an Addition-Crosslinkable, Silicon-Modified Polyolefin via Reactive Extrusion Monitored by In-Line Raman Spectroscopy.",1970,"We present the modification of ethylene-propylene rubber (EPM) with vinyltetra-methydisiloxane (VTMDS) via reactive extrusion to create a new silicone-based material with the potential for high-performance applications in the automotive, industrial and biomedical sectors. The radical-initiated modification is achieved with a peroxide catalyst starting the grafting reaction. The preparation process of the VTMDS-grafted EPM was systematically investigated using process analytical technology (in-line Raman spectroscopy) and the statistical design of experiments (DoE). By applying an orthogonal factorial array based on a face-centered central composite experimental design, the identification, quantification and mathematical modeling of the effects of the process factors on the grafting result were undertaken. Based on response surface models, process windows were defined that yield high grafting degrees and good grafting efficiency in terms of grafting agent utilization. To control the grafting process in terms of grafting degree and grafting efficiency, the chemical changes taking place during the modification procedure in the extruder were observed in real-time using a spectroscopic in-line Raman probe which was directly inserted into the extruder. Successful grafting of the EPM was validated in the final product by ",https://doi.org/10.3390/polym13081246,,Yes,potent(1)
0000-0002-3692-9175,Steffen Ulitzsch,Reutlingen,Optimizing the Process Efficiency of Reactive Extrusion in the Synthesis of Vinyltrimethoxysilane-Grafted Ethylene-Octene-Copolymer (EOC-g-VTMS) by Response Surface Methodology.,1970,"Thermoplastic polymers like ethylene-octene copolymer (EOC) may be grafted with silanes via reactive extrusion to enable subsequent crosslinking for advanced biomaterials manufacture. However, this reactive extrusion process is difficult to control and it is still challenging to reproducibly arrive at well-defined products. Moreover, high grafting degrees require a considerable excess of grafting reagent. A large proportion of the silane passes through the process without reacting and needs to be removed at great expense by subsequent purification. This results in unnecessarily high consumption of chemicals and a rather resource-inefficient process. It is thus desired to be able to define desired grafting degrees with optimum grafting efficiency by means of suitable process control. In this study, the continuous grafting of vinyltrimethoxysilane (VTMS) on ethylene-octene copolymer (EOC) via reactive extrusion was investigated. Successful grafting was verified and quantified by ",https://doi.org/10.3390/polym12122798,,No,
0009-0001-1925-0216,Sara Nester,Aalen,Joining of Aluminum and CFRP via Laser Powder Bed Fusion: Influence of Experimental Set-Up and Laser Processing on Microstructure and Mechanical Properties.,1970,"Additive-manufacturing-based joining methods enable tailored or even functionalized joints and allow for hybridization at small scales. The current study explored an innovative joining method for aluminum cast alloys (AlSi12) with thermoset carbon-fiber-reinforced polymers (CFRPs) via laser powder bed fusion (LPBF). The direct build-up of AlSi12 on a CFRP substrate proved to be challenging due to the dissimilar thermal properties of the considered materials, which led to substrate damage and low joint adhesion. These effects could be overcome by introducing an AlSi12 foil as an interlayer between the two joining partners, acting as a thermal barrier and further improving the AlSi12 melt wettability of the substrate. Within LPBF, the energy input in the form of volumetric laser energy density influenced both the porosity of the fused layers and the formation of thermally induced stresses due to the high cooling rates and different thermal expansion properties of the materials. While the AlSi12 volume density increased with a higher laser energy input, simultaneously increasing thermal stresses caused the debonding and deformation of the AlSi12 foil. However, within a narrow processing window of laser parameters, the samples achieved remarkably high shear strengths of τ > 20 MPa, comparable to those of conventional joining methods.",https://doi.org/10.3390/polym15183839,,Yes,innovative(1)
0000-0003-2310-5895,Dominik Merli,Augsburg University of Applied Sciences,Digitalizing Nursing in the Bavarian Swabia Region of Germany - Presentation of the Joint Project CARE REGIO.,1970,"The joint research project CARE REGIO aims to modernize the care system with digital solutions. We focus on the development of a uniform electronic care record, uniform data exchange between care facilities, and technical assistive systems, which shall all be unified in a standardized care-based storage solution.",https://doi.org/10.3233/SHTI200545,,No,
0000-0002-9886-5592,Lukas Kleybolte,Augsburg University of Applied Sciences,Review of Care Transition Records and Their Transmission Process in Nursing Facilities and Hospitals in Germany - Results of an Online Questionnaire.,1970,"A quantitative approach in the form of an online questionnaire was used to identify challenges and desires related to the Care Records Transmission Process and Care Transition Records (CTR). The questionnaire was sent to nurses, nursing assistants, and trainees working in ambulatory, acute inpatient, or long-term care settings. The survey revealed that creating CTRs is time-consuming, and the lack of standardization of CTRs makes the process even more cumbersome. In addition, most facilities transmit the CTR by physically handing it over to the patient or resident, resulting in little or no preparation time for the individual(s) receiving care. The key findings also suggest that most respondents are only partially satisfied with the completeness of the CTRs and that they must conduct additional interviews to obtain missing information. However, most respondents hoped that digital transmission of CTRs would lead to less administrative burden and that standardization of CTRs would be encouraged.",https://doi.org/10.3233/SHTI230473,,No,
0000-0003-1462-6875,Anica Mertins,Osnabrück University of Applied Sciences,How to use biogas?: A systematic review of biogas utilization pathways and business models.,1970,"There are many options for the utilization of biogas in different energy sectors (power, heat, mobility). The technical possibilities of using biogas are more diverse than the actual business models applied in the biogas industry. This paper shows the possible utilization pathways of biogas, divided into coupled power and heat generation, direct utilization and upgrading to a gas of a higher value. Subsequently, an overview of the business models discussed is given by a systematic literature review. The latter shows that the investigation of biogas business models is focused mainly on the last decade and has increased slightly over time. The regions of investigation can be found worldwide, with a clear focus on Europe. Direct use is studied mainly in the Asian and African regions. In the European context, a shift from investigating combined heat and power use to upgrading the biogas produced is evident.",https://doi.org/10.1186/s40643-022-00545-z,,No,
0009-0002-1609-3793,Milena Jäger,Osnabrück University of Applied Sciences,Grand Theft Auto-Based Cycling Simulator for Cognitive Enhancement Technologies in Dangerous Traffic Situations.,1970,"While developing traffic-based cognitive enhancement technology (CET), such as bike accident prevention systems, it can be challenging to test and evaluate them properly. After all, the real-world scenario could endanger the subjects' health and safety. Therefore, a simulator is needed, preferably one that is realistic yet low cost. This paper introduces a way to use the video game ",https://doi.org/10.3390/s23073672,,No,
0000-0001-9161-4603,Nicole Egbert,Osnabrück University of Applied Sciences,Discovering the importance of health informatics education competencies in healthcare practice. A focus group interview.,1970,"As healthcare and especially health technology evolve rapidly, new challenges require healthcare professionals to take on new roles. Consequently, the demand for health informatics competencies is increasing, and achieving these competencies using frameworks, such as Technology Informatics Guiding Reform (TIGER), is crucial for future healthcare.",https://doi.org/10.1016/j.ijmedinf.2024.105463,,No,
0000-0001-9161-4603,Nicole Egbert,Hochschule Osnabrück,Clinical Information Systems - Seen through the Ethics Lens.,1970,"The more people there are who use clinical information systems (CIS) beyond their traditional intramural confines, the more promising the benefits are, and the more daunting the risks will be. This review thus explores the areas of ethical debates prompted by CIS conceptualized as smart systems reaching out to patients and citizens. Furthermore, it investigates the ethical competencies and education needed to use these systems appropriately.",https://doi.org/10.1055/s-0040-1701996,,No,
0000-0001-9161-4603,Nicole Egbert,Hochschule Osnabrück,Evaluating a Proof-of-Concept Approach of the German Health Telematics Infrastructure in the Context of Discharge Management.,1970,"Although national eHealth strategies have existed now for more than a decade in many countries, they have been implemented with varying success. In Germany, the eHealth strategy so far has resulted in a roll out of electronic health cards for all citizens in the statutory health insurance, but in no clinically meaningful IT-applications. The aim of this study was to test the technical and organisation feasibility, usability, and utility of an eDischarge application embedded into a laboratory Health Telematics Infrastructure (TI). The tests embraced the exchange of eDischarge summaries based on the multiprofessional HL7 eNursing Summary standard between a municipal hospital and a nursing home. All in all, 36 transmissions of electronic discharge documents took place. They demonstrated the technical-organisation feasibility and resulted in moderate usability ratings. A comparison between eDischarge and paper-based summaries hinted at higher ratings of utility and information completeness for eDischarges. Despite problems with handling the electronic health card, the proof-of-concept for the first clinically meaningful IT-application in the German Health TI could be regarded as successful.",Kein DOI-Link verfügbar,,No,
0000-0002-9063-2508,Florian Avermann,Osnabrück University of Applied Sciences,Musculoskeletal Health Complaints and Associated Risk Factors in Freshmen Music Students.,1970,Evidence concerning the development of musculoskeletal health complaints (MHCs) among music students is limited due to inappropriate study designs. We aimed to assess the occurrences of MHCs and associated risk factors in freshmen music students compared to students from other disciplines.,https://doi.org/10.3390/ijerph20043169,,Yes,fresh(1)
0000-0002-4509-929X,Harry von Piekartz,Osnabrück University of Applied Sciences,Is There a Difference in Facial Emotion Recognition after Stroke with vs. without Central Facial Paresis?,1970,"The Facial Feedback Hypothesis (FFH) states that facial emotion recognition is based on the imitation of facial emotional expressions and the processing of physiological feedback. In the light of limited and contradictory evidence, this hypothesis is still being debated. Therefore, in the present study, emotion recognition was tested in patients with central facial paresis after stroke. Performance in facial vs. auditory emotion recognition was assessed in patients with vs. without facial paresis. The accuracy of objective facial emotion recognition was significantly lower in patients with vs. without facial paresis and also in comparison to healthy controls. Moreover, for patients with facial paresis, the accuracy measure for facial emotion recognition was significantly worse than that for auditory emotion recognition. Finally, in patients with facial paresis, the subjective judgements of their own facial emotion recognition abilities differed strongly from their objective performances. This pattern of results demonstrates a specific deficit in facial emotion recognition in central facial paresis and thus provides support for the FFH and points out certain effects of stroke.",https://doi.org/10.3390/diagnostics12071721,,No,
0000-0002-4509-929X,Harry von Piekartz,Osnabrück University of Applied Sciences,Facial Emotion Recognition in Patients with Post-Paralytic Facial Synkinesis-A Present Competence.,1970,"Facial palsy is a movement disorder with impacts on verbal and nonverbal communication. The aim of this study is to investigate the effects of post-paralytic facial synkinesis on facial emotion recognition. In a prospective cross-sectional study, we compared facial emotion recognition between ",https://doi.org/10.3390/diagnostics12051138,,No,
0000-0002-4509-929X,Harry von Piekartz,Hochschule Osnabrück,[Effects of preoperative neurobiological education on postoperative outcome : A systematic review].,1970,"Pain may have a crucial impact on human quality of life. An increase in knowledge about neurobiological and neuroscientific processes alone can positively influence the subjective perception of pain as well as psychometric variables. There are different forms of preoperative patient education with the aim to explain postoperative pain. Based on current literature, preoperative biomedical education has a low level of evidence. It can increase the preoperative anxiety and stress level of patients, which has a negative impact on the postoperative outcome. In contrast, the neuroscientific understanding considers postoperative pain from the viewpoints of the plasticity of the nervous system and involves sensitizational processes in the central and peripheral nervous systems.",https://doi.org/10.1007/s00482-021-00608-8,,No,
0000-0002-4509-929X,Harry von Piekartz,Osnabrück University of Applied Sciences,Cross-Cultural Adaption and Psychometric Evaluation of the German Craniofacial Pain and Disability Inventory (CF-PDI).,1970,"The Craniofacial Pain and Disability Inventory (CF-PDI) is a cross-culturally adapted instrument designed from a biopsychosocial perspective to measure pain, disability, and function in orofacial head and neck pain with shown psychometric properties; however, the German cross-cultural adaption is lacking.",Kein DOI-Link verfügbar,,No,
0000-0002-4509-929X,Harry von Piekartz,Osnabrück University of Applied Sciences,Validity of increasing the number of motor control tests within a test battery for discrimination of low back pain conditions in people attending a physiotherapy clinic: a case-control study.,1970,To develop a time-efficient motor control (MC) test battery while maximising diagnostic accuracy of both a two-level and three-level classification system for patients with non-specific low back pain (LBP).,https://doi.org/10.1136/bmjopen-2019-032340,,No,
0000-0002-4509-929X,Harry von Piekartz,Hochschule Osnabrück,Evidence and recommendations for the use of segmental motion testing for patients with LBP - A systematic review.,1970,"Assessment of low back pain (LBP) includes segmental motion tests. Although often used in clinical practice, the validity, inter- and intra-rater reliability of such tests in individuals with LBP are not universally accepted, making it difficult to interpret findings in clinical practice.",https://doi.org/10.1016/j.msksp.2019.102076,,No,
0000-0002-7211-271X,Christoph Budke,Osnabrück University of Applied Sciences,Selenium biofortification of different varieties of apples (Malus domestica) - Influence on protein content and the allergenic proteins Mal d 1 and Mal d 3.,1970,"As allergy towards apples is widespread, the evaluation of various cultivation and postharvest influences on the allergenic potential is of great importance. Therefore, the analysis of the Mal d 1 content was the focus of this study, originally dealing with investigating the influence of a selenium biofortification on apple quality. The Mal d 1 content of apples was in most cases reduced when the fruits were biofortified with selenium. Apple variety and climatic conditions were identified as further influencing factors for the Mal d 1 content of the fruits. The separate analysis of the peel and the fruit flesh showed that the content of Mal d 1 in the fruit flesh was significantly lower in the biofortified samples than in the controls. In conclusion, the results indicate that the selenium biofortification of apples and biochemical mechanism behind can reduce the allergenic potential regarding the content of Mal d 1.",https://doi.org/10.1016/j.foodchem.2021.130134,,Yes,potent(2)
0000-0002-7211-271X,Christoph Budke,Osnabrück University of Applied Sciences,"Relationship between Phenolic Compounds, Antioxidant Properties, and the Allergenic Protein Mal d 1 in Different Selenium-Biofortified Apple Cultivars (",1970,"Notable parts of the population in Europe suffer from allergies towards apples. To address this health problem, the analysis of the interactions of relevant allergens with other substances such as phenolic compounds is of particular importance. The aim of this study was to evaluate the correlations between the total phenolic content (TPC), polyphenol oxidase (PPO) activity, antioxidant activity (AOA), and the phenolic compound profile and the content of the allergenic protein Mal d 1 in six apple cultivars. It was found that the PPO activity and the content of individual phenolic compounds had an influence on the Mal d 1 content. With regard to the important constituents, flavan-3-ols and phenolic acids, it was found that apples with a higher content of chlorogenic acid and a low content of procyanidin trimers and/or epicatechin had a lower allergenic potential. This is probably based on the reaction of phenolic compounds (when oxidized by the endogenous PPO) with proteins, thus being able to change the conformation of the (allergenic) proteins, which further corresponds to a loss of antibody recognition. When apples were additionally biofortified with selenium, the composition of the apples, with regard to TPC, phenolic profile, AOA, and PPO, was significantly affected. Consequently, this innovative agronomic practice seems to be promising for reducing the allergenic potential of apples.",https://doi.org/10.3390/molecules26092647,,Yes,"innovative(1), notable(1), potent(2)"
0000-0002-7211-271X,Christoph Budke,Osnabrück University of Applied Sciences,Iodine Biofortification of Apples and Pears in an Orchard Using Foliar Sprays of Different Composition.,1970,"Many people across the world suffer from iodine (I) deficiency and related diseases. The I content in plant-based foods is particularly low, but can be enhanced by agronomic biofortification. Therefore, in this study two field experiments were conducted under orchard conditions to assess the potential of I biofortification of apples and pears by foliar fertilization. Fruit trees were sprayed at various times during the growing season with solutions containing I in different concentrations and forms. In addition, tests were carried out to establish whether the effect of I sprays can be improved by co-application of potassium nitrate (KNO",https://doi.org/10.3389/fpls.2021.638671,,Yes,potent(1)
0000-0002-7211-271X,Christoph Budke,Osnabrück University of Applied Sciences,Influence of a Selenium Biofortification on Antioxidant Properties and Phenolic Compounds of Apples (,1970,"Biofortified apples seem to be a suitable produce. In this study, different selenium forms and application levels were applied to the two apple varieties 'Golden Delicious' and 'Jonagold', grown in the years 2017 and 2018 in order to increase the selenium uptake within a typical Western diet. It was shown that the biofortification, which was performed as a foliar application implemented in usual calcium fertilization, led to significantly increased selenium contents in the fruits. Furthermore, biofortification affected the total phenolic content (TPC), the polyphenol oxidase activity (PPO), as well as the antioxidant activity (AOA), the latter measured with the two well-known assays Trolox Equivalent Antioxidant Capacity Assay (TEAC) and Oxygen Radical Absorbance Capacity Assays (ORAC). The varying selenium forms and application levels showed a differing influence on the parameters mentioned before. Higher fertilizer levels resulted in higher selenium accumulation. It was found that PPO activity fluctuates less in biofortified apples. With regard to TPC, selenate led to higher amounts when compared to the untreated controls and selenite resulted in lower TPC. AOA analysis showed no clear tendencies as a result of the selenium biofortification. In the case of 'Jonagold', a higher AOA was generally measured when being biofortified, whereas, in the case of 'Golden Delicious', only one form of application led to higher AOA. Additionally, differences in the amount of major phenolic compounds, measured with High Performance Liquid Chromatography Mass Spectrometry (HPLC-DAD-ESI-MS",https://doi.org/10.3390/antiox9020187,,No,
0000-0002-5693-3390,Sibylle Gaisser,Ansbach University of Applied Sciences,Synthetic biology in the view of European public funding organisations.,1970,"We analysed the decisions of major European public funding organisations to fund or not to fund synthetic biology (SB) and related ethical, legal and social implication (ELSI) studies. We investigated the reaction of public organisations in six countries (Austria, France, Germany, the Netherlands, Switzerland and the U.K.) towards SB that may influence SB's further development in Europe. We examined R&D and ELSI communities and their particular funding situation. Our results show that the funding situation for SB varies considerably among the analysed countries, with the U.K. as the only country with an established funding scheme for R&D and ELSI that successfully integrates these research communities. Elsewhere, we determined a general lack of funding (France), difficulties in funding ELSI work (Switzerland), lack of an R&D community (Austria), too small ELSI communities (France, Switzerland, Netherlands), or difficulties in linking existing communities with available funding sources (Germany), partly due to an unclear SB definition.",https://doi.org/10.1177/0963662510393624,,No,
0000-0001-9629-0759,Matthias Süncksen,Flensburg University of Applied Sciences,Simulation of scattered radiation during intraoperative imaging in a virtual reality learning environment.,1970,"Scattered radiation, which occurs when using a C-arm for intraoperative radiography, can be better understood through interactive visualization. We developed a virtual reality (VR) approach for the simulation of scattered radiation (SSR) as part of a C-arm training system. In VR, it is important to avoid cyber sickness, which is often caused by increased latency between head motion and image presentation inside the head-mounted display. As the latency requirement interferes with the computational complexity of the SSR, the goal has been to maintain a low latency during the simultaneous computation of the SSR on moderate-cost consumer hardware.",https://doi.org/10.1007/s11548-020-02126-x,,No,
0000-0001-8969-400X,Patricia Möbius-Lerch,Kempten University of Applied Sciences,Challenges and conditions for successfully implementing and adopting the telematics infrastructure in German outpatient healthcare: A qualitative study applying the NASSS framework.,1970,"Germany's healthcare system provides high-quality, universal health coverage to almost all residents. However, a major challenge lies in the strong separation of healthcare structures, which hinders efficient interprofessional and intersectoral communication and collaboration. The mandatory nationwide implementation of the telematics infrastructure may offer a solution to enhance healthcare professionals' communication and collaboration.",https://doi.org/10.1177/20552076241259855,,No,
0000-0002-8777-8805,Arsalan Haider,Kempten University of Applied Sciences,Velocity Estimation from LiDAR Sensors Motion Distortion Effect.,1970,"Many modern automated vehicle sensor systems use light detection and ranging (LiDAR) sensors. The prevailing technology is scanning LiDAR, where a collimated laser beam illuminates objects sequentially point-by-point to capture 3D range data. In current systems, the point clouds from the LiDAR sensors are mainly used for object detection. To estimate the velocity of an object of interest (OoI) in the point cloud, the tracking of the object or sensor data fusion is needed. Scanning LiDAR sensors show the motion distortion effect, which occurs when objects have a relative velocity to the sensor. Often, this effect is filtered, by using sensor data fusion, to use an undistorted point cloud for object detection. In this study, we developed a method using an artificial neural network to estimate an object's velocity and direction of motion in the sensor's field of view (FoV) based on the motion distortion effect without any sensor data fusion. This network was trained and evaluated with a synthetic dataset featuring the motion distortion effect. With the method presented in this paper, one can estimate the velocity and direction of an OoI that moves independently from the sensor from a single point cloud using only one single sensor. The method achieves a root mean squared error (RMSE) of 0.1187 m s",https://doi.org/10.3390/s23239426,,No,
0000-0002-8777-8805,Arsalan Haider,Kempten University of Applied Sciences,A Methodology to Model the Rain and Fog Effect on the Performance of Automotive LiDAR Sensors.,1970,"In this work, we introduce a novel approach to model the rain and fog effect on the light detection and ranging (LiDAR) sensor performance for the simulation-based testing of LiDAR systems. The proposed methodology allows for the simulation of the rain and fog effect using the rigorous applications of the Mie scattering theory on the time domain for transient and point cloud levels for spatial analyses. The time domain analysis permits us to benchmark the virtual LiDAR signal attenuation and signal-to-noise ratio (SNR) caused by rain and fog droplets. In addition, the detection rate (DR), false detection rate (FDR), and distance error derror of the virtual LiDAR sensor due to rain and fog droplets are evaluated on the point cloud level. The mean absolute percentage error (MAPE) is used to quantify the simulation and real measurement results on the time domain and point cloud levels for the rain and fog droplets. The results of the simulation and real measurements match well on the time domain and point cloud levels if the simulated and real rain distributions are the same. The real and virtual LiDAR sensor performance degrades more under the influence of fog droplets than in rain.",https://doi.org/10.3390/s23156891,,No,
0000-0002-8777-8805,Arsalan Haider,Kempten University of Applied Sciences,Performance Evaluation of MEMS-Based Automotive LiDAR Sensor and Its Simulation Model as per ASTM E3125-17 Standard.,1970,"Measurement performance evaluation of real and virtual automotive light detection and ranging (LiDAR) sensors is an active area of research. However, no commonly accepted automotive standards, metrics, or criteria exist to evaluate their measurement performance. ASTM International released the ASTM E3125-17 standard for the operational performance evaluation of 3D imaging systems commonly referred to as terrestrial laser scanners (TLS). This standard defines the specifications and static test procedures to evaluate the 3D imaging and point-to-point distance measurement performance of TLS. In this work, we have assessed the 3D imaging and point-to-point distance estimation performance of a commercial micro-electro-mechanical system (MEMS)-based automotive LiDAR sensor and its simulation model according to the test procedures defined in this standard. The static tests were performed in a laboratory environment. In addition, a subset of static tests was also performed at the proving ground in natural environmental conditions to determine the 3D imaging and point-to-point distance measurement performance of the real LiDAR sensor. In addition, real scenarios and environmental conditions were replicated in the virtual environment of a commercial software to verify the LiDAR model's working performance. The evaluation results show that the LiDAR sensor and its simulation model under analysis pass all the tests specified in the ASTM E3125-17 standard. This standard helps to understand whether sensor measurement errors are due to internal or external influences. We have also shown that the 3D imaging and point-to-point distance estimation performance of LiDAR sensors significantly impacts the working performance of the object recognition algorithm. That is why this standard can be beneficial in validating automotive real and virtual LiDAR sensors, at least in the early stage of development. Furthermore, the simulation and real measurements show good agreement on the point cloud and object recognition levels.",https://doi.org/10.3390/s23063113,,No,
0000-0002-8777-8805,Arsalan Haider,Kempten University of Applied Sciences,Development of High-Fidelity Automotive LiDAR Sensor Model with Standardized Interfaces.,1970,"This work introduces a process to develop a tool-independent, high-fidelity, ray tracing-based light detection and ranging (LiDAR) model. This virtual LiDAR sensor includes accurate modeling of the scan pattern and a complete signal processing toolchain of a LiDAR sensor. It is developed as a functional mock-up unit (FMU) by using the standardized open simulation interface (OSI) 3.0.2, and functional mock-up interface (FMI) 2.0. Subsequently, it was integrated into two commercial software virtual environment frameworks to demonstrate its exchangeability. Furthermore, the accuracy of the LiDAR sensor model is validated by comparing the simulation and real measurement data on the time domain and on the point cloud level. The validation results show that the mean absolute percentage error (MAPE) of simulated and measured time domain signal amplitude is 1.7%. In addition, the MAPE of the number of points Npoints and mean intensity Imean values received from the virtual and real targets are 8.5% and 9.3%, respectively. To the author's knowledge, these are the smallest errors reported for the number of received points Npoints and mean intensity Imean values up until now. Moreover, the distance error derror is below the range accuracy of the actual LiDAR sensor, which is 2 cm for this use case. In addition, the proving ground measurement results are compared with the state-of-the-art LiDAR model provided by commercial software and the proposed LiDAR model to measure the presented model fidelity. The results show that the complete signal processing steps and imperfections of real LiDAR sensors need to be considered in the virtual LiDAR to obtain simulation results close to the actual sensor. Such considerable imperfections are optical losses, inherent detector effects, effects generated by the electrical amplification, and noise produced by the sunlight.",https://doi.org/10.3390/s22197556,,No,
0009-0003-3438-6191,Ludwig Kastner,Kempten University of Applied Sciences,Velocity Estimation from LiDAR Sensors Motion Distortion Effect.,1970,"Many modern automated vehicle sensor systems use light detection and ranging (LiDAR) sensors. The prevailing technology is scanning LiDAR, where a collimated laser beam illuminates objects sequentially point-by-point to capture 3D range data. In current systems, the point clouds from the LiDAR sensors are mainly used for object detection. To estimate the velocity of an object of interest (OoI) in the point cloud, the tracking of the object or sensor data fusion is needed. Scanning LiDAR sensors show the motion distortion effect, which occurs when objects have a relative velocity to the sensor. Often, this effect is filtered, by using sensor data fusion, to use an undistorted point cloud for object detection. In this study, we developed a method using an artificial neural network to estimate an object's velocity and direction of motion in the sensor's field of view (FoV) based on the motion distortion effect without any sensor data fusion. This network was trained and evaluated with a synthetic dataset featuring the motion distortion effect. With the method presented in this paper, one can estimate the velocity and direction of an OoI that moves independently from the sensor from a single point cloud using only one single sensor. The method achieves a root mean squared error (RMSE) of 0.1187 m s",https://doi.org/10.3390/s23239426,,No,
0009-0003-3438-6191,Ludwig Kastner,Kempten University of Applied Sciences,Performance Evaluation of MEMS-Based Automotive LiDAR Sensor and Its Simulation Model as per ASTM E3125-17 Standard.,1970,"Measurement performance evaluation of real and virtual automotive light detection and ranging (LiDAR) sensors is an active area of research. However, no commonly accepted automotive standards, metrics, or criteria exist to evaluate their measurement performance. ASTM International released the ASTM E3125-17 standard for the operational performance evaluation of 3D imaging systems commonly referred to as terrestrial laser scanners (TLS). This standard defines the specifications and static test procedures to evaluate the 3D imaging and point-to-point distance measurement performance of TLS. In this work, we have assessed the 3D imaging and point-to-point distance estimation performance of a commercial micro-electro-mechanical system (MEMS)-based automotive LiDAR sensor and its simulation model according to the test procedures defined in this standard. The static tests were performed in a laboratory environment. In addition, a subset of static tests was also performed at the proving ground in natural environmental conditions to determine the 3D imaging and point-to-point distance measurement performance of the real LiDAR sensor. In addition, real scenarios and environmental conditions were replicated in the virtual environment of a commercial software to verify the LiDAR model's working performance. The evaluation results show that the LiDAR sensor and its simulation model under analysis pass all the tests specified in the ASTM E3125-17 standard. This standard helps to understand whether sensor measurement errors are due to internal or external influences. We have also shown that the 3D imaging and point-to-point distance estimation performance of LiDAR sensors significantly impacts the working performance of the object recognition algorithm. That is why this standard can be beneficial in validating automotive real and virtual LiDAR sensors, at least in the early stage of development. Furthermore, the simulation and real measurements show good agreement on the point cloud and object recognition levels.",https://doi.org/10.3390/s23063113,,No,
0000-0002-4345-6142,Sarah Vermeeren,Bonn-Rhein-Sieg University of Applied Sciences,Evaluating Release Kinetics from Alginate Beads Coated with Polyelectrolyte Layers for Sustained Drug Delivery.,1970,"Current approaches in stem cell-based bone tissue engineering require a release of bioactive compounds over up to 2 weeks. This study presents a polyelectrolyte-layered system featuring sustained release of water-soluble drugs with decreased burst release. The bioactive compounds adenosine 5'-triphosphate (ATP), suramin, and A740003 (a less water-soluble purinergic receptor ligand) were incorporated into alginate hydrogel beads subsequently layered with different polyelectrolytes (chitosan, poly(allyl amine), alginate, or lignosulfonate). Drug release into aqueous medium was monitored over 14 days and evaluated using Korsmeyer-Peppas, Peppas-Sahlin, Weibull models, and a Langmuir-like ""Two-Stage"" model. Release kinetics strongly depended on both the drug and the polyelectrolyte system. For ATP, five alternating layers of poly(allyl amine) and alginate proved to be most effective in sustaining the release. Release of suramin could be prolonged best with lignosulfonate as polyanion. A740003 showed prolonged release even without layering. Applying polyelectrolyte layers significantly slowed down the burst release. Release curves could be best described with the Langmuir-like model.",https://doi.org/10.1021/acsabm.1c00417,,No,
0000-0003-1133-9424,Alexander Asteroth,Bonn-Rhein-Sieg University of Applied Sciences,Efficient Quality Diversity Optimization of 3D Buildings through 2D Pre-Optimization.,1970,"Quality diversity algorithms can be used to efficiently create a diverse set of solutions to inform engineers' intuition. But quality diversity is not efficient in very expensive problems, needing hundreds of thousands of evaluations. Even with the assistance of surrogate models, quality diversity needs hundreds or even thousands of evaluations, which can make its use infeasible. In this study, we try to tackle this problem by using a pre-optimization strategy on a lower-dimensional optimization problem and then map the solutions to a higher-dimensional case. For a use case to design buildings that minimize wind nuisance, we show that we can predict flow features around 3D buildings from 2D flow features around building footprints. For a diverse set of building designs, by sampling the space of 2D footprints with a quality diversity algorithm, a predictive model can be trained that is more accurate than when trained on a set of footprints that were selected with a space-filling algorithm like the Sobol sequence. Simulating only 16 buildings in 3D, a set of 1,024 building designs with low predicted wind nuisance is created. We show that we can produce better machine learning models by producing training data with quality diversity instead of using common sampling techniques. The method can bootstrap generative design in a computationally expensive 3D domain and allow engineers to sweep the design space, understanding wind nuisance in early design phases.",https://doi.org/10.1162/evco_a_00326,,No,
0000-0003-1133-9424,Alexander Asteroth,Bonn-Rhein-Sieg University of Applied Sciences,"Determining Lennard-Jones Parameters Using Multiscale Target Data through Presampling-Enhanced, Surrogate-Assisted Global Optimization.",1970,"Force field-based models are a Newtonian mechanics approximation of reality and are inherently noisy. Coupling models from different molecular scale domains (including single, gas-phase molecules up to multimolecule, condensed phase ensembles) is difficult, which is also the case for finding solutions that transfer well between the scales. In this contribution, we introduce a surrogate-assisted algorithm to optimize Lennard-Jones parameters for target data from different scale domains to overcome the difficulties named above. Specifically, our approach combines a surrogate-assisted global evolutionary optimization method with a presampling phase that takes advantage of one scale domain being less computationally expensive to evaluate. The algorithm's components were evaluated individually, elucidating their individual merits. Our findings show that the process of parametrizing force fields can significantly benefit from both the presampling method, which alleviates the need to have a good initial guess for the parameters, and the surrogate model, which improves efficiency.",https://doi.org/10.1021/acs.jcim.2c01231,,No,
0000-0003-1133-9424,Alexander Asteroth,Bonn-Rhein-Sieg University of Applied Sciences,Artificial Intelligence in Elite Sports-A Narrative Review of Success Stories and Challenges.,1970,"This paper explores the role of artificial intelligence (AI) in elite sports. We approach the topic from two perspectives. Firstly, we provide a literature based overview of AI success stories in areas other than sports. We identified multiple approaches in the area of Machine Perception, Machine Learning and Modeling, Planning and Optimization as well as Interaction and Intervention, holding a potential for improving training and competition. Secondly, we discover the present status of AI use in elite sports. Therefore, in addition to another literature review, we interviewed leading sports scientist, which are closely connected to the main national service institute for elite sports in their countries. The analysis of this literature review and the interviews show that the most activity is carried out in the methodical categories of signal and image processing. However, projects in the field of modeling & planning have become increasingly popular within the last years. Based on these two perspectives, we extract deficits, issues and opportunities and summarize them in six key challenges faced by the sports analytics community. These challenges include data collection, controllability of an AI by the practitioners and explainability of AI results.",https://doi.org/10.3389/fspor.2022.861466,,Yes,potent(1)
0000-0003-1133-9424,Alexander Asteroth,Bonn-Rhein-Sieg University of Applied Sciences,"Measurement, Prediction, and Control of Individual Heart Rate Responses to Exercise-Basics and Options for Wearable Devices.",1970,"The use of wearable devices or ""wearables"" in the physical activity domain has been increasing in the last years. These devices are used as training tools providing the user with detailed information about individual physiological responses and feedback to the physical training process. Advantages in sensor technology, miniaturization, energy consumption and processing power increased the usability of these wearables. Furthermore, available sensor technologies must be reliable, valid, and usable. Considering the variety of the existing sensors not all of them are suitable to be integrated in wearables. The application and development of wearables has to consider the characteristics of the physical training process to improve the effectiveness and efficiency as training tools. During physical training, it is essential to elicit individual optimal strain to evoke the desired adjustments to training. One important goal is to neither overstrain nor under challenge the user. Many wearables use heart rate as indicator for this individual strain. However, due to a variety of internal and external influencing factors, heart rate kinetics are highly variable making it difficult to control the stress eliciting individually optimal strain. For optimal training control it is essential to model and predict individual responses and adapt the external stress if necessary. Basis for this modeling is the valid and reliable recording of these individual responses. Depending on the heart rate kinetics and the obtained physiological data, different models and techniques are available that can be used for strain or training control. Aim of this review is to give an overview of measurement, prediction, and control of individual heart rate responses. Therefore, available sensor technologies measuring the individual heart rate responses are analyzed and approaches to model and predict these individual responses discussed. Additionally, the feasibility for wearables is analyzed.",https://doi.org/10.3389/fphys.2018.00778,,No,
0000-0003-1133-9424,Alexander Asteroth,Bonn-Rhein-Sieg University of Applied Sciences,Data-Efficient Design Exploration through Surrogate-Assisted Illumination.,1970,"Design optimization techniques are often used at the beginning of the design process to explore the space of possible designs. In these domains illumination algorithms, such as MAP-Elites, are promising alternatives to classic optimization algorithms because they produce diverse, high-quality solutions in a single run, instead of only a single near-optimal solution. Unfortunately, these algorithms currently require a large number of function evaluations, limiting their applicability. In this article, we introduce a new illumination algorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate modeling techniques to create a map of the design space according to user-defined features while minimizing the number of fitness evaluations. On a two-dimensional airfoil optimization problem, SAIL produces hundreds of diverse but high-performing designs with several orders of magnitude fewer evaluations than MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of producing maps of high-performing designs in realistic three-dimensional aerodynamic tasks with an accurate flow simulation. Data-efficient design exploration with SAIL can help designers understand what is possible, beyond what is optimal, by considering more than pure objective-based optimization.",https://doi.org/10.1162/evco_a_00231,,No,
0000-0001-8406-6717,Martin Hamer,Bonn-Rhein-Sieg University of Applied Sciences,Remineralizing soils? The agricultural usage of silicate rock powders: A review.,1970,"Soil nutrient depletion threatens global food security and has been seriously underestimated for potassium (K) and several micronutrients. This is particularly the case for highly weathered soils in tropical countries, where classical soluble fertilizers are often not affordable or not accessible. One way to replenish macro- and micronutrients are ground silicate rock powders (SRPs). Rock forming silicate minerals contain most nutrients essential for higher plants, yet slow and inconsistent weathering rates have restricted their use in the past. Recent findings, however, challenge past agronomic objections which insufficiently addressed the factorial complexity of the weathering process. This review therefore first presents a framework with the most relevant factors for the weathering of SRPs through which several outcomes of prior studies can be explained. A subsequent analysis of 48 crop trials reveals the potential as alternative K source and multi-nutrient soil amendment for tropical soils, whereas the benefits for temperate soils are currently inconclusive. Beneficial results prevail for mafic and ultramafic rocks like basalts and rocks containing nepheline or glauconite. Several rock modifications are highly efficient in increasing the agronomic effectiveness of SRPs. Enhanced weathering of SRPs could additionally sequester substantial amounts of CO",https://doi.org/10.1016/j.scitotenv.2021.150976,,Yes,potent(1)
0000-0002-5092-3677,Robin Strickstrock,Bonn-Rhein-Sieg University of Applied Sciences,"Determining Lennard-Jones Parameters Using Multiscale Target Data through Presampling-Enhanced, Surrogate-Assisted Global Optimization.",1970,"Force field-based models are a Newtonian mechanics approximation of reality and are inherently noisy. Coupling models from different molecular scale domains (including single, gas-phase molecules up to multimolecule, condensed phase ensembles) is difficult, which is also the case for finding solutions that transfer well between the scales. In this contribution, we introduce a surrogate-assisted algorithm to optimize Lennard-Jones parameters for target data from different scale domains to overcome the difficulties named above. Specifically, our approach combines a surrogate-assisted global evolutionary optimization method with a presampling phase that takes advantage of one scale domain being less computationally expensive to evaluate. The algorithm's components were evaluated individually, elucidating their individual merits. Our findings show that the process of parametrizing force fields can significantly benefit from both the presampling method, which alleviates the need to have a good initial guess for the parameters, and the surrogate model, which improves efficiency.",https://doi.org/10.1021/acs.jcim.2c01231,,No,
0000-0002-9712-598X,Rainer Herpers,Bonn-Rhein-Sieg University of Applied Sciences,Neutral buoyancy and the static perception of upright.,1970,"The perceptual upright results from the multisensory integration of the directions indicated by vision and gravity as well as a prior assumption that upright is towards the head. The direction of gravity is signalled by multiple cues, the predominant of which are the otoliths of the vestibular system and somatosensory information from contact with the support surface. Here, we used neutral buoyancy to remove somatosensory information while retaining vestibular cues, thus ""splitting the gravity vector"" leaving only the vestibular component. In this way, neutral buoyancy can be used as a microgravity analogue. We assessed spatial orientation using the oriented character recognition test (OChaRT, which yields the perceptual upright, PU) under both neutrally buoyant and terrestrial conditions. The effect of visual cues to upright (the visual effect) was reduced under neutral buoyancy compared to on land but the influence of gravity was unaffected. We found no significant change in the relative weighting of vision, gravity, or body cues, in contrast to results found both in long-duration microgravity and during head-down bed rest. These results indicate a relatively minor role for somatosensation in determining the perceptual upright in the presence of vestibular cues. Short-duration neutral buoyancy is a weak analogue for microgravity exposure in terms of its perceptual consequences compared to long-duration head-down bed rest.",https://doi.org/10.1038/s41526-023-00296-x,,No,
0000-0002-9712-598X,Rainer Herpers,Hochschule Bonn-Rhein-Sieg,Vection underwater illustrates the limitations of neutral buoyancy as a microgravity analog.,1970,"Neutral buoyancy has been used as an analog for microgravity from the earliest days of human spaceflight. Compared to other options on Earth, neutral buoyancy is relatively inexpensive and presents little danger to astronauts while simulating some aspects of microgravity. Neutral buoyancy removes somatosensory cues to the direction of gravity but leaves vestibular cues intact. Removal of both somatosensory and direction of gravity cues while floating in microgravity or using virtual reality to establish conflicts between them has been shown to affect the perception of distance traveled in response to visual motion (vection) and the perception of distance. Does removal of somatosensory cues alone by neutral buoyancy similarly impact these perceptions? During neutral buoyancy we found no significant difference in either perceived distance traveled nor perceived size relative to Earth-normal conditions. This contrasts with differences in linear vection reported between short- and long-duration microgravity and Earth-normal conditions. These results indicate that neutral buoyancy is not an effective analog for microgravity for these perceptual effects.",https://doi.org/10.1038/s41526-023-00282-3,,No,
0000-0002-9712-598X,Rainer Herpers,Bonn-Rhein-Sieg University of Applied Sciences,Long-duration head down bed rest as an analog of microgravity: Effects on the static perception of upright.,1970,"Humans demonstrate many physiological changes in microgravity for which long-duration head down bed rest (HDBR) is a reliable analog. However, information on how HDBR affects sensory processing is lacking.",https://doi.org/10.3233/VES-210016,,No,
0000-0002-9712-598X,Rainer Herpers,Bonn-Rhein-Sieg University of Applied Sciences,Short Bouts of Intensive Exercise During the Workday Have a Positive Effect on Neuro-cognitive Performance.,1970,"Beside its positive impact on physical health, exercise is indicated to positively affect cognitive performance based on a relocation of cortical activity. This study examined the influence of different types of breaks on cognitive performance and related cortical activity in office-based employees. Breaks were filled with exercise, resting or a usual break and a control condition where employees continued working without any break. Cognitive performance was assessed using the d2-R test and two commercially available cognitive tasks. Brain cortical activity was recorded using electroencephalography before and after breaks. Individual's mood was analysed using a profile of mood state. Results indicate a positive effect of a 3-min boxing intervention on cognitive performance, mirrored by a decrease in prefrontal cortex activity. Although perceived psychological state was increased after the usual break, this is reflected in neither cortical activity nor cognitive performance. With respect to the fact that also bike activity resulted an increase in prefrontal alpha-2 activity, a positive effect of exercise on neuro-cognitive performance can be stated. Health and economic benefits may result from brief physical activity breaks and help to maintain workplace performance and job satisfaction. Copyright © 2015 John Wiley & Sons, Ltd.",https://doi.org/10.1002/smi.2654,,No,
0000-0002-9712-598X,Rainer Herpers,Bonn-Rhein-Sieg University of Applied Sciences,Effects of Exercise in Immersive Virtual Environments on Cortical Neural Oscillations and Mental State.,1970,"Virtual reality environments are increasingly being used to encourage individuals to exercise more regularly, including as part of treatment those with mental health or neurological disorders. The success of virtual environments likely depends on whether a sense of presence can be established, where participants become fully immersed in the virtual environment. Exposure to virtual environments is associated with physiological responses, including cortical activation changes. Whether the addition of a real exercise within a virtual environment alters sense of presence perception, or the accompanying physiological changes, is not known. In a randomized and controlled study design, moderate-intensity Exercise (i.e., self-paced cycling) and No-Exercise (i.e., automatic propulsion) trials were performed within three levels of virtual environment exposure. Each trial was 5 minutes in duration and was followed by posttrial assessments of heart rate, perceived sense of presence, EEG, and mental state. Changes in psychological strain and physical state were generally mirrored by neural activation patterns. Furthermore, these changes indicated that exercise augments the demands of virtual environment exposures and this likely contributed to an enhanced sense of presence.",https://doi.org/10.1155/2015/523250,,No,
0000-0002-9712-598X,Rainer Herpers,Hochschule Bonn-Rhein-Sieg,How much gravity is needed to establish the perceptual upright?,1970,"Might the gravity levels found on other planets and on the moon be sufficient to provide an adequate perception of upright for astronauts? Can the amount of gravity required be predicted from the physiological threshold for linear acceleration? The perception of upright is determined not only by gravity but also visual information when available and assumptions about the orientation of the body. Here, we used a human centrifuge to simulate gravity levels from zero to earth gravity along the long-axis of the body and measured observers' perception of upright using the Oriented Character Recognition Test (OCHART) with and without visual cues arranged to indicate a direction of gravity that differed from the body's long axis. This procedure allowed us to assess the relative contribution of the added gravity in determining the perceptual upright. Control experiments off the centrifuge allowed us to measure the relative contributions of normal gravity, vision, and body orientation for each participant. We found that the influence of 1 g in determining the perceptual upright did not depend on whether the acceleration was created by lying on the centrifuge or by normal gravity. The 50% threshold for centrifuge-simulated gravity's ability to influence the perceptual upright was at around 0.15 g, close to the level of moon gravity but much higher than the threshold for detecting linear acceleration along the long axis of the body. This observation may partially explain the instability of moonwalkers but is good news for future missions to Mars.",https://doi.org/10.1371/journal.pone.0106207,,No,
0000-0002-3963-687X,Jonas Bergrath,Bonn-Rhein-Sieg University of Applied Sciences,Mechanochemical Tailoring of Lignin Structure: Influence of Different Particle Sizes in the Organosolv Process.,1970,"The autocatalyzed ethanolic organosolv process is gaining increasing attention for the sulfur-free isolation of lignin, which is subsequently used as a renewable substitute for various fossil-based applications. For the first time, the mechanochemical influence of seven different particle sizes of two different biomasses on the respective organosolv lignin structure is examined. Wine pruning (Pinot Noir) and wine pomace (Accent) are used for organosolv process with particle sizes ranging from 2.0-1.6 mm to less than 0.25 mm. As particle size decreases, the weight-average molecular weight increases, while the total phenol content decreases significantly. Additionally, the distribution of the lignin-typical monolignols and relevant substructures, as determined by two-dimensional heteronuclear nuclear magnetic resonance spectra single quantum coherence (HSQC), is observed. The degree of grinding of the biomass has a clear chemical-structural influence on the isolated HG and HGS organosolv lignins. Therefore, it is crucial to understand this influence to apply organosolv lignins in a targeted manner. In the future, particle size specifications in the context of the organosolv process should be expressed in terms of distribution densities rather than in terms of a smaller than specification.",https://doi.org/10.1002/mabi.202400090,,No,
0000-0002-6921-7393,Philipp Gillemot,Bonn-Rhein-Sieg University of Applied Sciences,Analyses of used engine oils via atomic spectroscopy - Influence of sample pre-treatment and machine learning for engine type classification and lifetime assessment.,1970,"The analysis of used engine oils from industrial engines enables the study of engine wear and oil degradation in order to evaluate the necessity of oil changes. As the matrix composition of an engine oil strongly depends on its intended application, meaningful diagnostic oil analyses bear considerable challenges. Owing to the broad spectrum of available oil matrices, we have evaluated the applicability of using an internal standard and/or preceding sample digestion for elemental analysis of used engine oils via inductively coupled plasma optical emission spectroscopy (ICP OES). Elements originating from both wear particles and additives as well as particle size influence could be clearly recognized by their distinct digestion behaviour. While a precise determination of most wear elements can be achieved in oily matrix, the measurement of additives is performed preferably after sample digestion. Considering a dataset of physicochemical parameters and elemental composition for several hundred used engine oils, we have further investigated the feasibility of predicting the identity and overall condition of an unknown combustion engine using the machine learning system XGBoost. A maximum accuracy of 89.6% in predicting the engine type was achieved, a mean error of less than 10% of the observed timeframe in predicting the oil running time and even less than 4% for the total engine running time, based purely on common oil check data. Furthermore, obstacles and possibilities to improve the performance of the machine learning models were analysed and the factors that enabled the prediction were explored with SHapley Additive exPlanation (SHAP). Our results demonstrate that both the identification of an unknown engine as well as a lifetime assessment can be performed for a first estimation of the actual sample without requiring meticulous documentation.",https://doi.org/10.1016/j.talanta.2021.122431,,Yes,meticulous(1)
0000-0001-7062-2217,Sarah Shoushrah,Bonn-Rhein-Sieg University of Applied Sciences,Therapeutic Treatments for Osteoporosis-Which Combination of Pills Is the Best among the Bad?,1970,"Osteoporosis is a chronical, systemic skeletal disorder characterized by an increase in bone resorption, which leads to reduced bone density. The reduction in bone mineral density and therefore low bone mass results in an increased risk of fractures. Osteoporosis is caused by an imbalance in the normally strictly regulated bone homeostasis. This imbalance is caused by overactive bone-resorbing osteoclasts, while bone-synthesizing osteoblasts do not compensate for this. In this review, the mechanism is presented, underlined by in vitro and animal models to investigate this imbalance as well as the current status of clinical trials. Furthermore, new therapeutic strategies for osteoporosis are presented, such as anabolic treatments and catabolic treatments and treatments using biomaterials and biomolecules. Another focus is on new combination therapies with multiple drugs which are currently considered more beneficial for the treatment of osteoporosis than monotherapies. Taken together, this review starts with an overview and ends with the newest approaches for osteoporosis therapies and a future perspective not presented so far.",https://doi.org/10.3390/ijms23031393,,No,
0000-0001-7062-2217,Sarah Shoushrah,Bonn-Rhein-Sieg University of Applied Sciences,Sinking Our Teeth in Getting Dental Stem Cells to Clinics for Bone Regeneration.,1970,"Dental stem cells have been isolated from the medical waste of various dental tissues. They have been characterized by numerous markers, which are evaluated herein and differentiated into multiple cell types. They can also be used to generate cell lines and iPSCs for long-term in vitro research. Methods for utilizing these stem cells including cellular systems such as organoids or cell sheets, cell-free systems such as exosomes, and scaffold-based approaches with and without drug release concepts are reported in this review and presented with new pictures for clarification. These in vitro applications can be deployed in disease modeling and subsequent pharmaceutical research and also pave the way for tissue regeneration. The main focus herein is on the potential of dental stem cells for hard tissue regeneration, especially bone, by evaluating their potential for osteogenesis and angiogenesis, and the regulation of these two processes by growth factors and environmental stimulators. Current in vitro and in vivo publications show numerous benefits of using dental stem cells for research purposes and hard tissue regeneration. However, only a few clinical trials currently exist. The goal of this review is to pinpoint this imbalance and encourage scientists to pick up this research and proceed one step further to translation.",https://doi.org/10.3390/ijms22126387,,Yes,potent(2)
0000-0001-7062-2217,Sarah Shoushrah,Bonn-Rhein-Sieg University of Applied Sciences,Polysaccharide-Based Systems for Targeted Stem Cell Differentiation and Bone Regeneration.,1970,"Bone tissue engineering is an ever-changing, rapidly evolving, and highly interdisciplinary field of study, where scientists try to mimic natural bone structure as closely as possible in order to facilitate bone healing. New insights from cell biology, specifically from mesenchymal stem cell differentiation and signaling, lead to new approaches in bone regeneration. Novel scaffold and drug release materials based on polysaccharides gain increasing attention due to their wide availability and good biocompatibility to be used as hydrogels and/or hybrid components for drug release and tissue engineering. This article reviews the current state of the art, recent developments, and future perspectives in polysaccharide-based systems used for bone regeneration.",https://doi.org/10.3390/biom9120840,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,Special Issue on Whole Genome Amplification.,1970,"The development of whole-genome amplification (WGA) techniques has opened up new avenues for genetic analysis and genome research, in particular by facilitating the genome-wide analysis of few or even single copies of genomic DNA, such as from single cells (prokaryotic or eukaryotic) or virions [...].",https://doi.org/10.3390/ijms24119626,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,"Cost-Effective Next Generation Sequencing-Based STR Typing with Improved Analysis of Minor, Degraded and Inhibitor-Containing DNA Samples.",1970,"Forensic DNA profiles are established by multiplex PCR amplification of a set of highly variable short tandem repeat (STR) loci followed by capillary electrophoresis (CE) as a means to assign alleles to PCR products of differential length. Recently, CE analysis of STR amplicons has been supplemented by high-throughput next generation sequencing (NGS) techniques that are able to detect isoalleles bearing sequence polymorphisms and allow for an improved analysis of degraded DNA. Several such assays have been commercialised and validated for forensic applications. However, these systems are cost-effective only when applied to high numbers of samples. We report here an alternative, cost-efficient shallow-sequence output NGS assay called maSTR assay that, in conjunction with a dedicated bioinformatics pipeline called SNiPSTR, can be implemented with standard NGS instrumentation. In a back-to-back comparison with a CE-based, commercial forensic STR kit, we find that for samples with low DNA content, with mixed DNA from different individuals, or containing PCR inhibitors, the maSTR assay performs equally well, and with degraded DNA is superior to CE-based analysis. Thus, the maSTR assay is a simple, robust and cost-efficient NGS-based STR typing method applicable for human identification in forensic and biomedical contexts.",https://doi.org/10.3390/ijms24043382,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,New Perspectives for Whole Genome Amplification in Forensic STR Analysis.,1970,"Modern PCR-based analytical techniques have reached sensitivity levels that allow for obtaining complete forensic DNA profiles from even tiny traces containing genomic DNA amounts as small as 125 pg. Yet these techniques have reached their limits when it comes to the analysis of traces such as fingerprints or single cells. One suggestion to overcome these limits has been the usage of whole genome amplification (WGA) methods. These methods aim at increasing the copy number of genomic DNA and by this means generate more template DNA for subsequent analyses. Their application in forensic contexts has so far remained mostly an academic exercise, and results have not shown significant improvements and even have raised additional analytical problems. Until very recently, based on these disappointments, the forensic application of WGA seems to have largely been abandoned. In the meantime, however, novel improved methods are pointing towards a perspective for WGA in specific forensic applications. This review article tries to summarize current knowledge about WGA in forensics and suggests the forensic analysis of single-donor bioparticles and of single cells as promising applications.",https://doi.org/10.3390/ijms23137090,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,Ribosomal DNA as target for the assessment of DNA degradation of human and canine DNA.,1970,"The assessment of DNA amount and DNA integrity can support forensic DNA analysis, in particular of problematic traces such as single telogen hairs where STR typing success is often hampered by low amounts and strong degradation of nuclear DNA. Common strategies consist of quantitative polymerase chain reaction (qPCR)-based analysis of the abundance of a short versus a long nuclear amplicon, the latter prone to DNA degradation. To increase sensitivity, commercial qPCR solutions rest on amplification of multi-copy DNA sequences. Here we show that ribosomal DNA (rDNA) sequences are well suited for the same purpose. Because rDNA sequences are present in high copy number in most eukaryotic species, qPCR strategies can easily be adapted to non-human species. In this paper, we establish qPCR-based assays for human or dog DNA, respectively, which allow for sensitive analysis of DNA amounts and DNA degradation. We show that the human system can be applied to DNA of single telogen hairs, where STR typing success correlates with measured amounts and integrity of the DNA. By adapting the system to dog rDNA sequences we found that single telogen dog hairs often displayed less DNA degradation than human telogen hairs, in most cases allowing for successful STR typing. Thus, qPCR-based analysis of rDNA represents a cost-effective, highly sensitive strategy to assess DNA amount and integrity that can be adapted to hairs or other traces from various animal species.",https://doi.org/10.1016/j.legalmed.2020.101819,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,The Unfolded Protein Response in Breast Cancer.,1970,"In 2018, in the US alone, it is estimated that 268,670 people will be diagnosed with breast cancer, and that 41,400 will die from it. Since breast cancers often become resistant to therapies, and certain breast cancers lack therapeutic targets, new approaches are urgently required. A cell-stress response pathway, the unfolded protein response (UPR), has emerged as a promising target for the development of novel breast cancer treatments. This pathway is activated in response to a disturbance in endoplasmic reticulum (ER) homeostasis but has diverse physiological and disease-specific functions. In breast cancer, UPR signalling promotes a malignant phenotype and can confer tumours with resistance to widely used therapies. Here, we review several roles for UPR signalling in breast cancer, highlighting UPR-mediated therapy resistance and the potential for targeting the UPR alone or in combination with existing therapies.",https://doi.org/10.3390/cancers10100344,,Yes,potent(1)
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,New roles for old enzymes: killer caspases as the engine of cell behavior changes.,1970,"It has become increasingly clear that caspases, far from being merely cell death effectors, have a much wider range of functions within the cell. These functions are as diverse as signal transduction and cytoskeletal remodeling, and caspases are now known to have an essential role in cell proliferation, migration, and differentiation. There is also evidence that apoptotic cells themselves can direct the behavior of nearby cells through the caspase-dependent secretion of paracrine signaling factors. In some processes, including the differentiation of skeletal muscle myoblasts, both caspase activation in differentiating cells as well as signaling from apoptotic cells has been reported. Here, we review the non-apoptotic outcomes of caspase activity in a range of different model systems and attempt to integrate this knowledge.",https://doi.org/10.3389/fphys.2014.00149,,No,
0000-0002-1623-1917,Richard Jäger,Bonn-Rhein-Sieg University of Applied Sciences,"""Dead Cells Talking"": The Silent Form of Cell Death Is Not so Quiet.",1970,"After more than twenty years of research, the molecular events of apoptotic cell death can be succinctly stated; different pathways, activated by diverse signals, increase the activity of proteases called caspases that rapidly and irreversibly dismantle condemned cell by cleaving specific substrates. In this time the ideas that apoptosis protects us from tumourigenesis and that cancer chemotherapy works by inducing apoptosis also emerged. Currently, apoptosis research is shifting away from the intracellular events within the dying cell to focus on the effect of apoptotic cells on surrounding tissues. This is producing counterintuitive data showing that our understanding of the role of apoptosis in tumourigenesis and cancer therapy is too simple, with some interesting and provocative implications. Here, we will consider evidence supporting the idea that dying cells signal their presence to the surrounding tissue and, in doing so, elicit repair and regeneration that compensates for any loss of function caused by cell death. We will discuss evidence suggesting that cancer cell proliferation may be driven by inappropriate or corrupted tissue-repair programmes that are initiated by signals from apoptotic cells and show how this may dramatically modify how we view the role of apoptosis in both tumourigenesis and cancer therapy.",https://doi.org/10.1155/2012/453838,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Consensus guidelines for the diagnosis and management of isolated sulfite oxidase deficiency and molybdenum cofactor deficiencies.,1970,"Sulfite intoxication is the hallmark of four ultrarare disorders that are caused by impaired sulfite oxidase activity due to genetic defects in the synthesis of the molybdenum cofactor or of the apoenzyme sulfite oxidase. Delays on the diagnosis of these disorders are common and have been caused by their unspecific presentation of acute neonatal encephalopathy with high early mortality, followed by the evolution of dystonic cerebral palsy and also by the lack of easily available and reliable diagnostic tests. There is significant variation in survival and in the quality of symptomatic management of affected children. One of the four disorders, molybdenum cofactor deficiency type A (MoCD-A) has recently become amenable to causal treatment with synthetic cPMP (fosdenopterin). The evidence base for the rational use of cPMP is very limited. This prompted the formulation of these clinical guidelines to facilitate diagnosis and support the management of patients. The guidelines were developed by experts in diagnosis and treatment of sulfite intoxication disorders. It reflects expert consensus opinion and evidence from a systematic literature search.",https://doi.org/10.1002/jimd.12730,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,N-Acetylglutamate and N-acetylmethionine compromise mitochondrial bioenergetics homeostasis and glutamate oxidation in brain of developing rats: Potential implications for the pathogenesis of ACY1 deficiency.,1970,"Aminoacylase 1 (ACY1) deficiency is an inherited metabolic disorder biochemically characterized by high urinary concentrations of aliphatic N-acetylated amino acids and associated with a broad clinical spectrum with predominant neurological signs. Considering that the pathogenesis of ACY1 is practically unknown and the brain is highly dependent on energy production, the in vitro effects of N-acetylglutamate (NAG) and N-acetylmethionine (NAM), major metabolites accumulating in ACY1 deficiency, on the enzyme activities of the citric acid cycle (CAC), of the respiratory chain complexes and glutamate dehydrogenase (GDH), as well as on ATP synthesis were evaluated in brain mitochondrial preparations of developing rats. NAG mildly inhibited mitochondrial isocitrate dehydrogenase 2 (IDH2) activity, moderately inhibited the activities of isocitrate dehydrogenase 3 (IDH3) and complex II-III of the respiratory chain and markedly suppressed the activities of complex IV and GDH. Of note, the NAG-induced inhibitory effect on IDH3 was competitive, whereas that on GDH was mixed. On the other hand, NAM moderately inhibited the activity of respiratory complexes II-III and GDH activities and strongly decreased complex IV activity. Furthermore, NAM was unable to modify any of the CAC enzyme activities, indicating a selective effect of NAG toward IDH mitochondrial isoforms. In contrast, the activities of citrate synthase, α-ketoglutarate dehydrogenase, malate dehydrogenase, and of the respiratory chain complexes I and II were not changed by these N-acetylated amino acids. Finally, NAG and NAM strongly decreased mitochondrial ATP synthesis. Taken together, the data indicate that NAG and NAM impair mitochondrial brain energy homeostasis.",https://doi.org/10.1016/j.bbrc.2023.149123,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Disturbance of mitochondrial functions caused by N-acetylglutamate and N-acetylmethionine in brain of adolescent rats: Potential relevance in aminoacylase 1 deficiency.,1970,"Aminoacylase 1 (ACY1) deficiency is a rare genetic disorder that affects the breakdown of short-chain aliphatic N-acetylated amino acids, leading to the accumulation of these amino acid derivatives in the urine of patients. Some of the affected individuals have presented with heterogeneous neurological symptoms such as psychomotor delay, seizures, and intellectual disability. Considering that the pathological mechanisms of brain damage in this disorder remain mostly unknown, here we investigated whether major metabolites accumulating in ACY1 deficiency, namely N-acetylglutamate (NAG) and N-acetylmethionine (NAM), could be toxic to the brain by examining their in vitro effects on important mitochondrial properties. We assessed the effects of NAG and NAM on membrane potential, swelling, reducing equivalents, and Ca",https://doi.org/10.1016/j.neuint.2023.105631,,Yes,potent(1)
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,The glycine ,1970,"Isovaleric acidemia (IVA), due to isovaleryl-CoA dehydrogenase (IVD) deficiency, results in the accumulation of isovaleryl-CoA, isovaleric acid and secondary metabolites. The increase in these metabolites decreases mitochondrial energy production and increases oxidative stress. This contributes to the neuropathological features of IVA. A general assumption in the literature exists that glycine ",https://doi.org/10.1016/j.csbj.2023.01.041,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,3-Hydroxyisobutyric acid dehydrogenase deficiency: Expanding the clinical spectrum and quantitation of D- and L-3-Hydroxyisobutyric acid by an LC-MS/MS method.,1970,"A deficiency of 3-hydroxyisobutyric acid dehydrogenase (HIBADH) has been recently identified as a cause of primary 3-hydroxyisobutyric aciduria in two siblings; the only previously recognized primary cause had been a deficiency of methylmalonic semialdehyde dehydrogenase, the enzyme that is immediately downstream of HIBADH in the valine catabolic pathway and is encoded by the ALDH6A1 gene. Here we report on three additional patients from two unrelated families who present with marked and persistent elevations of urine L-3-hydroxyisobutyric acid (L-3HIBA) and a range of clinical findings. Molecular genetic analyses revealed novel, homozygous variants in the HIBADH gene that are private within each family. Evidence for pathogenicity of the identified variants is presented, including enzymatic deficiency of HIBADH in patient fibroblasts. This report describes new variants in HIBADH as an underlying cause of primary 3-hydroxyisobutyric aciduria and expands the clinical spectrum of this recently identified inborn error of valine metabolism. Additionally, we describe a quantitative method for the measurement of D- and L-3HIBA in plasma and urine and present the results of a valine restriction therapy in one of the patients.",https://doi.org/10.1002/jimd.12486,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Biomarkers for drug development in propionic and methylmalonic acidemias.,1970,"There is an unmet need for the development and validation of biomarkers and surrogate endpoints for clinical trials in propionic acidemia (PA) and methylmalonic acidemia (MMA). This review examines the pathophysiology and clinical consequences of PA and MMA that could form the basis for potential biomarkers and surrogate endpoints. Changes in primary metabolites such as methylcitric acid (MCA), MCA:citric acid ratio, oxidation of ",https://doi.org/10.1002/jimd.12478,,Yes,potent(1)
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Correction to: 3-hydroxy-3-methylglutaryl-coenzyme A lyase deficiency: one disease - many faces.,1970,Kein Abstract verfügbar,https://doi.org/10.1186/s13023-021-02154-z,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Diagnosis of atypical myopathy based on organic acid and acylcarnitine profiles and evolution of biomarkers in surviving horses.,1970,"Atypical myopathy (AM), an acquired multiple acyl-CoA dehydrogenase deficiency (MADD) in horses, induce changes in mitochondrial metabolism. Only few veterinary laboratories offer diagnostic testing for this disease. Inborn and acquired MADD exist in humans, therefore determination of organic acids (OA) in urine and acylcarnitines (AC) in blood by assays available in medical laboratories can serve as AM diagnostics. The evolution of OA and AC profiles in surviving horses is unreported.",https://doi.org/10.1016/j.ymgmr.2021.100827,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,3-Hydroxyisobutyrate dehydrogenase (HIBADH) deficiency-A novel disorder of valine metabolism.,1970,"3-Hydroxyisobutyric acid (3HiB) is an intermediate in the degradation of the branched-chain amino acid valine. Disorders in valine degradation can lead to 3HiB accumulation and its excretion in the urine. This article describes the first two patients with a new metabolic disorder, 3-hydroxyisobutyrate dehydrogenase (HIBADH) deficiency, its phenotype and its treatment with a low-valine diet. The detected mutation in the HIBADH gene leads to nonsense-mediated mRNA decay of the mutant allele and to a complete loss-of-function of the enzyme. Under strict adherence to a low-valine diet a rapid decrease of 3HiB excretion in the urine was observed. Due to limited patient numbers and intrafamilial differences in phenotype with one affected and one unaffected individual, the clinical phenotype of HIBADH deficiency needs further evaluation.",https://doi.org/10.1002/jimd.12410,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Succinyl-CoA:3-oxoacid coenzyme A transferase (SCOT) deficiency: A rare and potentially fatal metabolic disease.,1970,"Succinyl-CoA:3-oxoacid coenzyme A transferase deficiency (SCOTD) is a rare autosomal recessive disorder of ketone body utilization caused by mutations in OXCT1. We performed a systematic literature search and evaluated clinical, biochemical and genetic data on 34 previously published and 10 novel patients with SCOTD. Structural mapping and in silico analysis of protein variants is also presented. All patients presented with severe ketoacidotic episodes. Age at first symptoms ranged from 36 h to 3 years (median 7 months). About 70% of patients manifested in the first year of life, approximately one quarter already within the neonatal period. Two patients died, while the remainder (95%) were alive at the time of the report. Almost all the surviving patients (92%) showed normal psychomotor development and no neurologic abnormalities. A total of 29 missense mutations are reported. Analysis of the published crystal structure of the human SCOT enzyme, paired with both sequence-based and structure-based methods to predict variant pathogenicity, provides insight into the biochemical consequences of the reported variants. Pathogenic variants cluster in SCOT protein regions that affect certain structures of the protein. The described pathogenic variants can be viewed in an interactive map of the SCOT protein at https://michelanglo.sgc.ox.ac.uk/r/oxct. This comprehensive data analysis provides a systematic overview of all cases of SCOTD published to date. Although SCOTD is a rather benign disorder with often favourable outcome, metabolic crises can be life-threatening or even fatal. As the diagnosis can only be made by enzyme studies or mutation analyses, SCOTD may be underdiagnosed.",https://doi.org/10.1016/j.biochi.2021.02.003,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Frequent sequence variants of human glycine N-acyltransferase (GLYAT) and inborn errors of metabolism.,1970,"Glycine conjugation is an important phase II reaction and represents a central detoxification pathway which is essential for the recycling of free coenzyme A. Only few sequence variants have been reported in the human GLYAT gene and only two studies have overexpressed the human protein in bacterial systems and partially characterized it. This has prompted us to study the wild-type enzyme and two sequence variants not only in the E. coli strain Origami 2(DE3), but also to overexpress GLYAT in HEK293 cells, a human-derived cell line. Following purification of the recombinant proteins from E. coli the wild-type GLYAT protein and sequence variants, p.(Gln61Leu) yielded decreased specific activity than the wild-type enzyme, while specific activity of p.(Asn156Ser) activity of the latter variant was somewhat increased. K",https://doi.org/10.1016/j.biochi.2021.02.002,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,Impaired ketone body utilisation as a cause of life-threatening ketoacidosis.,1970,Kein Abstract verfügbar,https://doi.org/10.1136/postgradmedj-2021-139710,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,The impact of COVID-19 pandemic on the diagnosis and management of inborn errors of metabolism: A global perspective.,1970,"Quantitative estimates for the global impact of COVID-19 on the diagnosis and management of patients with inborn errors of metabolism (IEM) are lacking. We collected relevant data from 16 specialized medical centers treating IEM patients in Europe, Asia and Africa. The median decline of reported IEM related services in March 1st-May 31st 2020 compared to the same period in 2019 were as high as 60-80% with a profound impact on patient management and care for this vulnerable patient group. More representative data along with outcome data and guidelines for managing IEM disorders under such extraordinary circumstances are needed.",https://doi.org/10.1016/j.ymgme.2020.09.004,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,2-methylacetoacetyl-coenzyme A thiolase (beta-ketothiolase) deficiency: one disease - two pathways.,1970,"2-methylacetoacetyl-coenzyme A thiolase deficiency (MATD; deficiency of mitochondrial acetoacetyl-coenzyme A thiolase T2/ ""beta-ketothiolase"") is an autosomal recessive disorder of ketone body utilization and isoleucine degradation due to mutations in ACAT1.",https://doi.org/10.1186/s13023-020-01357-0,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,3-hydroxy-3-methylglutaryl-coenzyme A lyase deficiency: one disease - many faces.,1970,3-hydroxy-3-methylglutaryl-coenzyme A lyase deficiency (HMGCLD) is an autosomal recessive disorder of ketogenesis and leucine degradation due to mutations in HMGCL.,https://doi.org/10.1186/s13023-020-1319-7,,No,
0000-0003-2903-4872,Jörn Oliver Sass,Bonn-Rhein-Sieg University of Applied Sciences,d-Glycerate kinase deficiency in a neuropediatric patient.,1970,"d-Glyceric aciduria (DGA) due to d-glycerate kinase deficiency (DGKD) is a rare autosomal-recessive inborn error of metabolism that is usually linked to the metabolism of fructose and serine. We describe a Moroccan patient with DGKD whose metabolic defect has been characterized by metabolite studies, sequencing of genomic DNA and by studies on the RNA level. Since birth the index patient presented with severe muscular hypotonia, joint hypermobility and tremor. Enantioselective analysis showed elevated d-glyceric acid in the urine of the patient, but not in that of his parents. DNA analysis revealed homozygosity in the GLYCTK gene for c.517G>T [p.(Val173Leu)], the first mutation reported for exon 3 of this gene, as well as for the c.530-4A>G polymorphism. RNA studies suggest that none of these sequence variants affects splicing. The mother was heterozygous for both sequence variants, the father heterozygous for the first one and homozygous for the polymorphism, which further supports that c.517G>T is the functionally relevant nucleotide change. The conservation of GLYCTK throughout evolution suggests an important biological role of this enzyme, although it is not known yet how mutations are linked to clinical features. Future studies should investigate the molecular defect in a more general way and search for additional roles of GLYCTK beyond its established role in catabolism of serine and fructose.",https://doi.org/10.1016/j.braindev.2019.11.008,,No,
0000-0002-0468-3051,Markus Witzler,Bonn-Rhein-Sieg University of Applied Sciences,Evaluating Release Kinetics from Alginate Beads Coated with Polyelectrolyte Layers for Sustained Drug Delivery.,1970,"Current approaches in stem cell-based bone tissue engineering require a release of bioactive compounds over up to 2 weeks. This study presents a polyelectrolyte-layered system featuring sustained release of water-soluble drugs with decreased burst release. The bioactive compounds adenosine 5'-triphosphate (ATP), suramin, and A740003 (a less water-soluble purinergic receptor ligand) were incorporated into alginate hydrogel beads subsequently layered with different polyelectrolytes (chitosan, poly(allyl amine), alginate, or lignosulfonate). Drug release into aqueous medium was monitored over 14 days and evaluated using Korsmeyer-Peppas, Peppas-Sahlin, Weibull models, and a Langmuir-like ""Two-Stage"" model. Release kinetics strongly depended on both the drug and the polyelectrolyte system. For ATP, five alternating layers of poly(allyl amine) and alginate proved to be most effective in sustaining the release. Release of suramin could be prolonged best with lignosulfonate as polyanion. A740003 showed prolonged release even without layering. Applying polyelectrolyte layers significantly slowed down the burst release. Release curves could be best described with the Langmuir-like model.",https://doi.org/10.1021/acsabm.1c00417,,No,
0000-0002-0468-3051,Markus Witzler,Bonn-Rhein-Sieg University of Applied Sciences,Polysaccharide-Based Systems for Targeted Stem Cell Differentiation and Bone Regeneration.,1970,"Bone tissue engineering is an ever-changing, rapidly evolving, and highly interdisciplinary field of study, where scientists try to mimic natural bone structure as closely as possible in order to facilitate bone healing. New insights from cell biology, specifically from mesenchymal stem cell differentiation and signaling, lead to new approaches in bone regeneration. Novel scaffold and drug release materials based on polysaccharides gain increasing attention due to their wide availability and good biocompatibility to be used as hydrogels and/or hybrid components for drug release and tissue engineering. This article reviews the current state of the art, recent developments, and future perspectives in polysaccharide-based systems used for bone regeneration.",https://doi.org/10.3390/biom9120840,,No,
0000-0002-0468-3051,Markus Witzler,Bonn-Rhein-Sieg University of Applied Sciences,Non-Cytotoxic Agarose/Hydroxyapatite Composite Scaffolds for Drug Release.,1970,"Healing of large bone defects requires implants or scaffolds that provide structural guidance for cell growth, differentiation, and vascularization. In the present work, an agarose-hydroxyapatite composite scaffold was developed that acts not only as a 3D matrix, but also as a release system. Hydroxyapatite (HA) was incorporated into the agarose gels in situ in various ratios by a simple procedure consisting of precipitation, cooling, washing, and drying. The resulting gels were characterized regarding composition, porosity, mechanical properties, and biocompatibility. A pure phase of carbonated HA was identified in the scaffolds, which had pore sizes of up to several hundred micrometers. Mechanical testing revealed elastic moduli of up to 2.8 MPa for lyophilized composites. MTT testing on Lw35human mesenchymal stem cells (hMSCs) and osteosarcoma MG-63 cells proved the biocompatibility of the scaffolds. Furthermore, scaffolds were loaded with model drug compounds for guided hMSC differentiation. Different release kinetic models were evaluated for adenosine 5'-triphosphate (ATP) and suramin, and data showed a sustained release behavior over four days.",https://doi.org/10.3390/ijms20143565,,No,
0000-0002-0468-3051,Markus Witzler,Bonn-Rhein-Sieg University of Applied Sciences,Lignin-Derived Biomaterials for Drug Release and Tissue Engineering.,1970,"Renewable resources are gaining increasing interest as a source for environmentally benign biomaterials, such as drug encapsulation/release compounds, and scaffolds for tissue engineering in regenerative medicine. Being the second largest naturally abundant polymer, the interest in lignin valorization for biomedical utilization is rapidly growing. Depending on its resource and isolation procedure, lignin shows specific antioxidant and antimicrobial activity. Today, efforts in research and industry are directed toward lignin utilization as a renewable macromolecular building block for the preparation of polymeric drug encapsulation and scaffold materials. Within the last five years, remarkable progress has been made in isolation, functionalization and modification of lignin and lignin-derived compounds. However, the literature so far mainly focuses lignin-derived fuels, lubricants and resins. The purpose of this review is to summarize the current state of the art and to highlight the most important results in the field of lignin-based materials for potential use in biomedicine (reported in 2014⁻2018). Special focus is placed on lignin-derived nanomaterials for drug encapsulation and release as well as lignin hybrid materials used as scaffolds for guided bone regeneration in stem cell-based therapies.",https://doi.org/10.3390/molecules23081885,,Yes,potent(1)
0000-0002-4893-1942,Sebastian Wolff,RheinMain University of Applied Sciences,Quantification of microplastics in wastewater systems of German industrial parks and their wastewater treatment plants.,1970,"Microplastics (MP) enter the aquatic environment via several pathways. Many research groups have focused on municipal discharge, while research on industrial sources is rare. This study provides one of the first insights into MP occurrence and distribution in the wastewater systems of industrial parks (IPs) and their wastewater treatment plants (IPWWTPs). The effluents from production plants as well as influent, effluent, and internal samples from the IPWWTPs were assessed. Sampling methods for parallel MP mass and number analyses were developed for varying conditions. The total item emissions of MP (≥10 μm) into the environment were analyzed using μ-Raman spectroscopy and ranged from 3 · 10",https://doi.org/10.1016/j.scitotenv.2023.163349,,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Investigation of microplastics contamination in drinking water of a German city.,1970,"The drinking water of a German city was investigated for microplastics. Random samples were taken from three house connections, one transfer station, and five consumption taps in an educational institution, an apartment, a single-family house, a residential building, and a commercial enterprise. The sample volumes ranged from 0.25-1.3 m",https://doi.org/10.1016/j.scitotenv.2020.143421,,No,
0000-0002-4893-1942,Sebastian Wolff,Hochschule RheinMain,Determination of the microplastics emission in the effluent of a municipal waste water treatment plant using Raman microspectroscopy.,1970,"Samples from the secondary clarifier effluent of a waste water treatment plant (serving 98500 inhabitants) were analyzed to determine the microplastics (MP) emission. The samples were collected using a stainless steel centrifugal pump and filtered through a 10 μm stainless steel cartridge filter. Microplastics particles (MPPs) and microplastics fibers (MPFs) were recovered by chemical and physical sample purification. To remove natural organic matter, the samples were first subjected to oxidative treatment with H",https://doi.org/10.1016/j.wroa.2018.100014,,No,
0000-0001-6197-9847,Victoria Klemm,RheinMain University of Applied Sciences,Application and Evaluation of a Multimodal Training on the Second Victim Phenomenon at the European Researchers' Network Working on Second Victims Training School: Mixed Methods Study.,1970,"Health care workers (HCWs) are often impacted by distressing situations during patient care and can experience the second victim phenomenon (SVP). Addressing an adequate response, training, and increasing awareness of the SVP can increase HCWs' well-being and ultimately improve the quality of care and patient safety.",https://doi.org/10.2196/58727,,No,
0000-0001-6197-9847,Victoria Klemm,RheinMain University of Applied Sciences,Applicability and Validity of Second Victim Assessment Instruments among General Practitioners and Healthcare Assistants (SEVID-IX Study).,1970,"The second victim phenomenon and moral injury are acknowledged entities of psychological harm for healthcare providers. Both pose risks to patients, healthcare workers, and medical institutions, leading to further adverse events, economic burden, and dysfunctionality. Preceding studies in Germany and Austria showed a prevalence of second victim phenomena exceeding 53 percent among physicians, nurses, emergency physicians, and pediatricians. Using two German instruments for assessing moral injury and second victim phenomena, this study aimed to evaluate their feasibility for general practitioners and healthcare assistants.",https://doi.org/10.3390/healthcare12030351,,No,
0000-0001-6197-9847,Victoria Klemm,RheinMain University of Applied Sciences,Second Victims among Austrian Pediatricians (SeViD-A1 Study).,1970,"(1) Background: The second victim phenomenon (SVP) plays a critical role in workplace and patient safety. So far, there are limited epidemiological data on the SVP in German-speaking countries. Some studies have been carried out in Germany, but so far, no quantitative studies have been carried out in Austria examining the prevalence, symptom load and preferred support measures for second victims (SVs). This study therefore examines the SVP among Austrian pediatricians. (2) Methods: A nationwide, cross-sectional and anonymous online study was conducted using the SeViD questionnaire (Second Victims in Deutschland) including the Big Five Inventory-10 (BFI-10). Statistical analysis included binary-logistic and multiple linear regression with the bootstrapping, bias-corrected and accelerated (BCa) method based on 1000 bootstrap samples. (3) Results: Of 414 Austrian pediatricians, 89% self-identified as SVs. The main cause of becoming an SV was the unexpected death or suicide of a patient. High neuroticism and extraversion values as well as working in outpatient care positively correlated with having experienced the SVP. A preferred support strategy was access to legal counseling. (4) Conclusions: Austrian pediatricians have the highest SVP prevalence measured with the SeViD questionnaire. Further research should focus on prevention strategies and intervention programs.",https://doi.org/10.3390/healthcare11182501,,No,
0000-0001-6197-9847,Victoria Klemm,RheinMain University of Applied Sciences,Second Victims among German Emergency Medical Services Physicians (SeViD-III-Study).,1970,"Patient care in the prehospital emergency setting is error-prone. Wu's publications on the second victim syndrome made very clear that medical errors may lead to severe emotional injury on the caregiver's part. So far, little is known about the extent of the problem within the field of prehospital emergency care. Our study aimed at identifying the prevalence of the Second Victim Phenomenon among Emergency Medical Services (EMS) physicians in Germany.",https://doi.org/10.3390/ijerph20054267,,No,
0000-0001-6197-9847,Victoria Klemm,RheinMain University of Applied Sciences,Second Victims in Intensive Care-Emotional Stress and Traumatization of Intensive Care Nurses in Western Austria after Adverse Events during the Treatment of Patients.,1970,"The second victim phenomenon is common among nurses in intensive care units. Apart from quantitative studies, little is known about individual cases among those high-risk groups. This study evaluates the natural history and cause of second victim traumatization in Western Austria for the first time to tailor specific intervention.",https://doi.org/10.3390/ijerph19063611,,No,
0000-0002-5097-9064,Thomas Neusius,RheinMain University of Applied Sciences,Estimated Annual Healthcare Costs After Acute Pulmonary Embolism: Results From a Prospective Multicentre Cohort Study.,1970,"Patients surviving acute pulmonary embolism (PE) necessitate long-term treatment and follow-up. However, the chronic economic impact of PE on European healthcare systems remains to be determined.",https://doi.org/10.1093/ehjqcco/qcae050,,No,
0000-0002-5097-9064,Thomas Neusius,RheinMain University of Applied Sciences,Drivers and recent trends of hospitalisation costs related to acute pulmonary embolism.,1970,The socio-economic burden imposed by acute pulmonary embolism (PE) on European healthcare systems is largely unknown. We sought to determine temporal trends and identify cost drivers of hospitalisation for PE in Germany.,https://doi.org/10.1007/s00392-024-02437-y,,No,
0000-0002-5097-9064,Thomas Neusius,RheinMain University of Applied Sciences,Modelling costs of interventional pulmonary embolism treatment: implications of US trends for a European healthcare system.,1970,"Catheter-directed treatment (CDT) of acute pulmonary embolism (PE) is entering a growth phase in Europe following a steady increase in the USA in the past decade, but the potential economic impact on European healthcare systems remains unknown.",https://doi.org/10.1093/ehjacc/zuae019,,Yes,potent(1)
0000-0001-7165-0041,Linda Rau,RheinMain University of Applied Sciences,Virtual reality content creation based on self-contained components in the e-learning domain: Re-using pattern-based vr content in different authoring toolkits.,1970,"In the context of e-learning, it is challenging to incorporate emerging technologies, such as alternate reality games or Virtual Reality (VR), within current learning trends. Microlearning is such a current trend. It divides large and complex chunks of content into small and elementary learning nuggets. These single self-contained nuggets are then composed to overarching lessons or courses. The concept of VR nuggets dovetails this educational trend. VR nuggets are standalone, self-contained, and rather short VR experiences that can be combined with other learning nuggets. By using initial implementations of VR nuggets, they can be used to let authors create VR earning content, for example, to let learners experience alternate realities. In this paper, we further investigate the VR nugget authoring concept and extent it. We introduce two novel authoring toolkits that rely on VR nuggets - one based on context-related module interaction (",https://doi.org/10.1007/s11042-022-13362-5,,No,
0000-0003-3548-0537,Konstantin Schall,HTW Berlin,Interactive video retrieval evaluation at a distance: comparing sixteen interactive video search systems in a remote setting at the 10th Video Browser Showdown.,1970,"The Video Browser Showdown addresses difficult video search challenges through an annual interactive evaluation campaign attracting research teams focusing on interactive video retrieval. The campaign aims to provide insights into the performance of participating interactive video retrieval systems, tested by selected search tasks on large video collections. For the first time in its ten year history, the Video Browser Showdown 2021 was organized in a fully remote setting and hosted a record number of sixteen scoring systems. In this paper, we describe the competition setting, tasks and results and give an overview of state-of-the-art methods used by the competing systems. By looking at query result logs provided by ten systems, we analyze differences in retrieval model performances and browsing times before a correct submission. Through advances in data gathering methodology and tools, we provide a comprehensive analysis of ad-hoc video search tasks, discuss results, task design and methodological challenges. We highlight that almost all top performing systems utilize some sort of joint embedding for text-image retrieval and enable specification of temporal context in queries for known-item search. Whereas a combination of these techniques drive the currently top performing systems, we identify several future challenges for interactive video search engines and the Video Browser Showdown competition itself.",https://doi.org/10.1007/s13735-021-00225-2,,No,
0000-0002-2341-9790,Michael Knop,Deggendorf Institute of Technology,Weaning-associated interventions for ventilated intensive care patients: A scoping review.,1970,"Mechanical ventilation is a core intervention in critical care, but may also lead to negative consequences. Therefore, ventilator weaning is crucial for patient recovery. Numerous weaning interventions have been investigated, but an overview of interventions to evaluate different foci on weaning research is still missing.",https://doi.org/10.1111/nicc.13143,,No,
0000-0002-2341-9790,Michael Knop,Deggendorf Institute of Technology,The impact of digital technology use on nurses' professional identity and relations of power: a literature review.,1970,This study seeks to review how the use of digital technologies in clinical nursing affects nurses' professional identity and the relations of power within clinical environments.,https://doi.org/10.1111/jan.16178,,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute of Technology,"Loss of miR-101-3p in melanoma stabilizes genomic integrity, leading to cell death prevention.",1970,"Malignant melanoma remains the most lethal form of skin cancer, exhibiting poor prognosis after forming distant metastasis. Owing to their potential tumor-suppressive properties by regulating oncogenes and tumor suppressor genes, microRNAs are important player in melanoma development and progression. We defined the loss of miR-101-3p expression in melanoma cells compared with melanocytes and melanoblast-related cells as an early event in tumor development and aimed to understand the tumor suppressive role of miR-101-3p and its regulation of important cellular processes. Reexpression of miR-101-3p resulted in inhibition of proliferation, increase in DNA damage, and induction of apoptosis. We further determined the nuclear structure protein Lamin B1, which influences nuclear processes and heterochromatin structure, ATRX, CASP3, and PARP as an important direct target of miR-101-3p. RNA sequencing and differential gene expression analysis after miR-101-3p reexpression supported our findings and the importance of loss of mir-101-3p for melanoma progression. The validated functional effects are related to genomic instability, as recent studies suggest miRNAs plays a key role in mediating this cellular process. Therefore, we concluded that miR-101-3p reexpression increases the genomic instability, leading to irreversible DNA damage, which leads to apoptosis induction. Our findings suggest that the loss of miR-101-3p in melanoma serves as an early event in melanoma progression by influencing the genomic integrity to maintain the increased bioenergetic demand.",https://doi.org/10.1186/s11658-024-00552-2,,Yes,potent(1)
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute of Technology,Alternative Wnt-signaling axis leads to a break of oncogene-induced senescence.,1970,"Oncogene-induced senescence (OIS) is an important process that suppresses tumor development, but the molecular mechanisms of OIS are still under investigation. It is known that BRAF",https://doi.org/10.1038/s41419-024-06550-8,,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute of Technology,Sox9 regulates melanocytic fate decision of adult hair follicle stem cells.,1970,The bulge of hair follicles harbors Nestin,https://doi.org/10.1016/j.isci.2023.106919,,No,
0000-0002-9399-7078,Stefan Fischer,Deggendorf Institute of Technology,Two novel CreER,1970,"The skin of adult mammals protects from radiation, physical and chemical insults. While melanocytes and melanocyte-producing stem cells contribute to proper skin function in healthy organisms, dysfunction of these cells can lead to the generation of malignant melanoma-the deadliest type of skin cancer. Addressing cells of the melanocyte lineage in vivo represents a prerequisite for the understanding of melanoma on cellular level and the development of preventive and treatment strategies. Here, the inducible Cre-loxP-system has emerged as a promising tool to specifically target, monitor, and modulate cells in adult mice. Re-analysis of existing sequencing data sets of melanocytic cells revealed that genes with a known function in neural cells, including neural stem cells (Aldh1L1 and Nestin), are also expressed in melanocytic cells. Therefore, in this study, we explored whether the promoter activity of Nestin and Aldh1L1 can serve to target cells of the melanocyte lineage using the inducible CreER",https://doi.org/10.1111/pcmr.13061,,No,
0009-0005-6904-0208,Zubeir  El Ahmad,Deggendorf Institute of Technology,Transcription factor activating enhancer-binding protein 2ε (AP2ε) modulates phenotypic plasticity and progression of malignant melanoma.,1970,"Malignant melanoma, the most aggressive form of skin cancer, is often incurable once metastatic dissemination of cancer cells to distant organs has occurred. We investigated the role of Transcription Factor Activating Enhancer-Binding Protein 2ε (AP2ε) in the progression of metastatic melanoma. Here, we observed that AP2ε is a potent activator of metastasis and newly revealed AP2ε to be an important player in melanoma plasticity. High levels of AP2ε lead to worsened prognosis of melanoma patients. Using a transgenic melanoma mouse model with a specific loss of AP2ε expression, we confirmed the impact of AP2ε to modulate the dynamic switch from a migratory to a proliferative phenotype. AP2ε deficient melanoma cells show a severely reduced migratory potential in vitro and reduced metastatic behavior in vivo. Consistently, we revealed increased activity of AP2ε in quiescent and migratory cells compared to heterogeneously proliferating cells in bioprinted 3D models. In conclusion, these findings disclose a yet-unknown role of AP2ε in maintaining plasticity and migration in malignant melanoma cells.",https://doi.org/10.1038/s41419-024-06733-3,,Yes,potent(2)
0000-0002-1163-1399,Florian Wahl,Deggendorf Institute of Technology,"Nurses' perceptions, experience and knowledge regarding artificial intelligence: results from a cross-sectional online survey in Germany.",1970,"Nursing faces increasing pressure due to changing demographics and a shortage of skilled workers. Artificial intelligence (AI) offers an opportunity to relieve nurses and reduce pressure. The perception of AI by nurses is crucial for successful implementation. Due to a limited research state, our study aims to investigate nurses' knowledge and perceptions of AI.",https://doi.org/10.1186/s12912-024-01884-2,,No,
0000-0002-1163-1399,Florian Wahl,Deggendorf Institute of Technology,"Potential of Assistive Robots in Clinical Nursing: An Observational Study of Nurses' Transportation Tasks in Rural Clinics of Bavaria, Germany.",1970,"Transportation tasks in nursing are common, often overlooked, and directly impact patient care time in the context of staff shortages and an aging society. Current studies lack a specific focus on transportation tasks, a gap our research aims to fill. By providing detailed data on transportation needs in nursing, our study establishes a crucial foundation for the development and integration of assistive robots in clinical settings. In July and September 2023, we conducted weekly observations of nurses to assess clinical transportation needs. We aim to understand the economic impact and the methods nurses use for transportation tasks. We conducted a participant observation using a standardized app-based form over a seven-day observation period in two rural clinics. N = 1830 transports were made by nurses and examined by descriptive analysis. Non-medical supplies account for 27.05% (n = 495) of all transports, followed by medical supplies at 17.32% (n = 317), pharmacotherapy at 14.10% (n = 258) and other other categories like meals or drinks contributing 12.68% (n = 232). Most transports had a factual transport time of under a minute, with patient transport and lab samples displaying more variability. In total, 77.15% of all transports were made by hand. Requirements to collect items or connect transports with patient care were included in 5% of all transports. Our economic evaluation highlighted meals as the most costly transport, with 9596.16 € per year in the observed clinics. Budget-friendly robots would amortize these costs over one year by transporting meals. We support understanding nurses' transportation needs via further research on assistive robots to validate our findings and determine the feasibility of transport robots.",https://doi.org/10.3390/nursrep14010021,,No,
0000-0002-1163-1399,Florian Wahl,Deggendorf Institute of Technology,Nurses' Workplace Perceptions in Southern Germany-Job Satisfaction and Self-Intended Retention towards Nursing.,1970,"Our cross-sectional study, conducted from October 2022 to January 2023, aims to assess post-COVID job satisfaction, crucial work dimensions, and self-reported factors influencing nursing retention. Using an online survey, we surveyed 2572 nurses in different working fields in Bavaria, Germany. We employed a quantitative analysis, including a multivariable regression, to assess key influence factors on nursing retention. In addition, we evaluated open-ended questions via a template analysis to use in a joint display. In the status quo, 43.2% of nurses were not committed to staying in the profession over the next 12 months. A total of 66.7% of our surveyed nurses were found to be dissatisfied with the (i) time for direct patient care. Sources of dissatisfaction above 50% include (ii) service organization, (iii) documentation, (iv) codetermination, and (v) payment. The qualitative data underline necessary improvements in these areas. Regarding retention factors, we identified that nurses with (i) older age, (ii) living alone, (iii) not working in elder care, (iv) satisfactory working hours, (v) satisfactory career choice, (vi) career opportunities, (vii) satisfactory payment, and (viii) adequate working and rest times are more likely to remain in the profession. Conversely, dissatisfaction in (ix) supporting people makes nurses more likely to leave their profession and show emotional constraints. We uncovered a dichotomy where nurses have strong empathy for their profession but yearn for improvements due to unmet expectations. Policy implications should include measures for younger nurses and those in elderly care. Nevertheless, there is a need for further research, because our research is limited by potential bias from convenience sampling, and digitalization will soon show up as a potential solution to improve, e.g., documentation and enhanced time for direct patient time.",https://doi.org/10.3390/healthcare12020172,,Yes,potent(2)
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Splicing control by PHF5A is crucial for melanoma cell survival.,1970,"Abnormalities in alternative splicing are a hallmark of cancer formation. In this study, we investigated the role of the splicing factor PHD finger protein 5A (PHF5A) in melanoma. Malignant melanoma is the deadliest form of skin cancer, and patients with a high PHF5A expression show poor overall survival. Our data revealed that an siRNA-mediated downregulation of PHF5A in different melanoma cell lines leads to massive splicing defects of different tumour-relevant genes. The loss of PHF5A results in an increased rate of apoptosis by triggering Fas- and unfolded protein response (UPR)-mediated apoptosis pathways in melanoma cells. These findings are tumour-specific because we did not observe this regulation in fibroblasts. Our study identifies a crucial role of PHF5A as driver for melanoma malignancy and the described underlying splicing network provides an interesting basis for the development of new therapeutic targets for this aggressive form of skin cancer.",https://doi.org/10.1111/cpr.13741,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Transcription factor activating enhancer-binding protein 2ε (AP2ε) modulates phenotypic plasticity and progression of malignant melanoma.,1970,"Malignant melanoma, the most aggressive form of skin cancer, is often incurable once metastatic dissemination of cancer cells to distant organs has occurred. We investigated the role of Transcription Factor Activating Enhancer-Binding Protein 2ε (AP2ε) in the progression of metastatic melanoma. Here, we observed that AP2ε is a potent activator of metastasis and newly revealed AP2ε to be an important player in melanoma plasticity. High levels of AP2ε lead to worsened prognosis of melanoma patients. Using a transgenic melanoma mouse model with a specific loss of AP2ε expression, we confirmed the impact of AP2ε to modulate the dynamic switch from a migratory to a proliferative phenotype. AP2ε deficient melanoma cells show a severely reduced migratory potential in vitro and reduced metastatic behavior in vivo. Consistently, we revealed increased activity of AP2ε in quiescent and migratory cells compared to heterogeneously proliferating cells in bioprinted 3D models. In conclusion, these findings disclose a yet-unknown role of AP2ε in maintaining plasticity and migration in malignant melanoma cells.",https://doi.org/10.1038/s41419-024-06733-3,,Yes,potent(2)
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,"Loss of miR-101-3p in melanoma stabilizes genomic integrity, leading to cell death prevention.",1970,"Malignant melanoma remains the most lethal form of skin cancer, exhibiting poor prognosis after forming distant metastasis. Owing to their potential tumor-suppressive properties by regulating oncogenes and tumor suppressor genes, microRNAs are important player in melanoma development and progression. We defined the loss of miR-101-3p expression in melanoma cells compared with melanocytes and melanoblast-related cells as an early event in tumor development and aimed to understand the tumor suppressive role of miR-101-3p and its regulation of important cellular processes. Reexpression of miR-101-3p resulted in inhibition of proliferation, increase in DNA damage, and induction of apoptosis. We further determined the nuclear structure protein Lamin B1, which influences nuclear processes and heterochromatin structure, ATRX, CASP3, and PARP as an important direct target of miR-101-3p. RNA sequencing and differential gene expression analysis after miR-101-3p reexpression supported our findings and the importance of loss of mir-101-3p for melanoma progression. The validated functional effects are related to genomic instability, as recent studies suggest miRNAs plays a key role in mediating this cellular process. Therefore, we concluded that miR-101-3p reexpression increases the genomic instability, leading to irreversible DNA damage, which leads to apoptosis induction. Our findings suggest that the loss of miR-101-3p in melanoma serves as an early event in melanoma progression by influencing the genomic integrity to maintain the increased bioenergetic demand.",https://doi.org/10.1186/s11658-024-00552-2,,Yes,potent(1)
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Alternative Wnt-signaling axis leads to a break of oncogene-induced senescence.,1970,"Oncogene-induced senescence (OIS) is an important process that suppresses tumor development, but the molecular mechanisms of OIS are still under investigation. It is known that BRAF",https://doi.org/10.1038/s41419-024-06550-8,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Two novel CreER,1970,"The skin of adult mammals protects from radiation, physical and chemical insults. While melanocytes and melanocyte-producing stem cells contribute to proper skin function in healthy organisms, dysfunction of these cells can lead to the generation of malignant melanoma-the deadliest type of skin cancer. Addressing cells of the melanocyte lineage in vivo represents a prerequisite for the understanding of melanoma on cellular level and the development of preventive and treatment strategies. Here, the inducible Cre-loxP-system has emerged as a promising tool to specifically target, monitor, and modulate cells in adult mice. Re-analysis of existing sequencing data sets of melanocytic cells revealed that genes with a known function in neural cells, including neural stem cells (Aldh1L1 and Nestin), are also expressed in melanocytic cells. Therefore, in this study, we explored whether the promoter activity of Nestin and Aldh1L1 can serve to target cells of the melanocyte lineage using the inducible CreER",https://doi.org/10.1111/pcmr.13061,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Knockdown of Lamin B1 and the Corresponding Lamin B Receptor Leads to Changes in Heterochromatin State and Senescence Induction in Malignant Melanoma.,1970,"Modifications in nuclear structures of cells are implicated in several diseases including cancer. They result in changes in nuclear activity, structural dynamics and cell signalling. However, the role of the nuclear lamina and related proteins in malignant melanoma is still unknown. Its molecular characterisation might lead to a deeper understanding and the development of new therapy approaches. In this study, we analysed the functional effects of dysregulated nuclear lamin B1 (LMNB1) and its nuclear receptor (LBR). According to their cellular localisation and function, we revealed that these genes are crucially involved in nuclear processes like chromatin organisation. RNA sequencing and differential gene expression analysis after knockdown of LMNB1 and LBR revealed their implication in important cellular processes driving ER stress leading to senescence and changes in chromatin state, which were also experimentally validated. We determined that melanoma cells need both molecules independently to prevent senescence. Hence, downregulation of both molecules in a BRAF",https://doi.org/10.3390/cells11142154,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Impact of ,1970,The tumor suppressive role of CYLD lysine 63 deubiquitinase (,https://doi.org/10.3892/ijmm.2022.5122,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,HDAC2 Is Involved in the Regulation of BRN3A in Melanocytes and Melanoma.,1970,"The neural crest transcription factor BRN3A is essential for the proliferation and survival of melanoma cells. It is frequently expressed in melanoma but not in normal melanocytes or benign nevi. The mechanisms underlying the aberrant expression of BRN3A are unknown. Here, we investigated the epigenetic regulation of ",https://doi.org/10.3390/ijms23020849,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,"Loss of Gene Information: Discrepancies between RNA Sequencing, cDNA Microarray, and qRT-PCR.",1970,"Molecular analyses of normal and diseased cells give insight into changes in gene expression and help in understanding the background of pathophysiological processes. Years after cDNA microarrays were established in research, RNA sequencing (RNA-seq) became a key method of quantitatively measuring the transcriptome. In this study, we compared the detection of genes by each of the transcriptome analysis methods: cDNA array, quantitative RT-PCR, and RNA-seq. As expected, we found differences in the gene expression profiles of the aforementioned techniques. Here, we present selected genes that exemplarily demonstrate the observed differences and calculations to reveal that a strong RNA secondary structure, as well as sample preparation, can affect RNA-seq. In summary, this study addresses an important issue with a strong impact on gene expression analysis in general. Therefore, we suggest that these findings need to be considered when dealing with data from transcriptome analyses.",https://doi.org/10.3390/ijms22179349,,No,
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Molecular Changes Induced in Melanoma by Cell Culturing in 3D Alginate Hydrogels.,1970,"Alginate hydrogels have been used as a biomaterial for 3D culturing for several years. Here, gene expression patterns in melanoma cells cultivated in 3D alginate are compared to 2D cultures. It is well-known that 2D cell culture is not resembling the complex in vivo situation well. However, the use of very intricate 3D models does not allow performing high-throughput screening and analysis is highly complex. 3D cell culture strategies in hydrogels will better mimic the in vivo situation while they maintain feasibility for large-scale analysis. As alginate is an easy-to-use material and due to its favorable properties, it is commonly applied as a bioink component in the growing field of cell encapsulation and biofabrication. Yet, only a little information about the transcriptome in 3D cultures in hydrogels like alginate is available. In this study, changes in the transcriptome based on RNA-Seq data by cultivating melanoma cells in 3D alginate are analyzed and reveal marked changes compared to cells cultured on usual 2D tissue culture plastic. Deregulated genes represent valuable cues to signaling pathways and molecules affected by the culture method. Using this as a model system for tumor cell plasticity and heterogeneity, EGR1 is determined to play an important role in melanoma progression.",https://doi.org/10.3390/cancers13164111,,Yes,intricate(1)
0000-0003-0312-1687,Melanie  Kappelmann-Fenzl,Deggendorf Institute of Technology,Role of melanoma inhibitory activity in melanocyte senescence.,1970,"The protein melanoma inhibitory activity (MIA) is known to be expressed in melanoma and to support melanoma progression. Interestingly, previous studies also observed the expression of MIA in nevi. Concentrating on these findings, we revealed that MIA expression is correlated with a senescent state in melanocytes. Induction of replicative or oncogene-induced senescence resulted in increased MIA expression in vitro. Notably, MIA knockdown in senescent melanocytes reduced the percentage of senescence-associated beta-Gal-positive cells and enhanced proliferation. Using the melanoma mouse model Tg(Grm1), MIA-deficient mice supported the impact of MIA on senescence by showing a significantly earlier tumor onset compared to controls. In melanocytes, MIA knockdown led to a downregulation of the cell cycle inhibitor p21 in vitro and in vivo. In contrast, after induction of hTERT in human melanoma cells, p21 regulation by MIA was lost. In summary, our data show for the first time that MIA is a regulator of cellular senescence in human and murine melanocytes.",https://doi.org/10.1111/pcmr.12801,,No,
0000-0001-6412-7666,Markus Pillmayer,Munich University of Applied Sciences,"Geography Matters, But… Evolving Success Factors for Nature-Oriented Health Tourism within Selected Alpine Destinations.",1970,"This paper analyzes the success factors of health tourism based on natural attractions in selected European spa and health destinations. The natural resources included in the offers, such as water, salt, and air, play a central role in this context, as their evidence-based effects have a high relevance for the health and wellbeing of tourists. Due to its specific geographical location and considering the threat of climate change, however, this offer is facing increasing challenges which make adaptation strategies necessary. In addition to a conceptional introduction to the topic, this paper contains a descriptive analysis of tourism statistics and the results from self-administered questionnaires with six selected representatives from alpine health destinations (DE, FR, IT, AT, CH, SI). The results show varying forms of health tourism based on natural attractions, which are also reflected in online marketing, with potential for optimization. The web research and the responses to the questionnaire revealed that evidence-based studies hardly play a role in promoting health touristic offers. Furthermore, climate change effects on natural attractions are considered extremely small and tend to prompt the development of new offers. Health destinations are advised to generate a clearer focus on the risks of climate change regarding natural resources.",https://doi.org/10.3390/ijerph18105389,,Yes,potent(1)
0000-0003-1676-1376,Patrick Hanisch,Munich University of Applied Sciences,Squalene production under oxygen limitation by Schizochytrium sp. S31 in different cultivation systems.,1970,"The triterpene squalene is widely used in the food, cosmetics and pharmaceutical industries due to its antioxidant, antistatic and anti-carcinogenic properties. It is usually obtained from the liver of deep sea sharks, which are facing extinction. Alternative production organisms are marine protists from the family Thraustochytriaceae, which produce and store large quantities of various lipids. Squalene accumulation in thraustochytrids is complex, as it is an intermediate in sterol biosynthesis. Its conversion to squalene 2,3-epoxide is the first step in sterol synthesis and is heavily oxygen dependent. Hence, the oxygen supply during cultivation was investigated in our study. In shake flask cultivations, a reduced oxygen supply led to increased squalene and decreased sterol contents and yields. Oxygen-limited conditions were applied to bioreactor scale, where squalene accumulation and growth of Schizochytrium sp. S31 was determined in batch, fed-batch and continuous cultivation. The highest dry matter (32.03 g/L) was obtained during fed-batch cultivation, whereas batch cultivation yielded the highest biomass productivity (0.2 g/L*h",https://doi.org/10.1007/s00253-024-13051-3,,No,
0000-0002-2430-1655,Markus Lindner,Munich University of Applied Sciences,Fiber Bragg Sensors Embedded in Cast Aluminum Parts: Axial Strain and Temperature Response.,1970,"In this study, the response of fiber Bragg gratings (FBGs) embedded in cast aluminum parts under thermal and mechanical load were investigated. Several types of FBGs in different types of fibers were used in order to verify general applicability. To monitor a temperature-induced strain, an embedded regenerated FBG (RFBG) in a cast part was placed in a climatic chamber and heated up to 120 ∘C within several cycles. The results show good agreement with a theoretical model, which consists of a shrink-fit model and temperature-dependent material parameters. Several cast parts with different types of FBGs were machined into tensile test specimens and tensile tests were executed. For the tensile tests, a cyclic procedure was chosen, which allowed us to distinguish between the elastic and plastic deformation of the specimen. An analytical model, which described the elastic part of the tensile test, was introduced and showed good agreement with the measurements. Embedded FBGs - integrated during the casting process - showed under all mechanical and thermal load conditions no hysteresis, a reproducible sensor response, and a high reliable operation, which is very important to create metallic smart structures and packaged fiber optic sensors for harsh environments.",https://doi.org/10.3390/s21051680,,No,
0000-0002-2430-1655,Markus Lindner,Munich University of Applied Sciences,Strain Measurement in Aluminium Alloy during the Solidification Process Using Embedded Fibre Bragg Gratings.,1970,"In recent years, the observation of the behaviour of components during the production process and over their life cycle is of increasing importance. Structural health monitoring, for example of carbon composites, is state-of-the-art research. The usage of Fibre Bragg Gratings (FBGs) in this field is of major advantage. Another possible area of application is in foundries. The internal state of melts during the solidification process is of particular interest. By using embedded FBGs, temperature and stress can be monitored during the process. In this work, FBGs were embedded in aluminium alloys in order to observe the occurring strain. Two different FBG positions were chosen in the mould in order to compare its dependence. It was shown that FBGs can withstand the solidification process, although a compression in the range of one percent was measured, which is in agreement with the literature value. Furthermore, different lengths of the gratings were applied, and it was shown that shorter gratings result in more accurate measurements. The obtained results prove that FBGs are applicable as sensors for temperatures up to 740 °C.",https://doi.org/10.3390/s16111853,,No,
0000-0002-2132-6267,Herbert  Plischke,Munich University of Applied Sciences,Spectral dependency of the human pupillary light reflex. Influences of pre-adaptation and chronotype.,1970,"Non-visual photoreceptors (ipRGCs) and rods both exert a strong influence on the human pupil, yet pupil models regularly use cone-derived sensitivity as their basis. This inconsistency is further exacerbated by the fact that circadian effects can modulate the wavelength sensitivity. We assessed the pupillary reaction to narrowband light stimuli in the mesopic range. Pupil size for eighty-three healthy participants with normal color vision was measured in nine experimental protocols with varying series of continuous or discontinuous light stimuli under Ganzfeld conditions, presented after 90 seconds of dark adaptation. One hundred and fifty series of stimulation were conducted across three experiments, and were analyzed for wavelength-dependency on the normalized pupillary constriction (nPC), conditional on experimental settings and individual traits. Traits were surveyed by questionnaire; color vision was tested by Ishihara plates or the Lanthony D15 test. Data were analyzed with generalized additive mixed models (GAMM). The normalized pupillary constriction response is consistent with L+M-cone derived sensitivity when the series of light stimuli is continuous, i.e., is not interrupted by periods of darkness, but not otherwise. The results also show that a mesopic illuminance weighing led to an overall best prediction of pupillary constriction compared to other types of illuminance measures. IpRGC influence on nPC is not readily apparent from the results. When we explored the interaction of chronotype and time of day on the wavelength dependency, differences consistent with ipRGC influence became apparent. The models indicate that subjects of differing chronotype show a heightened or lowered sensitivity to short wavelengths, depending on their time of preference. IpRGC influence is also seen in the post-illumination pupil reflex if the prior light-stimulus duration is one second. However, shorter wavelengths than expected become more important if the light-stimulus duration is fifteen or thirty seconds. The influence of sex on nPC was present, but showed no interaction with wavelength. Our results help to define the conditions, under which the different wavelength sensitivities in the literature hold up for narrowband light settings. The chronotype effect might signify a mechanism for strengthening the individual´s chronotype. It could also be the result of the participant's prior exposure to light (light history). Our explorative findings for this effect demand replication in a controlled study.",https://doi.org/10.1371/journal.pone.0253030,,No,
0000-0002-2132-6267,Herbert  Plischke,Munich University of Applied Sciences,Influence of common lighting conditions and time-of-day on the effort-related cardiac response.,1970,"Melanopic stimuli trigger diverse non-image-forming effects. However, evidence of a melanopic contribution to acute effects on alertness and performance is inconclusive, especially under common lighting situations. Effects on cognitive performance are likely mediated by effort-related physiological changes. We assessed the acute effects of lighting in three scenarios, at two times of day, on effort-related changes to cardiac contraction as indexed by the cardiac pre-ejection period (PEP). In a within-subject design, twenty-seven participants performed a cognitive task thrice during a morning and a late-afternoon session. We set the lighting at 500 lux in all three lighting scenarios, measured horizontally at the desk level, but with 54 lux, 128 lux, or 241 lux melanopic equivalent daylight illuminance at the eye level. Impedance cardiography and electrocardiography measurements were used to calculate PEP, for the baseline and task period. A shorter PEP during the task represents a sympathetic heart activation and therefore increased effort. Data were analysed with linear mixed-effect models. PEP changes depended on both the light scene and time of day (p = 0.01 and p = 0.002, respectively). The highest change (sympathetic activation) occurred for the medium one of the three stimuli (128 lux) during the late-afternoon session. However, effect sizes for the singular effects were small, and only for the combined effect of light and time of day middle-sized. Performance scores or self-reported scores on alertness and task demand did not change with the light scene. In conclusion, participants reached the same performance most efficiently at both the highest and lowest melanopic setting, and during the morning session. The resulting U-shaped relation between melanopic stimulus intensity and PEP is likely not dependent solely on intrinsic ipRGC stimuli, and might be moderated by extrinsic cone input. Since lighting situations were modelled according to current integrative lighting strategies and real-life indoor light intensities, the result has implications for artificial lighting in a work environment.",https://doi.org/10.1371/journal.pone.0239553,,No,
0000-0002-2132-6267,Herbert  Plischke,Munich University of Applied Sciences,Pupillary light reflex and circadian synchronization in the elderly.,1970,"Most elderly lack a synchronized circadian rhythm and often cannot benefit from light therapy. Non-visual effects of light are mediated through intrinsic photosensitive retinal ganglion cells (ipRGCs). With chromatic pupillometry, the functionality of ipRGCs and their functional circuit in the brain can be tested.",https://doi.org/10.1002/pchj.186,,No,
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,Profiling native pulmonary basement membrane stiffness using atomic force microscopy.,1970,"Mammalian cells sense and react to the mechanics of their immediate microenvironment. Therefore, the characterization of the biomechanical properties of tissues with high spatial resolution provides valuable insights into a broad variety of developmental, homeostatic and pathological processes within living organisms. The biomechanical properties of the basement membrane (BM), an extracellular matrix (ECM) substructure measuring only ∼100-400 nm across, are, among other things, pivotal to tumor progression and metastasis formation. Although the precise assignment of the Young's modulus E of such a thin ECM substructure especially in between two cell layers is still challenging, biomechanical data of the BM can provide information of eminent diagnostic potential. Here we present a detailed protocol to quantify the elastic modulus of the BM in murine and human lung tissue, which is one of the major organs prone to metastasis. This protocol describes a streamlined workflow to determine the Young's modulus E of the BM between the endothelial and epithelial cell layers shaping the alveolar wall in lung tissues using atomic force microscopy (AFM). Our step-by-step protocol provides instructions for murine and human lung tissue extraction, inflation of these tissues with cryogenic cutting medium, freezing and cryosectioning of the tissue samples, and AFM force-map recording. In addition, it guides the reader through a semi-automatic data analysis procedure to identify the pulmonary BM and extract its Young's modulus E using an in-house tailored user-friendly AFM data analysis software, the Center for Applied Tissue Engineering and Regenerative Medicine processing toolbox, which enables automatic loading of the recorded force maps, conversion of the force versus piezo-extension curves to force versus indentation curves, calculation of Young's moduli and generation of Young's modulus maps, where the pulmonary BM can be identified using a semi-automatic spatial filtering tool. The entire protocol takes 1-2 d.",https://doi.org/10.1038/s41596-024-00955-7,,Yes,"pivotal(1), potent(1)"
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,Fibroblast-derived matrix models desmoplastic properties and forms a prognostic signature in cancer progression.,1970,"The desmoplastic reaction observed in many cancers is a hallmark of disease progression and prognosis, particularly in breast and pancreatic cancer. Stromal-derived extracellular matrix (ECM) is significantly altered in desmoplasia, and as such plays a critical role in driving cancer progression. Using fibroblast-derived matrices (FDMs), we show that cancer cells have increased growth on cancer associated FDMs, when compared to FDMs derived from non-malignant tissue (normal) fibroblasts. We assess the changes in ECM characteristics from normal to cancer-associated stroma at the primary tumor site. Compositional, structural, and mechanical analyses reveal significant differences, with an increase in abundance of core ECM proteins, coupled with an increase in stiffness and density in cancer-associated FDMs. From compositional changes of FDM, we derived a 36-ECM protein signature, which we show matches in large part with the changes in pancreatic ductal adenocarcinoma (PDAC) tumor and metastases progression. Additionally, this signature also matches at the transcriptomic level in multiple cancer types in patients, prognostic of their survival. Together, our results show relevance of FDMs for cancer modelling and identification of desmoplastic ECM components for further mechanistic studies.",https://doi.org/10.3389/fimmu.2023.1154528,,No,
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,ER Stress in ERp57 Knockout Knee Joint Chondrocytes Induces Osteoarthritic Cartilage Degradation and Osteophyte Formation.,1970,"Ageing or obesity are risk factors for protein aggregation in the endoplasmic reticulum (ER) of chondrocytes. This condition is called ER stress and leads to induction of the unfolded protein response (UPR), which, depending on the stress level, restores normal cell function or initiates apoptotic cell death. Here the role of ER stress in knee osteoarthritis (OA) was evaluated. It was first tested in vitro and in vivo whether a knockout (KO) of the protein disulfide isomerase ERp57 in chondrocytes induces sufficient ER stress for such analyses. ER stress in ERp57 KO chondrocytes was confirmed by immunofluorescence, immunohistochemistry, and transmission electron microscopy. Knee joints of wildtype (WT) and cartilage-specific ERp57 KO mice (ERp57 cKO) were analyzed by indentation-type atomic force microscopy (IT-AFM), toluidine blue, and immunofluorescence/-histochemical staining. Apoptotic cell death was investigated by a TUNEL assay. Additionally, OA was induced via forced exercise on a treadmill. ER stress in chondrocytes resulted in a reduced compressive stiffness of knee cartilage. With ER stress, 18-month-old mice developed osteoarthritic cartilage degeneration with osteophyte formation in knee joints. These degenerative changes were preceded by apoptotic death in articular chondrocytes. Young mice were not susceptible to OA, even when subjected to forced exercise. This study demonstrates that ER stress induces the development of age-related knee osteoarthritis owing to a decreased protective function of the UPR in chondrocytes with increasing age, while apoptosis increases. Therefore, inhibition of ER stress appears to be an attractive therapeutic target for OA.",https://doi.org/10.3390/ijms23010182,,No,
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,Mitochondrial respiratory chain function promotes extracellular matrix integrity in cartilage.,1970,"Energy metabolism and extracellular matrix (ECM) function together orchestrate and maintain tissue organization, but crosstalk between these processes is poorly understood. Here, we used single-cell RNA-Seq (scRNA-Seq) analysis to uncover the importance of the mitochondrial respiratory chain for ECM homeostasis in mature cartilage. This tissue produces large amounts of a specialized ECM to promote skeletal growth during development and maintain mobility throughout life. A combined approach of high-resolution scRNA-Seq, mass spectrometry/matrisome analysis, and atomic force microscopy was applied to mutant mice with cartilage-specific inactivation of respiratory chain function. This genetic inhibition in cartilage results in the expansion of a central area of 1-month-old mouse femur head cartilage, showing disorganized chondrocytes and increased deposition of ECM material. scRNA-Seq analysis identified a cell cluster-specific decrease in mitochondrial DNA-encoded respiratory chain genes and a unique regulation of ECM-related genes in nonarticular chondrocytes. These changes were associated with alterations in ECM composition, a shift in collagen/noncollagen protein content, and an increase of collagen crosslinking and ECM stiffness. These results demonstrate that mitochondrial respiratory chain dysfunction is a key factor that can promote ECM integrity and mechanostability in cartilage and presumably also in many other tissues.",https://doi.org/10.1016/j.jbc.2021.101224,,No,
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,Basement membrane stiffness determines metastases formation.,1970,"The basement membrane (BM) is a special type of extracellular matrix and presents the major barrier cancer cells have to overcome multiple times to form metastases. Here we show that BM stiffness is a major determinant of metastases formation in several tissues and identify netrin-4 (Net4) as a key regulator of BM stiffness. Mechanistically, our biophysical and functional analyses in combination with mathematical simulations show that Net4 softens the mechanical properties of native BMs by opening laminin node complexes, decreasing cancer cell potential to transmigrate this barrier despite creating bigger pores. Our results therefore reveal that BM stiffness is dominant over pore size, and that the mechanical properties of 'normal' BMs determine metastases formation and patient survival independent of cancer-mediated alterations. Thus, identifying individual Net4 protein levels within native BMs in major metastatic organs may have the potential to define patient survival even before tumour formation. The ratio of Net4 to laminin molecules determines BM stiffness, such that the more Net4, the softer the BM, thereby decreasing cancer cell invasion activity.",https://doi.org/10.1038/s41563-020-00894-0,,Yes,potent(2)
0000-0002-0234-5446,Lutz Fleischhauer,Munich University of Applied Sciences,Mice Lacking the Matrilin Family of Extracellular Matrix Proteins Develop Mild Skeletal Abnormalities and Are Susceptible to Age-Associated Osteoarthritis.,1970,"Matrilins (MATN1, MATN2, MATN3 and MATN4) are adaptor proteins of the cartilage extracellular matrix (ECM), which bridge the collagen II and proteoglycan networks. In humans, dominant-negative mutations in MATN3 lead to various forms of mild chondrodysplasias. However, single or double matrilin knockout mice generated previously in our laboratory do not show an overt skeletal phenotype, suggesting compensation among the matrilin family members. The aim of our study was to establish a mouse line, which lacks all four matrilins and analyze the consequence of matrilin deficiency on endochondral bone formation and cartilage function. ",https://doi.org/10.3390/ijms21020666,,No,
0000-0002-0258-5570,Markus Pietras,Munich University of Applied Sciences,Design and Operational Elements of the Robotic Subsystem for the e.deorbit Debris Removal Mission.,1970,"This paper presents a robotic capture concept that was developed as part of the e.deorbit study by ESA. The defective and tumbling satellite ENVISAT was chosen as a potential target to be captured, stabilized, and subsequently de-orbited in a controlled manner. A robotic capture concept was developed that is based on a chaser satellite equipped with a seven degrees-of-freedom dexterous robotic manipulator, holding a dedicated linear two-bracket gripper. The satellite is also equipped with a clamping mechanism for achieving a stiff fixation with the grasped target, following their combined satellite-stack de-tumbling and prior to the execution of the de-orbit maneuver. Driving elements of the robotic design, operations and control are described and analyzed. These include pre and post-capture operations, the task-specific kinematics of the manipulator, the intrinsic mechanical arm flexibility and its effect on the arm's positioning accuracy, visual tracking, as well as the interaction between the manipulator controller and that of the chaser satellite. The kinematics analysis yielded robust reachability of the grasp point. The effects of intrinsic arm flexibility turned out to be noticeable but also effectively scalable through robot joint speed adaption throughout the maneuvers. During most of the critical robot arm operations, the internal robot joint torques are shown to be within the design limits. These limits are only reached for a limiting scenario of tumbling motion of ENVISAT, consisting of an initial pure spin of 5 deg/s about its unstable intermediate axis of inertia. The computer vision performance was found to be satisfactory with respect to positioning accuracy requirements. Further developments are necessary and are being pursued to meet the stringent mission-related robustness requirements. Overall, the analyses conducted in this study showed that the capture and de-orbiting of ENVISAT using the proposed robotic concept is feasible with respect to relevant mission requirements and for most of the operational scenarios considered. Future work aims at developing a combined chaser-robot system controller. This will include a visual servo to minimize the positioning errors during the contact phases of the mission (grasping and clamping). Further validation of the visual tracking in orbital lighting conditions will be pursued.",https://doi.org/10.3389/frobt.2018.00100,,Yes,potent(1)
0000-0002-1572-842X,Marion Gödel,Munich University of Applied Sciences,Modelling airborne transmission of SARS-CoV-2 at a local scale.,1970,"The coronavirus disease (COVID-19) pandemic has changed our lives and still poses a challenge to science. Numerous studies have contributed to a better understanding of the pandemic. In particular, inhalation of aerosolised pathogens has been identified as essential for transmission. This information is crucial to slow the spread, but the individual likelihood of becoming infected in everyday situations remains uncertain. Mathematical models help estimate such risks. In this study, we propose how to model airborne transmission of SARS-CoV-2 at a local scale. In this regard, we combine microscopic crowd simulation with a new model for disease transmission. Inspired by compartmental models, we describe virtual persons as infectious or susceptible. Infectious persons exhale pathogens bound to persistent aerosols, whereas susceptible ones absorb pathogens when moving through an aerosol cloud left by the infectious person. The transmission depends on the pathogen load of the aerosol cloud, which changes over time. We propose a 'high risk' benchmark scenario to distinguish critical from non-critical situations. A parameter study of a queue shows that the new model is suitable to evaluate the risk of exposure qualitatively and, thus, enables scientists or decision-makers to better assess the spread of COVID-19 and similar diseases.",https://doi.org/10.1371/journal.pone.0273820,,No,
0000-0002-1572-842X,Marion Gödel,Munich University of Applied Sciences,Avoiding numerical pitfalls in social force models.,1970,"The social force model of Helbing and Molnár is one of the best known approaches to simulate pedestrian motion, a collective phenomenon with nonlinear dynamics. It is based on the idea that the Newtonian laws of motion mostly carry over to pedestrian motion so that human trajectories can be computed by solving a set of ordinary differential equations for velocity and acceleration. The beauty and simplicity of this ansatz are strong reasons for its wide spread. However, the numerical implementation is not without pitfalls. Oscillations, collisions, and instabilities occur even for very small step sizes. Classic solution ideas from molecular dynamics do not apply to the problem because the system is not Hamiltonian despite its source of inspiration. Looking at the model through the eyes of a mathematician, however, we realize that the right hand side of the differential equation is nondifferentiable and even discontinuous at critical locations. This produces undesirable behavior in the exact solution and, at best, severe loss of accuracy in efficient numerical schemes even in short range simulations. We suggest a very simple mollified version of the social force model that conserves the desired dynamic properties of the original many-body system but elegantly and cost efficiently resolves several of the issues concerning stability and numerical resolution.",https://doi.org/10.1103/PhysRevE.87.063305,,No,
0009-0008-3988-2836,Marcel Schmucker,Esslingen University of Applied Sciences,The usability of rollators as part of the human-centred quality of mobility devices: a systematic narrative literature review.,1970,,https://doi.org/10.1080/17483107.2024.2368651,,No,
0000-0002-0298-4723,Petra Wihofszky,Esslingen University of Applied Sciences,Impact and Lessons Learned from a National Consortium for Participatory Health Research: PartKommPlus-German Research Consortium for Healthy Communities (2015-2018).,1970,"Integrated strategies of health promotion at the municipal level are receiving particular attention in public health policy and practice in Germany. These strategies are intended to provide a coordinated approach to health promotion during the entire lifespan, with a particular focus on vulnerable communities. They are also intended to be participatory in both their design and implementation, involving all sectors of the social welfare, educational and healthcare systems, civil society, and the general public. PartKommPlus-German Research Consortium for Healthy Communities is examining such strategies using participatory forms of research. The goal is to determine how participation can best be planned and implemented and what effects this participation has. In this article the work of PartKommPlus from the first funding phase (2015-2018) will be described with particular attention to the lessons learned and the forms of impact which are being considered as part of the participatory research process.",https://doi.org/10.1155/2018/5184316,,No,
0000-0001-7758-5885,Kristina Weber,Medizinische Hochschule Hannover,"New research strategy with ambiguous implications: A comment on ""Planning future studies based on the conditional power of a meta-analysis"".",1970,Kein Abstract verfügbar,https://doi.org/10.1002/sim.7595,,No,
0000-0002-5035-6473,Daniel Friemert,Koblenz University of Applied Sciences,Influences of smart glasses on postural control under single- and dual-task conditions for ergonomic risk assessment.,1970,"Head worn displays have become increasingly popular at workplaces in logistics and assembly lines in recent years. Such displays are expected to improve productivity and safety at the workplace. However, their impact on balance in the workforce is still an open research question. Therefore, we investigated the influence of the Vuzix M400 and Realwear HMT1 smart glasses on postural control. A laboratory study was conducted with eleven participants. Balance parameters were recorded during bilateral quiet stance, together with parameters of cognitive load. The two different smart glasses used in this study were compared with a monitor and a tablet under single-task conditions and while performing a spatial 2-back task. As balance parameters, the prediction ellipse and sample entropy in anteroposterior as well as mediolateral direction of the center-of-pressure data were examined. No significant differences were observed in the cognitive task performance between the devices. The prediction ellipse of the smart glasses was smaller than the tablets but larger than the smartboard. The dynamic of sample entropy data suggests that the use of the spatial 2-back task induces postural sway in the participants. This effect was most profound when looking at the monitor and least recognizable in the data of the tablet.",https://doi.org/10.1515/bmt-2022-0404,,No,
0000-0003-2271-8630,Oliver Dürr,Konstanz University of Applied Sciences,Deep transformation models for functional outcome prediction after acute ischemic stroke.,1970,"In many medical applications, interpretable models with high prediction performance are sought. Often, those models are required to handle semistructured data like tabular and image data. We show how to apply deep transformation models (DTMs) for distributional regression that fulfill these requirements. DTMs allow the data analyst to specify (deep) neural networks for different input modalities making them applicable to various research questions. Like statistical models, DTMs can provide interpretable effect estimates while achieving the state-of-the-art prediction performance of deep neural networks. In addition, the construction of ensembles of DTMs that retain model structure and interpretability allows quantifying epistemic and aleatoric uncertainty. In this study, we compare several DTMs, including baseline-adjusted models, trained on a semistructured data set of 407 stroke patients with the aim to predict ordinal functional outcome three months after stroke. We follow statistical principles of model-building to achieve an adequate trade-off between interpretability and flexibility while assessing the relative importance of the involved data modalities. We evaluate the models for an ordinal and dichotomized version of the outcome as used in clinical practice. We show that both tabular clinical and brain imaging data are useful for functional outcome prediction, whereas models based on tabular data only outperform those based on imaging data only. There is no substantial evidence for improved prediction when combining both data modalities. Overall, we highlight that DTMs provide a powerful, interpretable approach to analyzing semistructured data and that they have the potential to support clinical decision-making.",https://doi.org/10.1002/bimj.202100379,,Yes,potent(1)
0000-0003-2271-8630,Oliver Dürr,Konstanz University of Applied Sciences,Integrating uncertainty in deep neural networks for MRI based stroke analysis.,1970,"At present, the majority of the proposed Deep Learning (DL) methods provide point predictions without quantifying the model's uncertainty. However, a quantification of the reliability of automated image analysis is essential, in particular in medicine when physicians rely on the results for making critical treatment decisions. In this work, we provide an entire framework to diagnose ischemic stroke patients incorporating Bayesian uncertainty into the analysis procedure. We present a Bayesian Convolutional Neural Network (CNN) yielding a probability for a stroke lesion on 2D Magnetic Resonance (MR) images with corresponding uncertainty information about the reliability of the prediction. For patient-level diagnoses, different aggregation methods are proposed and evaluated, which combine the individual image-level predictions. Those methods take advantage of the uncertainty in the image predictions and report model uncertainty at the patient-level. In a cohort of 511 patients, our Bayesian CNN achieved an accuracy of 95.33% at the image-level representing a significant improvement of 2% over a non-Bayesian counterpart. The best patient aggregation method yielded 95.89% of accuracy. Integrating uncertainty information about image predictions in aggregation models resulted in higher uncertainty measures to false patient classifications, which enabled to filter critical patient diagnoses that are supposed to be closer examined by a medical doctor. We therefore recommend using Bayesian approaches not only for improved image-level prediction and uncertainty estimation but also for the detection of uncertain aggregations at the patient-level.",https://doi.org/10.1016/j.media.2020.101790,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Improving Agri-environmental Schemes: Suggestions from Farmers and Nature Managers in a Central European Region.,1970,"Agri-environmental schemes (AES) are important policy instruments within the Common Agricultural Policy (CAP) of the European Union for environmental protection. Due to the voluntary nature of AES, their attractiveness to farmers and stakeholders involved in nature management and protection (nature managers) is essential for high participation levels. This study aims to assess farmers' and nature managers' ideas to improve agri-environmental schemes. We analyzed suggestions of 825 farmers and 118 nature managers for improvements of AES collected in a large-scale survey in Bavaria, Germany. A content analysis was applied to categorize and compare suggestions by farmers (differentiated into two groups through a cluster analysis) and nature managers. The results reveal that stakeholders were highly willing to share ideas and made detailed suggestions for improvements and individual measures. They were aware of the importance of protecting nature and promoting biodiversity in agricultural landscapes and acknowledged the necessity of (financial) support programs. Farmers placed more emphasis on the practicability and profitability of measures on arable land, while nature managers tended to propose policy-related ideas focusing on nature protection, biodiversity, and specific species. Among farmers, suggestions differed with farm characteristics such as the operation mode (full-time, part-time). These findings can support the design of future AES, accounting for different background situations and thereby increasing acceptability. This includes considering perspectives from different stakeholder groups and creating regionally adapted programs with varying levels of flexibility and practicability.",https://doi.org/10.1007/s00267-023-01922-w,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Earlier and more uniform spring green-up linked to lower insect richness and biomass in temperate forests.,1970,"Urbanization and agricultural intensification are considered the main causes of recent insect decline in temperate Europe, while direct climate warming effects are still ambiguous. Nonetheless, higher temperatures advance spring leaf emergence, which in turn may directly or indirectly affect insects. We therefore investigated how Sentinel-2-derived start of season (SOS) and its spatial variability (SV-SOS) are affected by spring temperature and whether these green-up variables can explain insect biomass and richness across a climate and land-use gradient in southern Germany. We found that the effects of both spring green-up variables on insect biomass and richness differed between land-use types, but were strongest in forests. Here, insect richness and biomass were higher with later green-up (SOS) and higher SV-SOS. In turn, higher spring temperatures advanced SOS, while SV-SOS was lower at warmer sites. We conclude that with a warming climate, insect biomass and richness in forests may be affected negatively due to earlier and more uniform green-up. Promising adaptation strategies should therefore focus on spatial variability in green-up in forests, thus plant species and structural diversity.",https://doi.org/10.1038/s42003-023-05422-9,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Plant species composition and local habitat conditions as primary determinants of terrestrial arthropod assemblages.,1970,"Arthropods respond to vegetation in multiple ways since plants provide habitat and food resources and indicate local abiotic conditions. However, the relative importance of these factors for arthropod assemblages is less well understood. We aimed to disentangle the effects of plant species composition and environmental drivers on arthropod taxonomic composition and to assess which aspects of vegetation contribute to the relationships between plant and arthropod assemblages. In a multi-scale field study in Southern Germany, we sampled vascular plants and terrestrial arthropods in typical habitats of temperate landscapes. We compared independent and shared effects of vegetation and abiotic predictors on arthropod composition distinguishing between four large orders (Lepidoptera, Coleoptera, Hymenoptera, Diptera), and five functional groups (herbivores, pollinators, predators, parasitoids, detritivores). Across all investigated groups, plant species composition explained the major fraction of variation in arthropod composition, while land-cover composition was another important predictor. Moreover, the local habitat conditions depicted by the indicator values of the plant communities were more important for arthropod composition than trophic relationships between certain plant and arthropod species. Among trophic groups, predators showed the strongest response to plant species composition, while responses of herbivores and pollinators were stronger than those of parasitoids and detritivores. Our results highlight the relevance of plant community composition for terrestrial arthropod assemblages across multiple taxa and trophic levels and emphasize the value of plants as a proxy for characterizing habitat conditions that are hardly accessible to direct environmental measurements.",https://doi.org/10.1007/s00442-023-05345-6,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,"Dung-visiting beetle diversity is mainly affected by land use, while community specialization is driven by climate.",1970,"Dung beetles are important actors in the self-regulation of ecosystems by driving nutrient cycling, bioturbation, and pest suppression. Urbanization and the sprawl of agricultural areas, however, destroy natural habitats and may threaten dung beetle diversity. In addition, climate change may cause shifts in geographical distribution and community composition. We used a space-for-time approach to test the effects of land use and climate on α-diversity, local community specialization (",https://doi.org/10.1002/ece3.9386,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,"Plant richness, land use and temperature differently shape invertebrate leaf-chewing herbivory on plant functional groups.",1970,"Higher temperatures can increase metabolic rates and carbon demands of invertebrate herbivores, which may shift leaf-chewing herbivory among plant functional groups differing in C:N (carbon:nitrogen) ratios. Biotic factors influencing herbivore species richness may modulate these temperature effects. Yet, systematic studies comparing leaf-chewing herbivory among plant functional groups in different habitats and landscapes along temperature gradients are lacking. This study was conducted on 80 plots covering large gradients of temperature, plant richness and land use in Bavaria, Germany. We investigated proportional leaf area loss by chewing invertebrates ('herbivory') in three plant functional groups on open herbaceous vegetation. As potential drivers, we considered local mean temperature (range 8.4-18.8 °C), multi-annual mean temperature (range 6.5-10.0 °C), local plant richness (species and family level, ranges 10-51 species, 5-25 families), adjacent habitat type (forest, grassland, arable field, settlement), proportion of grassland and landscape diversity (0.2-3 km scale). We observed differential responses of leaf-chewing herbivory among plant functional groups in response to plant richness (family level only) and habitat type, but not to grassland proportion, landscape diversity and temperature-except for multi-annual mean temperature influencing herbivory on grassland plots. Three-way interactions of plant functional group, temperature and predictors of plant richness or land use did not substantially impact herbivory. We conclude that abiotic and biotic factors can assert different effects on leaf-chewing herbivory among plant functional groups. At present, effects of plant richness and habitat type outweigh effects of temperature and landscape-scale land use on herbivory among legumes, forbs and grasses.",https://doi.org/10.1007/s00442-022-05199-4,,Yes,potent(1)
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Interactive effects of climate and land use on pollinator diversity differ among taxa and scales.,1970,"Changes in climate and land use are major threats to pollinating insects, an essential functional group. Here, we unravel the largely unknown interactive effects of both threats on seven pollinator taxa using a multiscale space-for-time approach across large climate and land-use gradients in a temperate region. Pollinator community composition, regional gamma diversity, and community dissimilarity (beta diversity) of pollinator taxa were shaped by climate-land-use interactions, while local alpha diversity was solely explained by their additive effects. Pollinator diversity increased with reduced land-use intensity (forest < grassland < arable land < urban) and high flowering-plant diversity at different spatial scales, and higher temperatures homogenized pollinator communities across regions. Our study reveals declines in pollinator diversity with land-use intensity at multiple spatial scales and regional community homogenization in warmer and drier climates. Management options at several scales are highlighted to mitigate impacts of climate change on pollinators and their ecosystem services.",https://doi.org/10.1126/sciadv.abm9359,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,"Landscape diversity and local temperature, but not climate, affect arthropod predation among habitat types.",1970,"Arthropod predators are important for ecosystem functioning by providing top-down regulation of insect herbivores. As predator communities and activity are influenced by biotic and abiotic factors on different spatial scales, the strength of top-down regulation ('arthropod predation') is also likely to vary. Understanding the combined effects of potential drivers on arthropod predation is urgently needed with regard to anthropogenic climate and land-use change. In a large-scale study, we recorded arthropod predation rates using artificial caterpillars on 113 plots of open herbaceous vegetation embedded in contrasting habitat types (forest, grassland, arable field, settlement) along climate and land-use gradients in Bavaria, Germany. As potential drivers we included habitat characteristics (habitat type, plant species richness, local mean temperature and mean relative humidity during artificial caterpillar exposure), landscape diversity (0.5-3.0-km, six scales), climate (multi-annual mean temperature, 'MAT') and interactive effects of habitat type with other drivers. We observed no substantial differences in arthropod predation rates between the studied habitat types, related to plant species richness and across the Bavarian-wide climatic gradient, but predation was limited when local mean temperatures were low and tended to decrease towards higher relative humidity. Arthropod predation rates increased towards more diverse landscapes at a 2-km scale. Interactive effects of habitat type with local weather conditions, plant species richness, landscape diversity and MAT were not observed. We conclude that landscape diversity favours high arthropod predation rates in open herbaceous vegetation independent of the dominant habitat in the vicinity. This finding may be harnessed to improve top-down control of herbivores, e.g. agricultural pests, but further research is needed for more specific recommendations on landscape management. The absence of MAT effects suggests that high predation rates may occur independent of moderate increases of MAT in the near future.",https://doi.org/10.1371/journal.pone.0264881,,Yes,potent(2)
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Modelling the Relative Abundance of Roe Deer (,1970,European roe deer (,https://doi.org/10.3390/ani12030222,,No,
0000-0002-2039-1462,Cynthia Tobisch,Weihenstephan-Triesdorf University of Applied Sciences,Relationship of insect biomass and richness with land use along a climate gradient.,1970,"Recently reported insect declines have raised both political and social concern. Although the declines have been attributed to land use and climate change, supporting evidence suffers from low taxonomic resolution, short time series, a focus on local scales, and the collinearity of the identified drivers. In this study, we conducted a systematic assessment of insect populations in southern Germany, which showed that differences in insect biomass and richness are highly context dependent. We found the largest difference in biomass between semi-natural and urban environments (-42%), whereas differences in total richness (-29%) and the richness of threatened species (-56%) were largest from semi-natural to agricultural environments. These results point to urbanization and agriculture as major drivers of decline. We also found that richness and biomass increase monotonously with increasing temperature, independent of habitat. The contrasting patterns of insect biomass and richness question the use of these indicators as mutual surrogates. Our study provides support for the implementation of more comprehensive measures aimed at habitat restoration in order to halt insect declines.",https://doi.org/10.1038/s41467-021-26181-3,,No,
0000-0002-5820-663X,Heike Mempel,Hochschule Weihenstephan-Triesdorf,OpenVNT: An Open Platform for VIS-NIR Technology.,1970,"Spectrometers measure diffuse reflectance and create a ""molecular fingerprint"" of the material under investigation. Ruggedized, small scale devices for ""in-field"" use cases exist. Such devices might for example be used by companies in the food supply chain for inward inspection of goods. However, their application for the industrial Internet of Things workflows or scientific research is limited due to their proprietary nature. We propose an open platform for visible and near-infrared technology (OpenVNT), an open platform for capturing, transmitting, and analysing spectral measurements. It is built for use in the field, as it is battery-powered and transmits data wireless. To achieve high accuracy, the OpenVNT instrument contains two spectrometers covering a wavelength range of 400-1700 nm. We conducted a study on white grapes to compare the performance of the OpenVNT instrument against the Felix Instruments F750, an established commercial instrument. Using a refractometer as ground truth, we built and validated models to estimate the Brix value. As a quality measure, we used coefficient of determination of the cross-validation (R2CV) between the instrument estimation and ground truth. With 0.94 for the OpenVNT and 0.97 for the F750, a comparable R2CV was achieved for both instruments. OpenVNT matches the performance of commercially available instruments at one tenth of the price. We provide an open bill of materials, building instructions, firmware, and analysis software to enable research and industrial IOT solutions without the limitations of walled garden platforms.",https://doi.org/10.3390/s23063151,,No,
0009-0001-8786-800X,Martina Hudler,Weihenstephan-Triesdorf University of Applied Sciences,Microbiota and Nutrient Portraits of European Roe Deer (Capreolus capreolus) Rumen Contents in Characteristic Southern German Habitats.,1970,"Roe deer (Capreolus capreolus) are found in various habitats, from pure forest cultures to agricultural areas and mountains. In adapting to the geographically and seasonally differentiating food supply, they depend, above all, on an adapted microbiome. However, knowledge about the microbiome of wild ruminants still needs to be improved. There are only a few publications for individual species with a low number of samples. This study aims to identify a core microbiota for Bavarian roe deer and present nutrient and microbiota portraits of the individual habitat types. This study investigated the roe deer's rumen (reticulorumen) content from seven different characteristic Bavarian habitat types. The focus was on the composition of nutrients, fermentation products, and the rumen bacterial community. A total of 311 roe deer samples were analysed, with the most even possible distribution per habitat, season, age class, and gender. Significant differences in nutrient concentrations and microbial composition were identified for the factors habitat, season, and age class. The highest crude protein content (plant protein and microbial) in the rumen was determined in the purely agricultural habitat (AG), the highest value of non-fibre carbohydrates in the alpine mountain forest, and the highest fibre content (neutral detergent fibre, NDF) in the pine forest habitat. Maximum values for fibre content go up to 70% NDF. The proportion of metabolites (ammonia, lactate, total volatile fatty acids) was highest in the Agriculture-Beech-Forest habitat (ABF). Correlations can be identified between adaptations in the microbiota and specific nutrient concentrations, as well as in strong fluctuations in ingested forage. In addition, a core bacterial community comprising five genera could be identified across all habitats, up to 44% of total relative abundance. As with all wild ruminants, many microbial genera remain largely unclassified at various taxonomic levels. This study provides a more in-depth insight into the diversity and complexity of the roe deer rumen microbiota. It highlights the key microorganisms responsible for converting naturally available nutrients of different botanical origins.",https://doi.org/10.1007/s00248-023-02308-5,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,Ways of integrating eating into everyday lives - a qualitative study in Germany.,1970,"Food-related behaviour is a very complex topic. A common way to reduce complex issues to their essential content is to create a typology. In Germany, with regard to food-related behaviour, the creation of a typology has often been carried out by commercial research institutes, but also by (international) scientific institutes. The former have mostly used quantitative methods, the latter usually have a specific content focus. Within this study, we want to investigate how people integrate eating into their everyday lives while engaging with themselves and the environment, thereby living out personality development and related socialisation.",https://doi.org/10.1186/s40795-024-00883-5,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,Determining the Distinguishing Features of Different Eating Action Types in Germany Using a Mixed-Method Approach.,1970,"Food-related behavior is a very complex topic, as it affects the most diverse areas of life. Accordingly, wide varieties of disciplines have already dealt with the topic to understand it better. The result is that there is neither a uniform nutrition knowledge nor a uniform nutrition behavior. In order to reduce the complexity of a field of study, there is the methodical means of type-building. Both commercial and academic studies have already formed nutrition types, either by means of standardized questionnaires or with a specific content focus. However, since both individual and social aspects influence food-related behavior, we investigate how people integrate eating into their everyday life against the background of (competing) individual and social demands by focusing on the individual point of view, for which a mixed methods approach is used. Based on 42 semi-structured, problem-centered interviews conducted in Germany in 2017, we built qualitative food-related types in a first step, which are analyzed in this article using a quantitative content analysis and cross-over analysis to identify the particular distinguishing feature(s) of each type and test them for significance. The results show the prominent characteristics for each type and indicate furthermore that subjectivization, self-determination, the body as an instrument of power, adaptation to the environment and being overstrained with the own behavior are particularly prominent when it comes to eating. Moreover, we clearly identified ",https://doi.org/10.3389/fnut.2021.720392,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,Protein for Community-Dwelling Older People: Aspects That Influence the Perception of Commercially Available Protein Drinks.,1970,"In an aging population, support for independent living is increasingly critical for older generations. Currently, sarcopenia is a major cause of frailty, which increases the risk of decreased mobility, falls, morbidity, and mortality and leads to dependence on third parties. Sarcopenia is preventable by consumption of adequate protein. However, many older people do not meet the recommended daily allowance of protein, thereby supporting dependence rather than independent living. Current literature indicates that a protein drink could be an appropriate product for older peoples' protein consumption. We were interested in autonomous persons whose nutritional decisions were still self-determined and thus could preventively influence their personal health. This study evaluated three commercially available protein drinks in three focus groups (",https://doi.org/10.3389/fnut.2020.00100,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,"The importance of herbal medicine use in the German health-care system: prevalence, usage pattern, and influencing factors.",1970,"Prevalence rates for herbal medicine (HM) have been increasing worldwide. However, little is known about prevalence, user characteristics, usage pattern and factors influencing HM usage for the general German population.",https://doi.org/10.1186/s12913-019-4739-0,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,The virtual doctor: An interactive clinical-decision-support system based on deep learning for non-invasive prediction of diabetes.,1970,"Artificial intelligence (AI) will pave the way to a new era in medicine. However, currently available AI systems do not interact with a patient, e.g., for anamnesis, and thus are only used by the physicians for predictions in diagnosis or prognosis. However, these systems are widely used, e.g., in diabetes or cancer prediction. In the current study, we developed an AI that is able to interact with a patient (virtual doctor) by using a speech recognition and speech synthesis system and thus can autonomously interact with the patient, which is particularly important for, e.g., rural areas, where the availability of primary medical care is strongly limited by low population densities. As a proof-of-concept, the system is able to predict type 2 diabetes mellitus (T2DM) based on non-invasive sensors and deep neural networks. Moreover, the system provides an easy-to-interpret probability estimation for T2DM for a given patient. Besides the development of the AI, we further analyzed the acceptance of young people for AI in healthcare to estimate the impact of such a system in the future.",https://doi.org/10.1016/j.artmed.2019.101706,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,"What motivates new, established and long-term users of herbal medicine: is there more than push and pull?",1970,"The use of herbal medicine (HM) has become an essential form of treatment and it is more and more common around the world. Little is known about the reasons that drive people to initially use HM or to maintain their behaviour, and whether the so-called ""push and pull factors"" known in the context of decision making for complementary and alternative medicine, also play a role for HM use. Here, our goal was to provide answers to these open questions and to analyse the reasons that motivate new, established and long-term HM consumers in detail.",https://doi.org/10.1186/s12906-019-2584-7,,No,
0000-0003-1079-1056,Klaus Menrad,Weihenstephan-Triesdorf University of Applied Sciences,Why people use herbal medicine: insights from a focus-group study in Germany.,1970,"The use of herbal medicine, as one element of complementary and alternative medicine, is increasing worldwide. Little is known about the reasons for and factors associated with its use. This study derives insights for the use of herbal medicine in Germany regarding the usage aims, role played by the type of illness, reasons for preferred usage and sources of information.",https://doi.org/10.1186/s12906-018-2160-6,,No,
0000-0001-6810-9545,Maria-Franziska Hohmann,Hannover,Bacterial Load of the Teat Apex Skin and Associated Factors at Herd Level.,1970,"In order to reduce antimicrobial treatment and prevent environmental mastitis, the aim of the present study was to investigate associations between herd level factors and microbial load on teat ends with environmental mastitis pathogens. Quarterly farm visits of 31 dairy farms over a one-year period were used for statistical analysis. During each farm visit, teat-skin swabs, bedding and air samples were taken and management practices and herd parameters were documented. Total mesophilic bacteria, esculin-positive streptococci and coliform bacteria were examined in the laboratory procedures from teat skin and environmental samples. Esculin-positive streptococci and coliform bacteria on teat ends increased with high temperature-humidity indices (THI) in the barn during the spring and summer. Significantly more coliform bacteria on teat ends were found in herds with an increased percentage of normal or slightly rough teat ends. Cleaning cubicles more frequently, pre-cleaning teats before milking as well as post-dipping them after milking had a decreasing effect of teat-skin load with total mesophilic and coliform bacteria at the herd level. To conclude, teat-skin bacterial load with environmental pathogens is subject to fluctuations and can be influenced by aspects of farm hygiene.",https://doi.org/10.3390/ani10091647,,No,
0000-0001-5016-3738,Andreas Eickhorst,Hochschule Hannover,[Adverse Childhood Experiences in Mothers and Intergenerational Family Violence].,1970,"For children, own adverse experiences, as well as their exposure to intimate partner violence poses a severe risk for health and development. In order to answer the question of intergenerational transmission of family violence, adverse childhood experiences in mothers are considered to be a significant risk factor for the occurrence of child maltreatment and intimate partner violence in families, which, however, has been little studied in Germany. Therefore, this paper uses cross-sectional data of 5.646 mothers that was taken fromthe representative study ""Kinder in Deutschland - KiD 0-3"". Multiple binary-logical regression models were calculated in order to examine the influence ofmaternal adverse childhood experiences on various forms of family violence. As a result, 823 mothers (9,3 %) reported adverse childhood experiences; 157 (2,8 %) admitted that their child had already been exposed to physical harm or harsh punishment, and 168 (3,0%) reported intimate partner violence since the birth of their child, respectively 493 (8,7 %) since any past relationship. Taking demographic and socioeconomic factors into account, the occurrence of all three forms of violence becamemore likely inmothers with adverse childhood experiences: physical harm of the child (OR = 2,78, p ≤ 0,001), current intimate partner violence of themother (OR = 3,76, p ≤ 0,001), as well as her lifetime experiences in general (OR = 3,67, p ≤ 0,001). Therefore, the support and guidance of families (e.g., by early childhood interventions) should take into account the connection between negative maternal childhood experiences, as well as familial forms of violence, and, if applicable, make generous preventative offers. In case of signs for familial violence, additional protective steps should be applied.",https://doi.org/10.13109/prkk.2023.72.6.483,,No,
0000-0001-5016-3738,Andreas Eickhorst,Hochschule Hannover,[Working with Fathers to Prevent Child Abuse. A Pilot Evaluation of the Caring Dads Program in Germany].,1970,,https://doi.org/10.13109/prkk.2021.70.2.115,,No,
0000-0001-5016-3738,Andreas Eickhorst,Hannover University of Applied Sciences,"Risk factors for child abuse, neglect and exposure to intimate partner violence in early childhood: Findings in a representative cross-sectional sample in Germany.",1970,"The KiD 0-3 national main study is a cross-sectional study on adversity in early childhood and parental access to support services, conducted as part of a long-term policy program for early intervention services in Germany.",https://doi.org/10.1016/j.chiabu.2020.104487,,No,
0000-0001-5016-3738,Andreas Eickhorst,Hochschule Hannover,Is the Brief Child Abuse Potential Inventory (BCAPI) a valid measure of child abuse potential among mothers and fathers of young children in Germany?,1970,"In order to prevent child abuse, instruments measuring child abuse potential (CAP) need to be appropriate, reliable and valid.",https://doi.org/10.1016/j.chiabu.2018.11.008,,Yes,potent(1)
0000-0001-5016-3738,Andreas Eickhorst,Hochschule Hannover,[Psychosocial Risk Factors and Negative Emotionality in Early Childhood: Mothers' Perspective].,1970,"Psychosocial Risk Factors and Negative Emotionality in Early Childhood: Mothers' Perspective Based on a nationally representative study of parents, this study examines risk factors for mothers' perceptions of young children's negative emotionality, focusing the role of mothers' educational resources and related psychosocial risk factors. Participants were 7,311 mothers with children below age 48 month. Mothers' perception of child emotionality was assessed through two factors, irritability and defiance. Findings from regression analyses showed a stable negative relationship between maternal education and perceived defiance of the child. Although this effect was partly mediated by further psychosocial risk factors, lower education was consistently related with higher perceived defiance. Perceived irritability, in contrast, was not affected by mother's education. Further analyses showed age-specific effects (stronger effects for younger children) as well as a predictive value of mothers' perceptions regarding the occurrence of child abuse or neglect. The results are discussed in the context of early prevention programs in Germany, emphasizing the relevance of identifying risk-groups and offering early and multidimensional prevention.",https://doi.org/10.13109/prkk.2018.67.5.405,,No,
0000-0003-3567-1173,Jürgen Dunkel,Hannover University of Applied Sciences,Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing.,1970,"Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing.",https://doi.org/10.3390/s23020614,,No,
0000-0001-5842-4218,Ralf Bruns,Hannover University of Applied Sciences,Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing.,1970,"Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing.",https://doi.org/10.3390/s23020614,,No,
0000-0002-4028-0604,Cornelia Frömke,Hannover University of Applied Sciences,"Suitability and user acceptance of the eResearch system ""Prospective Monitoring and Management App (PIA)""-The example of an epidemiological study on infectious diseases.",1970,"The eResearch system ""Prospective Monitoring and Management App (PIA)"" allows researchers to implement questionnaires on any topic and to manage biosamples. Currently, we use PIA in the longitudinal study ZIFCO (Integrated DZIF Infection Cohort within the German National Cohort) in Hannover (Germany) to investigate e.g. associations of risk factors and infectious diseases. Our aim was to assess user acceptance and compliance to determine suitability of PIA for epidemiological research on transient infectious diseases.",https://doi.org/10.1371/journal.pone.0279969,,No,
0000-0002-4028-0604,Cornelia Frömke,Hochschule Hannover,"[Erratum to: Ethical consideration of studies involving human subjects outside the regulatory framework: not mandatory, but of high relevance].",1970,Kein Abstract verfügbar,https://doi.org/10.1007/s00103-019-03014-9,,No,
0000-0002-4028-0604,Cornelia Frömke,Hochschule Hannover,"[Ethical consideration of studies involving human subjects outside the regulatory framework: not mandatory, but of high relevance].",1970,"According to the Declaration of Helsinki, ethics committees are obliged to evaluate any type of medical research involving human subjects in order to ensure an objective view on ethical considerations. This does not only mean considering whether the risks to study participants are ethically justifiable or not, but also checking whether the scientific quality of a study is sufficient. However, the role of ethics committees differs depending on whether the study to be considered is, for example, an approval study according to the German Medicines Act (AMG) or whether the study is outside the regulatory framework. For these so-called unregulated studies it is not always mandatory to obtain approval from an ethics committee or an institutional review board.In this paper, we first explain the term ""unregulated studies"" in detail and elaborate for which types of unregulated studies an application for ethical approval is required before we deal with the application for ethical approval as such and in particular with the study protocol as one of its major components. Registry studies, postmarketing surveillance studies, analyses of secondary data, surveys, intervention, and prognostic studies serve as examples to illustrate the broad range of unregulated studies.Finally, we discuss crucial aspects of the role of ethics committees with respect to the consideration of unregulated studies. In our conclusion, we point out the necessity of having ethics committees at each university in Germany that are also responsible for unregulated studies. In addition, the German legislature should define a stricter regulation such that unregulated studies also have to adhere to the vote of the ethics committee.",https://doi.org/10.1007/s00103-019-02947-5,,No,
0000-0002-4028-0604,Cornelia Frömke,Hochschule Hannover,Inflation of the type I error: investigations on regulatory recommendations for bioequivalence of highly variable drugs.,1970,"We investigated different evaluation strategies for bioequivalence trials with highly variable drugs on their resulting empirical type I error and empirical power. The classical 'unscaled' crossover design with average bioequivalence evaluation, the Add-on concept of the Japanese guideline, and the current 'scaling' approach of EMA were compared.",https://doi.org/10.1007/s11095-014-1450-z,,No,
0000-0003-3659-5336,Thomas Klefoth,Hochschule Bremen,Ecosystem-based management outperforms species-focused stocking for enhancing fish populations.,1970,"Ecosystem-based management is costly. Therefore, without rigorously showing that it can outperform traditional species-focused alternatives, its broad-scale adoption in conservation is unlikely. We present a large-scale replicated and controlled set of whole-lake experiments in fish conservation (20 lakes monitored over 6 years with more than 150,000 fish sampled) to examine the outcomes of ecosystem-based habitat enhancement (coarse woody habitat addition and shallow littoral zone creation) versus a widespread, species-focused alternative that has long dominated fisheries management practice (i.e., fish stocking). Adding coarse woody habitats alone did not, on average, enhance fish abundance, but creating shallow water habitat consistently did, especially for juvenile fish. Species-focused fish stocking completely failed. We provide strong evidence questioning the performance of species-focused conservation actions in aquatic ecosystems and instead recommend ecosystem-based management of key habitats.",https://doi.org/10.1126/science.adf0895,,No,
0000-0003-3659-5336,Thomas Klefoth,Hochschule Bremen,Big-data approaches lead to an increased understanding of the ecology of animal movement.,1970,"Understanding animal movement is essential to elucidate how animals interact, survive, and thrive in a changing world. Recent technological advances in data collection and management have transformed our understanding of animal ""movement ecology"" (the integrated study of organismal movement), creating a big-data discipline that benefits from rapid, cost-effective generation of large amounts of data on movements of animals in the wild. These high-throughput wildlife tracking systems now allow more thorough investigation of variation among individuals and species across space and time, the nature of biological interactions, and behavioral responses to the environment. Movement ecology is rapidly expanding scientific frontiers through large interdisciplinary and collaborative frameworks, providing improved opportunities for conservation and insights into the movements of wild animals, and their causes and consequences.",https://doi.org/10.1126/science.abg1780,,No,
0000-0003-3659-5336,Thomas Klefoth,Bremen,The battle between harvest and natural selection creates small and shy fish.,1970,"Harvest of fish and wildlife, both commercial and recreational, is a selective force that can induce evolutionary changes to life history and behavior. Naturally selective forces may create countering selection pressures. Assessing natural fitness represents a considerable challenge in broadcast spawners. Thus, our understanding about the relative strength of natural and fisheries selection is slim. In the field, we compared the strength and shape of harvest selection to natural selection on body size over four years and behavior over one year in a natural population of a freshwater top predator, the northern pike (",https://doi.org/10.1073/pnas.2009451118,,Yes,fresh(1)
0000-0002-5123-9391,Dieter Kraus,Hochschule Bremen,Real-time biscuit tile image segmentation method based on edge detection.,1970,"In this paper we propose a novel real-time Biscuit Tile Segmentation (BTS) method for images from ceramic tile production line. BTS method is based on signal change detection and contour tracing with a main goal of separating tile pixels from background in images captured on the production line. Usually, human operators are visually inspecting and classifying produced ceramic tiles. Computer vision and image processing techniques can automate visual inspection process if they fulfill real-time requirements. Important step in this process is a real-time tile pixels segmentation. BTS method is implemented for parallel execution on a GPU device to satisfy the real-time constraints of tile production line. BTS method outperforms 2D threshold-based methods, 1D edge detection methods and contour-based methods. Proposed BTS method is in use in the biscuit tile production line.",https://doi.org/10.1016/j.isatra.2018.03.015,,No,
0000-0001-6239-8358,Nina Graupner,Hochschule Bremen,"Structure, Properties and Degradation of Self-Assembled Fibrinogen Nanofiber Scaffolds.",1970,"Self-assembled fibrinogen nanofibers are promising candidates for skin tissue engineering due to their biocompatibility and ability to mimic the native blood clot architecture. Here, we studied the structure-property relationship and degradation of rehydrated fibrinogen nanofibers prepared by salt-induced self-assembly, focusing on the effect of scaffold layering, cross-linking time and freeze-drying. Optimal fiber stability was achieved with cross-linking by formaldehyde (FA) vapor, while treatment with liquid aldehydes, genipin, EDC, and transglutaminase failed to preserve the nanofibrous architecture upon rehydration. Scaffold layering did not significantly influence the mechanical properties but changed the scaffold architecture, with bulk fiber scaffolds being more compact than layered scaffolds. Freeze-drying maintained the mechanical properties and interconnected pore network with average pore diameters around 20 μm, which will enhance the storage stability of self-assembled fibrinogen scaffolds. Varying cross-linking times altered the scaffold mechanics without affecting the swelling behavior, indicating that scaffold hydration can be controlled independently of the mechanical characteristics. Cross-linking times of 240 min increased scaffold stiffness and decreased elongation, while 30 min resulted in mechanical properties similar to native skin. Cross-linking for 120 min was found to reduce scaffold degradation by various enzymes in comparison to 60 min. Overall, after 35 days of incubation, plasmin and a combination of urokinase and plasminogen exhibited the strongest degradative effect, with nanofibers being more susceptible to enzymatic degradation than planar fibrinogen due to their higher specific surface area. Based on these results, self-assembled fibrinogen fiber scaffolds show great potential for future applications in soft tissue engineering that require controlled structure-function relationships and degradation characteristics.",https://doi.org/10.1021/acsabm.4c00761,,Yes,potent(1)
0000-0001-6239-8358,Nina Graupner,Hochschule Bremen,A Bio-Inspired Approach to Improve the Toughness of Brittle Bast Fibre-Reinforced Composites Using Cellulose Acetate Foils.,1970,"Bast fibre-reinforced plastics are characterised by good strength and stiffness but are often brittle due to the stiff and less ductile fibres. This study uses a biomimetic approach to improve impact strength. Based on the structure of the spicules of a deep-sea glass sponge, in which hard layers of bioglass alternate with soft layers of proteins, the toughness of kenaf/epoxy composites was significantly improved by a multilayer structure of kenaf and cellulose acetate (CA) foils as impact modifiers. Due to the alternating structure, cracks are deflected, and toughness is improved. One to five CA foils were stacked with kenaf layers and processed to composite plates with bio-based epoxy resin by compression moulding. Results have shown a significant improvement in toughness using CA foils due to increased crack propagation. The unnotched Charpy impact strength increased from 9.0 kJ/m",https://doi.org/10.3390/biomimetics9030131,,No,
0000-0001-6239-8358,Nina Graupner,Hochschule Bremen,Impact and hardness optimisation of composite materials inspired by the babassu nut (Orbignya speciosa).,1970,"The babassu nut is the fruit of the babassu palm Orbignya speciosa. The combination of hardness and impact strength is difficult to acquire for artificial materials, making the babassu nut a promising source for biomimetic inspiration. Unnotched Charpy impact tests, Shore D hardness tests and scanning electron microscopy were used for mechanical and microscopical analysis of the pericarp. Four major principles were found for a biomimetic approach: a hard core ((1); endocarp) is embedded in a soft outer layer of high impact strength ((2); epicarp) and is reinforced with fibres of variable fineness (3), some of which are oriented radial to the core (4). Biomimetic fibre-reinforced composites were produced using abstracted mechanisms of the babassu nut based on regenerated cellulose fibres (lyocell, L) with two different fineness values as reinforcement embedded in a polylactide (PLA) core matrix and polypropylene (PP) based outer layers. The biomimetic fibre composite reaches a significantly higher impact strength that is 1.6 times higher than the reference sample produced from a PLA/PP/L-blend. At the same time the hardness is slightly increased compared to PP/L.",https://doi.org/10.1088/1748-3190/10/5/056006,,No,
0000-0003-4901-3657,Sabrina Hegner,Bremen University of Applied Sciences,Does trust play a role when it comes to donations? A comparison of Italian and US higher education institutions.,1970,"Higher education institutions (HEIs) have experienced severe cutbacks in funding over the past few years, with universities examining options for alternative funding streams, such as alumni funding. Identifying the factors influencing their alumni's intentions to invest in their alma mater can be of significant importance when establishing a sustainable revenue stream. Within this context, empirical research on the potential role of trust is scarce. This paper aims to deepen the analysis of the relationship between alumni trust and engagement as well as three outcomes, namely support, commitment, and attitude toward donation. A structural equation model was tested on two samples of US (",https://doi.org/10.1007/s10734-020-00623-1,,Yes,potent(1)
0009-0001-7913-5869,Matthias Ochs,Fulda University of Applied Sciences,Pattern transitions in diary data of MDD patients: a mixed-methods multiple case study of psychotherapy dynamics.,1970,"Mixed-methods approaches promise a deep understanding of psychotherapeutic processes. This study uses qualitative and quantitative data from daily diary entries and daily self-assessments during inpatient treatment. The aim of the study is to get an insight into the similarities and differences between both types of data and how they represent self-organized pattern transitions in psychotherapy. While a complete correlation of results is not expected, we anticipate observing amplifying and subsidiary patterns from both perspectives.",https://doi.org/10.3389/fpsyg.2024.1259610,,No,
0000-0002-8840-0548,Friedrich-Karl Lücke,Fulda University of Applied Sciences,"Controlled fermentation of rapeseed presscake by Rhizopus, and its effect on some components with relevance to human nutrition.",1970,"The use of rapeseed protein could contribute to meeting the increasing demand for plant proteins with high biological value in human nutrition. In order to make rapeseed presscake fit for human consumption, the presscake was fermented by using the tempeh mould, Rhizopus microsporus var. oligosporus. Fermentation was satisfactory at initial levels of added acetic acid of 40-60 mmoles/Kg, a",https://doi.org/10.1016/j.foodres.2018.11.031,,No,
0000-0003-0798-2919,Rolf Huesmann,Hochschule Darmstadt,"Mobile Contactless Fingerprint Recognition: Implementation, Performance and Usability Aspects.",1970,"This work presents an automated contactless fingerprint recognition system for smartphones. We provide a comprehensive description of the entire recognition pipeline and discuss important requirements for a fully automated capturing system. In addition, our implementation is made publicly available for research purposes. During a database acquisition, a total number of 1360 contactless and contact-based samples of 29 subjects are captured in two different environmental situations. Experiments on the acquired database show a comparable performance of our contactless scheme and the contact-based baseline scheme under constrained environmental influences. A comparative usability study on both capturing device types indicates that the majority of subjects prefer the contactless capturing method. Based on our experimental results, we analyze the impact of the current COVID-19 pandemic on fingerprint recognition systems. Finally, implementation aspects of contactless fingerprint recognition are summarized.",https://doi.org/10.3390/s22030792,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,Comparing the motivational underpinnings of sustainable consumption across contexts using a scenario-based approach.,1970,A sample of tourists (,https://doi.org/10.3389/fpsyg.2022.854093,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,"Expectation of others' cooperation, efficacy beliefs, and willingness to sacrifice personal interests for the environment.",1970,"This paper departs from the view that the social dilemma literature provides a useful framework to delineate possible barriers to the adoption of environmentally friendly lifestyles. One domain in which tensions between personal and collective interests might occur are travel decisions in the context of tourism, where it has been shown that even those people who are very committed to environmental practices at home tend to reduce respective commitments on vacation. Data from a cross-sectional survey N = 771 were analyzed to investigate if the expectation that other tourists travel environmentally friendly can in part explain individual travel decisions with environmental implications. Results showed that this expectation of others' cooperation added explanatory value in willingness to sacrifice (personal interests) for the environment. Further analyses indicated that the relationship between expectation of others' cooperation and willingness to sacrifice for the environment is sequentially mediated by collective efficacy and self-efficacy. We discuss implications for initiatives to gain a better understanding of travel decisions that can help limit environmentally harmful impacts.",https://doi.org/10.1111/sjop.12812,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,Commentary: We Need to Change: Integrating Psychological Perspectives Into the Multilevel Perspective on Socio-Ecological Transformations.,1970,Kein Abstract verfügbar,https://doi.org/10.3389/fpsyg.2021.724768,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,"Relationships Between Exposure to Different Gambling Advertising Types, Advertising Impact and Problem Gambling.",1970,"People with gambling problems report more exposure and impact from gambling advertising, although less is known regarding the role of specific advertising types. Data on gamblers (n = 5830, 48.5% women, mean age = 44.27) was collected from a general population cross-sectional survey in Norway (32.7% response rate). We examined if problem gambling was associated with perceived advertising impact (on gambling involvement, awareness, and knowledge) or exposure (via internet, TV, retail outlet, newspaper, and direct advertising). We also investigated if advertising exposure was associated with advertising impact. ANOVAs revealed that problem gambling was associated with increased perceived advertising impact on gambling involvement (ω",https://doi.org/10.1007/s10899-021-10038-x,,No,
0000-0001-6692-3316,Daniel Hanss,Hochschule Darmstadt,Lessons Learned From Applications of the Stage Model of Self-Regulated Behavioral Change: A Review.,1970,"Stage models are becoming increasingly popular in explaining change from current behavior to more environmentally friendly alternatives. We review empirical applications of a recently introduced model, the stage model of self-regulated behavioral change (SSBC). In the SSBC, change toward pro-environmental behavior takes place in four, qualitatively different stages (predecisional, preactional, actional, and postactional) which are each influenced by constructs taken from theories previously established to describe and predict pro-environmental behavior. We performed a systematic literature search to retrieve peer-reviewed SSBC-based studies. The review includes 10 studies published between 2013 and 2018, six of which employed a cross-sectional, three an interventional and one a correlational longitudinal design. The cross-sectional and longitudinal studies generally support the model, although there are some irregularities that warrant further investigation. The interventional studies found stage-tailored informational measures to be more effective than non-stage-tailored measures in promoting stage progression and behavioral change. Furthermore, we identified several challenges that researchers may face when applying the SSBC. These include whether and how to analyze multiple behavioral alternatives; how to address the challenge of measuring a comprehensive model while keeping questionnaire length manageable; selecting and defining the role of model constructs in a behavioral context while keeping results comparable; and establishing a validated and reliable tool to diagnose a person's stage of change. Based on these insights, we develop recommendations for researchers designing SSBC studies, in order to support a founded and efficient advancement of the theory which will then serve both researchers and practitioners aiming to promote pro-environmental behavior.",https://doi.org/10.3389/fpsyg.2019.01091,,No,
0000-0001-6692-3316,Daniel Hanss,Hochschule Darmstadt,Using Card Sorting to Explore the Mental Representation of Energy Transition Pathways Among Laypeople.,1970,"Meeting international emission targets will require major changes in the energy system. This paper addresses the public perception of different pathways to energy transition, and their mental representation in particular. A study is reported that employed card sorting to explore how laypeople categorize possible pathway components with respect to their perceived similarity (Norwegian sample, ",https://doi.org/10.3389/fpsyg.2018.02322,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,The effects of alcohol expectancy and intake on slot machine gambling behavior.,1970,"Background and aims Although alcohol intake and gambling often co-occur in related venues, there is conflicting evidence regarding the effects of alcohol expectancy and intake on gambling behavior. We therefore conducted an experimental investigation of the effects of alcohol expectancy and intake on slot machine gambling behavior. Methods Participants were 184 (females = 94) individuals [age range: 18-40 (mean = 21.9) years] randomized to four independent conditions differing in information/expectancy about beverage (told they received either alcohol or placebo) and beverage intake [actually ingesting low (target blood alcohol concentration [BAC] < 0.40 mg/L) vs. moderate (target BAC > 0.40 mg/L; ≈0.80 mg/L) amounts of alcohol]. All participants completed self-report questionnaires assessing demographic variables, subjective intoxication, alcohol effects (stimulant and sedative), and gambling factors (behavior and problems, evaluation, and beliefs). Participants also gambled on a simulated slot machine. Results A significant main effect of beverage intake on subjective intoxication and alcohol effects was detected as expected. No significant main or interaction effects were detected for number of gambling sessions, bet size and variation, remaining credits at termination, reaction time, and game evaluation. Conclusion Alcohol expectancy and intake do not affect gambling persistence, dissipation of funds, reaction time, or gambling enjoyment.",https://doi.org/10.1556/2006.6.2017.031,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,The Relationships between Mental Health Symptoms and Gambling Behavior in the Transition from Adolescence to Emerging Adulthood.,1970,"There is a paucity of longitudinal investigations of gambling behavior in the transition from adolescence to emerging adulthood. We conducted a longitudinal investigation of the associations and patterns of change between mental health symptoms and gambling behavior. A representative sample of Norwegians completed questionnaires containing demographic, mental health, and gambling measures at age 17 (",https://doi.org/10.3389/fpsyg.2017.00478,,No,
0000-0001-6692-3316,Daniel Hanss,Hochschule Darmstadt,Prevalence and Predictors of Video Game Addiction: A Study Based on a National Representative Sample of Gamers.,1970,"Video gaming has become a popular leisure activity in many parts of the world, and an increasing number of empirical studies examine the small minority that appears to develop problems as a result of excessive gaming. This study investigated prevalence rates and predictors of video game addiction in a sample of gamers, randomly selected from the National Population Registry of Norway (",https://doi.org/10.1007/s11469-015-9592-8,,No,
0000-0001-6692-3316,Daniel Hanss,Darmstadt University of Applied Sciences,Aggression Is Associated With Increased Anabolic-Androgenic Steroid Use Contemplation Among Adolescents.,1970,"We investigated the relationship between aggression and anabolic-androgenic steroid (AAS) use intent among adolescents. A nationally representative sample of Norwegian 18-year-olds (N = 1,334, females = 58.7%) took part in a survey in 2013 (response rate = 64.9%). Participants completed the physical and verbal subscales of the Short-Form Buss-Perry Aggression Questionnaire, the Intent to use AAS Scale, the Alcohol Use Disorders Identification Test-Consumption, and the Hospital Anxiety and Depression Scale. They also provided demographic information and answered questions about AAS use, gambling participation, as well as cigarette and snus use. Descriptive statistics and multinomial logistic regression were used to analyze the data. Lifetime and past year prevalence of AAS use was 0.1%. Between 0.4% and 1.7% of participants disclosed intent to use while between 1.1% and 2.5% expressed neutral intent to initiate AAS use. Compared to persons low on aggression, individuals high on aggression were more likely to report intent and curiosity towards initiating AAS use. Our findings indicate that aggression is a risk factor for AAS use contemplation among adolescents.",https://doi.org/10.1080/10826084.2016.1186696,,No,
0000-0001-6692-3316,Daniel Hanss,Hochschule Darmstadt,Problem gambling and the five-factor model of personality: a large population-based study.,1970,"Knowledge of the personality characteristics of individuals who develop gambling problems is important for designing targeted prevention efforts. Previous studies of the relationship between the five-factor model of personality and gambling problems were based on small samples not representative of the general population. We estimated differences in neuroticism, extroversion, intellect, agreeableness and conscientiousness between non-problem gamblers and individuals with low, moderate and severe gambling problems.",https://doi.org/10.1111/add.13388,,No,
0000-0001-7805-1981,Bernhard Humm,Hochschule Darmstadt,Decentralized Real-Time Anomaly Detection in Cyber-Physical Production Systems under Industry Constraints.,1970,"Anomaly detection is essential for realizing modern and secure cyber-physical production systems. By detecting anomalies, there is the possibility to recognize, react early, and in the best case, fix the anomaly to prevent the rise or the carryover of a failure throughout the entire manufacture. While current centralized methods demonstrate good detection abilities, they do not consider the limitations of industrial setups. To address all these constraints, in this study, we introduce an unsupervised, decentralized, and real-time process anomaly detection concept for cyber-physical production systems. We employ several 1D convolutional autoencoders in a sliding window approach to achieve adequate prediction performance and fulfill real-time requirements. To increase the flexibility and meet communication interface and processing constraints in typical cyber-physical production systems, we decentralize the execution of the anomaly detection into each separate cyber-physical system. The installation is fully automated, and no expert knowledge is needed to tackle data-driven limitations. The concept is evaluated in a real industrial cyber-physical production system. The test result confirms that the presented concept can be successfully applied to detect anomalies in all separate processes of each cyber-physical system. Therefore, the concept is promising for decentralized anomaly detection in cyber-physical production systems.",https://doi.org/10.3390/s23094207,,No,
0000-0001-7805-1981,Bernhard Humm,Darmstadt University of Applied Sciences,Autoencoder-Ensemble-Based Unsupervised Selection of Production-Relevant Variables for Context-Aware Fault Diagnosis.,1970,"Smart factories are complex; with the increased complexity of employed cyber-physical systems, the complexity evolves further. Cyber-physical systems produce high amounts of data that are hard to capture and challenging to analyze. Real-time recording of all data is not possible due to limited network capabilities. Limited network capabilities are the reason for a chain of faults introduced via active surveillance during fault diagnosis. These introduced faults may slow down production or lead to an outage of the production line. Here, we present a novel approach to automatically select production-relevant shop floor parameters to decrease the number of surveyed variables and, at the same time, maintain quality in fault diagnosis without overloading the network. We were able to achieve higher throughput, mitigate communication losses and prevent the disruption of factory instructions. Our approach uses an autoencoder ensemble via minority voting to differentiate between normal-always on-variables and production variables that may yield a higher entropy. Our approach has been tested in a production-equal smart factory and was cross-validated by a domain expert.",https://doi.org/10.3390/s22218259,,No,
0009-0009-7915-3432,Holger Kirsch,Evangelische Hochschule Darmstadt,[Mentalizing as ,1970,"Based on the psychological stress caused by theCovid 19 pandemic in families, this article explores the fundamental question of how the psychological process of mentalizing - metaphorically speaking - can act as a psychosocial vaccination in stressful times. To this end, we look at the developments in the psychosocial context under the conditions of the pandemic and consider the effects on child and adolescent psychotherapy on the basis of a vignette of a group therapy session.",https://doi.org/10.13109/prkk.2023.72.1.14,,No,
0009-0006-6219-5642,Sabrina Pietzsch,Darmstadt University of Applied Sciences,"Separation and Analysis of Connected, Micrometer-Sized, High-Frequency Damage on Glass Plates due to Laser-Accelerated Material Fragments in Vacuum.",1970,"In this paper, we present a new processing method, called MOSES-Impacts, for the detection of micrometer-sized damage on glass plate surfaces. It extends existing methods by a separation of damaged areas, called impacts, to support state-of-the-art recycling systems in optimizing their parameters. These recycling systems are used to repair process-related damages on glass plate surfaces, caused by accelerated material fragments, which arise during a laser-matter interaction in a vacuum. Due to a high number of impacts, the presented MOSES-Impacts algorithm focuses on the separation of connected impacts in two-dimensional images. This separation is crucial for the extraction of relevant features such as centers of gravity and radii of impacts, which are used as recycling parameters. The results show that the MOSES-Impacts algorithm effectively separates impacts, achieves a mean agreement with human users of (82.0 ± 2.0)%, and improves the recycling of glass plate surfaces by identifying around 7% of glass plate surface area as being not in need of repair compared to existing methods.",https://doi.org/10.3390/jimaging10050101,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,The interplay of multiple unconditioned stimuli in evaluative conditioning: A weighted averaging framework for attitude formation via stimulus co-occurrences.,1970,"Evaluative conditioning (EC) is a key effect in attitude formation, leading to changes in the liking of neutral attitude objects due to their pairing with positive or negative stimuli. Despite EC's significance, current theories and most empirical findings are limited to stimulus pairings with a single affective stimulus at a time. In contrast, social environments often involve more complex combinations of affective stimuli. In this article, we introduce a novel framework grounded in information integration research to understand how conditioned attitudes develop in the presence of multiple affective stimuli. Through 10 experiments with different designs, measures, materials, and pairing procedures, we find that individuals' conditioned attitudes follow the average valence of all affective stimuli present with a stronger weighting of negative stimuli. This weighted averaging rule bears two implications for EC in more complex stimulus combinations. First, EC effects are nonmonotonous, such that additional stimuli of the same valence do not produce incremental EC effects. Second, EC effects are interdependent, such that the impact of one stimulus is weakest when accompanied by another negative stimulus and strongest when no other affective stimulus is present. We examine different cognitive processes underlying this weighted averaging rule, including potential differences in pairing memory or changes in the affective stimuli's valence when other stimuli are present. Our findings present a novel theoretical perspective on EC and offer valuable insights into attitude change from stimulus co-occurrences in stimulus-rich environments. (PsycInfo Database Record (c) 2024 APA, all rights reserved).",https://doi.org/10.1037/pspa0000401,,Yes,potent(1)
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,The role of category valence in prototype preference.,1970,"People prefer prototypical stimuli over atypical stimuli. The dominant explanation for this prototype preference effect is that prototypical stimuli are processed more fluently. However, a more recent account proposes that prototypes are more strongly associated with their category's valence, leading to a reversed prototype preference effect for negative categories. One critical but untested assumption of this category-valence account is that no prototype preference should emerge for entirely neutral categories. We tested this prediction by conditioning categories of dot patterns positively, negatively, or neutrally. In line with previous findings on the category-valence account, prototype preference reversed for negatively conditioned categories. However, prototype preference was similarly strong for positive and neutral categories. These findings imply that prototype preferences do not only reflect a transfer of category valence to exemplars. Instead, the results suggest that prototype preference is a multi-process phenomenon arising from the activated category valence and a fluency-based process. We discuss further implications for theories on fluency and prototype preference.",https://doi.org/10.1080/02699931.2024.2335536,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,Can sequencing of articulation ease explain the in-out effect? A preregistered test.,1970,"Words whose consonantal articulation places move from the front of the mouth to the back (e.g. BADAKA; inward) receive more positive evaluations than words whose consonantal articulation places move from the back of the mouth to the front (e.g. KADABA; outward). This in-out effect has a variety of affective, cognitive, and even behavioural consequences, but its underlying mechanisms remain elusive. Most recently, a linguistic explanation has been proposed applying the linguistic easy-first account and the so-called labial-coronal effect from developmental speech research and phonology to the in-out effect: Labials (front) are easier to process than coronals (middle); and people prefer easy followed by harder motor components. Disentangling consonantal articulation direction and articulation place, the present three preregistered experiments (total ",https://doi.org/10.1080/02699931.2024.2326072,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,"Articulation dynamics and evaluative conditioning: investigating the boundary conditions, mental representation, and origin of the in-out effect.",1970,"People prefer linguistic stimuli with an inward (e.g. BODIKA) over those with an outward articulation dynamic (e.g. KODIBA), a phenomenon known as the articulatory in-out effect. Despite its robustness across languages and contexts, the phenomenon is still poorly understood. To learn more about the effect's boundary conditions, mental representation, and origin, we crossed the in-out effect with evaluative conditioning research. In five experiments (",https://doi.org/10.1080/02699931.2023.2228538,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,Transparent by choice: Proactive disclosures increase compliance with digital defaults.,1970,"Default nudges successfully guide choices across multiple domains. Online use cases for defaults range from promoting sustainable purchases to inducing acceptance of behavior tracking scripts, or ""cookies."" However, many scholars view defaults as unethical due to the covert ways in which they influence behavior. Hence, opt-outs and other digital decision aids are progressively being regulated in an attempt to make them more transparent. The current practice of transparency boils down to saturating the decision environment with convoluted legal information. This approach might be informed by researchers, who hypothesized that nudges could become less effective once they are clearly laid out: People can retaliate against influence attempts if they are aware of them. A recent line of research has shown that such concerns are unfounded when the default-setters proactively discloses the purpose of the intervention. Yet, it remained unclear whether the effect persists when defaults reflect the current practice of such mandated transparency boils down to the inclusion of information disclosures, containing convoluted legal information. In two empirical studies (",https://doi.org/10.3389/fpsyg.2022.981497,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,Can sequencing explain the in-out effect?,1970,Kein Abstract verfügbar,https://doi.org/10.1016/j.tics.2022.03.008,,No,
0000-0002-5768-1391,Tobias Vogel,Darmstadt University of Applied Sciences,"The articulatory in-out effect: replicable, but inexplicable.",1970,"People prefer inward over outward articulation dynamics, a phenomenon referred to as the articulatory in-out effect. It is empirically robust and generalizes across languages, settings, and stimuli. However, the theoretical explanation of the effect is still a matter of lively debate and in need of novel research directions.",https://doi.org/10.1016/j.tics.2021.10.008,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Sufficiency for PSS tracking gait disorders in multiple sclerosis: A managerial perspective.,1970,"This study primarily aimed to explore the capabilities of digitalisation in the healthcare context, focusing on a specific disease. In this case, the study examined the potential of remote monitoring of gait to address the sensitivity of multiple sclerosis progression to gait characteristics by adopting a non-invasive approach to remotely quantify gait disturbances in a patient's daily life. To better understand the managerial aspects associated with this approach, the researchers conducted a literature review along with a set of semi-structured interviews. The target population included MS patients as well as the key agents involved in their care: patients' family members, neurologists, MS nurses, physiotherapists, medical directors, and pharmacist. The study identifies the perceived barriers and drivers that could contribute to the successful deployment of PSS remote gait monitoring as a healthcare service: i) At mega-level governance. Implications on privacy and security data are notable barriers missing on the speech. ii) At macro level, funding is highlighted as main barrier. The cost and lack of health system subsidies may render initiatives unsustainable, as emphasised by the interviewees. iii) At meso level, useable data is recognised as a driver. The data collection process can align with diverse interests to create value and business opportunities for the ecosystem actors, enhance care, attract stakeholders, such as insurers and pharma, and form partnerships. iv) At micro-level processes, we find two potential barriers: wearable device and app usability (comfort, navigation, efficiency) and organisational/behavioural aspects (training, digital affinity, skills), which are crucial for value creation in innovation ecosystems among patients and healthcare professionals. Finally, we find an interesting gap in the literature and interviews. Stakeholders' limited awareness of technological demands, especially from information technologies, for a successful long-term service, can be consider two key barriers for PSS.",https://doi.org/10.1016/j.heliyon.2024.e30001,,Yes,"notable(1), potent(2)"
0000-0002-2423-1474,Javier Villalba-Diez,Heilbronn University of Applied Sciences,Socio-Technical Analysis of the Benefits and Barriers to Using a Digital Representation of the Global Horse Population in Equine Veterinary Medicine.,1970,"There is a consensus that future medicine will benefit from a comprehensive analysis of harmonized, interconnected, and interoperable health data. These data can originate from a variety of sources. In particular, data from veterinary diagnostics and the monitoring of health-related life parameters using the Internet of Medical Things are considered here. To foster the usage of collected data in this way, not only do technical aspects need to be addressed but so do organizational ones, and to this end, a socio-technical matrix is first presented that complements the literature. It is used in an exemplary analysis of the system. Such a socio-technical matrix is an interesting tool for analyzing the process of data sharing between actors in the system dependent on their social relations. With the help of such a socio-technical tool and using equine veterinary medicine as an example, the social system of veterinarians and owners as actors is explored in terms of barriers and enablers of an effective digital representation of the global equine population.",https://doi.org/10.3390/ani13223557,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Quantum cyber-physical systems.,1970,"This paper aims to promote a quantum framework that analyzes Industry 4.0 cyber-physical systems more efficiently than traditional simulations used to represent integrated systems. The paper proposes a novel configuration of distributed quantum circuits in multilayered complex networks that enable the evaluation of industrial value creation chains. In particular, two different mechanisms for the integration of information between circuits operating at different layers are proposed, where their behavior is analyzed and compared with the classical conditional probability tables linked to the Bayesian networks. With the proposed method, both linear and nonlinear behaviors become possible while the complexity remains bounded. Applications in the case of Industry 4.0 are discussed when a component's health is under consideration, where the effect of integration between different quantum cyber-physical digital twin models appears as a relevant implication.",https://doi.org/10.1038/s41598-022-11691-x,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Optimisation of Maintenance Policies Based on Right-Censored Failure Data Using a Semi-Markovian Approach.,1970,"This paper exposes the existing problems for optimal industrial preventive maintenance intervals when decisions are made with right-censored data obtained from a network of sensors or other sources. A methodology based on the use of the z transform and a semi-Markovian approach is presented to solve these problems and obtain a much more consistent mathematical solution. This methodology is applied to a real case study of the maintenance of large marine engines of vessels dedicated to coastal surveillance in Spain to illustrate its usefulness. It is shown that the use of right-censored failure data significantly decreases the value of the optimal preventive interval calculated by the model. In addition, that optimal preventive interval increases as we consider older failure data. In sum, applying the proposed methodology, the maintenance manager can modify the preventive maintenance interval, obtaining a noticeable economic improvement. The results obtained are relevant, regardless of the number of data considered, provided that data are available with a duration of at least 75% of the value of the preventive interval.",https://doi.org/10.3390/s22041432,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Improvement of Quantum Approximate Optimization Algorithm for Max-Cut Problems.,1970,"The objective of this short letter is to study the optimal partitioning of value stream networks into two classes so that the number of connections between them is maximized. Such kind of problems are frequently found in the design of different systems such as communication network configuration, and industrial applications in which certain topological characteristics enhance value-stream network resilience. The main interest is to improve the Max-Cut algorithm proposed in the quantum approximate optimization approach (QAOA), looking to promote a more efficient implementation than those already published. A discussion regarding linked problems as well as further research questions are also reviewed.",https://doi.org/10.3390/s22010244,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Human-Machine Integration in Processes within Industry 4.0 Management.,1970,"The aim of this work is to use IIoT technology and advanced data processing to promote integration strategies between these elements to achieve a better understanding of the processing of information and thus increase the integrability of the human-machine binomial, enabling appropriate management strategies. Therefore, the major objective of this paper is to evaluate how human-machine integration helps to explain the variability associated with value creation processes. It will be carried out through an action research methodology in two different case studies covering different sectors and having different complexity levels. By covering cases from different sectors and involving different value stream architectures, with different levels of human influence and organisational requirements, it will be possible to assess the transparency increases reached as well as the benefits of analysing processes with higher level of integration between them.",https://doi.org/10.3390/s21175928,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Quantum JIDOKA. Integration of Quantum Simulation on a CNC Machine for In-Process Control Visualization.,1970,"With the advent of the Industry 4.0 paradigm, the possibilities of controlling manufacturing processes through the information provided by a network of sensors connected to work centers have expanded. Real-time monitoring of each parameter makes it possible to determine whether the values yielded by the corresponding sensor are in their normal operating range. In the interplay of the multitude of parameters, deterministic analysis quickly becomes intractable and one enters the realm of ""uncertain knowledge"". Bayesian decision networks are a recognized tool to control the effects of conditional probabilities in such systems. However, determining whether a manufacturing process is out of range requires significant computation time for a decision network, thus delaying the triggering of a malfunction alarm. From its origins, JIDOKA was conceived as a means to provide mechanisms to facilitate real-time identification of malfunctions in any step of the process, so that the production line could be stopped, the cause of the disruption identified for resolution, and ultimately the number of defective parts minimized. Our hypothesis is that we can model the internal sensor network of a computer numerical control (CNC) machine with quantum simulations that show better performance than classical models based on decision networks. We show a successful test of our hypothesis by implementing a quantum digital twin that allows for the integration of quantum computing and Industry 4.0. This quantum digital twin simulates the intricate sensor network within a machine and permits, due to its high computational performance, to apply JIDOKA in real time within manufacturing processes.",https://doi.org/10.3390/s21155031,,Yes,intricate(1)
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Industry 4.0 Quantum Strategic Organizational Design Configurations. The Case of 3 Qubits: Two Report to One.,1970,"The goal of this work is to explore how the relationship between two subordinates reporting to a leader influences the alignment of the latter with the company's strategic objectives in an Industry 4.0 environment. We do this through the implementation of quantum circuits that represent decision networks. In fact, through the quantum simulation of strategic organizational design configurations (QSOD) through five hundred quantum circuit simulations. We conclude that the alignment probability of the leader is never higher than the average alignment value of his subordinates, i.e., the leader never has a better alignment than his subordinates. In other words, the leader cannot present asymptotic stability better than that of his subordinates. The most relevant conclusion of this work is the clear recommendation to the leaders of Industry 4.0 not to add hierarchical levels to their organization if they have not achieved high levels of stability in the lower levels.",https://doi.org/10.3390/e23040426,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Industry 4.0 Quantum Strategic Organizational Design Configurations. The Case of 3 Qubits: One Reports to Two.,1970,"In this work we explore how the relationship between one subordinate reporting to two leaders influences the alignment of the latter with the company's strategic objectives in an Industry 4.0 environment. We do this through the implementation of quantum circuits that represent decision networks. This is done for two cases: One in which the leaders do not communicate with each other, and one in which they do. Through the quantum simulation of strategic organizational design configurations (QSOD) through 500 quantum circuit simulations, we conclude that in the first case both leaders are not simultaneously in alignment, and in the second case that both reporting nodes need to have an alignment probability higher than 90% to support the leader node.",https://doi.org/10.3390/e23030374,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Industry 4.0 Quantum Strategic Organizational Design Configurations. The Case of Two Qubits: One Reports to One.,1970,"In this paper we investigate how the relationship with a subordinate who reports to him influences the alignment of an Industry 4.0 leader. We do this through the implementation of quantum circuits that represent decision networks. In fact, through the quantum simulation of strategic organizational design configurations (QSOD) through five hundred simulations of quantum circuits, we conclude that there is an influence of the subordinate on the leader that resembles that of a harmonic under-damped oscillator around the value of 50% probability of alignment for the leader. Likewise, we have observed a fractal behavior in this type of relationship, which seems to conjecture that there is an exchange of energy between the two agents that oscillates with greater or lesser amplitude depending on certain parameters of interdependence. Fractality in this QSOD context allows for a quantification of these complex dynamics and its pervasive effect offers robustness and resilience to the two-",https://doi.org/10.3390/s20236977,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Quantum Strategic Organizational Design: Alignment in Industry 4.0 Complex-Networked Cyber-Physical Lean Management Systems.,1970,"The strategic design of organizations in an environment where complexity is constantly increasing, as in the cyber-physical systems typical of Industry 4.0, is a process full of uncertainties. Leaders are forced to make decisions that affect other organizational units without being sure that their decisions are the right ones. Previously to this work, genetic algorithms were able to calculate the state of alignment of industrial processes that were measured through certain key performance indicators (KPIs) to ensure that the leaders of the Industry 4.0 make decisions that are aligned with the strategic objectives of the organization. However, the computational cost of these algorithms increases exponentially with the number of KPIs. That is why this work makes use of the principles of quantum computing to present the strategic design of organizations from a novel point of view: Quantum Strategic Organizational Design (QSOD). The effectiveness of the application of these principles is shown with a real case study, in which the computing time is reduced from hundreds of hours to seconds. This has very powerful practical applications for industry leaders, since, with this new approach, they can potentially allow a better understanding of the complex processes underlying the strategic design of organizations and, above all, make decisions in real-time.",https://doi.org/10.3390/s20205856,,Yes,potent(1)
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Data Handling in Industry 4.0: Interoperability Based on Distributed Ledger Technology.,1970,"Information-intensive transformation is vital to realize the Industry 4.0 paradigm, where processes, systems, and people are in a connected environment. Current factories must combine different sources of knowledge with different technological layers. Taking into account data interconnection and information transparency, it is necessary to enhance the existing frameworks. This paper proposes an extension to an existing framework, which enables access to knowledge about the different data sources available, including data from operators. To develop the interoperability principle, a specific proposal to provide a (public and encrypted) data management solution to ensure information transparency is presented, which enables semantic data treatment and provides an appropriate context to allow data fusion. This proposal is designed also considering the Privacy by Design option. As a proof of application case, an implementation was carried out regarding the logistics of the delivery of industrial components in the construction sector, where different stakeholders may benefit from shared knowledge under the proposed architecture.",https://doi.org/10.3390/s20113046,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Industry 4.0 Lean Shopfloor Management Characterization Using EEG Sensors and Deep Learning.,1970,"Achieving the shift towards Industry 4.0 is only feasible through the active integration of the shopfloor into the transformation process. Several shopfloor management (SM) systems can aid this conversion. They form two major factions. The first includes methodologies such as Balanced Scorecard (BSC). A defining feature is rigid structures to fixate on pre-defined goals. Other SM strategies instead concentrate on continuous improvement by giving directions. An example of this group is the ""HOSHIN KANRI TREE"" (HKT). One way of analyzing the dissimilarities, the advantages and disadvantages of these groups, is to examine the neurological patterns of workers as they are applying these. This paper aims to achieve this evaluation through non-invasive electroencephalography (EEG) sensors, which capture the electrical activity of the brain. A deep learning (DL) soft sensor is used to classify the recorded data with an accuracy of 96.5%. Through this result and an analysis using the correlations of the EEG signals, it has been possible to detect relevant characteristics and differences in the brain's activity. In conclusion, these findings are expected to help assess SM systems and give guidance to Industry 4.0 leaders.",https://doi.org/10.3390/s20102860,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Geometric Deep Lean Learning: Deep Learning in Industry 4.0 Cyber-Physical Complex Networks.,1970,"In the near future, value streams associated with Industry 4.0 will be formed by interconnected cyber-physical elements forming complex networks that generate huge amounts of data in real time. The success or failure of industry leaders interested in the continuous improvement of lean management systems in this context is determined by their ability to recognize behavioral patterns in these big data structured within non-Euclidean domains, such as these dynamic sociotechnical complex networks. We assume that artificial intelligence in general and deep learning in particular may be able to help find useful patterns of behavior in 4.0 industrial environments in the lean management of cyber-physical systems. However, although these technologies have meant a paradigm shift in the resolution of complex problems in the past, the traditional methods of deep learning, focused on image or video analysis, both with regular structures, are not able to help in this specific field. This is why this work focuses on proposing geometric deep lean learning, a mathematical methodology that describes deep-lean-learning operations such as convolution and pooling on cyber-physical Industry 4.0 graphs. Geometric deep lean learning is expected to positively support sustainable organizational growth because customers and suppliers ought to be able to reach new levels of transparency and traceability on the quality and efficiency of processes that generate new business for both, hence generating new products, services, and cooperation opportunities in a cyber-physical environment.",https://doi.org/10.3390/s20030763,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Indoor Air-Quality Data-Monitoring System: Long-Term Monitoring Benefits.,1970,"Indoor air pollution has been ranked among the top five environmental risks to public health. Indoor Air Quality (IAQ) is proven to have significant impacts on people's comfort, health, and performance. Through a systematic literature review in the area of IAQ, two gaps have been identified by this study: short-term monitoring bias and IAQ data-monitoring solution challenges. The study addresses those gaps by proposing an Internet of Things (IoT) and Distributed Ledger Technologies (DLT)-based IAQ data-monitoring system. The developed data-monitoring solution allows for the possibility of low-cost, long-term, real-time, and summarized IAQ information benefiting all stakeholders contributing to define a rich context for Industry 4.0. The solution helps the penetration of Industrial Internet of Things (IIoT)-based monitoring strategies in the specific case of Occupational Safety Health (OSH). The study discussed the corresponding benefits OSH regulation, IAQ managerial, and transparency perspectives based on two case studies conducted in Spain.",https://doi.org/10.3390/s19194157,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Deep Learning for Industrial Computer Vision Quality Control in the Printing Industry 4.0.,1970,"Rapid and accurate industrial inspection to ensure the highest quality standards at a competitive price is one of the biggest challenges in the manufacturing industry. This paper shows an application of how a Deep Learning soft sensor application can be combined with a high-resolution optical quality control camera to increase the accuracy and reduce the cost of an industrial visual inspection process in the Printing Industry 4.0. During the process of producing gravure cylinders, mistakes like holes in the printing cylinder are inevitable. In order to improve the defect detection performance and reduce quality inspection costs by process automation, this paper proposes a deep neural network (DNN) soft sensor that compares the scanned surface to the used engraving file and performs an automatic quality control process by learning features through exposure to training data. The DNN sensor developed achieved a fully ",https://doi.org/10.3390/s19183987,,No,
0000-0002-2423-1474,Javier Villalba-Diez,Hochschule Heilbronn,Characterization of Industry 4.0 Lean Management Problem-Solving Behavioral Patterns Using EEG Sensors and Deep Learning.,1970,"Industry 4.0 leaders solve problems all of the time. Successful problem-solving behavioral pattern choice determines organizational and personal success, therefore a proper understanding of the problem-solving-related neurological dynamics is sure to help increase business performance. The purpose of this paper is two-fold: first, to discover relevant neurological characteristics of problem-solving behavioral patterns, and second, to conduct a characterization of two problem-solving behavioral patterns with the aid of deep-learning architectures. This is done by combining electroencephalographic non-invasive sensors that capture process owners' brain activity signals and a deep-learning soft sensor that performs an accurate characterization of such signals with an accuracy rate of over 99% in the presented case-study dataset. As a result, the deep-learning characterization of lean management (LM) problem-solving behavioral patterns is expected to help Industry 4.0 leaders in their choice of adequate manufacturing systems and their related problem-solving methods in their future pursuit of strategic organizational goals.",https://doi.org/10.3390/s19132841,,No,
0000-0002-7048-5088,Wendelin Schramm,Hochschule Heilbronn,"Explorative cost-effectiveness analysis of colorectal cancer recurrence detection with next-generation sequencing liquid biopsy in Spain, France, and Germany.",1970,"Next-generation sequencing liquid biopsy (NGS-LB) for colorectal cancer (CRC) detection and surveillance remains an expensive technology as economies of scale have not yet been realized. Nevertheless, the cost of sequencing has decreased while sensitivity has increased, raising the question of whether cost-effectiveness (CE) has already been achieved from the perspective of European healthcare systems.",https://doi.org/10.1177/17562848241248246,,No,
0000-0002-7048-5088,Wendelin Schramm,Hochschule Heilbronn,Assessment of SARS-CoV-2 Infection among Healthcare Workers of a German COVID-19 Treatment Center.,1970,"To date, more than 160 million people have been infected with COVID-19 worldwide. In the present study, we investigated the history of SARS-CoV-2 infection among 3067 healthcare workers (HCW) in a German COVID-19 treatment center during the early phase of the pandemic (July 2020) based on the seroprevalence of SARS-CoV-2 antibodies and self-reported previous PCR results. The results demonstrate a low prevalence of SARS-CoV-2 infection (",https://doi.org/10.3390/ijerph18137057,,No,
0000-0002-7048-5088,Wendelin Schramm,Hochschule Heilbronn,Benefit of Digital Tools Used for Integrated Personalized Diabetes Management: Results From the PDM-ProValue Study Program.,1970,"Integrated personalized diabetes management (iPDM) is a digitally supported therapeutic concept to improve patient-physician interaction to overcome the aspects of clinical inertia. Integrated personalized diabetes management can support decision making and improve therapeutic outcomes of suboptimally controlled persons with insulin-treated type 2 diabetes (T2D). In this paper, we report the results of an analysis of the PDM-ProValue study program on the effectiveness and perceived benefit of this approach, with a focus on how physicians used and assessed the digital tools provided for the iPDM process.",https://doi.org/10.1177/1932296819867686,,No,
0000-0002-7048-5088,Wendelin Schramm,Hochschule Heilbronn,Digital Diabetes Self-Management: A Trilateral Serial.,1970,"The way diabetes patients cope with their disease in day-to-day routines is decisive for the development or the prevention of medical complications. Smartphones have created the ubiquitous environment to support health care with mobile applications (mHealth). This article comments on the publication by Offringa et al in JDST, which is one of few studies that tries to isolate the effects of a diabetes app. At the same time, it is a good example to discuss general aspects of mHealth in diabetes care. Treatment context, eHealth literacy, interoperability, and efficiency will determine the success of diabetes apps. The development has not yet reached its end. A triple quality feedback loop linking persons with diabetes, health care providers, and mHealth providers is suggested.",https://doi.org/10.1177/1932296818761973,,No,
0000-0002-9249-6810,Christina Wacker,Frankfurt University of Applied Sciences,Approaching the Causes of Unintentional Injuries in the School Environment: A Panel Analysis of Survey Data From Germany.,1970,"Previous research on the correlates of unintentional school injuries is based on either process or cross-sectional data. This study aims at approaching the causal effects of risk-seeking behavior, mental health problems, physical activity, and exposure to bullying on unintentional injuries in the school environment by relying on longitudinal survey data.",https://doi.org/10.1111/josh.13112,,No,
0000-0003-3717-7929,Tobias Hagen,Frankfurt University of Applied Sciences,Evaluation of a Placement Coaching Program for Recipients of Disability Insurance Benefits in Switzerland.,1970,"Purpose During 2009‒2013 a pilot project was carried out in Zurich which aimed to increase the income of disability insurance (DI) benefit recipients in order to reduce their entitlement to DI benefits. The project consisted of placement coaching carried out by a private company that specialized in this field. It was exceptional with respect to three aspects: firstly, it did not include any formal training and/or medical aid; secondly, the coaches did not have the possibility of providing additional financial incentives or sanctioning lack of effort; and thirdly due to performance bonuses, the company not only had incentives to bring the participants into (higher paid) work, but also to keep them there for 52 weeks. This paper estimates the medium-run effects of the pilot project and assesses the net benefit from the Swiss social security system. Methods Different propensity score matching estimators are applied to administrative longitudinal data in order to construct suitable control groups. Results The estimates indicate a reduction in DI benefits and an increase in income even in the medium-run. A simple cost-benefit analysis suggests that the pilot project was a profitable investment for the social security system. Conclusion Given a healthy labor market, it seems possible to enhance the employment prospects of disabled persons with a relatively inexpensive intervention, which does not include any explicit investments in human capital.",https://doi.org/10.1007/s10926-018-9766-x,,No,
0000-0002-5343-4940,Andreas Ladurner,Hochschule Aalen,[Not Available].,1970,Kein Abstract verfügbar,https://doi.org/10.1007/s00350-021-6097-2,,No,
0000-0001-9226-3649,Denise Wetzel,Hochschule Aalen,Benchmarking Visual Performance with Monofocal Intraocular Lenses with and without Enhanced Optical Properties in a Nighttime Driving Simulator Environment: A Proof-of-Concept Study.,1970,"The purpose of this study is to introduce a method for benchmarking intraocular lenses during driving activities under highly standardized conditions, specifically with regard to visual acuity (VA) and contrast sensitivity (CS). Therefore, patients with intraocular lens (IOL) implants ICB00 (Tecnis Eyhance, Johnson & Johnson, Santa Ana, CA, USA) vs. CNA0T0 (Clareon, Alcon Laboratories Inc., Fort Worth, TX, USA) were examined clinically and in a nighttime driving simulator.",https://doi.org/10.1055/a-1773-1197,,No,
0000-0001-9226-3649,Denise Wetzel,Aalen University of Applied Sciences,Dissociation between red and white stimulus perception: A perimetric quantification of protanopic color vision deficiencies.,1970,"Horizontal visual field extension was assessed for red and white stimuli in subjects with protanopia using semi-automated kinetic perimetry. In contrast to a conventional anomaloscope, the ""red/white dissociation ratio"" (RWR) allows to describe protanopia numerically. For the majority of subjects with protanopia a restriction for faint red stimuli was found.",https://doi.org/10.1371/journal.pone.0260362,,No,
0009-0007-3358-4152,Kerstin Rieder,Aalen University of Applied Sciences,Evaluation of two study demands-resources-based interventions: a randomized controlled trial.,1970,"Higher education students experience significant levels of exhaustion in their studies, yet there are limited evidence-based support programs available. Therefore, this study evaluated a novel intervention approach by testing the effectiveness of two online interventions based on the study demands-resources framework. These interventions aimed to balance demands and resources. Derived from the theoretical assumptions of the framework, we hypothesized that the interventions would increase study and personal resources, engagement, and study crafting, and decrease study demands, exhaustion, and self-undermining. Additionally, we hypothesized that demands and resources would mediate the effects of the intervention on engagement, exhaustion, study crafting, and self-undermining.",https://doi.org/10.3389/fpsyg.2024.1368267,,No,
0009-0007-3358-4152,Kerstin Rieder,Hochschule Aalen,[Exposing occupational stress].,1970,Kein Abstract verfügbar,Kein DOI-Link verfügbar,,No,
0000-0003-4005-7354,Markus Merkel,Aalen University of Applied Sciences,Additive Manufacturing of WC-Co Specimens with Internal Channels.,1970,"Most material removal in modern manufacturing is currently performed using tools with indexable inserts. Additive manufacturing allows for the creation of new, experimental insert shapes and, more importantly, internal structures, such as channels for coolant. This study deals with developing a process for efficiently manufacturing WC-Co specimens with internal coolant channels with a focus on obtaining a suitable microstructure and surface finish, especially inside the channels. The first part of this study covers the development of process parameters to achieve a microstructure without cracks and with minimal porosity. The next stage focuses solely on improving the surface quality of the parts. Special attention is given to the internal channels, where true surface area and surface quality are evaluated, as these characteristics greatly influence coolant flow. To conclude, WC-Co specimens were successfully manufactured and a microstructure with low porosity and no cracks was achieved and an effective parameter set was found. We have developed a process that produces parts with a surface roughness comparable to those of standard SLS manufacturing of steel parts, while still providing a high-quality internal microstructure. The most suitable parameter set resulted in a profile surface roughness of Ra 4 μm and Rz 31 μm and areal surface roughness of Sa 7 µm and Sz 125 µm.",https://doi.org/10.3390/ma16113907,,No,
0000-0003-4005-7354,Markus Merkel,Aalen University of Applied Sciences,Heat Treatments and Critical Quenching Rates in Additively Manufactured Al-Si-Mg Alloys.,1970,"Laser powder-bed fusion (LPBF) has significantly gained in importance and has become one of the major fabrication techniques within metal additive manufacturing. The fast cooling rates achieved in LPBF due to a relatively small melt pool on a much larger component or substrate, acting as heat sink, result in fine-grained microstructures and high oversaturation of alloying elements in the α-aluminum. Al-Si-Mg alloys thus can be effectively precipitation hardened. Moreover, the solidified material undergoes an intrinsic heat treatment, whilst the layers above are irradiated and the elevated temperature in the built chamber starts the clustering process of alloying elements directly after a scan track is fabricated. These silicon-magnesium clusters were observed with atom probe tomography in as-built samples. Similar beneficial clustering behavior at higher temperatures is known from the direct-aging approach in cast samples, whereby the artificial aging is performed immediately after solution annealing and quenching. Transferring this approach to LPBF samples as a possible post-heat treatment revealed that even after direct aging, the outstanding hardness of the as-built condition could, at best, be met, but for most instances it was significantly lower. Our investigations showed that LPBF Al-Si-Mg exhibited a high dependency on the quenching rate, which is significantly more pronounced than in cast reference samples, requiring two to three times higher quenching rate after solution annealing to yield similar hardness results. This suggests that due to the finer microstructure and the shorter diffusion path in Al-Si-Mg fabricated by LPBF, it is more challenging to achieve a metastable oversaturation necessary for precipitation hardening. This may be especially problematic in larger components.",https://doi.org/10.3390/ma13030720,,No,
0000-0003-4005-7354,Markus Merkel,Aalen University of Applied Sciences,On the Anisotropic Mechanical Properties of Selective Laser-Melted Stainless Steel.,1970,"The thorough description of the peculiarities of additively manufactured (AM) structures represents a current challenge for aspiring freeform fabrication methods, such as selective laser melting (SLM). These methods have an immense advantage in the fast fabrication (no special tooling or moulds required) of components, geometrical flexibility in their design, and efficiency when only small quantities are required. However, designs demand precise knowledge of the material properties, which in the case of additively manufactured structures are anisotropic and, under certain circumstances, inhomogeneous in nature. Furthermore, these characteristics are highly dependent on the fabrication settings. In this study, the anisotropic tensile properties of selective laser-melted stainless steel (1.4404, 316L) are investigated: the Young's modulus ranged from 148 to 227 GPa, the ultimate tensile strength from 512 to 699 MPa, and the breaking elongation ranged, respectively, from 12% to 43%. The results were compared to related studies in order to classify the influence of the fabrication settings. Furthermore, the influence of the chosen raw material was addressed by comparing deviations on the directional dependencies reasoned from differing microstructural developments during manufacture. Stainless steel was found to possess its maximum strength at a 45° layer versus loading offset, which is precisely where AlSi10Mg was previously reported to be at its weakest.",https://doi.org/10.3390/ma10101136,,No,
0000-0003-1632-7334,Laura Haase,Berlin School of Economics and Law,User Drafts for the Design of an mHealth Application for Equestrians.,1970,A study was conducted to find out user ideas for the design and workflow of an mHealth application supporting triage of horses by equestrians. Results focus on information input elements and also include some requirements for information.,https://doi.org/10.3233/SHTI230800,,No,
0000-0003-1632-7334,Laura Haase,Berlin School of Economics and Law,Analysis of the Usage Context of an mHealth Application for Equestrians.,1970,"One possibility to support veterinarians in times of a vet shortage is by providing animal owners with a technical decision support for deciding whether their animal needs to be seen by a vet. As the first step in the user-centered development of such an mHealth application for equestrians, an analysis of the context of use was done.",https://doi.org/10.3233/SHTI230702,,No,
0000-0003-1632-7334,Laura Haase,Berlin School of Economics and Law,Requirements to mHealth Applications for Animal Owners: A Narrative Review.,1970,"A narrative literature review was conducted to determine software requirements to mHealth applications for animal owners. Results focus on securing a complete user input, providing adequately formulated information to the users and ensuring the understanding of the applications limitations.",https://doi.org/10.3233/SHTI230332,,No,
0000-0002-6652-9535,Alain Loh,Hamburg University of Applied Sciences,Multivariate Analysis of Photoacoustic Spectra for the Detection of Short-Chained Hydrocarbon Isotopologues.,1970,"We report, to our knowledge, the first optical detection scheme for short-chained hydrocarbon isotopologues. The sensor system is based on photoacoustic spectroscopy (PAS). Two continuous wave, thermoelectrically cooled, distributed feedback interband cascade lasers (DFB-ICLs) with emission wavelengths around 3.33 and 3.38 μm, respectively, served as light sources. The investigations comprised the main stable carbon isotopologues of methane (",https://doi.org/10.3390/molecules25092266,,No,
0000-0002-6040-8337,Anja Dessauvagie,Hamburg University of Applied Sciences,Development of a Mobile Application for Detection of Adolescent Mental Health Problems and Feasibility Assessment with Primary Health Care Workers.,1970,,https://doi.org/10.1080/01612840.2022.2124003,,No,
0000-0002-6040-8337,Anja Dessauvagie,Hamburg University of Applied Sciences,Mental Health of University Students in Southeastern Asia: A Systematic Review.,1970,"Mental health in young people is a public health challenge worldwide, with around one-fifth of university students suffering from a 12-month mental disorder. In low- and middle-income countries (LMICs) of Southeastern Asia, resources for mental health are limited and counseling services are not regularly established at universities. This review aims to determine the prevalence of mental health problems among university students in six ASEAN (Association of Southeast Asian Nations) countries (Cambodia, Laos, Malaysia, Myanmar, Thailand, and Vietnam) and to identify the determinants of mental health. A systematic database search (PubMed, CINAHL, PsycINFO, PubPsych, and Scopus) for peer-reviewed, English language articles, published 2010-2020, reporting prevalence data based on standardized screening instruments resulted in 335 articles; 108 were eligible for full-text analysis, of which 34 could be included in the review. Median point prevalence was 29.4% for depression, 42.4% for anxiety, 16.4% for stress, and 13.9% for disordered eating. Current suicidality was present in 7% to 8% of students. There was a high rate of psychiatric comorbidity. Despite the high prevalence of mental health problems, the willingness to seek professional help was comparatively low. Implications for mental health promotion and prevention in university settings are discussed.",https://doi.org/10.1177/10105395211055545,,No,
0000-0001-7310-7253,Cintia Nunes,Hamburg University of Applied Sciences,Planetary Health and Health Education in Brazil: Towards Better Trained Future Health Professionals.,1970,"Brazil is Latin America's largest country and has a strong economy, but it is also characterised by many inequalities. These are very conspicuous in the health sector, particularly in health education, which is expected to modernise according to the planetary health (PH) perspective. This paper describes the health education scenario in Brazil and undertakes an analysis of the postgraduate health programmes and policies in place, identifying the extent to which these support the cause of PH. To achieve this goal, this paper deploys a bibliometric analysis to gain a better understanding of the research streams related to higher education and PH. In addition, it presents and discusses selected case studies in the field and cross-checks documents from the Brazilian Ministry of Education against five domains of PH in education. The results indicate that despite some progress to date and the fact that some programmes are in place, there is a perceived need for policies and efforts from education organisations towards connecting PH principles in the education of current and future health professionals.",https://doi.org/10.3390/ijerph191610041,,No,
0000-0002-9678-2259,York Zoellner,Hamburg University of Applied Sciences,The role of galenic innovation in improving treatment compliance and persistence: three case studies.,1970,The purpose of this study was to explore whether newer galenic formulations with lower treatment burdens are associated with better patient compliance and persistence compared with older more burdensome modalities.,https://doi.org/10.2147/CEOR.S23158,,No,
0000-0001-9947-8722,Gunter Groen,Hamburg University of Applied Sciences,Development of a Mobile Application for Detection of Adolescent Mental Health Problems and Feasibility Assessment with Primary Health Care Workers.,1970,,https://doi.org/10.1080/01612840.2022.2124003,,No,
0000-0001-9947-8722,Gunter Groen,Hamburg University of Applied Sciences,Mental Health of University Students in Southeastern Asia: A Systematic Review.,1970,"Mental health in young people is a public health challenge worldwide, with around one-fifth of university students suffering from a 12-month mental disorder. In low- and middle-income countries (LMICs) of Southeastern Asia, resources for mental health are limited and counseling services are not regularly established at universities. This review aims to determine the prevalence of mental health problems among university students in six ASEAN (Association of Southeast Asian Nations) countries (Cambodia, Laos, Malaysia, Myanmar, Thailand, and Vietnam) and to identify the determinants of mental health. A systematic database search (PubMed, CINAHL, PsycINFO, PubPsych, and Scopus) for peer-reviewed, English language articles, published 2010-2020, reporting prevalence data based on standardized screening instruments resulted in 335 articles; 108 were eligible for full-text analysis, of which 34 could be included in the review. Median point prevalence was 29.4% for depression, 42.4% for anxiety, 16.4% for stress, and 13.9% for disordered eating. Current suicidality was present in 7% to 8% of students. There was a high rate of psychiatric comorbidity. Despite the high prevalence of mental health problems, the willingness to seek professional help was comparatively low. Implications for mental health promotion and prevention in university settings are discussed.",https://doi.org/10.1177/10105395211055545,,No,
0000-0003-1623-5309,Marina Tropmann-Frick,Hamburg University of Applied Sciences,Robot-assisted partial nephrectomy: A single-center matched-pair analysis of the retroperitoneal versus the transperitoneal approach.,1970,Comparison of the retroperitoneal (RRPN) perioperative variables and the transperitoneal (TRPN) robot-assisted partial nephrectomy (RPN) using a matched-pair analysis.,https://doi.org/10.5152/tud.2021.21008,,No,
0000-0003-1623-5309,Marina Tropmann-Frick,Hamburg University of Applied Sciences,Language Representation Models: An Overview.,1970,"In the last few decades, text mining has been used to extract knowledge from free texts. Applying neural networks and deep learning to natural language processing (NLP) tasks has led to many accomplishments for real-world language problems over the years. The developments of the last five years have resulted in techniques that have allowed for the practical application of transfer learning in NLP. The advances in the field have been substantial, and the milestone of outperforming human baseline performance based on the general language understanding evaluation has been achieved. This paper implements a targeted literature review to outline, describe, explain, and put into context the crucial techniques that helped achieve this milestone. The research presented here is a targeted review of neural language models that present vital steps towards a general language representation model.",https://doi.org/10.3390/e23111422,,No,
0000-0003-1623-5309,Marina Tropmann-Frick,Hamburg University of Applied Sciences,Retroperitoneal Versus Transperitoneal Robotic Partial Nephrectomy: A Multicenter Matched-pair Analysis.,1970,"With increasing acceptance of robotic partial nephrectomy over the last decade, there is an ongoing discussion about the transperitoneal versus retroperitoneal access.",https://doi.org/10.1016/j.euf.2020.08.012,,No,
0000-0002-6597-4960,Claudia Limmer,Hamburg University of Applied Sciences,Key dimensions of women's and their partners' experiences of childbirth: A systematic review of reviews of qualitative studies.,1970,"The World Health Organization 2018 intrapartum guideline for a positive birth experience emphasized the importance of maternal emotional and psychological well-being during pregnancy and the need for safe childbirth. Today, in many countries birth is safe, yet many women report negative and traumatic birth experiences, with adverse effects on their and their families' well-being. Many reviews have attempted to understand the complexity of women's and their partners' birth experience; however, it remains unclear what the key dimensions of the birth experience are.",https://doi.org/10.1371/journal.pone.0299151,,No,
0000-0002-9724-5586,Franziska Wolf,Hamburg University of Applied Sciences,Climate change adaptation responses among riparian settlements: A case study from Bangladesh.,1970,"As transition areas between aquatic ecosystems and the adjacent terrestrial ones, riparian regions are highly exposed to coastal climate hazards. This article describes how climate change and extreme weather impact vulnerable riparian communities and settlements. The analysis is done by reviewing past research and empirical case studies from riparian rural communities of the impact zone of the Sundarbans in Bangladesh, the world's most extensive mangrove forest. The article discusses the climate-related impacts on households through a Severity Index of Vulnerability and assesses the adaptation responses they may pursue. The principal climate-related vulnerabilities and impacts due to increases in temperature, storm surges, sea flooding, and sea-level rise are seawater intrusion and riverbank erosion. Many households have adopted several autonomous reactive adaptation strategies rather than planned ones, to cope with these impacts. However, government organisations and NGOs provide less than optimal technical and financial support to households for planned and anticipatory adaptive responses. The main barriers to adaptation were the high cost of improved crop varieties, inadequate agricultural extension services, and a lack of knowledge on effective climate adaptation. The restoration of the mangrove ecosystem may increase its resilience and, among other things, make local communities less exposed. The article also presents some adaptation measures proper to reduce the climate-related vulnerability of riparian settlements.",https://doi.org/10.1371/journal.pone.0278605,,No,
0000-0002-9724-5586,Franziska Wolf,Hamburg University of Applied Sciences,Heading towards an unsustainable world: some of the implications of not achieving the SDGs.,1970,"The Sustainable Development Goals (SDGs) were conceived at the United Nations Conference on Sustainable Development, held in Rio de Janeiro in 2012 (Rio + 20), and adopted by the United Nations General Assembly in September 2015. They are part of a larger framework, namely the UN 2030 Agenda for Sustainable Development. Since then, many countries round the world have been engaging in respect of their implementation. The slow progress seen in the implementation of the SDGs, is in contrast with the many negative implications of not implementing them. This paper outlines the relevance of the SDGs, the barriers currently seen in respect of their implementation and outlines what is at stake, if they are not duly implemented. To accomplish this, a thorough literature review of contributions published in the field of SDGs in English between the years 2012-2020 was performed.",https://doi.org/10.1007/s43621-020-00002-x,,No,
0000-0001-6371-3073,Ulf Baumgärtner,MSH Medical School Hamburg University of Applied Sciences and Medical University,Review of techniques useful for the assessment of sensory small fiber neuropathies: Report from an IFCN expert group.,1970,"Nerve conduction studies (NCS) are an essential aspect of the assessment of patients with peripheral neuropathies. However, conventional NCS do not reflect activation of small afferent fibers, including Aδ and C fibers. A definitive gold standard for laboratory evaluation of these fibers is still needed and therefore, clinical evaluation remains fundamental in patients with small fiber neuropathies (SFN). Several clinical and research techniques have been developed for the assessment of small fiber function, such as (i) microneurography, (ii) laser evoked potentials, (iii) contact heat evoked potentials, (iv) pain-related electrically evoked potentials, (v) quantitative thermal sensory testing, (vi) skin biopsy-intraepidermal nerve fiber density and (vii) corneal confocal microscopy. The first five are physiological techniques, while the last two are morphological. They all have advantages and limitations, but the combined use of an appropriate selection of each of them would lead to gathering invaluable information for the diagnosis of SFN. In this review, we present an update on techniques available for the study of small afferent fibers and their clinical applicability. A summary of the anatomy and important physiological aspects of these pathways, and the clinical manifestations of their dysfunction is also included, in order to have a minimal common background.",https://doi.org/10.1016/j.clinph.2022.01.002,,Yes,"invaluable(1), potent(3)"
0000-0003-3230-9582,Thomas Martens,MSH Medical School Hamburg University of Applied Sciences and Medical University,Evaluating student's ability to assess treatment claims: validating a German version of the Claim Evaluation Tools.,1970,"The Claim Evaluation Tools measure the ability to assess claims about treatment effects. The aim of this study was to adapt the German item sets to the target group of secondary school students (aged 11 to 16 years, grade 6 to 10) and to validate them accordingly. The scale's reliability and validity using Rasch's probabilistic test theory should be determined.",https://doi.org/10.1186/s12889-022-14700-w,,No,
0000-0003-3416-6829,Christoph Boehmert,IU International University of Applied Sciences,Affective evaluation and exposure perception of everyday mobile phone usage situations.,1970,"To understand citizens' reactions to the 5G rollout, their affective reaction and perception of radiofrequency electromagnetic fields (RF-EMF) exposure are of interest. Although precursor studies on 2G-4G have investigated exposure perception mostly quantitatively, the present study applied a qualitative exploratory approach. A number of 35 individual interviews and 6 focus groups with the same participants were conducted in December 2022. Participants were recruited from several locations in Germany, where 5G rollout was at different stages. Interactive tasks, particularly an affective evaluation task and a ranking task, encouraged participants to consider their affect regarding mobile communications and their exposure perception. This approach allowed the participants to first engage with the topic of mobile communications/5G in an intuitive way, before talking about their specific beliefs on RF-EMF exposure. Several pictures showing a person (1) interacting with a mobile phone, (2) surrounded by other peoples' mobile phones, or (3) in the vicinity of mobile phone base stations (antennas) were used as stimulus materials. Data were analyzed using an exploratory content analysis. In the affective evaluation task participants revealed more negative associations with base stations than with mobile phones. The analysis showed that the reasons for their evaluation were very diverse, whereby exposure to RF-EMF only played a subordinate role. Further, the ranking task indicated that most participants (n = 20) felt more exposed from base stations than from mobile devices. Results are mostly in-line with the literature on 2G-4G and do not indicate a substantially different exposure perception for 5G.",https://doi.org/10.1111/risa.17641,,No,
