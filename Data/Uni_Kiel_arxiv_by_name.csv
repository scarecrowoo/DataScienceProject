Authors,PubDate,Title,Abstract,IDlist
"['Christian Jung', 'Claudia Redenbach']",2022-10-11T02:12:11Z,Crack Modeling via Minimum-Weight Surfaces in 3d Voronoi Diagrams,"  Shortest paths play an important role in mathematical modeling and image processing. Usually, shortest path problems are formulated on planar graphs that consist of vertices and weighted arcs. In this context, one is interested in finding a path of minimum weight from a start vertex to an end vertex. The concept of minimum-weight surfaces extends shortest paths to 3d. The minimum-weight surface problem is formulated on a cellular complex with weighted facets. A cycle on the arcs of the complex serves as input and one is interested in finding a surface of minimum weight bounded by that cycle. In practice, minimum-weight surfaces can be used to segment 3d images. Vice versa, it is possible to use them as a modeling tool for geometric structures such as cracks. In this work, we present an approach for using minimum-weight surfaces in bounded Voronoi diagrams to generate synthetic 3d images of cracks. ",2210.05093v1
"['Christian Jung', 'Claudia Redenbach']",2023-03-27T15:03:00Z,An Analytical Representation of the 2d Generalized Balanced Power   Diagram,"  Tessellations are an important tool to model the microstructure of cellular and polycrystalline materials. Classical tessellation models include the Voronoi diagram and Laguerre tessellation whose cells are polyhedra. Due to the convexity of their cells, those models may be too restrictive to describe data that includes possibly anisotropic grains with curved boundaries. Several generalizations exist. The cells of the generalized balanced power diagram are induced by elliptic distances leading to more diverse structures. So far, methods for computing the generalized balanced power diagram are restricted to discretized versions in the form of label images. In this work, we derive an analytic representation of the vertices and edges of the generalized balanced power diagram in 2d. Based on that, we propose a novel algorithm to compute the whole diagram. ",2303.15275v1
"['Tin Barisin', 'Christian Jung', 'Franziska Müsebeck', 'Claudia Redenbach', 'Katja Schladitz']",2021-12-17T13:02:30Z,Methods for segmenting cracks in 3d images of concrete: A comparison   based on semi-synthetic images,"  Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast. ",2112.09493v1
"['Christian Jung', 'Daniel Karch', 'Sebastian Knopp', 'Dennis Luxen', 'Peter Sanders']",2011-02-16T11:16:51Z,Efficient Error-Correcting Geocoding,  We study the problem of resolving a perhaps misspelled address of a location into geographic coordinates of latitude and longitude. Our data structure solves this problem within a few milliseconds even for misspelled and fragmentary queries. Compared to major geographic search engines such as Google or Bing we achieve results of significantly better quality. ,1102.3306v1
"['Johannes Brünger', 'Maria Gentz', 'Imke Traulsen', 'Reinhard Koch']",2020-05-21T07:36:03Z,Panoptic Instance Segmentation on Pigs,"  The behavioural research of pigs can be greatly simplified if automatic recognition systems are used. Especially systems based on computer vision have the advantage that they allow an evaluation without affecting the normal behaviour of the animals. In recent years, methods based on deep learning have been introduced and have shown pleasingly good results. Especially object and keypoint detectors have been used to detect the individual animals. Despite good results, bounding boxes and sparse keypoints do not trace the contours of the animals, resulting in a lot of information being lost. Therefore this work follows the relatively new definition of a panoptic segmentation and aims at the pixel accurate segmentation of the individual pigs. For this a framework of a neural network for semantic segmentation, different network heads and postprocessing methods is presented. With the resulting instance segmentation masks further information like the size or weight of the animals could be estimated. The method is tested on a specially created data set with 1000 hand-labeled images and achieves detection rates of around 95% (F1 Score) despite disturbances such as occlusions and dirty lenses. ",2005.10499v1
"[""Francesco D'Angelo"", 'Christian Henning']",2021-10-12T14:11:37Z,On out-of-distribution detection with Bayesian neural networks,"  The question whether inputs are valid for the problem a neural network is trying to solve has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNNs) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and show that proper Bayesian inference with function space priors induced by neural networks does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact due to the correspondence with Gaussian processes. Strikingly, the kernels derived from common architectural choices lead to function space priors which induce predictive uncertainties that do not reflect the underlying input data distribution and are therefore unsuited for OOD detection. Importantly, we find the OOD behavior in this limiting case to be consistent with the corresponding finite-width case. To overcome this limitation, useful function space properties can also be encoded in the prior in weight space, however, this can currently only be applied to a specified subset of the domain and thus does not inherently extend to OOD data. Finally, we argue that a trade-off between generalization and OOD capabilities might render the application of BNNs for OOD detection undesirable in practice. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research. ",2110.06020v2
"['Christian Henning', ""Francesco D'Angelo"", 'Benjamin F. Grewe']",2021-07-26T14:53:14Z,Are Bayesian neural networks intrinsically good at out-of-distribution   detection?,"  The need to avoid confident predictions on unfamiliar data has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNN) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and provide empirical evidence that proper Bayesian inference with common neural network architectures does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact considering the corresponding Gaussian process. Strikingly, the kernels induced under common architectural choices lead to uncertainties that do not reflect the underlying data generating process and are therefore unsuited for OOD detection. Finally, we study finite-width networks using HMC, and observe OOD behavior that is consistent with the infinite-width case. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research. ",2107.12248v1
"['Jeffrey Wrighton', 'James Dufty', 'Christian Henning', 'Michael Bonitz']",2008-09-18T06:23:02Z,Linear Response for Confined Particles,  The dynamics of fluctuations is considered for electrons near a positive ion or for charges in a confining trap. The stationary nonuniform equilibrium densities are discussed and contrasted. The linear response function for small perturbations of this nonuniform state is calculated from a linear Markov kinetic theory whose generator for the dynamics is exact in the short time limit. The kinetic equation is solved in terms of an effective mean field single particle dynamics determined by the local density and dynamical screening by a dielectric function for the non-uniform system. The autocorrelation function for the total force on the charges is discussed. ,0809.3071v1
"['Johannes von Oswald', 'Christian Henning', 'Benjamin F. Grewe', 'João Sacramento']",2019-06-03T10:45:08Z,Continual learning with hypernetworks,"  Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets. ",1906.00695v4
"['Johannes von Oswald', 'Seijin Kobayashi', 'Alexander Meulemans', 'Christian Henning', 'Benjamin F. Grewe', 'João Sacramento']",2020-07-25T13:23:37Z,Neural networks with late-phase weights,"  The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning. ",2007.12927v4
"['Maria R. Cervera', 'Rafael Dätwyler', ""Francesco D'Angelo"", 'Hamza Keurti', 'Benjamin F. Grewe', 'Christian Henning']",2021-11-23T10:18:41Z,Uncertainty estimation under model misspecification in neural network   regression,"  Although neural networks are powerful function approximators, the underlying modelling assumptions ultimately define the likelihood and thus the hypothesis class they are parameterizing. In classification, these assumptions are minimal as the commonly employed softmax is capable of representing any categorical distribution. In regression, however, restrictive assumptions on the type of continuous distribution to be realized are typically placed, like the dominant choice of training via mean-squared error and its underlying Gaussianity assumption. Recently, modelling advances allow to be agnostic to the type of continuous distribution to be modelled, granting regression the flexibility of classification models. While past studies stress the benefit of such flexible regression models in terms of performance, here we study the effect of the model choice on uncertainty estimation. We highlight that under model misspecification, aleatoric uncertainty is not properly captured, and that a Bayesian treatment of a misspecified model leads to unreliable epistemic uncertainty estimates. Overall, our study provides an overview on how modelling choices in regression may influence uncertainty estimation and thus any downstream decision making process. ",2111.11763v1
"['Christian Henning', 'Maria R. Cervera', ""Francesco D'Angelo"", 'Johannes von Oswald', 'Regina Traber', 'Benjamin Ehret', 'Seijin Kobayashi', 'Benjamin F. Grewe', 'João Sacramento']",2021-03-01T17:08:35Z,Posterior Meta-Replay for Continual Learning,"  Learning a sequence of tasks without access to i.i.d. observations is a widely studied form of continual learning (CL) that remains challenging. In principle, Bayesian learning directly applies to this setting, since recursive and one-off Bayesian updates yield the same result. In practice, however, recursive updating often leads to poor trade-off solutions across tasks because approximate inference is necessary for most models of interest. Here, we describe an alternative Bayesian approach where task-conditioned parameter distributions are continually inferred from data. We offer a practical deep learning implementation of our framework based on probabilistic task-conditioned hypernetworks, an approach we term posterior meta-replay. Experiments on standard benchmarks show that our probabilistic hypernetworks compress sequences of posterior parameter distributions with virtually no forgetting. We obtain considerable performance gains compared to existing Bayesian CL methods, and identify task inference as our major limiting factor. This limitation has several causes that are independent of the considered sequential setting, opening up new avenues for progress in CL. ",2103.01133v3
"['Helena U. Zacharias', 'Christoph Kaleta', 'Francois Cossais', 'Eva Schaeffer', 'Henry Berndt', 'Lena Best', 'Thomas Dost', 'Svea Glüsing', 'Mathieu Groussin', 'Mathilde Poyet', 'Sebastian Heinzel', 'Corinna Bang', 'Leonard Siebert', 'Tobias Demetrowitsch', 'Frank Leypoldt', 'Rainer Adelung', 'Thorsten Bartsch', 'Anja Bosy-Westphal', 'Karin Schwarz', 'Daniela Berg']",2022-08-19T13:39:23Z,Microbiome and metabolome insights into the role of the   gastrointestinal-brain axis in neurodegenerative diseases: unveiling   potential therapeutic targets,"  Due to the aging of the world population and westernization of lifestyles, the prevalence of neurodegenerative diseases such as Alzheimer's disease (AD) and Parkinson's disease (PD) is rapidly rising and is expected to put a strong socioeconomic burden on health systems worldwide. Due to the limited success of clinical trials of therapies against neurodegenerative diseases, research has extended its scope to a systems medicine point of view, with a particular focus on the gastrointestinal-brain axis as a potential main actor in disease development and progression. Microbiome as well as metabolome studies along the gastrointestinal-brain axis have already revealed important insights into disease pathomechanisms. Both the microbiome and metabolome can be easily manipulated by dietary and lifestyle interventions, and might thus offer novel, readily available therapeutic options to prevent the onset as well as the progression of PD and AD. This review summarizes our current knowledge on the association between microbiota, metabolites, and neurodegeneration in light of the gastrointestinal-brain axis. In this context, we also illustrate state-of-the art methods of microbiome and metabolome research as well as metabolic modeling that facilitate the identification of disease pathomechanisms. We conclude our review with therapeutic options to modulate microbiome composition to prevent or delay neurodegeneration and illustrate potential future research directions to fight PD and AD. ",2208.09338v1
"['Ludwig A. Hothorn', 'Mario Hasler']",2023-03-16T10:56:24Z,The Dunnett procedure with possibly heterogeneous variances,"  Most comparisons of treatments or doses against a control are performed by the original Dunnett single step procedure \cite{Dunnett1955} providing both adjusted p-values and simultaneous confidence intervals for differences to the control. Motivated by power arguments, unbalanced designs with higher sample size in the control are recommended. When higher variance occur in the treatment of interest or in the control, the related per-pairs power is reduced, as expected. However, if the variance is increased in a non-affected treatment group, e.g. in the highest dose (which is highly significant), the per-pairs power is also reduced in the remaining treatment groups of interest. I.e., decisions about the significance of certain comparisons may be seriously distorted. To avoid this nasty property, three modifications for heterogeneous variances are compared by a simulation study with the original Dunnett procedure. For small and medium sample sizes, a Welch-type modification can be recommended. For medium to high sample sizes, the use of a sandwich estimator instead of the common mean square estimator is useful. Related CRAN packages are provided. Summarizing we recommend not to use the original Dunnett procedure in routine and replace it by a robust modification. Particular care is needed in small sample size studies. ",2303.09222v1
"['Jan Benedikt', 'Dirk Ellerweg', 'Achim von Keudell']",2011-05-13T11:07:56Z,Diagnostics of low and atmospheric pressure plasmas by means of mass   spectrometry,"  The knowledge of absolute fluxes of reactive species such as radicals or energetic ions to the surface is crucial for understanding the growth or etching of thin films. These species have due to their high reactivity very low densities and their detection is therefore a challenging task. Mass spectrometry is a very sensitive technique and it will be demonstrated that it is a good choice for the study of plasma chemistry. Mass spectrometry measures the plasma composition directly at the surface and is not limited by existence of accessible optical transitions. When properly designed and carefully calibrated mass spectrometry provides absolute densities of the measured species. It can even measure internally excited metastable species. Here, measurement of neutral species and positive ions generated in an atmospheric pressure plasmas jet operated with He, hexamethyldisiloxane and O2 will be presented. ",1105.2687v1
"['Dirk Ellerweg', 'Achim von Keudell', 'Jan Benedikt']",2011-12-06T12:29:08Z,The mystery of O and O3 production in the effluent of a He/O2   atmospheric pressure microplasma jet,"  Microplasma jets are commonly used to treat samples in ambient air atmosphere. The effect of admixing air into the effluent may severely affect the composition of the emerging species. Here, the effluent of a He/O2 microplasma jet has been analyzed in a helium and in an air atmosphere by molecular beam mass spectrometry. First, the composition of the effluent in air has been recorded as a function of the distance to determine how fast air admixes into the effluent. Then, the spatial distribution of atomic oxygen and ozone in the effluent has been recorded in ambient air and compared to measurements in a helium atmosphere. Additionally, a fluid model of the gas flow with reaction kinetics of reactive oxygen species in the effluent has been constructed. In ambient air, the O density declines only slightly faster with the distance compared to a helium atmosphere. On the contrary, the O3 density in ambient air increases significantly faster with the distance compared to a helium atmosphere. This mysterious behavior can have big implication for the use of similar jets in plasma medicine. It is shown that photodissociation of O2 and O3 is not responsible for the observed effect. A reaction scheme involving the reaction of plasma produced highly vibrationally excited O2 with ground state O2 molecules is proposed as a possible explanation of the observed densities. A very good agreement between measured and simulated densities is achieved. ",1112.1252v1
"['Tristan Winzer', 'Natascha Blosczyk', 'Jan Benedikt', 'Judith Golda']",2021-02-24T08:40:53Z,Multi-diagnostic approach to energy transport in an atmospheric pressure   helium-oxygen plasma jet,"  The energy balance of a plasma holds fundamental information not only about basic plasma physics, but it is also important for tailoring plasmas to specific applications. Especially RF-driven atmospheric pressure plasma jets (APPJs) operated in helium with oxygen admixture have high application potential in industry and medicine. Many types of plasma jets have been studied up to now, leading to the challenge how to compare results from various sources. We have developed a method for measuring the power deposited in the plasma as the parameter to compare different sources and gas mixtures with each other. Furthermore, we studied energy transport as a function of this input power and molecular gas admixture in a newly developed APPJ based on the COST-reference jet with a capillary as a dielectric in between the electrodes. The gas temperature, atomic oxygen density, ozone density and absolute emission intensity in the visible wavelength range have been determined. Combining the results gave an energy balance with most of the energy deposited into gas heating. Production of final chemical products made up a small amount of the deposited power while radiation was negligible for all combinations of external parameters studied. ",2102.12126v1
"['Jan Benedikt', 'Rüdiger Reuter', 'Dirk Ellerweg', 'Katja Rügner', 'Achim von Keudell']",2011-05-13T11:23:12Z,Deposition of SiOx films by means of atmospheric pressure microplasma   jets,"  Atmospheric pressure plasma jet sources are currently in the focus of many researchers for their promising applications in medical industry (e.g. treatment of living tissues), surface modification or material etching or synthesis. Here we report on the study of fundamental principles of deposition of SiOx films from microplasma jets with admixture of hexamethyldisiloxane [(CH3)3SiOSi(CH3)3, HMDSO] molecules and oxygen. The properties of the deposited films, the composition of the plasma as measured by molecular beam mass spectrometry and the effect of additional treatment of grown film by oxygen or hydrogen atoms will be presented. ",1105.2691v1
"['Felix Mitschker', 'Marina Prenzel', 'Jan Benedikt', 'Christian Maszl', 'Achim von Keudell']",2013-05-30T11:09:19Z,Time resolved measurement of film growth during reactive high power   pulsed magnetron sputtering (HIPIMS) of titanium nitride,"  The growth rate during reactive high power pulsed magnetron sputtering (HIPIMS) of titanium nitride is measured with a temporal resolution of up to 25 us using a rotating shutter concept. According to that concept a 200 um slit is rotated in front of the substrate synchronous with the HIPIMS pulses. Thereby, the growth flux is laterally distributed over the substrate. By measuring the resulting deposition profile with profilometry and with x-ray photoelectron spectroscopy, the temporal variation of the titanium and nitrogen growth flux per pulse is deduced. The analysis reveals that film growth occurs mainly during a HIPIMS pulse, with the growth rate following the HIPIMS phases ignition, current rise, gas rarefaction, plateau and afterglow. The growth fluxes of titanium and nitrogen follow slightly different behaviors with titanium dominating at the beginning of the HIPIMS pulse and nitrogen at the end of the pulse. This is explained by the gas rarefaction effect resulting in a dense initial metal plasma and metal films which are subsequently being nitrified. ",1305.7063v1
"['Christian Maszl', 'Wolfgang Breilmann', 'Lars Berscheid', 'Jan Benedikt', 'Achim von Keudell']",2014-05-20T15:30:21Z,Fast time resolved techniques as key to the understanding of energy and   particle transport in HPPMS-plasmas,  High power pulsed magnetron sputtering (HPPMS) plasmas are pulsed discharges where the plasma composition as well as the fluxes and energies of ions are changing during the pulse. The time resolved energy distribution for Ar$^{1+}$ ions was measured and Phase Resolved Optical Emission Spectroscopy (PROES) for the Ar I line at 760 nm was done to get more insight in the transport properties of the plasma forming noble gas. These measurements were performed during HPPMS of titanium with argon at 0.5 Pa. The peak power density during the 50 $\mu$s pulses was 1.8 kW/cm$^2$. In this contribution we demonstrate how time resolved mass spectrometry and ICCD cameras can be used to shed more light on energy and particle transport in HPPMS-plasmas. ,1405.5123v1
"['Simon Schneider', 'Jan-Wilm Lackmann', 'Dirk Ellerweg', 'Benjamin Denis', 'Franz Narberhaus', 'Julia E. Bandow', 'Jan Benedikt']",2011-05-31T12:55:26Z,The role of VUV radiation in the inactivation of bacteria with an   atmospheric pressure plasma jet,"  A modified version of a micro scale atmospheric pressure plasma jet (\mu-APPJ) source, so-called X-Jet, is used to study the role of plasma generated VUV photons in the inactivation of E. coli bacteria. The plasma is operated in He gas or a He/O2 mixture and the X-Jet modification of the jet geometry allows effective separation of heavy reactive particles (such as O atoms or ozone molecules) from the plasma-generated photons. The measurements of the evolution of zone of inhibitions formed in monolayers of vegetative E. coli bacteria, of VUV emission intensity and of positive ion spectra show that photochemistry in the gas phase followed by photochemistry products impacting on bacteria can result in bacterial inactivation. Interestingly, this process is more effective than direct inactivation by VUV radiation damage. Mainly protonated water cluster ions are detected by mass spectrometry indicating that water impurity has to be carefully considered. The measurements indicate that the combination of the presence of water cluster ions and O2 molecules at the surface leads to the strongest effect. Additionally, it seem that the interaction of VUV photons with effluent of He/O2 plasma leads to enhanced formation of O3, which is not the case when only O2 molecules and gas impurities at room temperature interacts with plasma generated VUV photons. ",1105.6260v1
"['Christian Maszl', 'Wolfgang Breilmann', 'Jan Benedikt', 'Achim von Keudell']",2013-11-29T12:38:51Z,Origin of the energetic ions at the substrate generated during high   power pulsed magnetron sputtering of titanium,"  HiPIMS plasmas generate energetic metal ions at the substrate as a major difference to conventional direct current magnetron sputtering. The origin of these ions is still an open issue, which is unraveled by using two fast diagnostics: time resolved mass spectrometry with a temporal resolution of 2 $\mu$s and phase resolved optical emission spectroscopy with a temporal resolution of 1 $\mu$s. A power scan from dcMS-like to HiPIMS plasmas was performed, with a 2-inch magnetron and a titanium target as sputter source and argon as working gas. Clear differences in the transport as well in the energetic properties of Ar$^{+}$, Ar$^{2+}$, Ti$^+$ and Ti$^{2+}$ were observed. For discharges with highest peak power densities a high energetic group of Ti$^+$ and Ti$^{2+}$ could be identified. A cold group of ions is always present. It is found that hot ions are observed only, when the plasma enters the spokes regime, which can be monitored by oscillations in the IV-characteristics in the MHz range that are picked up by the used VI-probes. These oscillations are correlated with the spokes phenomenon and are explained as an amplification of the Hall current inside the spokes as hot ionization zones. To explain the presence of energetic ions, we propose a double layer (DL) confining the hot plasma inside a spoke: if an atom becomes ionized inside the spokes region it is accelerated because of the DL to higher energies whereas its energy remains unchanged if it is ionized outside. In applying this DL model to our measurements the observed phenomena as well as several measurements from other groups can be explained. Only if spokes and a double layer are present the confined particles can gain enough energy to leave the magnetic trap. We conclude from our findings that the spoke phenomenon represents the essence of HiPIMS plasmas, explaining their good performance for material synthesis applications. ",1311.7545v3
"['Carles Corbella', 'Simon Grosse-Kreul', 'Oliver Kreiter', 'Teresa de los Arcos', 'Jan Benedikt', 'Achim von Keudell']",2013-06-06T15:13:00Z,Particle beam experiments for the investigation of plasma-surface   interactions: application to magnetron sputtering and polymer treatment,"  A beam experiment is presented to study heterogeneous reactions relevant to plasma-surface interactions. Atom and ion beams are focused onto the sample to expose it to quantified beams of oxygen, nitrogen, hydrogen, noble gas ions and metal vapor. The heterogeneous surface processes are monitored in-situ and in real time by means of a quartz crystal microbalance (QCM) and Fourier transform infrared spectroscopy (FTIR). Two examples illustrate the capabilities of the particle beam setup: oxidation and nitriding of aluminum as a model of target poisoning during reactive magnetron sputtering, and plasma treatment of polymers (PET, PP). ",1306.1431v1
"['Teresa de los Arcos', 'Peter Awakowicz', 'Jan Benedikt', 'Beatrix Biskup', 'Marc Böke', 'Nils Boysen', 'Rahel Buschhaus', 'Rainer Dahlmann', 'Anjana Devi', 'Tobias Gergs', 'Jonathan Jenderny', 'Achim von Keudell', 'Thomas D. Kühne', 'Simon Kusmierz', 'Hendrik Müller', 'Thomas Mussenbrock', 'Jan Trieschmann', 'David Zanders', 'Frederik Zysk', 'Guido Grundmeier']",2023-06-21T12:06:11Z,PECVD and PEALD on polymer substrates (part I): Fundamentals and   analysis of plasma activation and thin film growth,"  This feature article considers the analysis of the initial states of film growth on polymer substrates. The assembled results are based on the cooperation between research groups in the field of plasma physics, chemistry, electric as well as mechanical engineering over the last years, mostly within the frame of the transregional project SFB-TR 87 (""Pulsed high power plasmas for the synthesis of nanostructured functional layers""). This feature article aims at bridging the gap between the understanding of plasma processes in the gas phase and the resulting surface and interface processes of the polymer. The results show that interfacial adhesion and initial film growth can be well controlled and even predicted based on the combination of analytical approaches. ",2307.00011v1
"['Michael Bauer', 'Kay Brandner', 'Udo Seifert']",2016-02-12T16:58:43Z,"Optimal performance of periodically driven, stochastic heat engines   under limited control","  We consider the performance of periodically driven stochastic heat engines in the linear response regime. Reaching the theoretical bounds for efficiency and efficiency at maximum power typically requires full control over the design and the driving of the system. We develop a framework which allows to quantify the role that limited control over the system has on the performance. Specifically, we show that optimizing the driving entering the work extraction for a given temperature protocol leads to a universal, one-parameter dependence for both maximum efficiency and maximum power as a function of efficiency. In particular, we show that reaching Carnot efficiency (and, hence, Curzon-Ahlborn efficiency at maximum power) requires to have control over the amplitude of the full Hamiltonian of the system. Since the kinetic energy cannot be controlled by an external parameter, heat engines based on underdamped dynamics can typically not reach Carnot efficiency. We illustrate our general theory with a paradigmatic case study of a heat engine consisting of an underdamped charged particle in a modulated two-dimensional harmonic trap in the presence of a magnetic field. ",1602.04119v1
"['Kay Brandner', 'Michael Bauer', 'Michael T. Schmid', 'Udo Seifert']",2015-03-16T21:38:46Z,Coherence-enhanced efficiency of feedback-driven quantum engines,"  A genuine feature of projective quantum measurements is that they inevitably alter the mean energy of the observed system if the measured quantity does not commute with the Hamiltonian. Compared to the classical case, Jacobs proved that this additional energetic cost leads to a stronger bound on the work extractable after a single measurement from a system initially in thermal equilibrium [Phys. Rev. A 80, 012322 (2009)]. Here, we extend this bound to a large class of feedback-driven quantum engines operating periodically and in finite time. The bound thus implies a natural definition for the efficiency of information to work conversion in such devices. For a simple model consisting of a laser-driven two-level system, we maximize the efficiency with respect to the observable whose measurement is used to control the feedback operations. We find that the optimal observable typically does not commute with the Hamiltonian and hence would not be available in a classical two level system. This result reveals that periodic feedback engines operating in the quantum realm can exploit quantum coherences to enhance efficiency. ",1503.04865v1
"['Kay Brandner', 'Michael Bauer', 'Udo Seifert']",2017-03-07T16:42:19Z,Universal Coherence-Induced Power Losses of Quantum Heat Engines in   Linear Response,"  We introduce a universal scheme to divide the power output of a periodically driven quantum heat engine into a classical contribution and one stemming solely from quantum coherence. Specializing to Lindblad-dynamics and small driving amplitudes, we derive general upper bounds on both, the coherent and the total power. These constraints imply that, in the linear-response regime, coherence inevitably leads to power losses. To illustrate our general analysis, we explicitly work out the experimentally relevant example of a single-qubit engine. ",1703.02464v1
"['Mario Heidernätsch', 'Michael Bauer', 'Günter Radons']",2013-03-07T14:34:35Z,Characterizing N-dimensional anisotropic Brownian motion by the   distribution of diffusivities,"  Anisotropic diffusion processes emerge in various fields such as transport in biological tissue and diffusion in liquid crystals. In such systems, the motion is described by a diffusion tensor. For a proper characterization of processes with more than one diffusion coefficient an average description by the mean squared displacement is often not sufficient. Hence, in this paper, we use the distribution of diffusivities to study diffusion in a homogeneous anisotropic environment. We derive analytical expressions of the distribution and relate its properties to an anisotropy measure based on the mean diffusivity and the asymptotic decay of the distribution. Both quantities are easy to determine from experimental data and reveal the existence of more than one diffusion coefficient, which allows the distinction between isotropic and anisotropic processes. We further discuss the influence on the analysis of projected trajectories, which are typically accessible in experiments. For the experimentally relevant cases of two- and three-dimensional anisotropic diffusion we derive specific expressions, determine the diffusion tensor, characterize the anisotropy, and demonstrate the applicability for simulated trajectories. ",1303.1702v2
"['Rohan Yadav', 'Shiv Sundram', 'Wonchan Lee', 'Michael Garland', 'Michael Bauer', 'Alex Aiken', 'Fredrik Kjolstad']",2024-06-26T06:51:43Z,Composing Distributed Computations Through Task and Kernel Fusion,"  We introduce Diffuse, a system that dynamically performs task and kernel fusion in distributed, task-based runtime systems. The key component of Diffuse is an intermediate representation of distributed computation that enables the necessary analyses for the fusion of distributed tasks to be performed in a scalable manner. We pair task fusion with a JIT compiler to fuse together the kernels within fused tasks. We show empirically that Diffuse's intermediate representation is general enough to be a target for two real-world, task-based libraries (cuNumeric and Legate Sparse), letting Diffuse find optimization opportunities across function and library boundaries. Diffuse accelerates unmodified applications developed by composing task-based libraries by 1.86x on average (geo-mean), and by between 0.93x--10.7x on up to 128 GPUs. Diffuse also finds optimization opportunities missed by the original application developers, enabling high-level Python programs to match or exceed the performance of an explicitly parallel MPI library. ",2406.18109v1
"['Rohan Yadav', 'Michael Bauer', 'David Broman', 'Michael Garland', 'Alex Aiken', 'Fredrik Kjolstad']",2024-06-26T06:57:26Z,Automatic Tracing in Task-Based Runtime Systems,"  Implicitly parallel task-based runtime systems often perform dynamic analysis to discover dependencies in and extract parallelism from sequential programs. Dependence analysis becomes expensive as task granularity drops below a threshold. Tracing techniques have been developed where programmers annotate repeated program fragments (traces) issued by the application, and the runtime system memoizes the dependence analysis for those fragments, greatly reducing overhead when the fragments are executed again. However, manual trace annotation can be brittle and not easily applicable to complex programs built through the composition of independent components. We introduce Apophenia, a system that automatically traces the dependence analysis of task-based runtime systems, removing the burden of manual annotations from programmers and enabling new and complex programs to be traced. Apophenia identifies traces dynamically through a series of dynamic string analyses, which find repeated program fragments in the stream of tasks issued to the runtime system. We show that Apophenia is able to come between 0.92x--1.03x the performance of manually traced programs, and is able to effectively trace previously untraced programs to yield speedups of between 0.91x--2.82x on the Perlmutter and Eos supercomputers. ",2406.18111v1
"['Michael Bauer', 'Rustem Valiullin', 'Günter Radons', 'Jörg Kärger']",2010-06-30T19:17:26Z,How to compare diffusion processes assessed by single-particle tracking   and pulsed field gradient nuclear magnetic resonance,"  Heterogeneous diffusion processes occur in many different fields such as transport in living cells or diffusion in porous media. A characterization of the transport parameters of such processes can be achieved by ensemble-based methods, such as pulsed field gradient nuclear magnetic resonance (PFG NMR), or by trajectory-based methods obtained from single-particle tracking (SPT) experiments. In this paper, we study the general relationship between both methods and its application to heterogeneous systems. We derive analytical expressions for the distribution of diffusivities from SPT and further relate it to NMR spin-echo diffusion attenuation functions. To exemplify the applicability of this approach, we employ a well-established two-region exchange model, which has widely been used in the context of PFG NMR studies of multiphase systems subjected to interphase molecular exchange processes. This type of systems, which can also describe a layered liquid with layer-dependent self-diffusion coefficients, has also recently gained attention in SPT experiments. We reformulate the results of the two-region exchange model in terms of SPT-observables and compare its predictions to that obtained using the exact transformation which we derived. ",1006.5953v2
"['Hermann Erk', 'Carl Eric Jensen', 'Stephan Jauernik', 'Michael Bauer']",2024-04-22T15:55:07Z,Ultrafast low-energy photoelectron diffraction for the study of   surface-adsorbate interactions with 100 femtosecond temporal resolution,"  An ultrafast photoemission-based low-energy electron diffraction experiment with monolayer surface sensitivity is presented. In a first experiment on tin-phthalocyanine adsorbed on graphite, we demonstrate a time resolution of approx. 100 fs. Analysis of the transient photoelectron diffraction signal indicates a heating of the adsorbate layer on a time scale of a few ps, suggesting coupling to phononic degrees of freedom of the substrate as the primary energy transfer channel for the vibrational excitation of the adsorbate layer. Remarkably, the transient photoelectron diffraction signal not only provides direct information about the structural dynamics of the adsorbate, but also about the charge carrier dynamics of the substrate. The presented concept combined with momentum microscopy could become a versatile tool for the comprehensive investigation of the coupled charge and vibrational dynamics of relevance for ultrafast surface processes. ",2404.14297v1
"['Michael Bauer', 'David Abreu', 'Udo Seifert']",2012-03-01T13:55:09Z,Efficiency of a Brownian information machine,"  A Brownian information machine extracts work from a heat bath through a feedback process that exploits the information acquired in a measurement. For the paradigmatic case of a particle trapped in a harmonic potential, we determine how power and efficiency for two variants of such a machine operating cyclically depend on the cycle time and the precision of the positional measurements. Controlling only the center of the trap leads to a machine that has zero efficiency at maximum power whereas additional optimal control of the stiffness of the trap leads to an efficiency bounded between 1/2, which holds for maximum power, and 1 reached even for finite cycle time in the limit of perfect measurements. ",1203.0184v3
"['Michael Bauer', 'Andre C. Barato', 'Udo Seifert']",2014-06-04T12:58:54Z,Optimized finite-time information machine,"  We analyze a periodic optimal finite-time two-state information-driven machine that extracts work from a single heat bath exploring imperfect measurements. Two models are considered, a memory-less one that ignores past measurements and an optimized model for which the feedback scheme consists of a protocol depending on the whole history of measurements. Depending on the precision of the measurement and on the period length the optimized model displays a phase transition to a phase where measurements are judged as non-reliable. We obtain the critical line exactly and show that the optimized model leads to more work extraction in comparison to the memory-less model, with the gain parameter being larger in the region where the frequency of non-reliable measurements is higher. We also demonstrate that the model has two second law inequalities, with the extracted work being bounded by the change of the entropy of the system and by the mutual information. ",1406.1030v1
"['Petra Hein', 'Stephan Jauernik', 'Hermann Erk', 'Lexian Yang', 'Yanpeng Qi', 'Yan Sun', 'Claudia Felser', 'Michael Bauer']",2020-02-18T14:31:52Z,A combined laser-based ARPES and 2PPES study of Td-WTe$_2$,  Laser-based angle-resolved photoemission spectroscopy (ARPES) and two-photon photoemission spectroscopy (2PPES) are employed to study the valence electronic structure of the Weyl semimetal candidate Td-WTe$_2$ along two high symmetry directions and for binding energies between $\approx$ -1 eV and 5 eV. The experimental data show a good agreement with band structure calculations. Polarization dependent measurements provide furthermore information on initial and intermediate state symmetry properties with respect to the mirror plane of the Td structure of WTe$_2$. ,2002.07581v1
"['Gerald Rohde', 'Ankatrin Stange', 'Arne Müller', 'Marcel Behrendt', 'Lars-Philip Oloff', 'Kerstin Hanff', 'Thies Albert', 'Petra Hein', 'Kai Rossnagel', 'Michael Bauer']",2018-04-04T13:41:43Z,Decoding the ultrafast formation of a Fermi-Dirac distributed electron   gas,"  Time- and angle-resolved photoelectron spectroscopy with 13 fs temporal resolution is used to follow the different stages in the formation of a Fermi-Dirac distributed electron gas in graphite after absorption of an intense 7 fs laser pulse. Within the first 50 fs after excitation a sequence of time frames is resolved which are characterized by different energy and momentum exchange processes among the involved photonic, electronic, and phononic degrees of freedom. The results reveal experimentally the complexity of the transition from a nascent non-thermal towards a thermal electron distribution due to the different timescales associated with the involved interaction processes. ",1804.01403v2
"['Yingchao Zhang', 'Xun Shi', 'Mengxue Guan', 'Wenjing You', 'Yigui Zhong', 'Tika R. Kafle', 'Yaobo Huang', 'Hong Ding', 'Michael Bauer', 'Kai Rossnagel', 'Sheng Meng', 'Henry C. Kapteyn', 'Margaret M. Murnane']",2020-11-15T20:25:55Z,Creation of a novel inverted charge density wave state,"  Charge density wave (CDW) order is an emergent quantum phase that is characterized by a periodic lattice distortion and charge density modulation, often present near superconducting transitions. Here we uncover a novel inverted CDW state by using a femtosecond laser to coherently over-drive the unique star-of-David lattice distortion in 1T-TaSe$_2$. We track the signature of this novel CDW state using time- and angle-resolved photoemission spectroscopy and time-dependent density functional theory, and validate that it is associated with a unique lattice and charge arrangement never before realized. The dynamic electronic structure further reveals its novel properties, that are characterized by an increased density of states near the Fermi level, high metallicity, and altered electron-phonon couplings. Our results demonstrate how ultrafast lasers can be used to create unique states in materials, by manipulating charge-lattice orders and couplings. ",2011.07623v2
"['Ryuichi Fujimoto', 'Kazuhisa Mitsuda', 'Dan McCammon', 'Yoh Takei', 'Michael Bauer', 'Yoshitaka Ishisaki', 'F. Scott Porter', 'Hiroya Yamaguchi', 'Kiyoshi Hayashida', 'Noriko Y. Yamasaki']",2006-09-12T03:14:39Z,Evidence for Solar-Wind Charge-Exchange X-Ray Emission from the Earth's   Magnetosheath,"  We report an apparent detection of the C VI 4p to 1s transition line at 459 eV, during a long-term enhancement (LTE) in the Suzaku north ecliptic pole (NEP) observation of 2005 September 2. The observed intensity of the line is comparable to that of the C VI 2p to 1s line at 367 eV. This is strong evidence for the charge-exchange process. In addition to the C VI lines, emission lines from O VII, O VIII, Ne X, and Mg XI lines showed clear enhancements. There are also features in the 750 to 900 eV range that could be due to some combination of Fe XVII and XVIII L-lines, higher order transitions of O VIII (3p to 1s and 6p to 1s), and a Ne IX line. From the correlation of the X-ray intensity with solar-wind flux on time scales of about half a day, and from the short-term (~10 minutes) variations of the X-ray intensity, these lines most likely arise from solar-wind heavy ions interacting with neutral material in the Earth's magnetosheath. A hard power-law component is also necessary to explain the LTE spectrum. The origin of this component is not yet known. Our results indicate that solar activity can significantly contaminate Suzaku cosmic X-ray spectra below ~1 keV. Recommendations are provided for recognizing such contamination in observations of extended sources. ",astro-ph/0609308v1
"['Xun Shi', 'Wenjing You', 'Yingchao Zhang', 'Zhensheng Tao', 'Peter M. Oppeneer', 'Xianxin Wu', 'Ronny Thomale', 'Kai Rossnagel', 'Michael Bauer', 'Henry Kapteyn', 'Margaret Murnane']",2019-01-24T03:28:36Z,Ultrafast electron calorimetry uncovers a new long-lived metastable   state in 1T-TaSe$_2$ mediated by mode-selective electron-phonon coupling,"  Quantum materials represent one of the most promising frontiers in the quest for faster, lightweight, energy efficient technologies. However, their inherent complexity and rich phase landscape make them challenging to understand or manipulate in useful ways. Here we present a new ultrafast electron calorimetry technique that can systematically uncover new phases of quantum matter. Using time- and angle-resolved photoemission spectroscopy, we measure the dynamic electron temperature, band structure and heat capacity. We then show that this is a very sensitive probe of phase changes in materials, because electrons react very quickly, and moreover generally are the smallest component of the total heat capacity. This allows us to uncover a new long-lived metastable state in the charge density wave material 1T-TaSe$_2$, that is distinct from all of the known equilibrium phases: it is characterized by a significantly reduced effective heat capacity that is only 30% of the normal value, due to selective electron-phonon coupling to a subset of phonon modes. As a result, significantly less energy is required to melt the charge order and transform the state of the material than under thermal equilibrium conditions. ",1901.08214v1
"['Yingchao Zhang', 'Xun Shi', 'Wenjing You', 'Zhensheng Tao', 'Yigui Zhong', 'Fairoja Cheenicode Kabeer', 'Pablo Maldonado', 'Peter M. Oppeneer', 'Michael Bauer', 'Kai Rossnagel', 'Henry Kapteyn', 'Margaret Murnane']",2019-06-23T03:31:05Z,Coherent modulation of the electron temperature and electron-phonon   couplings in a 2D material,"  Ultrashort light pulses can selectively excite charges, spins and phonons in materials, providing a powerful approach for manipulating their properties. Here we use femtosecond laser pulses to coherently manipulate the electron and phonon distributions, and their couplings, in the charge density wave (CDW) material 1T-TaSe$_2$. After exciting the material with a short light pulse, spatial smearing of the electrons launches a coherent lattice breathing mode, which in turn modulates the electron temperature. This indicates a bi-directional energy exchange between the electrons and the strongly-coupled phonons. By tuning the laser excitation fluence, we can control the magnitude of the electron temperature modulation, from ~ 200 K in the case of weak excitation, to ~ 1000 K for strong laser excitation. This is accompanied by a switching of the dominant mechanism from anharmonic phonon-phonon coupling to coherent electron-phonon coupling, as manifested by a phase change of $\pi$ in the electron temperature modulation. Our approach thus opens up possibilities for coherently manipulating the interactions and properties of quasi-2D and other quantum materials using light. ",1906.09545v1
"['Richard Berndt', 'Joerg Kliewer', 'S. Crampin']",2001-12-04T12:54:33Z,Scanning tunnelling spectroscopy of electron resonators,"  The electronic structure of artificial Mn atom arrays on Ag(111) is characterized in detail with scanning tunnelling spectroscopy and spectroscopic imaging at low temperature. We demonstrate the degree to which variations in geometry may be used to control spatial and spectral distributions of surface state electrons confined within the arrays, how these are influenced by atoms placed within the structure and how the ability to induce spectral features at specific energies may be exploited through lineshape analyses to deduce quasiparticle lifetimes near the Fermi level. Through extensive comparison of $dI/dV$ maps and spectra we demonstrate the utility of a model based upon two-dimensional s-wave scatterers for describing and predicting the characteristics of specific resonators. ",cond-mat/0112051v1
"['Jan Homberg', 'Alexander Weismann', 'Troels Markussen', 'Richard Berndt']",2022-02-17T18:43:02Z,Resonance-enhanced vibrational spectroscopy of molecules on a   superconductor,"  Molecular vibrational spectroscopy with the scanning tunneling microscope is feasible but usually detects few vibrational modes. We harness sharp Yu-Shiba-Rusinov (YSR) states observed from molecules on a superconductor to significantly enhance the vibrational signal. From a lead phthalocyanin molecule 46 vibrational peaks are resolved enabling a comparison with calculated modes. The energy resolution is improved beyond the thermal broadening limit and shifts induced by neighbor molecules or the position of the microscope tip are determined. Vice versa, spectra of vibrational modes are used to measure the effect of an electrical field on the energy of YSR states. The method may help to further probe the interaction of molecules with their environment and to better understand selection rules for vibrational excitations. ",2202.08820v1
"['Jan Homberg', 'Alexander Weismann', 'Richard Berndt']",2024-02-13T09:48:57Z,Making Closed-Shell Lead-Pthalocyanine Paramagnetic on Pb(100),"  Lead phthalocyanine (PbPc), a non-planar molecule, is studied on Pb(100) using scanning tunneling spectroscopy. A rigid shift of the molecular orbitals is found between molecules with the central Pb ion pointing to (PbPc$\downarrow$) or away (PbPc$\uparrow$) from the substrate and understood from the interaction between the molecules and their image charges. Inside the superconducting energy gap, Yu-Shiba-Rusinov (YSR) resonances are observed for PbPc$\uparrow$ molecules in islands indicating the presence of a magnetic moment. Such bound states are neither present on PbPc$\downarrow$ molecules nor isolated PbPc$\uparrow$ or molecules that lost the Pb ion during deposition (H$_0$Pc). The YSR energies vary depending on the orientation and type of the molecular neighbors. We analyze the role of the out-of-plane dipole moment of PbPc. ",2402.08330v1
"['Patrick Schmidt', 'Richard Berndt', 'Mikhail I. Vexler']",2007-08-28T16:10:38Z,Ultraviolet light emission from Si in a scanning tunneling microscope,"  Ultraviolet and visible radiation is observed from the contacts of a scanning tunneling microscope with Si(100) and (111) wafers. This luminescence relies on the presence of hot electrons in silicon, which are supplied, at positive bias on n- and p-type samples, through the injection from the tip, or, at negative bias on p-samples, by Zener tunneling. Measured spectra reveal a contribution of direct optical transitions in Si bulk. The necessary holes well below the valence band edge are injected from the tip or generated by Auger processes. ",0708.3769v1
"['Natalia L. Schneider', 'Guillaume Schull', 'Richard Berndt']",2010-07-01T07:58:39Z,Optical Probe of Quantum Shot Noise Reduction at a Single-Atom Contact,"  Visible and infra-red light emitted at a Ag-Ag(111) junction has been investigated from tunneling to single atom contact conditions with a scanning tunneling microscope. The light intensity varies in a highly nonlinear fashion with the conductance of the junction and exhibits a minimum at conductances close to the conductance quantum. The data are interpreted in terms of current noise at optical frequencies, which is characteristic of partially open transport channels. ",1007.0095v2
"['Johannes Schöneberg', 'Alexander Weismann', 'Richard Berndt']",2013-03-06T15:39:38Z,Scanning Tunneling Spectroscopy of Ni/W(110): bcc and fcc properties in   the second atomic layer,"  Nickel islands are grown on W(110) at elevated temperatures. Islands with a thickness of two layers are investigated with scanning tunneling microscopy. Spectroscopic measurements reveal that nanometer sized areas of the islands exhibit distinctly different apparent heights and dI/dV spectra. Spin polarized and paramagnetic band structure calculations indicate that the spectral features are due to fcc(111) and bcc(110) orientations of the Ni film, respectively. ",1303.1357v1
"['Alexander Sperl', 'Joerg Kroeger', 'Richard Berndt', 'Andreas Franke', 'Eckhard Pehlke']",2009-03-30T11:21:26Z,Evolution of unoccupied resonance during the synthesis of a silver dimer   on Ag(111),  Silver dimers were fabricated on Ag(111) by single-atom manipulation using the tip of a cryogenic scanning tunnelling microscope. An unoccupied electronic resonance was observed to shift toward the Fermi level with decreasing atom-atom distance as monitored by spatially resolved scanning tunnelling spectroscopy. Density functional calculations were used to analyse the experimental observations and revealed that the coupling between the adsorbed atoms is predominantly direct rather than indirect via the Ag(111) substrate. ,0903.5186v1
"['Guillaume Schull', 'Thomas Frederiksen', 'Mads Brandbyge', 'Richard Berndt']",2009-10-07T16:51:10Z,Passing current through touching molecules,"  The charge flow from a single C60 molecule to another one has been probed. The conformation and electronic states of both molecules on the contacting electrodes have been characterized using a cryogenic scanning tunneling microscope. While the contact conductance of a single molecule between two Cu electrodes can vary up to a factor of three depending on electrode geometry, the conductance of the C60-C60 contact is consistently lower by two orders of magnitude. First-principles transport calculations reproduce the experimental results, allow a determination of the actual C60-C60 distances, and identify the essential role of the intermolecular link in bi- and trimolecular chains. ",0910.1281v1
"['Manuel Gruber', 'Alexander Weismann', 'Richard Berndt']",2018-07-24T13:15:36Z,The Kondo Resonance Line Shape in Scanning Tunnelling Spectroscopy:   Instrumental Aspects,"  In the scanning tunnelling microscope, the many-body Kondo effect leads to a zero-bias feature of the differential conductance spectra of magnetic adsorbates on surfaces. The intrinsic line shape of this Kondo resonance and its temperature dependence in principle contain valuable information. We use measurements on a molecular Kondo system, all-trans retinoic acid on Au(111), and model calculations to discuss the role of instrumental broadening. The modulation voltage used for the lock-in detection, noise on the sample voltage, and the temperature of the microscope tip are considered. These sources of broadening affect the apparent line shapes and render difficult a determination of the intrinsic line width, in particular when variable temperatures are involved. ",1807.09082v1
"['Germar Hoffmann', 'Richard Berndt', 'Peter Johansson']",2002-07-31T15:32:51Z,Two-Electron Photon Emission From Metallic Quantum Wells,"  Unusual emission of visible light is observed in scanning tunneling microscopy of the quantum well system Na on Cu(111). Photons are emitted at energies exceeding the energy of the tunneling electrons. Model calculations of two-electron processes which lead to quantum well transitions reproduce the experimental fluorescence spectra, the quantum yield, and the power-law variation of the intensity with the excitation current. ",cond-mat/0207746v2
"['Nadine Hauptmann', 'Fabian Mohn', 'Leo Gross', 'Gerhard Meyer', 'Thomas Frederiksen', 'Richard Berndt']",2012-08-14T06:43:30Z,Force and Conductance during contact formation to a C60 molecule,  Force and conductance were simultaneously measured during the formation of Cu-C60 and C60-C60 contacts using a combined cryogenic scanning tunneling and atomic force microscope. The contact geometry was controlled with submolecular resolution. The maximal attractive forces measured for the two types of junctions were found to differ significantly. We show that the previously reported values of the contact conductance correspond to the junction being under maximal tensile stress. ,1208.2791v1
"['Nadine Hauptmann', 'César González', 'Fabian Mohn', 'Leo Gross', 'Gerhard Meyer', 'Richard Berndt']",2017-04-27T08:06:05Z,Interactions between two C60 molecules measured by scanning probe   microscopies,"  C60-functionalized tips are used to probe C60 molecules on Cu(111) with scanning tunneling and atomic force microscopy. Distinct and complex intramolecular contrasts are found. Maximal attractive forces are observed when for both molecules a [6,6] bond faces a hexagon of the other molecule. Density functional theory calculations including parameterized van der Waals interactions corroborate the observations. ",1704.08466v1
"['Peter-Jan Peters', 'Fei Xu', 'Kristen Kaasbjerg', 'Gianluca Rastelli', 'Wolfgang Belzig', 'Richard Berndt']",2018-09-08T12:22:18Z,Quantum Coherent Multielectron Processes in an Atomic Scale Contact,"  The light emission from a scanning tunneling microscope operated on a Ag(111) surface at 6 K is analyzed from low conductances to values approaching the conductance quantum. Optical spectra recorded at a sample voltages V reveal emission with photon energies hv> 2eV. A model of electrons interacting coherently via a localized plasmon-polariton mode reproduces the experimental data, in particular the kinks in the spectra at eV and 2eV as well as the scaling of the intensity at low and intermediate conductances. ",1809.02793v1
"['Takashi Uchihashi', 'Jianwei Zhang', 'Joerg Kroeger', 'Richard Berndt']",2008-04-18T08:16:30Z,Quantum modulation of the Kondo resonance of Co adatoms on Cu/Co/Cu(100),  Low-temperature scanning tunneling spectroscopy reveals that the Kondo temperature T_K of Co atoms adsorbed on Cu/Co/Cu(100) multilayers varies between 60 K and 134 K as the Cu film thickness decreases from 20 to 5 atomic layers. The observed change of T_K is attributed to a variation of the density of states at the Fermi level \rho_F induced by quantum well states confined to the Cu film. A model calculation based on the quantum oscillations of \rho_F at the belly and the neck of the Cu Fermi surface reproduces most of the features in the measured variation of T_K. ,0804.2967v2
"['Peter Johansson', 'Germar Hoffmann', 'Richard Berndt']",2002-08-06T12:46:26Z,Light emission from Na/Cu(111) induced by a scanning tunneling   microscope,"  Measurements of light emission from a scanning tunneling microscope probing a Na overlayer on the (111) surface of Cu are reported along with results of a model calculation that essentially agree with the experimental ones. The observed light emission spectra show two characteristic features depending on the bias voltage. When the bias voltage is smaller than the energy of the second quantum well state formed outside the Na overlayer the light emission is due to a plasmon-mediated process, while for larger biases light emission is mainly caused by quantum well transitions between the two levels. ",cond-mat/0208110v2
"['Michael Mohr', 'Torben Jasper-Toennies', 'Thomas Frederiksen', 'Aran Garcia-Lekue', 'Sandra Ulrich', 'Rainer Herges', 'Richard Berndt']",2019-05-21T12:58:02Z,Conductance channels of a platform molecule on Au(111) probed with shot   noise,  The shot noise of the current $I$ through junctions to single trioxatriangulenium cations (TOTA$^+$) on Au(111) is measured with a low temperature scanning tunneling microscope using Au tips. The noise is significantly reduced compared to the Poisson noise power of $2eI$ and varies linearly with the junction conductance. The data are consistent with electron transmission through a single spin-degenerate transport channel and show that TOTA$^+$ in a Au contact does not acquire an unpaired electron. Ab initio calculations reproduce the observations and show that the current involves the lowest unoccupied orbital of the molecule and tip states close to the Fermi level. ,1905.08591v1
"['Michael Mohr', 'Alexander Weismann', 'Dongzhe Li', 'Mads Brandbyge', 'Richard Berndt']",2021-10-18T09:15:20Z,Current shot noise in atomic contacts: Fe and FeH$_2$ between Au   electrodes,"  Single Fe atoms on Au(111) surfaces were hydrogenated and dehydrogenated with the Au tip of a low-temperature scanning tunneling microscope (STM). Fe and FeH$_2$ were contacted with the tip of the microscope and show distinctly different evolutions of the conductance with the tip-substrate distance. The current shot noise of these contacts has been measured and indicates a single relevant conductance channel with the spin-polarized transmission. For FeH$_2$ the spin polarization reaches values up to 80\% for low conductances and is reduced if the tip-surface distance is decreased. These observations are partially reproduced using density functional theory (DFT) based transport calculations. We suggest that the quantum motion of the hydrogen atoms, which is not taken into account in our DFT modeling, may have a significant effect on the results. ",2110.09120v1
"['Sujoy Karan', 'David Jacob', 'Michael Karolak', 'Christian Hamann', 'Yongfeng Wang', 'Alexander Weismann', 'Alexander I. Lichtenstein', 'Richard Berndt']",2015-06-15T14:44:53Z,Shifting the Voltage Drop in Electron Transport through a Single   Molecule,"  A Mn-porphyrin was contacted on Au(111) in a low-temperature scanning tunneling microscope (STM). Differential conductance spectra show a zero-bias resonance that is due to an underscreened Kondo effect according to many-body calculations. When the Mn center is contacted by the STM tip, the spectrum appears to invert along the voltage axis. A drastic change in the electrostatic potential of the molecule involving a small geometric relaxation is found to cause this observation. ",1506.04618v2
"['Michael Mohr', 'Manuel Gruber', 'Alexander Weismann', 'David Jacob', 'Paula Abufager', 'Nicolás Lorente', 'Richard Berndt']",2019-10-18T07:25:03Z,Spin dependent transmission of Nickelocene-Cu contacts probed with shot   noise,"  The current $I$ through nickelocene molecules and its noise are measured with a low temperature scanning tunneling microscope on a Cu(100) substrate. Density functional theory calculations and many-body modeling are used to analyze the data. During contact formation, two types of current evolution are observed, an abrupt jump to contact and a smooth transition. These data along with conductance spectra ($dI/dV$) recorded deep in the contact range are interpreted in terms of a transition from a spin-1 to a spin-1/2 state that is Kondo screened. Many-body calculations show that the smooth transition is also consistent with a renormalization of spin excitations of a spin-1 molecule by Kondo exchange coupling. The shot noise is significantly reduced compared to the Schottky value of $2eI$ but no influence of the Kondo effect or spin excitations are resolved. The noise can be described in the Landauer picture in terms of spin-polarized transmission of $\approx$35% through two degenerate $d_\pi$-orbitals of the Nickelocene molecule. ",1910.08289v2
"['Neda Noei', 'Roberto Mozara', 'Ana M. Montero', 'Sascha Brinker', 'Niklas Ide', 'Filipe S. M. Guimarães', 'Alexander I. Lichtenstein', 'Richard Berndt', 'Samir Lounis', 'Alexander Weismann']",2023-03-06T17:42:43Z,Spin excitation of Co atoms at monatomic Cu chains,"  The zero-bias anomaly in conductance spectra of single Co atoms on Cu(111) observed at $\approx$ 4 K, which has been interpreted as being due to a Kondo resonance, is strongly modified when the Co atoms are attached to monatomic Cu chains. Scanning tunneling spectra measured at 340 mK in magnetic fields exhibit all characteristics of spin-flip excitations. Their dependence on the magnetic field reveals a magnetic anisotropy and suggests a non-collinear spin state. This indicates that spin-orbit coupling (SOC), which has so far been neglected in theoretical studies of Co/Cu(111), has to be taken into account. According to our density functional theory and multi-orbital quantum Monte Carlo calculations SOC suppresses the Kondo effect for all studied geometries. ",2303.03317v1
"['Sujoy Karan', 'Na Li', 'Yajie Zhang', 'Yang He', 'I-Po Hong', 'Huanjun Song', 'Jing-Tao Lü', 'Yongfeng Wang', 'Lianmao Peng', 'Kai Wu', 'Georg S. Michelitsch', 'Reinhard J. Maurer', 'Katharina Diller', 'Karsten Reuter', 'Alexander Weismann', 'Richard Berndt']",2015-12-14T21:53:27Z,Spin Manipulation by Creation of Single-Molecule Radical Cations,"  All-trans-retinoic acid (ReA), a closed-shell organic molecule comprising only C, H, and O atoms, is investigated on a Au(111) substrate using scanning tunneling microscopy and spectroscopy. In dense arrays single ReA molecules are switched to a number of states, three of which carry a localized spin as evidenced by conductance spectroscopy in high magnetic fields. The spin of a single molecule may be reversibly switched on and off without affecting its neighbors. We suggest that ReA on Au is readily converted to a radical by the abstraction of an electron. ",1512.04576v1
"['Johannes Marquardt', 'Bernd Heber']",2019-05-03T07:24:54Z,Galactic cosmic ray hydrogen spectra and radial gradients in the inner   heliosphere measured by the HELIOS Experiment 6,"  Context: The HELIOS solar observation probes provide unique data regarding their orbit and operation time. One of the onboard instruments, the Experiment 6 (E6), is capable of measuring ions from 4 to several hundred MeV/nuc.   Aims: In this paper we aim to demonstrate the relevance of the E6 data for the calculation of galactic cosmic ray (GCR), anomolous cosmic ray (ACR), and solar energetic particle (SEP) fluxes for different distances from the sun and time periods.   Methods: Several corrections have been applied to the raw data: determination of the Quenching factor of the scintillator, correction of the temperature dependent electronics, degradation of the scintillator as well as the effects on the edge of semi-conductor detectors.   Results: Fluxes measured by the E6 are in accordance with the force field solution for the GCR and match models of the anomalous cosmic ray propagation. GCR radial gradients in the inner heliosphere show a different behaviour than in the outer heliosphere ",1905.01052v1
"['Jan Gieseler', 'Bernd Heber']",2016-02-01T14:05:10Z,Spatial gradients of GCR protons in the inner heliosphere derived from   Ulysses COSPIN/KET and PAMELA measurements,"  During the transition from solar cycle 23 to 24 from 2006 to 2009, the Sun was in an unusual solar minimum with very low activity over a long period. These exceptional conditions included a very low interplanetary magnetic field (IMF) strength and a high tilt angle, which both play an important role in the modulation of galactic cosmic rays (GCR) in the heliosphere. Thus, the radial and latitudinal gradients of GCRs are very much expected to depend not only on the solar magnetic epoch, but also on the overall modulation level. We determine the non-local radial and the latitudinal gradients of protons in the rigidity range from ~0.45 to 2 GV. This was accomplished by using data from the satellite-borne experiment Payload for Antimatter Matter Exploration and Light-nuclei Astrophysics (PAMELA) at Earth and the Kiel Electron Telescope (KET) onboard Ulysses on its highly inclined Keplerian orbit around the Sun with the aphelion at Jupiter's orbit. In comparison to the previous A>0 solar magnetic epoch, we find that the absolute value of the latitudinal gradient is lower at higher and higher at lower rigidities. This energy dependence is therefore a crucial test for models that describe the cosmic ray transport in the inner heliosphere. ",1602.00533v1
"['Nina Dresing', 'Solveig Theesen', 'Andreas Klassen', 'Bernd Heber']",2016-02-10T16:49:32Z,Efficiency of particle acceleration at interplanetary shocks:   Statistical study of STEREO observations,"  Context. Among others, shocks are known to be accelerators of energetic charged particles. However, many questions regarding the acceleration efficiency and the required conditions are not fully understood. In particular, the acceleration of electrons by shocks is often questioned. Aims. In this study we determine the efficiency of interplanetary shocks for $<$100 keV electrons, and for ions at $\sim$0.1 and $\sim$2 MeV energies, as measured by the Solar Electron and Proton Telescope (SEPT) instruments aboard the twin Solar Terrestrial Relations Observatory (STEREO) spacecraft. Methods. We employ an online STEREO in situ shock catalog that lists all shocks observed between 2007 and mid 2014 (observed by STEREO A) and until end of 2013 (observed by STEREO B). In total 475 shocks are listed. To determine the particle acceleration efficiency of these shocks, we analyze the associated intensity increases (shock spikes) during the shock crossings. For the near-relativistic electrons, we take into account the issue of possible ion contamination in the SEPT instrument. Results. The highest acceleration efficiency is found for low energy ions (0.1 MeV), which show a shock-associated increase at 27% of all shocks. The 2 MeV ions show an associated increase only during 5% of the shock crossings. In the case of the electrons, the shocks are nearly ineffective. Only five shock-associated electron increases were found, which correspond to only 1% of all shock crossings. ",1602.03440v1
"['Jan Gieseler', 'Bernd Heber', 'Konstantin Herbst']",2017-10-30T09:54:48Z,An empirical modification of the force field approach to describe the   modulation of galactic cosmic rays close to Earth in a broad range of   rigidities,"  On their way through the heliosphere, Galactic Cosmic Rays (GCRs) are modulated by various effects before they can be detected at Earth. This process can be described by the Parker equation, which calculates the phase space distribution of GCRs depending on the main modulation processes: convection, drifts, diffusion and adiabatic energy changes. A first order approximation of this equation is the force field approach, reducing it to a one-parameter dependency, the solar modulation potential $\phi$. Utilizing this approach, it is possible to reconstruct $\phi$ from ground based and spacecraft measurements. However, it has been shown previously that $\phi$ depends not only on the Local Interstellar Spectrum (LIS) but also on the energy range of interest. We have investigated this energy dependence further, using published proton intensity spectra obtained by PAMELA as well as heavier nuclei measurements from IMP-8 and ACE/CRIS. Our results show severe limitations at lower energies including a strong dependence on the solar magnetic epoch. Based on these findings, we will outline a new tool to describe GCR proton spectra in the energy range from a few hundred MeV to tens of GeV over the last solar cycles. In order to show the importance of our modification, we calculate the global production rates of the cosmogenic radionuclide $^{10}$Be which is a proxy for the solar activity ranging back thousands of years. ",1710.10834v1
"['Anamarija Kirin', 'Bojan Vršnak', 'Mateja Dumbović', 'Bernd Heber']",2020-02-13T17:24:25Z,On the Interaction of Galactic Cosmic Rays with Heliospheric Shocks   During Forbush Decreases,"  Forbush decreases (FDs) are depletions in the galactic cosmic ray (GCR) count rate that last typically for about a week and can be caused by coronal mass ejections (CMEs) or corotating interacting regions (CIRs). Fast CMEs that drive shocks cause large FDs that often show a two-step decrease where the first step is attributed to the shock/sheath region, while the second step is attributed to the closed magnetic structure. Since the difference in size of shock and sheath region is significant, and since there are observed effects that can be related to shocks and not necessarily to the sheath region we expect that the physical mechanisms governing the interaction with GCRs in these two regions are different. We therefore aim to analyse interaction of GCRs with heliospheric shocks only. We approximate the shock by a structure where the magnetic field linearly changes with position within this structure. We assume protons of different energy, different pitch angle and different incoming direction. We also vary the shock parameters such as the magnetic field strength and orientation, as well as the shock thickness. The results demonstrate that protons with higher energies are less likely to be reflected. Also, thicker shocks and shocks with stronger field reflect protons more efficiently. ",2002.09454v1
"['Rolf Bütikofer', 'Neus Agueda', 'Bernd Heber', 'Dennis Galsdorf', 'Rami Vainio']",2016-12-28T16:30:30Z,Assessment of Source and Transport Parameters of Relativistic SEPs Based   on Neutron Monitor Data,"  As part of the HESPERIA Horizon 2020 project, we developed a software package for the direct inversion of Ground Level Enhancements (GLEs) based on data of the worldwide network of Neutron Monitors (NMs). The new methodology to study the release processes of relativistic solar energetic particles (SEPs) makes use of several models, including: the propagation of relativistic SEPs from the Sun to the Earth, their transport in the Earth's magnetosphere and atmosphere, as well as the detection of the nucleon component of the secondary cosmic rays by ground based NMs. The combination of these models allows to compute the expected ground-level NM counting rates caused by a series of instantaneous particle releases from the Sun. The proton release-time profile at the Sun and the interplanetary transport conditions are then inferred by fitting NM observations with modeled NM counting rates. In the paper the used models for the different processes, the software and first findings with the new software are presented. ",1612.08922v1
"['Bojan Vrsnak', 'Mateja Dumbovic', 'Bernd Heber', 'Anamarija Kirin']",2022-01-24T11:53:35Z,Analytic modeling of recurrent Forbush decreases caused by corotating   interaction regions,"  On scales of days, the galactic cosmic ray (GCR) flux is affected by coronal mass ejections and corotating interaction regions (CIRs), causing so-called Forbush decreases and recurrent Forbush decreases (RFDs), respectively. We explain the properties and behavior of RFDs recorded at about 1 au that are caused by CIRs generated by solar wind high-speed streams (HSSs) that emanate from coronal holes. We employed a convection-diffusion GCR propagation model based on the Fokker-Planck equation and applied it to solar wind and interplanetary magnetic field properties at 1 au. Our analysis shows that the only two effects that are relevant for a plausible overall explanation of the observations are the enhanced convection effect caused by the increased velocity of the HSS and the reduced diffusion effect caused by the enhanced magnetic field and its fluctuations within the CIR and HSS structure. These two effects that we considered in the model are sufficient to explain not only the main signatures of RFDs, but also the sometimes observed ""over-recovery"" and secondary dips in RFD profiles. The explanation in terms of the convection-diffusion GCR propagation hypothesis is tested by applying our model to the observations of a long-lived CIR that recurred over 27 rotations in 2007-2008. Our analysis demonstrates a very good match of the model results and observations. ",2201.09619v1
"['Patrick Kühl', 'Nina Dresing', 'Bernd Heber', 'Andreas Klassen']",2016-11-10T12:55:48Z,Solar Energetic Particle Events with Protons above 500 MeV between 1995   and 2015 Measured with SOHO/EPHIN,"  The Sun is an effective particle accelerator producing solar energetic particle (SEP) events during which particles up to several GeVs can be observed. Those events observed at Earth with the neutron monitor network are called ground level enhancements (GLEs). Although these events with a high energy component have been investigated for several decades, a clear relation between the spectral shape of the SEPs outside the Earth's magnetosphere and the increase in neutron monitor count rate has yet to be established. Hence, an analysis of these events is of interest for the space weather as well as the solar event community. In this work, SEP events with protons accelerated to above 500 MeV have been identified using data from the Electron Proton Helium Instrument (EPHIN) aboard the Solar and Heliospheric Observatory (SOHO) between 1995 and 2015. For a statistical analysis, onset times have been determined for the events and the proton energy spectra were derived and fitted with a power law. As a result, a list of 42 SEP events with protons accelerated to above 500 MeV measured with the EPHIN instrument onboard SOHO is presented. The statistical analysis based on the fitted spectral slopes and absolute intensities is discussed with special emphasis on whether or not an event has been observed as GLE. Furthermore, a correlation between the derived intensity at 500 MeV and the observed increase in neutron monitor count rate has been found for a subset of events. ",1611.03289v1
"['Patrick Kühl', 'Bernd Heber', 'Raúl Gómez-Herrero', 'Olga Malandraki', 'Arik Posner', 'Holger Sierks']",2020-10-02T08:47:20Z,The Electron Proton Helium INstrument as an Example for a Space Weather   Radiation Instrument,"  The near-Earth energetic particle environment has been monitored since the 1970's. With the increasing importance of quantifying the radiation risk for, e.g. for the human exploration of the Moon and Mars, it is essential to continue and further improve these measurements. The Electron Proton Helium INstrument (EPHIN) on-board SOHO continually provides these data sets to the solar science and space weather communities since 1995. Here, we introduce the numerous data products developed over the years and present space weather related applications. Important design features that have led to EPHINs success as well as lessons learned and possible improvements to the instrument are also discussed with respect to the next generation of particle detectors. ",2010.00864v1
"['Nina Dresing', 'Raúl Gómez-Herrero', 'Andreas Klassen', 'Bernd Heber', 'Yulia Kartavykh', 'Wolfgang Dröge']",2012-06-07T15:17:48Z,"The large longitudinal spread of solar energetic particles during the   January 17, 2010 solar event","  We investigate multi-spacecraft observations of the January 17, 2010 solar energetic particle event. Energetic electrons and protons have been observed over a remarkable large longitudinal range at the two STEREO spacecraft and SOHO suggesting a longitudinal spread of nearly 360 degrees at 1AU. The flaring active region, which was on the backside of the Sun as seen from Earth, was separated by more than 100 degrees in longitude from the magnetic footpoints of each of the three spacecraft. The event is characterized by strongly delayed energetic particle onsets with respect to the flare and only small or no anisotropies in the intensity measurements at all three locations. The presence of a coronal shock is evidenced by the observation of a type II radio burst from the Earth and STEREO B. In order to describe the observations in terms of particle transport in the interplanetary medium, including perpendicular diffusion, a 1D model describing the propagation along a magnetic field line (model 1) (Dr\""oge, 2003) and the 3D propagation model (model 2) by (Dr\""oge et al., 2010) including perpendicular diffusion in the interplanetary medium have been applied, respectively. While both models are capable of reproducing the observations, model 1 requires injection functions at the Sun of several hours. Model 2, which includes lateral transport in the solar wind, reveals high values for the ratio of perpendicular to parallel diffusion. Because we do not find evidence for unusual long injection functions at the Sun we favor a scenario with strong perpendicular transport in the interplanetary medium as explanation for the observations. ",1206.1520v1
"['Mateja Dumbović', 'Bernd Heber', 'Bojan Vršnak', 'Manuela Temmer', 'Anamarija Kirin']",2018-05-02T17:23:53Z,An Analytical Diffusion-Expansion Model for Forbush Decreases Caused by   Flux Ropes,"  We present an analytical diffusion-expansion Forbush decrease (FD) model ForbMod which is based on the widely used approach of the initially empty, closed magnetic structure (i.e. flux rope) which fills up slowly with particles by perpendicular diffusion. The model is restricted to explain only the depression caused by the magnetic structure of the interplanetary coronal mass ejection (ICME). We use remote CME observations and a 3D reconstruction method (the Graduated Cylindrical Shell method) to constrain initial boundary conditions of the FD model and take into account CME evolutionary properties by incorporating flux rope expansion. Several flux rope expansion modes are considered, which can lead to different FD characteristics. In general, the model is qualitatively in agreement with observations, whereas quantitative agreement depends on the diffusion coefficient and the expansion properties (interplay of the diffusion and the expansion). A case study was performed to explain the FD observed 2014 May 30. The observed FD was fitted quite well by ForbMod for all expansion modes using only the diffusion coefficient as a free parameter, where the diffusion parameter was found to correspond to expected range of values. Our study shows that in general the model is able to explain the global properties of FD caused by FR and can thus be used to help understand the underlying physics in case studies. ",1805.00916v1
"['Stefan J. Hofmeister', 'Astrid Veronig', 'Manuela Temmer', 'Susanne Vennerstrom', 'Bernd Heber', 'Bojan Vršnak']",2018-04-25T14:09:39Z,The Dependence of the Peak Velocity of High-Speed Solar Wind Streams as   Measured in the Ecliptic by ACE and the STEREO satellites on the Area and   Co-Latitude of their Solar Source Coronal Holes,"  We study the properties of 115 coronal holes in the time-range from 2010/08 to 2017/03, the peak velocities of the corresponding high-speed streams as measured in the ecliptic at 1 AU, and the corresponding changes of the Kp index as marker of their geo-effectiveness. We find that the peak velocities of high-speed streams depend strongly on both the ar- eas and the co-latitudes of their solar source coronal holes with regard to the heliospheric latitude of the satellites. Therefore, the co-latitude of their source coronal hole is an im- portant parameter for the prediction of the high-speed stream properties near the Earth. We derive the largest solar wind peak velocities normalized to the coronal hole areas for coronal holes located near the solar equator, and that they linearly decrease with increas- ing latitudes of the coronal holes. For coronal holes located at latitudes & 60{\deg}, they turn statistically to zero, indicating that the associated high-speed streams have a high chance to miss the Earth. Similar, the Kp index per coronal hole area is highest for the coronal holes located near the solar equator and strongly decreases with increasing latitudes of the coronal holes. We interpret these results as an effect of the three-dimensional propaga- tion of high-speed streams in the heliosphere, i.e., high-speed streams arising from coro- nal holes near the solar equator propagate in direction towards and directly hit the Earth, whereas solar wind streams arising from coronal holes at higher solar latitudes only graze or even miss the Earth. ",1804.09579v2
"['Miikka Paassilta', 'Athanasios Papaioannou', 'Nina Dresing', 'Rami Vainio', 'Eino Valtonen', 'Bernd Heber']",2018-03-14T15:50:47Z,"Catalogue of >55 MeV Wide-longitude Solar Proton Events Observed by   SOHO, ACE, and the STEREOs at $\approx$1 AU during 2009-2016","  Based on energetic particle observations made at $\approx$1 AU, we present a catalogue of 46 wide-longitude (>45{\deg}) solar energetic particle (SEP) events detected at multiple locations during 2009-2016. The particle kinetic energies of interest were chosen as >55 MeV for protons and 0.18-0.31 MeV for electrons. We make use of proton data from the Solar and Heliospheric Observatory/Energetic and Relativistic Nuclei and Electron experiment (SOHO/ERNE) and the Solar Terrestrial Relations Observatory/High Energy Telescopes (STEREO/HET), together with electron data from the Advanced Composition Explorer/Electron, Proton, and Alpha Monitor (ACE/EPAM) and the STEREO/Solar Electron and Proton Telescopes (SEPT). We consider soft X-ray data from the Geostationary Operational Environmental Satellites (GOES) and coronal mass ejection (CME) observations made with the SOHO/Large Angle and Spectrometric Coronagraph (LASCO) and STEREO/Coronagraphs 1 and 2 (COR1, COR2) to establish the probable associations between SEP events and the related solar phenomena. Event onset times and peak intensities are determined; velocity dispersion analysis (VDA) and time-shifting analysis (TSA) are performed for protons; TSA is performed for electrons. In our event sample, there is a tendency for the highest peak intensities to occur when the observer is magnetically connected to solar regions west of the flare. Our estimates for the mean event width, derived as the standard deviation of a Gaussian curve modelling the SEP intensities (protons $\approx$44{\deg}, electrons $\approx$50{\deg}), largely agree with previous results for lower-energy SEPs. SEP release times with respect to event flares, as well as the event rise times, show no simple dependence on the observer's connection angle, suggesting that... ",1803.05370v3
"['Nina Dresing', 'Raúl Gómez-Herrero', 'Bernd Heber', 'Andreas Klassen', 'Manuela Temmer', 'Astrid Veronig']",2018-02-13T16:40:12Z,Long-lasting injection of solar energetic electrons into the heliosphere,"  The main sources of solar energetic particle (SEP) events are solar flares and shocks driven by coronal mass ejections (CMEs). While it is generally accepted that energetic protons can be accelerated by shocks, whether or not these shocks can also efficiently accelerate solar energetic electrons is still debated. In this study we present observations of the extremely widespread SEP event of 26 Dec 2013. To the knowledge of the authors, this is the widest longitudinal SEP distribution ever observed together with unusually long-lasting energetic electron anisotropies at all observer positions. Further striking features of the event are long-lasting SEP intensity increases, two distinct SEP components with the second component mainly consisting of high-energy particles, a complex associated coronal activity including a pronounced signature of a shock in radio type-II observations, and the interaction of two CMEs early in the event. The observations require a prolonged injection scenario not only for protons but also for electrons. We therefore analyze the data comprehensively to characterize the possible role of the shock for the electron event. Remote-sensing observations of the complex solar activity are combined with in-situ measurements of the particle event. We also apply a Graduated Cylindrical Shell (GCS) model to the coronagraph observations of the two associated CMEs to analyze their interaction. We find that the shock alone is likely not responsible for this extremely wide SEP event. Therefore we propose a scenario of trapped energetic particles inside the CME-CME interaction region which undergo further acceleration due to the shock propagating through this region, stochastic acceleration, or ongoing reconnection processes inside the interaction region. The origin of the second component of the SEP event is likely caused by a sudden opening of the particle trap. ",1802.04722v1
"['Mateja Dumbovic', 'Bojan Vrsnak', 'Manuela Temmer', 'Bernd Heber', 'Patrick Kuhl']",2022-01-24T12:04:12Z,Generic profile of a long-lived corotating interaction region and   associated recurrent Forbush decrease,"  We observe and analyse a long-lived corotating interaction region (CIR), originating from a single coronal hole (CH), recurring in 27 consecutive Carrington rotations 2057-2083 in the time period from June 2007 - May 2009. We studied the in situ measurements of this long-lived CIR as well as the corresponding depression in the cosmic ray (CR) count observed by SOHO/EPHIN throughout different rotations. We performed a statistical analysis, as well as the superposed epoch analysis, using relative values of the key parameters: the total magnetic field strength, B, the magnetic field fluctuations, dBrms, plasma flow speed, v, plasma density, n, plasma temperature, T , and the SOHO/EPHIN F-detector particle count, and CR count. We find that the mirrored CR count-time profile is correlated with that of the flow speed, ranging from moderate to strong correlation, depending on the rotation. In addition, we find that the CR count dip amplitude is correlated to the peak in the magnetic field and flow speed of the CIR. These results are in agreement with previous statistical studies. Finally, using the superposed epoch analysis, we obtain a generic CIR example, which reflects the in situ properties of a typical CIR well. Our results are better explained based on the combined convection-diffusion approach of the CIR-related GCR modulation. Furthermore, qualitatively, our results do not differ from those based on different CHs samples. This indicates that the change of the physical properties of the recurring CIR from one rotation to another is not qualitatively different from the change of the physical properties of CIRs originating from different CHs. Finally, the obtained generic CIR example, analyzed on the basis of superposed epoch analysis, can be used as a reference for testing future models. ",2201.09623v1
"['Jingnan Guo', 'Saša Banjac', 'Lennart Roestel', 'Jan C. Terasa', 'Konstantin Herbst', 'Bernd Heber', 'Robert F. Wimmer-Schweingruber']",2019-01-07T13:10:33Z,Implementation and validation of the GEANT4/AtRIS code to model the   radiation environment at Mars,"  A new GEANT4 particle transport model -- the Atmospheric Radiation Interaction Simulator (AtRIS, Banjac et al. 2018a. J. Geophys. Res.) -- has been recently developed in order to model the interaction of radiation with planets. The upcoming instrumentational advancements in the exoplanetary science, in particular transit spectroscopy capabilities of missions like JWST and E-ELT, have motivated the development of a particle transport code with a focus on providing the necessary flexibility in planet specification (atmosphere and soil geometry and composition, tidal locking, oceans, clouds, etc.) for the modeling of radiation environment for exoplanets. Since there are no factors limiting the applicability of AtRIS to Mars and Venus, AtRIS' unique flexibility opens possibilities for new studies. Following the successful validation against Earth measurements Banjac et al. 2018, J. Geophys. Res., this work applies AtRIS with a specific implementation of the Martian atmospheric and regolith structure to model the radiation environment at Mars. We benchmark these first modeling results based on different GEANT4 physics lists with the energetic particle spectra recently measured by the Radiation Assessment Detector (RAD) on the surface of Mars. The good agreement between AtRIS and the actual measurement provides one of the first and sound validations of AtRIS and the preferred physics list which could be recommended for predicting the radiation field of other conceivable (exo)planets with an atmospheric environment similar to Mars. ",1901.01787v1
"['Nina Dresing', 'Frederic Effenberger', 'Raul Gomez-Herrero', 'Bernd Heber', 'Andreas Klassen', 'Alexander Kollhoff', 'Ian Richardson', 'Solveig Theesen']",2019-12-21T15:32:52Z,Statistical results for solar energetic electron spectra observed over   12 years with STEREO/SEPT,"  We present a statistical analysis of near-relativistic (NR) solar energetic electron event spectra near 1au. We use measurements of the STEREO Solar Electron and Proton Telescope (SEPT) in the energyrange of 45-425 keV and utilize the SEPT electron event list containing all electron events observed bySTEREO A and STEREO B from 2007 through 2018. We select 781 events with significant signal tonoise ratios for our analysis and fit the spectra with single or broken power law functions of energy.We find 437 (344) events showing broken (single) power laws in the energy range of SEPT. The eventswith broken power laws show a mean break energy of about 120 keV. We analyze the dependence ofthe spectral index on the rise times and peak intensities of the events as well as on the presence ofrelativistic electrons. The results show a relation between the power law spectral index and the risetimes of the events with softer spectra belonging to rather impulsive events. Long rise-time events areassociated with hard spectra as well as with the presence of higher energy (>0.7 MeV) electrons. Thisgroup of events cannot be explained by a pure flare scenario but suggests an additional accelerationmechanism, involving a prolonged acceleration and/or injection of the particles. A dependence of thespectral index on the longitudinal separation from the parent solar source region was not found. Astatistical analysis of the spectral indices during impulsively rising events (rise times<20 minutes) isalso shown. ",1912.10279v1
"['Tobias Wiengarten', 'Jens Kleimann', 'Horst Fichtner', 'Patrick Kühl', 'Andreas Kopp', 'Bernd Heber', 'Ralf Kissmann']",2014-06-02T08:46:48Z,Cosmic Ray Transport in Heliospheric Magnetic Structures: I. Modeling   Background Solar Wind Using the CRONOS MHD Code,"  The transport of energetic particles such as Cosmic Rays is governed by the properties of the plasma being traversed. While these properties are rather poorly known for galactic and interstellar plasmas due to the lack of in situ measurements, the heliospheric plasma environment has been probed by spacecraft for decades and provides a unique opportunity for testing transport theories. Of particular interest for the 3D heliospheric transport of energetic particles are structures such as corotating interaction regions (CIRs), which, due to strongly enhanced magnetic field strengths, turbulence, and associated shocks, can act as diffusion barriers on the one hand, but also as accelerators of low energy CRs on the other hand as well. In a two-fold series of papers we investigate these effects by modeling inner-heliospheric solar wind conditions with a numerical magnetohydrodynamic (MHD) setup (this paper), which will serve as an input to a transport code employing a stochastic differential equation (SDE) approach (second paper). In this first paper we present results from 3D MHD simulations with our code CRONOS: for validation purposes we use analytic boundary conditions and compare with similar work by Pizzo. For a more realistic modeling of solar wind conditions, boundary conditions derived from synoptic magnetograms via the Wang-Sheeley-Arge (WSA) model are utilized, where the potential field modeling is performed with a finite-difference approach (FDIPS) in contrast to the traditional spherical harmonics expansion often utilized in the WSA model. Our results are validated by comparing with multi-spacecraft data for ecliptical (STEREO-A/B) and out-of-ecliptic (Ulysses) regions. ",1406.0293v1
"['Nina Dresing', 'Raúl Gómez-Herrero', 'Bernd Heber', 'Miguel Angel Hidalgo', 'Andreas Klassen', 'Manuela Temmer', 'Astrid Veronig']",2016-01-04T13:13:58Z,Injection of solar energetic particles into both loop legs of a magnetic   cloud,"  Each of the two STEREO spacecraft carries a SEPT Instrument which measures electrons and protons. Anisotropy observations are provided in four viewing directions. The SEP event on 7 Nov 2013 was observed by both STEREO spacecraft, which were longitudinally separated by 68{\deg} at that time. While STEREO A observed the expected characteristics of an SEP event at a well-connected position, STEREO B detected a very anisotropic bi-directional distribution of near-relativistic electrons and was situated inside a magnetic-cloud-like structure during the early phase of the event. We examine the source of the bi-directional SEP distribution at STEREO B. On the one hand this distribution could be caused by a double injection into both loop legs of the MC. On the other hand, a mirroring scenario where the incident beam is reflected in the opposite loop leg could be the reason. Furthermore, the energetic electron observations are used to probe the magnetic structure inside the magnetic cloud. We show that STEREO B was embedded in an MC-like structure ejected three days earlier. We apply a GCS model to the coronagraph observations from three viewpoints as well as the Global Magnetic Cloud model to the in situ measurements at STEREO B to determine the orientation and topology of the MC close to the Sun and at 1 AU. We also estimate the path lengths of the electrons propagating through the MC to estimate the amount of magnetic field line winding inside the structure. The relative intensity and timing of the energetic electron increases in the different SEPT telescopes at STEREO B strongly suggest that the bi-directional electron distribution is formed by SEP injections in both loop legs of the MC separately instead of by mirroring farther away beyond the STEREO orbit. Observations by the Nancay Radioheliograph of two distinct radio sources during the SEP injection further support the above scenario. ",1601.00491v1
"['Mateja Dumbović', 'Bojan Vršnak', 'Jingnan Guo', 'Bernd Heber', 'Karin Dissauer', 'Fernando Carcaboso', 'Manuela Temmer', 'Astrid Veronig', 'Tatiana Podladchikova', 'Christian Möstl', 'Tanja Amerstorfer', 'Anamarija Kirin']",2020-06-03T13:13:02Z,Evolution of coronal mass ejections and the corresponding Forbush   decreases: modelling vs multi-spacecraft observations,"  One of the very common in situ signatures of interplanetary coronal mass ejections (ICMEs), as well as other interplanetary transients, are Forbush decreases (FDs), i.e. short-term reductions in the galactic cosmic ray (GCR) flux. A two-step FD is often regarded as a textbook example, which presumably owes its specific morphology to the fact that the measuring instrument passed through the ICME head-on, encountering first the shock front (if developed), then the sheath and finally the CME magnetic structure. The interaction of GCRs and the shock/sheath region, as well as the CME magnetic structure, occurs all the way from Sun to Earth, therefore, FDs are expected to reflect the evolutionary properties of CMEs and their sheaths. We apply modelling to different ICME regions in order to obtain a generic two-step FD profile, which qualitatively agrees with our current observation-based understanding of FDs. We next adapt the models for energy dependence to enable comparison with different GCR measurement instruments (as they measure in different particle energy ranges). We test these modelling efforts against a set of multi-spacecraft observations of the same event, using the Forbush decrease model for the expanding flux rope (ForbMod). We find a reasonable agreement of the ForbMod model for the GCR depression in the CME magnetic structure with multi-spacecraft measurements, indicating that modelled FDs reflect well the CME evolution. ",2006.02253v1
"['Miikka Paassilta', 'Osku Raukunen', 'Rami Vainio', 'Eino Valtonen', 'Athanasios Papaioannou', 'Robert Siipola', 'Esa Riihonen', 'Mark Dierckxsens', 'Norma Crosby', 'Olga Malandraki', 'Bernd Heber', 'Karl-Ludwig Klein']",2017-07-03T12:07:41Z,Catalogue of 55-80 MeV solar proton events extending through solar   cycles 23 and 24,"  We present a new catalogue of solar energetic particle events near the Earth, covering solar cycle 23 and the majority of solar cycle 24 (1996-2016), based on the 55-80 MeV proton intensity data gathered by the SOHO/ERNE experiment. In addition to ERNE proton and heavy ion observations, data from the ACE/EPAM (near-relativistic electrons), SOHO/EPHIN (relativistic electrons), SOHO/LASCO (coronal mass ejections, CMEs), and GOES soft X-ray experiments are also considered and the associations between the particle and CME/X-ray events deduced to obtain a better understanding of each event. A total of 176 SEP events have been identified as having occurred during the time period of interest; their onset and solar release times have been estimated using both velocity dispersion analysis (VDA) and time-shifting analysis (TSA) for protons, as well as TSA for near-relativistic electrons. Additionally, a brief statistical analysis has been performed on the VDA and TSA results, as well as the X-rays and CMEs associated with the proton/electron events, both to test the viability of the VDA and to investigate possible differences between the two solar cycles. We find, in confirmation of a number of previous studies, that VDA results for protons that yield an apparent path length of 1 AU < s <~ 3 AU seem to be useful, but those outside this range are probably unreliable, as evidenced by the anticorrelation between apparent path length and release time estimated from the X-ray activity. It also appears that even the first-arriving energetic protons apparently undergo significant pitch angle scattering in the interplanetary medium, with the resulting apparent path length being on average about twice the length of the spiral magnetic field. The analysis indicates an increase in high-energy SEP events originating from the far eastern solar hemisphere; e.g., such an event... ",1707.00498v1
"['Konstantin Herbst', 'John Lee Grenfell', 'Miriam Sinnhuber', 'Heike Rauer', 'Bernd Heber', 'Saša Banjac', 'Markus Scheucher', 'Vanessa Schmidt', 'Stefanie Gebauer', 'Ralph Lehmann', 'Franz Schreier']",2019-09-25T17:25:53Z,A New Model Suite to Determine the Influence of Cosmic Rays on   (Exo)planetary Atmospheric Biosignatures -- Validation based on Modern Earth,"  The first opportunity to detect indications for life outside the Solar System may be provided already within the next decade with upcoming missions such as the James Webb Space Telescope (JWST), the European Extremely Large Telescope (E-ELT) and/or the Atmospheric Remote-sensing Infrared Exoplanet Large-survey (ARIEL) mission, searching for atmospheric biosignatures on planets in the habitable zone of cool K- and M-stars. Nevertheless, their harsh stellar radiation and particle environment could lead to photochemical loss of atmospheric biosignatures. We aim to study the influence of cosmic rays on exoplanetary atmospheric biosignatures and the radiation environment considering feedbacks between energetic particle precipitation, climate, atmospheric ionization, neutral and ion chemistry, and secondary particle generation. We describe newly-combined state-of-the-art modeling tools to study the impact of the radiation and particle environment on atmospheric particle interaction, the influence on the atmospheric chemistry, and the climate-chemistry coupling in a self-consistent model suite. To this end, models like the Atmospheric Radiation Interaction Simulator (AtRIS), the Exoplanetary Terrestrial Ion Chemistry model (ExoTIC), and the updated coupled climate-chemistry model are combined. Amongst others, we model the atmospheric response during quiescent solar periods and during a strong solar energetic particle event as well as the scenario-dependent terrestrial transit spectra, as seen by the NIR-Spec infrared spectrometer onboard the JWST. We find that the comparatively weak solar event drastically increases the spectral signal of HNO$_3$, while significantly suppressing the spectral feature of ozone. Because of the slow recovery after such events, the latter indicates that ozone might not be a good biomarker for planets orbiting stars with high flaring rates. ",1909.11632v1
"['Nat Gopalswamy', 'Joseph M. Davila', 'Frédéric Auchère', 'Jesper Schou', 'Clarence Korendike', 'Albert Shih', 'Janet C. Johnston', 'Robert J. MacDowall', 'Milan Maksimovic', 'Edward Sittler', 'Adam Szabo', 'Richard Wesenberg', 'Suzanne Vennerstrom', 'Bernd Heber']",2011-09-13T21:06:58Z,Earth-Affecting Solar Causes Observatory (EASCO): A mission at the   Sun-Earth L5,"  Coronal mass ejections (CMEs) and corotating interaction regions (CIRs) as well as their source regions are important because of their space weather consequences. The current understanding of CMEs primarily comes from the Solar and Heliospheric Observatory (SOHO) and the Solar Terrestrial Relations Observatory (STEREO) missions, but these missions lacked some key measurements: STEREO did not have a magnetograph; SOHO did not have in-situ magnetometer. SOHO and other imagers such as the Solar Mass Ejection Imager (SMEI) located on the Sun-Earth line are also not well-suited to measure Earth-directed CMEs. The Earth-Affecting Solar Causes Observatory (EASCO) is a proposed mission to be located at the Sun-Earth L5 that overcomes these deficiencies. The mission concept was recently studied at the Mission Design Laboratory (MDL), NASA Goddard Space Flight Center, to see how the mission can be implemented. The study found that the scientific payload (seven remote-sensing and three in-situ instruments) can be readily accommodated and can be launched using an intermediate size vehicle; a hybrid propulsion system consisting of a Xenon ion thruster and hydrazine has been found to be adequate to place the payload at L5. Following a 2-year transfer time, a 4-year operation is considered around the next solar maximum in 2025. ",1109.2929v1
"['Jingnan Guo', 'Cary Zeitlin', 'Robert F. Wimmer-Schweingruber', 'Scot Rafkin', 'Donald M. Hassler', 'Arik Posner', 'Bernd Heber', 'Jan Koehler', 'Bent Ehresmann', 'Jan K. Appel', 'Eckart Boehm', 'Stephan Boettcher', 'Soenke Burmeister', 'David E. Brinza', 'Henning Lohf', 'Cesar Martin', 'H. Kahanpaeae', 'Guenther Reitz']",2015-07-13T14:29:17Z,Modeling the variations of Dose Rate measured by RAD during the first   MSL Martian year: 2012-2014,"  The Radiation Assessment Detector (RAD), on board Mars Science Laboratory's (MSL) rover Curiosity, measures the {energy spectra} of both energetic charged and neutral particles along with the radiation dose rate at the surface of Mars. With these first-ever measurements on the Martian surface, RAD observed several effects influencing the galactic cosmic ray (GCR) induced surface radiation dose concurrently: [a] short-term diurnal variations of the Martian atmospheric pressure caused by daily thermal tides, [b] long-term seasonal pressure changes in the Martian atmosphere, and [c] the modulation of the primary GCR flux by the heliospheric magnetic field, which correlates with long-term solar activity and the rotation of the Sun. The RAD surface dose measurements, along with the surface pressure data and the solar modulation factor, are analysed and fitted to empirical models which quantitatively demonstrate} how the long-term influences ([b] and [c]) are related to the measured dose rates. {Correspondingly we can estimate dose rate and dose equivalents under different solar modulations and different atmospheric conditions, thus allowing empirical predictions of the Martian surface radiation environment. ",1507.03473v2
"['Jingnan Guo', 'Cary Zeitlin', 'Robert F. Wimmer-Schweingruber', 'Donald M. Hassler', 'Arik Posner', 'Bernd Heber', 'Jan Köhler', 'Scot Rafkin', 'Bent Ehresmann', 'Jan K. Appel', 'Eckart Böhm', 'Stephan Böttcher', 'Sönke Burmeister', 'David E. Brinza', 'Henning Lohf', 'Cesar Martin', 'Günther Reitz']",2015-03-23T13:23:56Z,Variations of dose rate observed by MSL/RAD in transit to Mars,"  Aims: To predict the cruise radiation environment related to future human missions to Mars, the correlation between solar modulation potential and the dose rate measured by the Radiation Assessment Detector (RAD) has been analyzed and empirical models have been employed to quantify this correlation. Methods: The instrument RAD, onboard Mars Science Laboratory's (MSL) rover Curiosity, measures a broad spectrum of energetic particles along with the radiation dose rate during the 253-day cruise phase as well as on the surface of Mars. With these first ever measurements inside a spacecraft from Earth to Mars, RAD observed the impulsive enhancement of dose rate during solar particle events as well as a gradual evolution of the galactic cosmic ray (GCR) induced radiation dose rate due to the modulation of the primary GCR flux by the solar magnetic field, which correlates with long-term solar activities and heliospheric rotation. Results: We analyzed the dependence of the dose rate measured by RAD on solar modulation potentials and estimated the dose rate and dose equivalent under different solar modulation conditions. These estimations help us to have approximate predictions of the cruise radiation environment, such as the accumulated dose equivalent associated with future human missions to Mars. Conclusions: The predicted dose equivalent rate during solar maximum conditions could be as low as one-fourth of the current RAD cruise measurement. However, future measurements during solar maximum and minimum periods are essential to validate our estimations. ",1503.06631v1
"['Daniel Pacheco', 'Neus Agueda', 'Angels Aran', 'Bernd Heber', 'David Lario', ' .']",2019-02-18T14:58:42Z,Full inversion of solar relativistic electron events measured by the   Helios spacecraft,"  Up to present, the largest data set of SEP events in the inner heliosphere are the observations by the two Helios spacecraft. We re-visit a sample of 15 solar relativistic electron events measured by the Helios mission with the goal of better characterising the injection histories of solar energetic particles and their interplanetary transport conditions at heliocentric distances <1 AU. The measurements provided by the E6 instrument on board Helios provide us with the electron directional distributions in eight different sectors that we use to infer the detailed evolution of the electron pitch-angle distributions. The results of a Monte Carlo interplanetary transport model, combined with a full inversion procedure, were used to fit the observed directional intensities in the 300-800 keV nominal energy channel. Unlike previous studies, we have considered both the energy and angular responses of the detector. This method allowed us to infer the electron release time profile at the source and determine the electron interplanetary transport conditions. We discuss the duration of the release time profiles and the values of the radial mean free path, and compare them with the values reported previously in the literature using earlier approaches. Five of the events show short injection histories (<30 min) at the Sun and ten events show long-lasting (>30 min) injections. The values of mean free path range from 0.02 AU to 0.27 AU. The inferred injection histories match with the radio and soft x-ray emissions found in literature. We find no dependence of the radial mean free path on the radial distance. In addition, we find no apparent relation between the strength of interplanetary scattering and the size of the solar particle release. ",1902.06602v2
"['Du Toit Strauss', 'Stepan Poluianov', 'Cobus van der Merwe', 'Hendrik Krüger', 'Corrie Diedericks', 'Helena Krüger', 'Ilya Usoskin', 'Bernd Heber', 'Rendani Nndanganeni', 'Juanjo Blanco-Ávalos', 'Ignacio García-Tejedor', 'Konstantin Herbst', 'Rogelio Caballero-Lopez', 'Katlego Moloto', 'Alejandro Lara', 'Michael Walter', 'Nigussie Mezgebe Giday', 'Rita Traversi']",2020-07-29T09:43:06Z,The mini-neutron monitor: A new approach in neutron monitor design,"  The near-Earth cosmic ray flux has been monitored for more than 70 years by a network of ground-based neutron monitors (NMs). With the ever-increasing importance of quantifying the radiation risk and effects of cosmic rays for, e.g., air and space-travel, it is essential to continue operating the existing NM stations, while expanding this crucial network. In this paper, we discuss a smaller and cost-effective version of the traditional NM, the mini-NM. These monitors can be deployed with ease, even to extremely remote locations, where they operate in a semi-autonomous fashion. We believe that the mini-NM, therefore, offers the opportunity to increase the sensitivity and expand the coverage of the existing NM network, making this network more suitable to near-real-time monitoring for space weather applications. In this paper, we present the technical details of the mini-NM's design and operation, and present a summary of the initial tests and science results. ",2007.14711v1
"['Johan L. Freiherr von Forstner', 'Jingnan Guo', 'Robert F. Wimmer-Schweingruber', 'Donald M. Hassler', 'Manuela Temmer', 'Mateja Dumbović', 'Lan K. Jian', 'Jan K. Appel', 'Jaša Čalogović', 'Bent Ehresmann', 'Bernd Heber', 'Henning Lohf', 'Arik Posner', 'Christian T. Steigies', 'Bojan Vršnak', 'Cary J. Zeitlin']",2017-12-19T12:34:20Z,Using Forbush decreases to derive the transit time of ICMEs propagating   from 1 AU to Mars,"  The propagation of 15 interplanetary coronal mass ejections (ICMEs) from Earth's orbit (1 AU) to Mars (~ 1.5 AU) has been studied with their propagation speed estimated from both measurements and simulations. The enhancement of magnetic fields related to ICMEs and their shock fronts cause the so-called Forbush decrease, which can be de- tected as a reduction of galactic cosmic rays measured on-ground. We have used galactic cosmic ray (GCR) data from in-situ measurements at Earth, from both STEREO A and B as well as GCR measurements by the Radiation Assessment Detector (RAD) instrument onboard Mars Science Laboratory (MSL) on the surface of Mars. A set of ICME events has been selected during the periods when Earth (or STEREO A or B) and Mars locations were nearly aligned on the same side of the Sun in the ecliptic plane (so-called opposition phase). Such lineups allow us to estimate the ICMEs' transit times between 1 and 1.5 AU by estimating the delay time of the corresponding Forbush decreases measured at each location. We investigate the evolution of their propagation speeds before and after passing Earth's orbit and find that the deceleration of ICMEs due to their interaction with the ambient solar wind may continue beyond 1 AU. We also find a substantial variance of the speed evolution among different events revealing the dynamic and diverse nature of eruptive solar events. Furthermore, the results are compared to simulation data obtained from two CME propagation models, namely the Drag-Based Model and ENLIL plus cone model. ",1712.07301v1
"['Jingnan Guo', 'Mateja Dumbović', 'Robert F. Wimmer-Schweingruber', 'Manuela Temmer', 'Henning Lohf', 'Yuming Wang', 'Astrid Veronig', 'Donald M. Hassler', 'Leila M. Mays', 'Cary Zeitlin', 'Bent Ehresmann', 'Oliver Witasse', 'Johan L. Freiherr von Forstner', 'Bernd Heber', 'Mats Holmström', 'Arik Posner']",2018-03-01T15:43:41Z,Modeling the evolution and propagation of the 2017 September 9th and   10th CMEs and SEPs arriving at Mars constrained by remote-sensing and in-situ   measurement,"  On 2017-09-10, solar energetic particles (SEPs) originating from the active region 12673 were registered as a ground level enhancement (GLE) at Earth and the biggest GLE on the surface of Mars as observed by the Radiation Assessment Detector (RAD) since the landing of the Curiosity rover in August 2012. Based on multi-point coronagraph images, we identify the initial 3D kinematics of an extremely fast CME and its shock front as well as another 2 CMEs launched hours earlier (with moderate speeds) using the Graduated Cylindrical Shell (GCS) model. These three CMEs interacted as they propagated outwards into the heliosphere and merged into a complex interplanetary CME (ICME). The arrival of the shock and ICME at Mars caused a very significant Forbush Decrease (FD) seen by RAD only a few hours later than that at Earth which is about 0.5 AU closer to the Sun. We investigate the propagation of the three CMEs and the consequent ICME together with the shock using the Drag Based Model (DBM) and the WSA-ENLIL plus cone model constrained by the in-situ SEP and FD/shock onset timing. The synergistic modeling of the ICME and SEP arrivals at Earth and Mars suggests that in order to better predict potentially hazardous space weather impacts at Earth and other heliospheric locations for human exploration missions, it is essential to analyze 1) the CME kinematics, especially during their interactions and 2) the spatially and temporally varying heliospheric conditions, such as the evolution and propagation of the stream interaction regions. ",1803.00461v4
"['Michael Bonitz', 'Andrea Scharnhorst']",2013-06-17T09:51:35Z,Remembering Manfred Bonitz (7.3.1931 -- 14.8.2012) on the first   anniversary of his death,  Biographical essay at the occasion of the first death anniversary of Manfred Bonitz ,1306.3789v1
"['Michael Bonitz', 'Anatoly Zagorodny']",2024-02-07T19:29:34Z,"Yuri Lvovich Klimontovich, his theory of fluctuations and its impact on   the kinetic theory",  Yuri L'vovich Klimontovich (28.09.1924--26.10.2002) was an outstanding theoretical physicist who made major contributions to kinetic theory. On the occasion of his 100th birthday we recall his main scientific achievements. ,2402.08689v1
"['Michael Bonitz', 'Antti-Pekka Jauho', 'Michael Sadovski', 'Sergei Tikhodeev']",2019-01-04T11:47:43Z,In memoriam Leonid V. Keldysh,"  Leonid Keldysh -- one of the most influential theoretical physicists of the 20th century -- passed away in November 2016. Keldysh is best known for the diagrammatic formulation of real-time (nonequilibrium) Green functions theory and for the theory of strong field ionization of atoms. Both theories profoundly changed large areas of theoretical physics and stimulated important experiments. Both these discoveries emerged almost simultaneously -- like Einstein, also Keldysh had his \textit{annus mirabilis} -- the year 1964. But the list of his theoretical developments is much broader and is briefly reviewed here. ",1901.01065v1
"['Torben Ott', 'Michael Bonitz', 'Zoltan Donko']",2015-06-11T10:00:20Z,The Effect of Correlations on the Heat Transport in a Magnetized Plasma,"  In a classical ideal plasma, a magnetic field is known to reduce the heat conductivity perpendicular to the field whereas it does not alter the one along the field. Here we show that, in strongly correlated plasmas that are observed at high pressure or/and low temperature, a magnetic field reduces the perpendicular heat transport much less and even {\it enhances} the parallel transport. These surprising observations are explained by the competition of kinetic, potential and collisional contributions to the heat conductivity. Our results are based on first principle molecular dynamics simulations of a one-component plasma. ",1506.03605v1
"['Miriam Scharnke', 'Niclas Schlünzen', 'Michael Bonitz']",2016-12-23T16:48:03Z,Time Reversal Invariance of quantum kinetic equations: Nonequilibrium   Green Functions Formalism,"  Time reversal symmetry is a fundamental property of many quantum mechanical systems. The relation between statistical physics and time reversal is subtle and not all statistical theories conserve this particular symmetry, most notably hydrodynamic equations and kinetic equations such as the Boltzmann equation. In this article it is shown analytically that quantum kinetic generalizations of the Boltzmann equation that are derived using the nonequilibrium Green functions formalism as well as all approximations that stem from $\Phi$-derivable selfenergies are time reversal invariant. ",1612.08033v1
"['Karsten Balzer', 'Michael Bonitz']",2021-03-09T15:51:57Z,Neutralization dynamics of slow highly charged ions passing through   graphene nanoflakes--an embedding self-energy approach,"  We study the time-dependent neutralization of a slow highly charged ion that penetrates a hexagonal hollow-centred graphene nanoflake. To compute the ultrafast charge transfer dynamics, we apply an effective Hubbard nanocluster model and use the method of nonequilibrium Green functions (NEGF) in conjunction with an embedding self-energy scheme which allows one to follow the temporal changes of the number of electrons in the nanoflake. We perform extensive simulations of the charge transfer dynamics for a broad range of ion charge states and impact velocities. The results are used to put forward a simple semi-analytical model of the neutralization dynamics that is in very good agreement with transmission experiments, in which highly charged xenon ions pass through sheets of single-layer graphene. ",2103.05507v1
"['Hanno Kählert', 'Michael Bonitz']",2023-01-09T15:24:04Z,Dynamic structure factor and excitation spectrum of the one-component   plasma: the case of weak to moderate magnetization,"  Magnetized plasmas are well known to exhibit a rich spectrum of collective modes. Here, we focus on the density modes in dense or cold plasmas, where strong coupling effects alter the mode spectrum known from traditional weakly coupled plasmas. In particular, we study the dynamic structure factor (DSF) of the magnetized one-component plasma with molecular dynamics simulations. Extending our previous results [H.~K\""ahlert and M.~Bonitz, Phys. Rev. Research \textbf{2022}, 4, 013197], it is shown that Bernstein modes can be observed in the weakly magnetized regime, where they are found below the upper hybrid frequency, provided the coupling strength is sufficiently low. We investigate the DSF for a variety of different wave numbers and plasma parameters and show that even small magnetization can give rise to a strong zero-frequency mode perpendicular to the magnetic field and change the dispersion as well as the damping of the upper hybrid mode. ",2301.03425v1
"['Erik Schroedter', 'Michael Bonitz']",2023-12-22T20:01:14Z,Two-Time Quantum Fluctuations Approach and its Relation to the   Bethe--Salpeter Equation,"  Correlated quantum many-particle systems out of equilibrium are of high interest in many fields, including correlated solids, ultracold atoms or dense plasmas. Accurate theoretical description of these systems is challenging both, conceptionally and with respect to computational resources. We have recently presented a quantum fluctuations approach which is equivalent to the nonequilibrium $GW$ approximation [E. Schroedter \textit{et al.}, Cond. Matt. Phys. \textbf{25}, 23401 (2022)] that promises high accuracy at low computational cost. In a second publication [E. Schroedter \textit{et al.}, Phys. Rev. B \textbf{108}, 205109 (2023)], this approach was extended to the two-time exchange-correlation functions and the density response properties. Here, we analyze the properties of this approach in more detail. We demonstrate that the method is equivalent to the Bethe--Salpeter equation for the two-time exchange-correlation function when the generalized Kadanoff-Baym ansatz with Hartree-Fock propagators is applied. ",2312.15034v2
"['Jan Willem Abraham', 'Karsten Balzer', 'David Hochstuhl', 'Michael Bonitz']",2012-03-09T09:03:30Z,Quantum Breathing Mode of Interacting Particles in a One-dimensional   Harmonic Trap,"  Extending our previous work, we explore the breathing mode---the [uniform] radial expansion and contraction of a spatially confined system. We study the breathing mode across the transition from the ideal quantum to the classical regime and confirm that it is not independent of the pair interaction strength (coupling parameter). We present the results of time-dependent Hartree-Fock simulations for 2 to 20 fermions with Coulomb interaction and show how the quantum breathing mode depends on the particle number. We validate the accuracy of our results, comparing them to exact Configuration Interaction results for up to 8 particles. ",1203.2019v1
"['Erik Schroedter', 'Björn Jakob Wurst', 'Jan-Philip Joost', 'Michael Bonitz']",2023-05-30T11:43:03Z,Quantum Fluctuations Approach to the Nonequilibrium $GW$-Approximation   II: Density Correlations and Dynamic Structure Factor,"  The quantum dynamics of correlated fermionic or bosonic many-body systems following external excitation can be successfully studied using nonequilibrium Green functions (NEGF) or reduced density matrix methods. Approximations are introduced via a proper choice of the many-particle selfenergy or decoupling of the BBGKY-hierarchy, respectively. These approximations are based on Feynman's diagram approaches or on cluster expansions into single-particle and correlation operators. In a recent paper [E. Schroedter, J.-P. Joost, and M. Bonitz, Cond. Matt. Phys. \textbf{25}, 23401 (2022)] we have presented a different approach where, instead of equations of motion for the many-particle NEGF (or density operators), equations for the correlation functions of fluctuations are analyzed. In particular, we derived the stochastic GW and polarization approximations that are closely related to the nonequilibrium GW approximation. Here, we extend this approach to the computation of two-time observables depending on the specific ordering of the underlying operators. In particular, we apply this extension to the calculation of the density correlation function and dynamic structure factor of correlated Hubbard clusters in and out of equilbrium. ",2305.18956v1
"['Shabbir A. Khan', 'Michael Bonitz']",2013-10-01T13:25:54Z,Quantum Hydrodynamics,"  Quantum plasma physics is a rapidly evolving research field with a very inter-disciplinary scope of potential applications, ranging from nano-scale science in condensed matter to the vast scales of astrophysical objects. The theoretical description of quantum plasmas relies on various approaches, microscopic or macroscopic, some of which have obvious relation to classical plasma models. The appropriate model should, in principle, incorporate the quantum mechanical effects such as diffraction, spin statistics and correlations, operative on the relevant scales. However, first-principle approaches such as quantum Monte Carlo and density functional theory or quantum-statistical methods such as quantum kinetic theory or non-equilibrium Green's functions require substantial theoretical and computational efforts. Therefore, for selected problems, alternative simpler methods have been put forward. In particular, the collective behavior of many-body systems is usually described within a self-consistent scheme of particles and fields on the mean-field level. In classical plasmas, further simplifications are achieved by a transition to hydrodynamic equations. % Similar fluid-type descriptions for quantum plasmas have been proposed and widely used in the recent decade. This chapter is devoted to an overview of the main concepts of quantum hydrodynamics (QHD), thereby critically analyzing its validity range and its main limitations. Furthermore, the results of the linearized QHD in unmagnetized and magnetized plasmas and a few nonlinear solutions are examined with illustrations. The basic concepts and formulation of particle-particle interactions are also reviewed at the end, indicating their possible consequences in quantum many-body problems. ",1310.0283v1
"['Erik Schroedter', 'Michael Bonitz']",2024-02-07T19:43:35Z,Classical and Quantum Theory of Fluctuations for Many-Particle Systems   out of Equilibrium,"  Correlated classical and quantum many-particle systems out of equilibrium are of high interest in many fields, including dense plasmas, correlated solids, and ultracold atoms. Accurate theoretical description of these systems is challenging both, conceptionally and with respect to computational resources. While for classical systems, in principle, exact simulations are possible via molecular dynamics, this is not the case for quantum systems. Alternatively, one can use many-particle approaches such as hydrodynamics, kinetic theory or nonequilibrium Green functions (NEGF). However, NEGF exhibit a very unfavorable cubic scaling of the CPU time with the number of time steps. An alternative is the G1--G2 scheme [N. Schl\""unzen et al., Phys. Rev. Lett. \textbf{124}, 076601 (2020)] which allows for NEGF simulations with time linear scaling, however, at the cost of large memory consumption. The reason is the need to store the two-particle correlation function. This problem can be overcome for a number of approximations by reformulating the kinetic equations in terms of fluctuations -- an approach that was developed, for classical systems, by Yu.L. Klimontovich [JETP \textbf{33}, 982 (1957)]. Here we present an overview of his ideas and extend them to quantum systems. In particular, we demonstrate that this quantum fluctuations approach can reproduce the nonequilibrium $GW$ approximation [E. Schroedter \textit{et al.}, Cond. Matt. Phys. \textbf{25}, 23401 (2022)] promising high accuracy at low computational cost which arises from an effective semiclassical stochastic sampling procedure. We also demonstrate how to extend the approach to the two-time exchange-correlation functions and the density response properties. [E. Schroedter \textit{et al.}, Phys. Rev. B \textbf{108}, 205109 (2023)]. ",2402.05214v1
"['Alexei Filinov', 'Jens Böning', 'Michael Bonitz']",2006-11-21T15:50:33Z,Path integral Monte Carlo simulation of charged particles in traps,"  This chapter is devoted to the computation of equilibrium (thermodynamic) properties of quantum systems. In particular, we will be interested in the situation where the interaction between particles is so strong that it cannot be treated as a small perturbation. For weakly coupled systems many efficient theoretical and computational techniques do exist. However, for strongly interacting systems such as nonideal gases or plasmas, strongly correlated electrons and so on, perturbation methods fail and alternative approaches are needed. Among them, an extremely successful one is the Monte Carlo (MC) method which we are going to consider in this chapter. ",cond-mat/0611558v1
"['Giedrius Kudelis', 'Hauke Thomsen', 'Michael Bonitz']",2013-04-16T15:05:25Z,Heat Transport in Confined Strongly Coupled 2D Dust Clusters,"  Dusty plasmas are a model system for studying strong correlation. The dust grains' size of a few micro-meters and their characteristic oscillation frequency of a few hertz allows for an investigation of many particle effects on an atomic level. In this article, we model the heat transport through an axially confined 2D dust cluster from the center to the outside. The system behaves particularly interesting since heat is not only conducted within the dust component but also transfered to the neutral gas. Fitting the analytical solution to the obtained radial temperature profiles allows to determine the heat conductivity $\kheat$. The heat conductivity is found to be constant over a wide range of coupling strengths even including the phase transition from solid to liquid here, as it was also found in extended systems by V. Nosenko et al. in 2008 \cite{PhysRevLett.100.025003} ",1304.4483v1
"['Tobias Dornheim', 'Simon Groth', 'Jan Vorberger', 'Michael Bonitz']",2017-06-01T14:16:20Z,Permutation Blocking Path Integral Monte Carlo approach to the Static   Density Response of the Warm Dense Electron Gas,"  The static density response of the uniform electron gas is of fundamental importance for numerous applications. Here, we employ the recently developed \textit{ab initio} permutation blocking path integral Monte Carlo (PB-PIMC) technique [T.~Dornheim \textit{et al.}, \textit{New J.~Phys.}~\textbf{17}, 073017 (2015)] to carry out extensive simulations of the harmonically perturbed electron gas at warm dense matter conditions. In particular, we investigate in detail the validity of linear response theory and demonstrate that PB-PIMC allows to obtain highly accurate results for the static density response function and, thus, the static local field correction. A comparison with dielectric approximations to our new \textit{ab initio} data reveals the need for an exact treatment of correlations. Finally, we consider a superposition of multiple perturbations and discuss the implications for the calculation of the static response function. ",1706.00315v1
"['Niclas Schluenzen', 'Jan-Philip Joost', 'Michael Bonitz']",2019-09-25T13:41:07Z,Achieving the Ultimate Scaling Limit for Nonequilibrium Green Functions   Simulations,"  The dynamics of strongly correlated fermions following an external excitation reveals extremely rich collective quantum effects. Examples are fermionic atoms in optical lattices, electrons in correlated materials, and dense quantum plasmas. Presently, the only quantum-dynamics approach that rigorously describes these processes in two and three dimensions is nonequilibrium Green functions (NEGF). However, NEGF simulations are computationally expensive due to their $T^3$ scaling with the simulation duration $T$. Recently, $T^2$ scaling was achieved with the generalized Kadanoff--Baym ansatz (GKBA) which has substantially extended the scope of NEGF simulations. Here we present a novel approach to GKBA-NEGF simulations that is of order $T$, and demonstrate its remarkable capabilities. ",1909.11489v2
"['Simon Groth', 'Tobias Dornheim', 'Michael Bonitz']",2016-11-17T14:24:38Z,Free Energy of the Uniform Electron Gas: Testing Analytical Models   against First Principle Results,"  The uniform electron gas is a key model system in the description of matter, including dense plasmas and solid state systems. However, the simultaneous occurence of quantum, correlation, and thermal effects makes the theoretical description challenging. For these reasons, over the last half century many analytical approaches have been developed the accuracy of which has remained unclear. We have recently obtained the first \textit{ab initio} data for the exchange correlation free energy of the uniform electron gas [T. Dornheim \textit{et al.}, Phys.~Rev.~Lett.~\textbf{117}, 156403 (2016)] which now provides the opportunity to assess the quality of the mentioned approaches and parametrizations. Particular emphasis is put on the warm dense matter regime, where we find significant discrepancies between the different approaches. ",1611.05695v1
"['Tobias Dornheim', 'Simon Groth', 'Michael Bonitz']",2017-09-07T12:16:25Z,\textit{Ab Initio} results for the Static Structure Factor of the Warm   Dense Electron Gas,"  The uniform electron gas at finite temperature is of high current interest for warm dense matter research. The complicated interplay of quantum degeneracy and Coulomb coupling effects is fully contained in the pair distribution function or, equivalently, the static strucutre factor. By combining exact quantum Monte Carlo results for large wave vectors with the long-range behavior from the Singwi-Tosi-Land-Sj\""olander approximation, we are able to obtain highly accurate data for the static structure factor over the entire $k$-range. This allows us to gauge the accuracy of previous approximations and discuss their respective shortcomings. Further, our new data will serve as valuable input for the computation of other quantities. ",1709.02199v1
"['Zhandos Moldabekov', 'Tim Schoof', 'Patrick Ludwig', 'Michael Bonitz', 'Tlekkabul Ramazanov']",2015-08-05T16:20:04Z,Statically screened ion potential and Bohm potential in a quantum plasma,"  The effective potential $\Phi$ of a classical ion in a weakly correlated quantum plasma in thermodynamic equilibrium at finite temperature is well described by the RPA screened Coulomb potential. Additionally, collision effects can be included via a relaxation time ansatz (Mermin dielectric function). These potentials are used to study the quality of various statically screened potentials that were recently proposed by Shukla and Eliasson (SE) [Phys. Rev. Lett. {\bf 108}, 165007 (2012)], Akbari--Moghanjoughi (AM) [Phys. Plasmas {\bf 22}, 022103 (2015)] and Stanton and Murillo (SM) [Phys. Rev. E {\bf 91}, 033104 (2015)] starting from quantum hydrodynamic theory (QHD). Our analysis reveals that the SE potential is qualitatively different from the full potential, whereas the SM potential (at any temperature) and the AM potential (at zero temperature) are significantly more accurate. This confirms the correctness of the recently derived [Michta {\em et al.}, Contrib. Plasma Phys. {\bf 55}, (2015)] pre-factor $1/9$ in front of the Bohm term of QHD for fermions. ",1508.01120v1
"['Tobias Dornheim', 'Simon Groth', 'Alexey Filinov', 'Michael Bonitz']",2015-04-15T11:13:36Z,Permutation blocking path integral Monte Carlo: A highly efficient   approach to the simulation of strongly degenerate non-ideal fermions,"  Correlated fermions are of high interest in condensed matter (Fermi liquids, Wigner molecules), cold atomic gases and dense plasmas. Here we propose a novel approach to path integral Monte Carlo (PIMC) simulations of strongly degenerate non-ideal fermions at finite temperature by combining a fourth-order factorization of the density matrix with antisymmetric propagators, i.e., determinants, between all imaginary time slices. To efficiently run through the modified configuration space, we introduce a modification of the widely used continuous space worm algorithm, which allows for an efficient sampling at arbitrary system parameters. We demonstrate how the application of determinants achieves an effective blocking of permutations with opposite signs, leading to a significant relieve of the fermion sign problem. To benchmark the capability of our method regarding the simulation of degenerate fermions, we consider multiple electrons in a quantum dot and compare our results with other ab initio techniques, where they are available. The present permutation blocking path integral Monte Carlo approach allows us to obtain accurate results even for $N=20$ electrons at low temperature and arbitrary coupling, where no other ab initio results have been reported, so far. ",1504.03859v1
"['Jan-Philip Joost', 'Antti-Pekka Jauho', 'Michael Bonitz']",2019-10-02T18:21:43Z,Correlated Topological States in Graphene Nanoribbon Heterostructures,"  Finite graphene nanoribbon (GNR) heterostructures host intriguing topological in-gap states (Rizzo, D. J. et al.~\textit{Nature} \textbf{2018}, \textit{560}, 204]). These states may be localized either at the bulk edges, or at the ends of the structure. Here we show that correlation effects (not included in previous density functional simulations) play a key role in these systems: they result in increased magnetic moments at the ribbon edges accompanied by a significant energy renormalization of the topological end states -- even in the presence of a metallic substrate. Our computed results are in excellent agreement with the experiments. Furthermore, we discover a striking, novel mechanism that causes an energy splitting of the non-zero-energy topological end states for a weakly screened system. We predict that similar effects should be observable in other GNR heterostructures as well. ",1910.01152v1
"['Tobias Dornheim', 'Jan Vorberger', 'Michael Bonitz']",2020-04-07T09:47:18Z,Nonlinear Electronic Density Response in Warm Dense Matter,"  Warm dense matter (WDM)---an extreme state with high temperatures and densities that occurs e.g. in astrophysical objects---constitutes one of the most active fields in plasma physics and materials science. These conditions can be realized in the lab by shock compression or laser excitation, and the most accurate experimental diagnostics is achieved with lasers and free electron lasers which is theoretically modeled using linear response theory. Here, we present first \textit{ab initio} path integral Monte Carlo results for the nonlinear density response of correlated electrons in WDM and show that for many situations of experimental relevance nonlinear effects cannot be neglected. ",2004.03229v1
"['Sebastian Hermanns', 'Niclas Schlünzen', 'Michael Bonitz']",2014-02-28T16:16:34Z,Hubbard nanoclusters far from equilibrium,"  The Hubbard model is a prototype for strongly correlated many-particle systems, including electrons in condensed matter and molecules, as well as for fermions or bosons in optical lattices. While the equilibrium properties of these systems have been studied in detail, the nonequilibrium dynamics following a strong non-perturbative excitation only recently came into the focus of experiments and theory. It is of particular interest how the dynamics depend on the coupling strength and on the particle number and whether there exist universal features in the time evolution. Here, we present results for the dynamics of finite Hubbard clusters based on a selfconsistent nonequilibrium Green functions (NEGF) approach invoking the generalized Kadanoff--Baym ansatz (GKBA). We discuss the conserving properties of the GKBA with Hartree--Fock propagators in detail and present a generalized form of the energy conservation criterion of Baym and Kadanoff for NEGF. Furthermore, we demonstrate that the HF-GKBA cures some artifacts of prior two-time NEGF simulations. Besides, this approach substantially speeds up the numerical calculations and thus presents the capability to study comparatively large systems and to extend the analysis to long times allowing for an accurate computation of the excitation spectrum via time propagation. Our data obtained within the second Born approximation compares favorably with exact diagonalization results (available for up to 13 particles) and are expected to have predictive capability for substantially larger systems in the weak coupling limit. ",1402.7300v2
"['Zhandos Moldabekov', 'Patrick Ludwig', 'Michael Bonitz', 'Tlekkabul Ramazano']",2014-09-29T11:34:45Z,The ion potential in warm dense matter: wake effects due to streaming   degenerate electrons,"  The effective dynamically screened potential of a classical ion in a stationary flowing quantum plasma at finite temperature is investigated. This is a key quantity for thermodynamics and transport of dense plasmas in the warm dense matter regime. This potential has been studied before within hydrodynamic approaches or based on the zero temperature Lindhard dielectric function. Here we extend the kinetic analysis by including the effects of finite temperature and of collisions based on the Mermin dielectric function. The resulting ion potential exhibits an oscillatory structure with attractive minima (wakes) and, thus, strongly deviates from the static Yukawa potential of equilibrium plasmas. This potential is analyzed in detail for high-density plasmas with values of the Brueckner parameter in the range $0.1 \le r_s \le 1$, for a broad range of plasma temperature and electron streaming velocity. It is shown that wake effects become weaker with increasing temperature of the electrons. Finally, we obtain the minimal electron streaming velocity for which attraction between ions occurs. This velocity turns out to be less than the electron Fermi velocity. Our results allow, for the first time, for reliable predictions of the strength of wake effects in nonequilibrium quantum plasmas with fast streaming electrons showing that these effects are crucial for transport under warm dense matter conditions, in particular for laser-matter interaction, electron-ion temperature equilibration and for stopping power. ",1409.8079v3
"['Simon Groth', 'Tobias Dornheim', 'Michael Bonitz']",2017-08-13T16:59:17Z,Configuration Path Integral Monte Carlo Approach to the Static Density   Response of the Warm Dense Electron Gas,"  Precise knowledge of the static density response function (SDRF) of the uniform electron gas (UEG) serves as key input for numerous applications, most importantly for density functional theory beyond generalized gradient approximations. Here we extend the configuration path integral Monte Carlo (CPIMC) formalism that was previously applied to the spatially uniform electron gas to the case of an inhomogeneous electron gas by adding a spatially periodic external potential. This procedure has recently been successfully used in permutation blocking path integral Monte Carlo simulations (PB-PIMC) of the warm dense electron gas [Dornheim \textit{et al.}, Phys. Rev. E in press, arXiv:1706.00315], but this method is restricted to low and moderate densities. Implementing this procedure into CPIMC allows us to obtain exact finite temperature results for the SDRF of the electron gas at \textit{high to moderate densities} closing the gap left open by the PB-PIMC data. In this paper we demonstrate how the CPIMC formalism can be efficiently extended to the spatially inhomogeneous electron gas and present the first data points. Finally, we discuss finite size errors involved in the quantum Monte Carlo results for the SDRF in detail and present a solution how to remove them that is based on a generalization of ground state techniques. ",1708.03934v1
"['David Hochstuhl', 'Michael Bonitz']",2012-07-24T13:29:41Z,Time-dependent restricted active space Configuration Interaction for the   photoionization of many-electron atoms,"  We introduce the time-dependent restricted active space Configuration Interaction method to solve the time-dependent Schr\""odinger equation for many-electron atoms, and particularly apply it to the treatment of photoionization processes in atoms. The method is presented in a very general formulation and incorporates a wide range of commonly used approximation schemes, like the single-active electron approximation, time-dependent Configuration Interaction with single-excitations, or the time-dependent R-matrix method. We proof the applicability of the method by calculating the photoionization cross sections of Helium and Beryllium, as well as the X-ray--IR pump-probe ionization in Beryllium ",1207.5693v3
"['Hanno Kählert', 'Michael Bonitz']",2021-12-22T15:52:51Z,Dynamic structure factor of the magnetized one-component plasma:   crossover from weak to strong coupling,"  Plasmas in strong magnetic fields have been mainly studied in two distinct limiting cases--that of weak and strong nonideality with very different physical properties. While the former is well described by the familiar theory of Braginskii, the latter regime is closer to the behavior of a Coulomb liquid. Here we study in detail the transition between both regimes. We focus on the evolution of the dynamic structure factor of the magnetized one-component plasma from weak to strong coupling, which is studied with first-principle molecular dynamics simulations. The simulations show the vanishing of Bernstein modes and the emergence of higher harmonics of the upper hybrid mode across the magnetic field, a redistribution of spectral power between the two main collective modes under oblique angles, and a suppression of plasmon damping along the magnetic field. Comparison with results from various models, including the random phase approximation, a Mermin-type dielectric function, and the Quasi-Localized Charge Approximation show that none of the theories is capable of reproducing the crossover that occurs when the coupling parameter is on the order of unity. The findings are relevant to the scattering spectra, stopping power, and transport coefficients of correlated magnetized plasmas. ",2112.11981v1
"['Jan Willem Abraham', 'Michael Bonitz', 'Chris McDonald', 'Gianfranco Orlando', 'Thomas Brabec']",2013-11-21T11:33:58Z,Quantum Breathing Mode of Trapped Systems in One and Two Dimensions,"  We investigate the quantum breathing mode (monopole oscillation) of trapped fermionic particles with Coulomb and dipole interaction in one and two dimensions. This collective oscillation has been shown to reveal detailed information on the many-particle state of interacting trapped systems and is thus a sensitive diagnostics for a variety of finite systems, including cold atomic and molecular gases in traps and optical lattics, electrons in metal clusters and in quantum confined semiconductor structures or nanoplasmas. An improved sum rule formalism allows us to accurately determine the breathing frequencies from the ground state of the system, avoiding complicated time-dependent simulations. In combination with the Hartree-Fock and the Thomas-Fermi approximations this enables us to extend the calculations to large particle numbers $N$ on the order of several million. Tracing the breathing frequency to large $N$ as a function of the coupling parameter of the system reveals a surprising difference of the asymptotic behavior of one-dimensional and two-dimensional harmonically trapped Coulomb systems. ",1311.5371v1
"['Sita Sundar', 'Hanno Kählert', 'Jan-Philip Joost', 'Patrick Ludwig', 'Michael Bonitz']",2017-02-23T09:54:46Z,Collision induced amplification of wakes in streaming plasmas,"  This work examines the formation of wake fields caused by ions streaming around a charged dust particle, using three-dimensional particle-in-cell (PIC) simulations with charge-neutral collisions included. The influence of an external driving electric field, which leads to a non-Maxwellian distribution of ions, is investigated in detail. The wake features formed for non-Maxwellian ions exhibit significant deviations from those observed within the model of a shifted Maxwellian distribution. The dependence of the peak amplitude and position of the wake potential upon the degree of collisionality is analyzed for a wide range of streaming velocities (Mach numbers). In contrast to a shifted Maxwellian distribution of ions, the drift-driven non-Maxwellian distribution exhibits an increase of the wake amplitude of the first attractive peak with increase in collisionality for high streaming velocities. At very low Mach numbers, collision-induced amplification is observed for Maxwellian as well as non-Maxwellian distributions. ",1702.07152v1
"['Karsten Balzer', 'Maximilian Rodriguez Rasmussen', 'Niclas Schlünzen', 'Jan-Philip Joost', 'Michael Bonitz']",2018-01-16T14:41:22Z,Doublon formation by ions impacting a strongly correlated finite lattice   system,"  Strongly correlated systems of fermions have a number of exciting collective properties. Among them, the creation of a lattice that is occupied by doublons, i.e. two quantum particles with opposite spins, offers interesting electronic properties. In the past a variety of methods have been proposed to control doublon formation, both, spatially and temporally. Here, a novel mechanism is proposed and verified by exact diagonalization and nonequilibrium Green functions simulations---fermionic doublon creation by the impact of energetic ions. We report the formation of a nonequilibrium steady state with homogeneous doublon distribution. The effect should be observable in strongly correlated solids in contact with a high-pressure plasma and in fermionic atoms in optical lattices. ",1801.05267v1
"['Kai Hunger', 'Tim Schoof', 'Tobias Dornheim', 'Michael Bonitz', 'Alexey Filinov']",2021-01-04T09:19:38Z,Momentum distribution function and short-range correlations of the warm   dense electron gas -- ab initio quantum Monte Carlo results,"  In a classical plasma the momentum distribution, $n(k)$, decays exponentially, for large $k$, and the same is observed for an ideal Fermi gas. However, when quantum and correlation effects are relevant simultaneously, an algebraic decay, $n_\infty(k)\sim k^{-8}$ has been predicted. This is of relevance for cross sections and threshold processes in dense plasmas that depend on the number of energetic particles. Here we present extensive \textit{ab initio} results for the momentum distribution of the nonideal uniform electron gas at warm dense matter conditions. Our results are based on first principle fermionic path integral Monte Carlo (CPIMC) simulations and clearly confirm the $k^{-8}$ asymptotic. This asymptotic behavior is directly linked to short-range correlations which are analyzed via the on-top pair distribution function (on-top PDF), i.e. the PDF of electrons with opposite spin. We present extensive results for the density and temperature dependence of the on-top PDF and for the momentum distribution in the entire momentum range. ",2101.00842v2
"['Lotte Borkowski', 'Niclas Schlünzen', 'Jan Philip Joost', 'Franziska Reiser', 'Michael Bonitz']",2021-10-13T11:15:11Z,Doublon production in correlated materials by multiple ion impacts,"  In a recent Letter [Balzer \textit{et al.}, Phys. Rev. Lett. \textbf{121}, 267602 (2018)] it was demonstrated that ions impacting a correlated graphene cluster can excite strongly nonequilibrium states. In particular, this can lead to an enhanced population of bound pairs of electrons with opposite spin -- doublons -- where the doublon number can be increased via multiple ion impacts. These predictions were made based on nonequilibrium Green functions (NEGF) simulations allowing for a time-dependent non-perturbative study of the energy loss of charged particles penetrating a strongly correlated system. Here we extend these simulations to larger clusters and longer simulation times, utilizing the recently developed G1--G2 scheme [Sch\""unzen \textit{et al.}, Phys. Rev. Lett. \textbf{124}, 076601 (2020)] which allows for a dramatic speedup of NEGF simulations. Furthermore, we investigate the dependence of the energy and doublon number on the time interval between ion impacts and on the impact point. ",2110.06644v1
"['James Dufty', 'Sandipan Dutta', 'Michael Bonitz', 'Alexei Filinov']",2009-03-17T14:01:44Z,Quantum Potential for Diffraction and Exchange Effects,"  Semi-classical methods of statistical mechanics can incorporate essential quantum effects by using effective quantum potentials. An ideal Fermi gas interacting with an impurity is represented by a classical fluid with effective electron-electron and electron-impurity quantum potentials. The electron-impurity quantum potential is evaluated at weak coupling, leading to a generalization of the Kelbg potential to include both diffraction and degeneracy effects. The electron-electron quantum potential for exchange effects only is the same as that discussed earlier by others. ",0903.2968v1
"['Peter Hartmann', 'Zoltán Donkó', 'Torben Ott', 'Hanno Kählert', 'Michael Bonitz']",2013-09-25T07:39:05Z,Magnetoplasmons in rotating dusty plasmas,"  A rotating dusty plasma apparatus was constructed to provide the possibility of experimental emulation of extremely high magnetic fields by means of the Coriolis force, observable in a co-rotating measurement frame. We present collective excitation spectra for different rotation rates with a magnetic induction equivalent of up to 3200 Tesla. We identify the onset of magnetoplasmon-equivalent mode dispersion in the rotating macroscopic two-dimensional single-layer dusty plasma. The experimental results are supported by molecular dynamics simulations of 2D magnetized Yukawa systems. ",1309.6416v1
"['Vladimir Filinov', 'Michael Bonitz', 'Alexei Filinov', 'Volodymyr Golubnychiy']",2006-11-21T15:57:48Z,Wigner function quantum molecular dynamics,"  Classical molecular dynamics (MD) is a well established and powerful tool in various fields of science, e.g. chemistry, plasma physics, cluster physics and condensed matter physics. Objects of investigation are few-body systems and many-body systems as well. The broadness and level of sophistication of this technique is documented in many monographs and reviews, see for example \cite{Allan,Frenkel,mdhere}. Here we discuss the extension of MD to quantum systems (QMD). There have been many attempts in this direction which differ from one another, depending on the type of system under consideration. One direction of QMD has been developed for condensed matter systems and will not discussed here, e.g. \cite{fermid}. In this chapter we are dealing with unbound electrons as they occur in gases, fluids or plasmas. Here, one strategy is to replace classical point particles by wave packets, e.g. \cite{fermid,KTR94,zwicknagel06} which is quite successful. At the same time, this method struggles with problems related to the dispersion of such a packet and difficulties to properly describe strong electron-ion interaction and bound state formation. We, therefore, avoid such restrictions and consider a completely general alternative approach. We start discussion of quantum dynamics from a general consideration of quantum distribution functions. ",cond-mat/0611560v1
"['Tobias Dornheim', 'Simon Groth', 'Travis Sjostrom', 'Fionn D. Malone', 'W. M. C. Foulkes', 'Michael Bonitz']",2016-07-27T13:14:47Z,{\em Ab initio} Quantum Monte Carlo simulation of the warm dense   electron gas in the thermodynamic limit,"  We perform \emph{ab initio} quantum Monte Carlo (QMC) simulations of the warm dense uniform electron gas in the thermodynamic limit. By combining QMC data with linear response theory we are able to remove finite-size errors from the potential energy over the entire warm dense regime, overcoming the deficiencies of the existing finite-size corrections by Brown \emph{et al.}~[PRL \textbf{110}, 146405 (2013)]. Extensive new QMC results for up to $N=1000$ electrons enable us to compute the potential energy $V$ and the exchange-correlation free energy $F_{xc}$ of the macroscopic electron gas with an unprecedented accuracy of $|\Delta V|/|V|, |\Delta F_{xc}|/|F|_{xc} \sim 10^{-3}$. A comparison of our new data to the recent parametrization of $F_{xc}$ by Karasiev {\em et al.} [PRL {\bf 112}, 076403 (2014)] reveals significant deviations to the latter. ",1607.08076v2
"['Simon Groth', 'Tobias Dornheim', 'Travis Sjostrom', 'Fionn D. Malone', 'W. M. C. Foulkes', 'Michael Bonitz']",2017-03-23T13:57:06Z,Ab initio Exchange-Correlation Free Energy of the Uniform Electron Gas   at Warm Dense Matter Conditions,"  In a recent Letter [T.~Dornheim \textit{et al.}, Phys. Rev. Lett. \textbf{117}, 156403 (2016)], we presented the first \textit{ab initio} quantum Monte-Carlo (QMC) results of the warm dense electron gas in the thermodynamic limit. However, a complete parametrization of the exchange-correlation free energy with respect to density, temperature, and spin polarization remained out of reach due to the absence of (i) accurate QMC results below $\theta=k_\text{B}T/E_\text{F}=0.5$ and (ii) of QMC results for spin polarizations different from the paramagnetic case. Here we overcome both remaining limitations. By closing the gap to the ground state and by performing extensive QMC simulations for different spin polarizations, we are able to obtain the first complete \textit{ab initio} exchange-correlation free energy functional; the accuracy achieved is an unprecedented $\sim 0.3\%$. This also allows us to quantify the accuracy and systematic errors of various previous approximate functionals. ",1703.08074v1
"['Tobias Dornheim', 'Simon Groth', 'Fionn Malone', 'Tim Schoof', 'Travis Sjostrom', 'W. M. C. Foulkes', 'Michael Bonitz']",2016-11-08T19:10:35Z,\emph{Ab initio} Quantum Monte Carlo simulation of the warm dense   electron gas,"  Warm dense matter is one of the most active frontiers in plasma physics due to its relevance for dense astrophysical objects as well as for novel laboratory experiments in which matter is being strongly compressed e.g. by high-power lasers. Its description is theoretically very challenging as it contains correlated quantum electrons at finite temperature---a system that cannot be accurately modeled by standard analytical or ground state approaches. Recently several breakthroughs have been achieved in the field of fermionic quantum Monte Carlo simulations. First, it was shown that exact simulations of a finite model system ($30 \dots 100$ electrons) is possible that avoid any simplifying approximations such as fixed nodes [Schoof {\em et al.}, Phys. Rev. Lett. {\bf 115}, 130402 (2015)]. Second, a novel way to accurately extrapolate these results to the thermodynamic limit was reported by Dornheim {\em et al.} [Phys. Rev. Lett. {\bf 117}, 156403 (2016)]. As a result, now thermodynamic results for the warm dense electron gas are available that have an unprecedented accuracy on the order of $0.1\%$. Here we present an overview on these results and discuss limitations and future directions. ",1611.02658v1
"['Alexey Filinov', 'Michael Bonitz']",2012-05-23T14:52:50Z,Collective and single-particle excitations in 2D dipolar Bose gases,"  The Berezinskii-Kosterlitz-Thouless transition in 2D dipolar systems has been studied recently by path integral Monte Carlo (PIMC) simulations [A. Filinov et al., PRL 105, 070401 (2010)]. Here, we complement this analysis and study temperature-coupling strength dependence of the density (particle-hole) and single-particle (SP) excitation spectra both in superfluid and normal phases. The dynamic structure factor, S(q,omega), of the longitudinal excitations is rigorously reconstructed with full information on damping. The SP spectral function, A(q,omega), is worked out from the one-particle Matsubara Green's function. A stochastic optimization method is applied for reconstruction from imaginary times. In the superfluid regime sharp energy resonances are observed both in the density and SP excitations. The involved hybridization of both spectra is discussed. In contrast, in the normal phase, when there is no coupling, the density modes, beyond acoustic phonons, are significantly damped. Our results generalize previous zero temperature analyses based on variational many-body wavefunctions [F. Mazzanti et al., PRL 102, 110405 (2009), D. Hufnagl et al., PRL 107, 065303 (2011)], where the underlying physics of the excitation spectrum and the role of the condensate has not been addressed. ",1205.5191v2
"['Tobias Dornheim', 'Simon Groth', 'Michael Bonitz']",2018-01-17T18:17:54Z,The Uniform Electron Gas at Warm Dense Matter Conditions,"  We review the uniform electron gas (UEG) at finite temperature and over a broad density range relevant for warm dense matter (WDM) applications. We provide an overview of different simulation techniques, focusing on recent developments in the dielectric formalism and quantum Monte Carlo (QMC). Our primary focus is on two novel QMC methods: Permutation blocking path integral MC (PB-PIMC) and configuration PIMC (CPIMC). In fact, a combination of PB-PIMC and CPIMC has allowed for a highly accurate description of the warm dense UEG over a broad density-temperature range. We are able to effectively avoid the notorious fermion sign problem, without invoking uncontrolled approximations such as the fixed node approximation. Furthermore, a new finite-size correction scheme is presented that makes it possible to treat the UEG in the thermodynamic limit without loss of accuracy. In addition, we in detail discuss the construction of a parametrization of the exchange-correlation free energy that provides a complete description of the UEG and is of crucial importance as input for the simulation of real WDM applications. Further, we test previous theories, including restricted PIMC, finite-temperature Green functions, the classical mapping by Perrot and Dharma-wardana, and various dielectric methods such as the random phase approximation, or the Singwi-Tosi-Land-Sj\""olander, Vashishta-Singwi and the recent Tanaka scheme for the local field correction. Thus, for the first time, thorough benchmarks of important approximation schemes regarding various quantities such as different energies, in particular the exchange-correlation free energy, and the static structure factor, are possible. Finally, we outline a way how to rigorously extend our QMC studies to the inhomogeneous electron gas and present first ab initio data for the static density response and for the static local field correction. ",1801.05783v1
"['Dietrich Kremp', 'Dirk Semkat', 'Thomas Bornath', 'Michael Bonitz', 'Manfred Schlanges', 'Paul Hilse']",2006-02-07T09:01:56Z,Partially ionized plasmas in electromagnetic fields,"  The interaction of partially ionized plasmas with an electromagnetic field is investigated using quantum statistical methods. A general statistical expression for the current density of a plasma in an electromagnetic field is presented and considered in the high field regime. Expressions for the collisional absorption are derived and discussed. Further, partially ionized plasmas are considered. Plasma Bloch equations for the description of bound-free transitions are given and the absorption coefficient as well as rate coefficients for multiphoton ionization are derived and numerical results are presented. ",cond-mat/0602171v1
"['Henrik R. Larsson', 'Sebastian Bauch', 'Lasse Kragh Sørensen', 'Michael Bonitz']",2015-07-15T07:44:51Z,Correlation effects in strong-field ionization of heteronuclear diatomic   molecules,"  We develop a time-dependent theory to investigate electron dynamics and photoionization processes of diatomic molecules interacting with strong laser fields including electron-electron correlation effects. We combine the recently formulated time-dependent generalized-active-space configuration interaction theory [D. Hochstuhl and M. Bonitz, Phys. Rev. A 86, 053424 (2012); S. Bauch, et al., Phys. Rev. A 90, 062508 (2014)] with a prolate spheroidal basis set including localized orbitals and continuum states to describe the bound electrons and the outgoing photoelectron. As an example, we study the strong-field ionization of the two-center four-electron lithium hydride molecule in different intensity regimes. By using single-cycle pulses, two orientations of the asymmetric heteronuclear molecule are investigated: Li-H, with the electrical field pointing from H to Li, and the opposite case of H-Li. The preferred orientation for ionization is determined and we find a transition from H-Li, for low intensity, to Li-H, for high intensity. The influence of electron correlations is studied at different levels of approximation, and we find a significant change in the preferred orientation. For certain intensity regimes, even an interchange of the preferred configuration is observed, relative to the uncorrelated simulations. Further insight is provided by detailed comparisons of photoelectron angular distributions with and without correlation effects taken into account. ",1507.04107v2
"['Tobias Dornheim', 'Tim Schoof', 'Simon Groth', 'Alexey Filinov', 'Michael Bonitz']",2015-08-13T14:11:49Z,Permutation blocking path integral Monte Carlo approach to the uniform   electron gas at finite temperature,"  The uniform electron gas (UEG) at finite temperature is of high current interest due to its key relevance for many applications including dense plasmas and laser excited solids. In particular, density functional theory heavily relies on accurate thermodynamic data for the UEG. Until recently, the only existing first-principle results had been obtained for $N=33$ electrons with restricted path integral Monte Carlo (RPIMC), for low to moderate density,   $r_s = \overline{r}/a_B \gtrsim 1$. This data has been complemented by Configuration path integral Monte Carlo (CPIMC) simulations for $r_s \leq 1$ that substantially deviate from RPIMC towards smaller $r_s$ and low temperature. In this work, we present results from an independent third method---the recently developed permutation blocking path integral Monte Carlo (PB-PIMC) approach [T. Dornheim \textit{et al.}, NJP \textbf{17}, 073017 (2015)] which we extend to the UEG. Interestingly, PB-PIMC allows us to perform simulations over the entire density range down to half the Fermi temperature ($\theta=k_BT/E_F=0.5$) and, therefore, to compare our results to both aforementioned methods. While we find excellent agreement with CPIMC, where results are available, we observe deviations from RPIMC that are beyond the statistical errors and increase with density. ",1508.03221v1
"['Jan-Philip Joost', 'Niclas Schlünzen', 'Hannes Ohldag', 'Michael Bonitz', 'Fabian Lackner', 'Iva Březinová']",2022-02-21T09:16:09Z,The dynamically screened ladder approximation: Simultaneous treatment of   strong electronic correlations and dynamical screening out of equilibrium,"  Dynamical screening is a key property of charged many-particle systems. Its theoretical description is based on the $GW$ approximation that is extensively applied for ground-state and equilibrium situations but also for systems driven out of equilibrium. The main limitation of the $GW$ approximation is the neglect of strong electronic correlation effects that are important in many materials as well as in dense plasmas. Here we derive the dynamically screened ladder (DSL) approximation that selfconsistently includes, in addition to the $GW$ diagrams, also particle--particle and particle--hole $T$-matrix diagrams. The derivation is based on reduced-density-operator theory and the result is equivalent to the recently presented G1--G2 scheme [Schl\""unzen \textit{et al.}, Phys. Rev. Lett. \textbf{124}, 076601 (2020); Joost \textit{et al.}, Phys. Rev. B \textbf{101}, 245101 (2020)]. We perform extensive time-dependent DSL simulations for finite Hubbard clusters and present tests against exact results that confirm excellent accuracy as well as total energy conservation of the approximation. At strong coupling and for long simulation durations, instabilities are observed. These problems are solved by enforcing contraction consistency and applying a purification approach. ",2202.10061v1
"['Karsten Balzer', 'Niclas Schlünzen', 'Hannes Ohldag', 'Jan-Philip Joost', 'Michael Bonitz']",2022-11-17T16:04:37Z,Accelerating Nonequilibrium Green functions simulations with embedding   selfenergies,"  Real-time nonequilibrium Green functions (NEGF) have been very successful to simulate the dynamics of correlated many-particle systems far from equilibrium. However, NEGF simulations are computationally expensive since the effort scales cubically with the simulation duration. Recently we have introduced the G1--G2 scheme that allows for a dramatic reduction to time-linear scaling [Schl\""unzen, Phys. Rev. Lett. 124, 076601 (2020); Joost et al., Phys. Rev. B 101, 245101 (2020)]. Here we tackle another problem: the rapid growth of the computational effort with the system size. In many situations where the system of interest is coupled to a bath, to electric contacts or similar macroscopic systems for which a microscopic resolution of the electronic properties is not necessary, efficient simplifications are possible. This is achieved by the introduction of an embedding selfenergy -- a concept that has been successful in standard NEGF simulations. Here, we demonstrate how the embedding concept can be introduced into the G1--G2 scheme, allowing us to drastically accelerate NEGF embedding simulations. The approach is compatible with all advanced selfenergies that can be represented by the G1--G2 scheme [as described in Joost et al., Phys. Rev. B 105, 165155 (2022)] and retains the memory-less structure of the equations and their time linear scaling. As a numerical illustration we investigate the charge transfer between a Hubbard nanocluster and an additional site which is of relevance for the neutralization of ions in matter. ",2211.09615v2
"['Michael Bonitz', 'Jan-Philip Joost', 'Christopher Makait', 'Erik Schroedter', 'Tim Karsberger', 'Karsten Balzer']",2023-12-22T19:52:57Z,Accelerating Nonequilibrium Green functions simulations: the G1-G2   scheme and beyond,"  The theory of Nonequilibrium Green functions (NEGF) has seen a rapid development over the recent three decades. Applications include diverse correlated many-body systems in and out of equilibrium. Very good agreement with experiments and available exact theoretical results could be demonstrated if the proper selfenergy approximations were used. However, full two-time NEGF simulations are computationally costly, as they suffer from a cubic scaling of the computation time with the simulation duration. Recently we have introduced the G1-G2 scheme that exactly reformulates the Kadanoff-Baym ansatz with Hartree-Fock propagators (HF-GKBA) into time-local equations, allowing for a dramatic reduction of the scaling to time-linear scaling [Schluenzen et al., Phys. Rev. Lett. \textbf{124}, 076601 (2020)]. Remarkably, this scaling is achieved quickly, and also for high-level selfenergies, including the nonequilibrium $GW$ and $T$-matrix approximations [Joost et al., Phys. Rev. B \textbf{101}, 245101 (2020)]. Even the dynamically screened ladder approximation is now feasible [Joost et al., Phys. Rev. B \textbf{105}, 165155 (2022)], and also applications to electron-boson systems were demonstrated. Here we present an overview on recent results that were achieved with the G1--G2 scheme. We discuss problems and open questions and present further ideas how to overcome the current limitations of the scheme.We illustrate the G1--G2 scheme by presenting applying it to the excitation dynamics of Hubbard clusters, to optical excitation of graphene, and to charge transfer during stopping of ions by correlated materials. ",2312.15030v2
"['Karsten Balzer', 'Sebastian Bauch', 'Michael Bonitz']",2009-11-23T09:50:15Z,Finite elements and the discrete variable representation in   nonequilibrium Green's function calculations. Atomic and molecular models,"  In this contribution, we discuss the finite-element discrete variable representation (FE-DVR) of the nonequilibrium Green's function and its implications on the description of strongly inhomogeneous quantum systems. In detail, we show that the complementary features of FEs and the DVR allows for a notably more efficient solution of the two-time Schwinger/Keldysh/Kadanoff-Baym equations compared to a general basis approach. Particularly, the use of the FE-DVR leads to an essential speedup in computing the self-energies.   As atomic and molecular examples we consider the He atom and the linear version of H$_3^+$ in one spatial dimension. For these closed-shell models we, in Hartree-Fock and second Born approximation, compute the ground-state properties and compare with the exact findings obtained from the solution of the few-particle time-dependent Schr\""odinger equation. ",0911.4348v1
"['Patrick Ludwig', 'Hanno Kählert', 'Michael Bonitz']",2012-01-09T08:40:29Z,Ion-Streaming Induced Order Transition in 3D Dust Clusters,"  Dust Dynamics Simulations utilizing a dynamical screening approach are performed to study the effect of ion-streaming on the self-organized structures in a three-dimensional spherically confined complex (dusty) plasma. Varying the Mach number M - the ratio of ion drift velocity to the sound velocity, the simulations reproduce the experimentally observed cluster configurations in the two limiting cases: at M=0 strongly correlated crystalline structures consisting of nested spherical shells (Yukawa balls) and, for M\geq1, flow-aligned dust chains, respectively. In addition, our simulations reveal a discontinuous transition between these two limits. It is found that already a moderate ion drift velocity (M\approx0.1) destabilizes the highly ordered Yukawa balls and initiates an abrupt melting transition. The critical value of M is found to be independent of the cluster size. ",1201.1711v1
"['Tobias Dornheim', 'Simon Groth', 'Alexei Filinov', 'Michael Bonitz']",2019-02-17T16:03:39Z,Path Integral Monte Carlo Simulation of Degenerate Electrons:   Permutation-Cycle Properties,"  Being motivated by the surge of fermionic quantum Monte Carlo simulations at finite temperature, we present a detailed analysis of the permutation-cycle properties of path integral Monte Carlo (PIMC) simulations of degenerate electrons. Particular emphasis is put onto the uniform electron gas in the warm dense matter regime. We carry out PIMC simulations of up to $N=100$ electrons and investigate exchange-cycle frequencies, which are found not to follow any simple exponential law even in the case of ideal fermions due to the finite size of the simulation box. Moreover, we introduce a permutation-cycle correlation function, which allows us to analyse the joint probability to simultaneously find cycles of different lengths within a single configuration. Again, we find that finite-size effects predominate the observed behaviour. Finally, we briefly consider an inhomogeneous system, namely electrons in a $2D$ harmonic trap. We expect our results to be of interest for the further development of fermionic PIMC methods, in particular to alleviate the notorious fermion sign problem. ",1902.06741v1
"['Zhandos Moldabekov', 'Tobias Dornheim', 'Michael Bonitz']",2020-09-19T07:13:21Z,Screening of a test charge in a free-electron gas at warm dense matter   and dense non-ideal plasma conditions,"  The screening of a test charge by partially degenerate non-ideal free electrons at conditions related to warm dense matter and dense plasmas is investigated using linear response theory and the local field correction based on ab inito Quantum Monte-Carlo simulations data. The analysis of the obtained results is performed by comparing to the random phase approximation and the Singwi-Tosi-Land-Sj\""olander approximation. The applicability of the long-wavelength approximation for the description of screening is investigated. The impact of electronic exchange-correlations effects on structural properties and the applicability of the screened potential from linear response theory for the simulation of the dynamics of ions are discussed. ",2009.09180v1
"['Hauke Thomsen', 'Patrick Ludwig', 'Michael Bonitz', 'Jan Schablinski', 'Dietmar Block', 'André Schella', 'André Melzer']",2014-02-28T10:05:10Z,Controlling strongly correlated dust clusters with lasers,"  The most attractive feature of dusty plasmas is the possibility to create strong correlations at room temperatures. At the same time, these plasmas allow for a precise diagnostics with single-particle resolution. From such measurements, the structural properties of finite two-dimensional (2D) clusters and three-dimensional (3D) spherical crystals in nearly harmonic traps-Yukawa balls-have been explored in great detail. Their structural properties-the shell compositions and the order within the shells-have been investigated and good agreement to theoretical predictions was found. Open questions on the agenda are the excitation behavior, the structural changes, and phase transitions that occur at elevated temperature.   In order to increase the dust temperature in the experiment various techniques have been used. Among them, laser heating appears to have unique capabilities because it affects only the dust particles, leaving the lighter plasma components unchanged. Here we report on recent experimental results where laser heating methods were further improved and applied to finite 2D and 3D clusters. Comparing to simulations, we demonstrate that this indeed allows to increase the temperature in a controlled manner. For the analysis of thermodynamics and phase transitions in these finite systems, we present theoretical and experimental results on the basis of the instantaneous normal modes, pair distribution function and the recently introduced center-two-particle distribution function. ",1402.7182v2
"['Patrick Ludwig', 'Wojciech J. Miloch', 'Hanno Kählert', 'Michael Bonitz']",2012-01-09T09:25:24Z,On the Wake Structure in Streaming Complex Plasmas,"  The theoretical description of complex (dusty) plasmas requires multiscale concepts that adequately incorporate the correlated interplay of streaming electrons and ions, neutrals, and dust grains. Knowing the effective dust-dust interaction, the multiscale problem can be effectively reduced to a one-component plasma model of the dust subsystem. The goal of the present publication is a systematic evaluation of the electrostatic potential distribution around a dust grain in the presence of a streaming plasma environment by means of two complementary approaches: (i) a high precision computation of the dynamically screened Coulomb potential from the dynamic dielectric function, and (ii) full 3D particle-in-cell simulations, which self-consistently include dynamical grain charging and non-linear effects. The applicability of these two approaches is addressed. ",1201.1714v2
"['Tobias Dornheim', 'Jan Vorberger', 'Zhandos Moldabekov', 'Michael Bonitz']",2021-10-25T05:58:12Z,Nonlinear electronic density response of the warm dense electron gas:   multiple perturbations and mode coupling,"  We present extensive new ab initio path integral Monte Carlo (PIMC) results for an electron gas at warm dense matter conditions that is subject to multiple harmonic perturbations. In addition to the previously investigated nonlinear effects at the original wave number [Dornheim \emph{et al.}, PRL \textbf{125}, 085001 (2020)] and the excitation of higher harmonics [Dornheim \emph{et al.}, PRR \textbf{3}, 033231 (2021)], the presence of multiple external potentials leads to mode-coupling effects, which constitute the dominant nonlinear effect and lead to a substantially more complicated density response compared to linear response theory. One possibility to estimate mode-coupling effects from a PIMC simulation of the unperturbed system is given in terms of generalized imaginary-time correlation functions that have been recently introduced by Dornheim \emph{et al.}~[JCP \textbf{155}, 054110 (2021)]. In addition, we extend our previous analytical theory of the nonlinear density response of the electron gas in terms of the static local field correction [Dornheim \emph{et al.}, PRL \textbf{125}, 235001 (2020)], which allows for a highly accurate description of the PIMC results with negligible computational cost. ",2110.12657v1
"['Karsten Balzer', 'Niclas Schlünzen', 'Michael Bonitz']",2016-02-22T20:30:21Z,Stopping dynamics of ions passing through correlated honeycomb clusters,"  A combined nonequilibrium Green functions-Ehrenfest dynamics approach is developed that allows for a time-dependent study of the energy loss of a charged particle penetrating a strongly correlated system at zero and finite temperature. Numerical results are presented for finite inhomogeneous two-dimensional Fermi-Hubbard models, where the many-electron dynamics in the target are treated fully quantum mechanically and the motion of the projectile is treated classically. The simulations are based on the solution of the two-time Dyson (Keldysh-Kadanoff-Baym) equations using the second-order Born, third-order and T-matrix approximations of the self-energy. As application, we consider protons and helium nuclei with a kinetic energy between 1 and 500 keV/u passing through planar fragments of the two-dimensional honeycomb lattice and, in particular, examine the influence of electron-electron correlations on the energy exchange between projectile and electron system. We investigate the time dependence of the projectile's kinetic energy (stopping power), the electron density, the double occupancy and the photoemission spectrum. Finally, we show that, for a suitable choice of the Hubbard model parameters, the results for the stopping power are in fair agreement with ab-initio simulations for particle irradiation of single-layer graphene. ",1602.06928v2
"['Jan-Philip Joost', 'Patrick Ludwig', 'Hanno Kählert', 'Christopher Arran', 'Michael Bonitz']",2014-07-07T09:55:42Z,Screened Coulomb potential in a flowing magnetized plasma,"  The electrostatic potential of a moving dust grain in a complex plasma with magnetized ions is computed using linear response theory, thereby extending our previous work for unmagnetized plasmas [P. Ludwig et al., New J. Phys. 14, 053016 (2012)]. In addition to the magnetic field, our approach accounts for a finite ion temperature as well as ion-neutral collisions. Our recently introduced code \texttt{Kielstream} is used for an efficient calculation of the dust potential. Increasing the magnetization of the ions, we find that the shape of the potential crucially depends on the Mach number $M$. In the regime of subsonic ion flow ($M<1$), a strong magnetization gives rise to a potential distribution that is qualitatively different from the unmagnetized limit, while for $M>1$ the magnetic field effectively suppresses the plasma wakefield. ",1407.1645v1
"['Markus M. Becker', 'Hanno Kählert', 'Anbang Sun', 'Michael Bonitz', 'Detlef Loffhagen']",2016-08-16T14:03:44Z,Advanced fluid modelling and PIC/MCC simulations of low-pressure ccrf   discharges,"  Comparative studies of capacitively coupled radio-frequency discharges in helium and argon at pressures between 10 and 80 Pa are presented applying two different fluid modelling approaches as well as two independently developed particle-in-cell/Monte Carlo collision (PIC/MCC) codes. The focus is on the analysis of the range of applicability of a recently proposed fluid model including an improved drift-diffusion approximation for the electron component as well as its comparison with fluid modelling results using the classical drift-diffusion approximation and benchmark results obtained by PIC/MCC simulations. Main features of this time- and space-dependent fluid model are given. It is found that the novel approach shows generally quite good agreement with the macroscopic properties derived by the kinetic simulations and is largely able to characterize qualitatively and quantitatively the discharge behaviour even at conditions when the classical fluid modelling approach fails. Furthermore, the excellent agreement between the two PIC/MCC simulation codes using the velocity Verlet method for the integration of the equations of motion verifies their accuracy and applicability. ",1608.04601v2
"['Jeffrey Wrighton', 'Hanno Kählert', 'Torben Ott', 'Patrick Ludwig', 'Hauke Thomsen', 'James Dufty', 'Michael Bonitz']",2011-10-11T18:57:44Z,Charge Correlations in a Harmonic Trap,"  A system of N classical Coulomb charges trapped in a harmonic potential displays shell structure and orientational ordering. The local density profile is well understood from theory, simulation, and experiment. Here, pair correlations are considered for this highly inhomogeneous system for both the fluid and ordered states. In the former, it is noted that there is a close relationship to pair correlations in the uniform OCP. For the ordered state, it is shown that the disordered ""tiling"" is closely related to the ground state Thomson sites for a single sphere. ",1110.2465v1
"['Henning Bruhn', 'Hanno Kählert', 'Torben Ott', 'Michael Bonitz', 'Jeffrey Wrighton', 'James Dufty']",2011-10-15T21:30:03Z,Theoretical description of spherically confined strongly correlated   Yukawa plasmas,"  A theoretical description of the radial density profile for charged particles with Yukawa interaction in a harmonic trap is described. At strong Coulomb coupling shell structure is observed in both computer simulations and experiments. Correlations responsible for such shell structure are described here using a recently developed model based in density functional theory. A wide range of particle number, Coulomb coupling, and screening lengths is considered within the fluid phase. A hypernetted chain approximation shows the formation of shell structure, but fails to give quantitative agreement with Monte Carlo simulation results at strong coupling. Significantly better agreement is obtained within the hypernetted chain structure using a renormalized coupling constant, representing bridge function corrections. ",1110.3440v1
"['Paul Hamann', 'Tobias Dornheim', 'Jan Vorberger', 'Zhandos A. Moldabekov', 'Michael Bonitz']",2020-07-30T14:10:06Z,Dynamic properties of the warm dense electron gas: an ab initio path   integral Monte Carlo approach,"  There is growing interest in warm dense matter (WDM) -- an exotic state on the border between condensed matter and plasmas. Due to the simultaneous importance of quantum and correlation effects WDM is complicated to treat theoretically. A key role has been played by \textit{ab initio} path integral Monte Carlo (PIMC) simulations, and recently extensive results for thermodynamic quantities have been obtained. The first extension of PIMC simulations to the dynamic structure factor of the uniform electron gas were reported by Dornheim \textit{et al.} [Phys. Rev. Lett. \textbf{121}, 255001 (2018)]. This was based on an accurate reconstruction of the dynamic local field correction. Here we extend this concept to other dynamical quantities of the warm dense electron gas including the dynamic susceptibility, the dielectric function and the conductivity. ",2007.15471v1
"['Paul Hamann', 'Jan Vorberger', 'Tobias Dornheim', 'Zhandos Moldabekov', 'Michael Bonitz']",2020-08-11T09:47:24Z,Ab initio results for the plasmon dispersion and damping of the warm   dense electron gas,"  Warm dense matter (WDM) is an exotic state on the border between condensed matter and dense plasmas. Important occurrences of WDM include dense astrophysical objects, matter in the core of our Earth, as well as matter produced in strong compression experiments. As of late, x-ray Thomson scattering has become an advanced tool to diagnose WDM. The interpretation of the data requires model input for the dynamic structure factor $S(q,\omega)$ and the plasmon dispersion $\omega(q)$. Recently the first \textit{ab initio} results for $S(q,\omega)$ of the homogeneous warm dense electron gas were obtained from path integral Monte Carlo simulations, [Dornheim \textit{et al.}, Phys. Rev. Lett. \textbf{121}, 255001 (2018)]. Here, we analyse the effects of correlations and finite temperature on the dynamic dielectric function and the plasmon dispersion. Our results for the plasmon dispersion and damping differ significantly from the random phase approximation and from earlier models of the correlated electron gas. Moreover, we show when commonly used weak damping approximations break down and how the method of complex zeros of the dielectric function can solve this problem for WDM conditions. ",2008.04605v1
"['Tobias Dornheim', 'Zhandos Moldabekov', 'Jan Vorberger', 'Hanno Kählert', 'Michael Bonitz']",2022-03-23T09:12:02Z,Electronic pair alignment and roton feature in the warm dense electron   gas,"  The study of matter under extreme densities and temperatures as they occur e.g. in astrophysical objects and nuclear fusion applications has emerged as one of the most active frontiers in physics, material science, and related disciplines. In this context, a key quantity is given by the dynamic structure factor $S(\mathbf{q},\omega)$, which is probed in scattering experiments -- the most widely used method of diagnostics at these extreme conditions. In addition to its crucial importance for the study of warm dense matter, the modelling of such dynamic properties of correlated quantum many-body systems constitutes one of the most fundamental theoretical challenges of our time. Here we report a hitherto unexplained \emph{roton feature} in $S(\mathbf{q},\omega)$ of the warm dense electron gas, and introduce a microscopic explanation in terms of a new \emph{electronic pair alignment} model. This new paradigm will be highly important for the understanding of warm dense matter, and has a direct impact on the interpretation of scattering experiments. Moreover, we expect our results to give unprecedented insights into the dynamics of a number of correlated quantum many-body systems such as ultracold helium, dipolar supersolids, and bilayer heterostructures. ",2203.12288v1
"['Paul Hamann', 'Linda Kordts', 'Alexey Filinov', 'Michael Bonitz', 'Tobias Dornheim', 'Jan Vorberger']",2023-04-21T08:27:40Z,Prediction of a roton-type feature in warm dense hydrogen,"  In a recent Letter [T. Dornheim \textit{et al.}, Phys. Rev. Lett. \textbf{121}, 255001 (2018)], it was predicted on the basis of \textit{ab initio} quantum Monte Carlo simulations that, in a uniform electron gas, the peak $\omega_0$ of the dynamic structure factor $S(q,\omega)$ exhibits an unusual non-monotonic wave number dependence, where $d\omega_0/dq < 0$, at intermediate $q$, under strong coupling conditions. This effect was subsequently explained by the pair alignment of electrons %at an intermediate range of wave numbers [T. Dornheim \textit{et al.}, Comm. Phys. \textbf{5}, 304 (2022)]. Here we predict that this non-monotonic dispersion resembling the roton-type behavior known from superfluids should be observable in a dense, partially ionized hydrogen plasma. Based on a combination of path integral Monte Carlo simulations and linear response results for the density response function, we present the approximate range of densities, temperatures and wave numbers and make predictions for possible experimental observations. ",2304.10807v1
"['Erik Schroedter', 'Jan-Philip Joost', 'Michael Bonitz']",2022-04-18T10:59:58Z,Quantum fluctuations approach to the nonequilibrium $GW$ approximation,"  The quantum dynamics of fermionic or bosonic many-body systems following external excitation can be successfully studied using two-time nonequilibrium Green's functions (NEGF) or single-time reduced density matrix methods. Approximations are introduced via a proper choice of the many-particle self-energy or decoupling of the BBGKY hierarchy. These approximations are based on Feynman's diagram approaches or on cluster expansions into single-particle and correlation operators. Here, we develop a different approach where, instead of equations of motion for the many-particle NEGF (or density operators), single-time equations for the correlation functions of fluctuations are analyzed. We present a derivation of the first two equations of the alternative hierarchy of fluctuations and discuss possible decoupling approximations. In particular, we derive the polarization approximation (PA) which is shown to be equivalent to the single-time version [following by applying the generalized Kadanoff-Baym ansatz (GKBA)] of the nonequilibrium $GW$ approximation with exchange effects of NEGF theory, for weak coupling. The main advantage of the quantum fluctuations approach is that the standard ensemble average can be replaced by a semiclassical average over different initial realizations, as was demonstrated before by Lacroix and co-workers [see e.g. D. Lacroix et al., Phys. Rev. B, 2014, 90, 125112]. Here, we introduce the stochastic $GW$ (SGW) approximation and the stochastic polarization approximation (SPA) which are demonstrated to be equivalent to the single-time $GW$ approximation without and with exchange, respectively, in the weak coupling limit. Our numerical tests confirm that our approach has the same favorable linear scaling with the computation time as the recently developed G1-G2 scheme [Schluenzen et al., Phys. Rev. Lett., 2020, 124, 076601]. ",2204.08250v2
"['Tobias Dornheim', 'Jan Vorberger', 'Simon Groth', 'Nico Hoffmann', 'Zhandos Moldabekov', 'Michael Bonitz']",2019-07-19T11:52:54Z,The Static Local Field Correction of the Warm Dense Electron Gas: An ab   Initio Path Integral Monte Carlo Study and Machine Learning Representation,"  The study of matter at extreme densities and temperatures as they occur in astrophysical objects and state-of-the art experiments with high-intensity lasers is of high current interest for many applications. While no overarching theory for this regime exists, accurate data for the density response of correlated electrons to an external perturbation are of paramount importance. In this context, the key quantity is given by the local field correction (LFC), which provides a wave-vector resolved description of exchange-correlation effects. In this work, we present extensive new path integral Monte Carlo (PIMC) results for the static LFC of the uniform electron gas, which are subsequently used to train a fully connected deep neural network. This allows us to present a continuous representation of the LFC with respect to wave-vector, density, and temperature covering the entire warm dense matter regime. Both the PIMC data and neural-net results are available online. Moreover, we expect the presented combination of ab initio calculations with machine-learning methods to be a promising strategy for many applications. ",1907.08473v1
"['Tobias Dornheim', 'Maximilian Böhme', 'Zhandos A. Moldabekov', 'Jan Vorberger', 'Michael Bonitz']",2021-04-06T10:19:56Z,Density Response of the Warm Dense Electron Gas beyond Linear Response   Theory: Excitation of Harmonics,"  In a recent Letter, Dornheim et al. [PRL 125, 085001 (2020)] have investigated the nonlinear density response of the uniform electron gas in the warm dense matter regime. More specifically, they have studied the cubic response function at the first harmonic, which cannot be neglected in many situations of experimental relevance. In this work, we go one step further and study the full spectrum of excitations at the higher harmonics of the original perturbation based on extensive new ab initio path integral Monte Carlo (PIMC) simulations. We find that the dominant contribution to the density response beyond linear response theory is given by the quadratic response function at the second harmonic in the moderately nonlinear regime. Furthermore, we show that the nonlinear density response is highly sensitive to exchange-correlation effects, which makes it a potentially valuable new tool of diagnostics. To this end, we present a new theoretical description of the nonlinear electronic density response based on the recent effective static approximation to the local field correction [PRL 125, 235001 (2020)], which accurately reproduces our PIMC data with negligible computational cost. ",2104.02405v1
"['Stefan Donsa', 'Fabian Lackner', 'Joachim Burgdörfer', 'Michael Bonitz', 'Benedikt Kloss', 'Angel Rubio', 'Iva Březinová']",2023-03-08T13:41:58Z,Non-equilibrium correlation dynamics in the one-dimensional   Fermi-Hubbard model: A testbed for the two-particle reduced density matrix   theory,"  We explore the non-equilibrium dynamics of a one-dimensional Fermi-Hubbard system as a sensitive testbed for the capabilities of the time-dependent two-particle reduced density matrix (TD2RDM) theory to accurately describe time-dependent correlated systems. We follow the time evolution of the out-of-equilibrium finite-size Fermi-Hubbard model initialized by a quench over extended periods of time. By comparison with exact calculations for small systems and with matrix product state (MPS) calculations for larger systems but limited to short times, we demonstrate that the TD2RDM theory can accurately account for the non-equilibrium dynamics in the regime from weak to moderately strong inter-particle correlations. We find that the quality of the approximate reconstruction of the three-particle cumulant (or correlation) required for the closure of the equations of motion for the reduced density matrix is key to the accuracy of the numerical TD2RDM results. We identify the size of the dynamically induced three-particle correlations and the amplitude of cross correlations between the two- and three-particle cumulants as critical parameters that control the accuracy of the TD2RDM theory when current state-of-the art reconstruction functionals are employed. ",2303.04576v1
"['Peter Hartmann', 'Jorge C. Reyes', 'Evdokiya G. Kostadinova', 'Lorin S. Matthews', 'Truell W. Hyde', 'Ranna U. Masheyeva', 'Karlygash N. Dzhumagulova', 'Tlekkabul S. Ramazanov', 'Torben Ott', 'Hanno Kählert', 'Michael Bonitz', 'Ihor Korolov', 'Zoltán Donkó']",2018-12-26T07:29:53Z,Self-diffusion in two-dimensional quasi-magnetized rotating dusty   plasmas,"  The self-diffusion phenomenon in a two-dimensional dusty plasma at extremely strong (effective) magnetic fields is studied experimentally and by means of molecular dynamics simulations. In the experiment the high magnetic field is introduced by rotating the particle cloud and observing the particle trajectories in a co-rotating frame, which allows reaching effective magnetic fields up to 3000 Tesla. The experimental results confirm the predictions of the simulations: (i) super-diffusive behavior is found at intermediate time-scales and (ii) the dependence of the self-diffusion coefficient on the magnetic field is well reproduced. ",1812.10253v1
"['Tobias Dornheim', 'Zhandos A. Moldabekov', 'Kushal Ramakrishna', 'Panagiotis Tolias', 'Andrew D. Baczewski', 'Dominik Kraus', 'Thomas R. Preston', 'David A. Chapman', 'Maximilian P. Böhme', 'Tilo Döppner', 'Frank Graziani', 'Michael Bonitz', 'Attila Cangi', 'Jan Vorberger']",2022-12-16T07:55:35Z,Electronic Density Response of Warm Dense Matter,"  Matter at extreme temperatures and pressures -- commonly known as warm dense matter (WDM) in the literature -- is ubiquitous throughout our Universe and occurs in a number of astrophysical objects such as giant planet interiors and brown dwarfs. Moreover, WDM is very important for technological applications such as inertial confinement fusion, and is realized in the laboratory using different techniques. A particularly important property for the understanding of WDM is given by its electronic density response to an external perturbation. Such response properties are routinely probed in x-ray Thomson scattering (XRTS) experiments, and, in addition, are central for the theoretical description of WDM. In this work, we give an overview of a number of recent developments in this field. To this end, we summarize the relevant theoretical background, covering the regime of linear-response theory as well as nonlinear effects, the fully dynamic response and its static, time-independent limit, and the connection between density response properties and imaginary-time correlation functions (ITCF). In addition, we introduce the most important numerical simulation techniques including ab initio path integral Monte Carlo (PIMC) simulations and different thermal density functional theory (DFT) approaches. From a practical perspective, we present a variety of simulation results for different density response properties, covering the archetypal model of the uniform electron gas and realistic WDM systems such as hydrogen. Moreover, we show how the concept of ITCFs can be used to infer the temperature from XRTS measurements of arbitrarily complex systems without the need for any models or approximations. Finally, we outline a strategy for future developments based on the close interplay between simulations and experiments. ",2212.08326v2
"['Michael Bonitz', 'Jan Vorberger', 'Mandy Bethkenhagen', 'Maximilian Böhme', 'David Ceperley', 'Alexey Filinov', 'Thomas Gawne', 'Frank Graziani', 'Gianluca Gregori', 'Paul Hamann', 'Stephanie Hansen', 'Markus Holzmann', 'S. X. Hu', 'Hanno Kählert', 'Valentin Karasiev', 'Uwe Kleinschmidt', 'Linda Kordts', 'Christopher Makait', 'Burkhard Militzer', 'Zhandos Moldabekov', 'Carlo Pierleoni', 'Martin Preising', 'Kushal Ramakrishna', 'Ronald Redmer', 'Sebastian Schwalbe', 'Pontus Svensson', 'Tobias Dornheim']",2024-05-17T08:42:36Z,First principles simulations of dense hydrogen,"  Accurate knowledge of the properties of hydrogen at high compression is crucial for astrophysics (e.g. planetary and stellar interiors, brown dwarfs, atmosphere of compact stars) and laboratory experiments, including inertial confinement fusion. There exists experimental data for the equation of state, conductivity, and Thomson scattering spectra. However, the analysis of the measurements at extreme pressures and temperatures typically involves additional model assumptions, which makes it difficult to assess the accuracy of the experimental data. rigorously. On the other hand, theory and modeling have produced extensive collections of data. They originate from a very large variety of models and simulations including path integral Monte Carlo (PIMC) simulations, density functional theory (DFT), chemical models, machine-learned models, and combinations thereof. At the same time, each of these methods has fundamental limitations (fermion sign problem in PIMC, approximate exchange-correlation functionals of DFT, inconsistent interaction energy contributions in chemical models, etc.), so for some parameter ranges accurate predictions are difficult. Recently, a number of breakthroughs in first principle PIMC and DFT simulations were achieved which are discussed in this review. Here we use these results to benchmark different simulation methods. We present an update of the hydrogen phase diagram at high pressures, the expected phase transitions, and thermodynamic properties including the equation of state and momentum distribution. Furthermore, we discuss available dynamic results for warm dense hydrogen, including the conductivity, dynamic structure factor, plasmon dispersion, imaginary-time structure, and density response functions. We conclude by outlining strategies to combine different simulations to achieve accurate theoretical predictions. ",2405.10627v1
"['Wolfgang J. Duschl', 'Harald Lesch']",1994-02-04T09:27:04Z,The spectrum of Sgr A* and its variability,"  We demonstrate that there is only one physical process required to explain the spectrum and the variability of the radio source at the dynamical center of our Galaxy, Sgr A*, in the frequency range from $\approx$1 to $\approx$1000 GHz, namely optically thin synchrotron radiation that is emitted from a population of relativistic electrons. We attribute the observed variability to variable energy input from an accretion disk around Sgr A* into the acceleration of the electrons. ",astro-ph/9402013v1
"['Wolfgang J. Duschl', 'Harald Lesch']",1994-11-03T14:05:17Z,The radio spectrum of Sgr A*,"  We discuss the radio spectrum of Sgr A* \index{Sgr A*, radio spectrum} in the frequency range between $\approx 1\,{\rm GHz}$ and $\approx 1\,000\,{\rm GHz}$, show that it can be explained by optically thin synchrotron radiation \index{Sgr A*, synchrotron radiation, optically thin} of relativistic electrons, and point toward a possible correlation between the spectrum of Sgr A* and larger-scale ($\la 50\,{\rm pc}$) radio emission from the Galactic Center \index{Galactic Center} region. ",astro-ph/9411015v1
"['Marvin Blank', 'Wolfgang J. Duschl']",2012-06-14T17:15:46Z,Mass flow rates in and outflow rates from AGN accretion discs,"  We derive analytical expressions for the mass flow rates in and from accretion discs, taking into account the Eddington limit. This allows us to connect the basic properties of outflows with those of the accretion flow. As an example, we derive the radial surface density distribution in the accretion disc of Mrk 231. ",1206.3185v1
"['Peter L. Biermann', 'Wolfgang J. Duschl', 'Susanne von Linden']",1993-05-12T07:34:21Z,Molecular Clouds Close to the Galactic Center,"  We demonstrate that the accretion disk model for the Galactic Center region by Linden et al (1993a) is applicable for at least one order of magnitude in radius from the Galactic Center (10 ... 100 pc). The viscosity $\nu$ is shown to be weakly dependent on the radius $s$: $\nu \sim s^{0.4}$. Finally, we discuss the influence of the inner boundary on the structure of the inner disk regions. ",astro-ph/9305013v1
"['Marvin Blank', 'Wolfgang J. Duschl']",2016-07-21T15:35:21Z,Viscous time lags between starburst and AGN activity,  There is strong observational evidence indicating a time lag of order of some 100 Myr between the onset of starburst and AGN activity in galaxies. Dynamical time lags have been invoked to explain this. We extend this approach by introducing a viscous time lag the gas additionally needs to flow through the AGN's accretion disc before it reaches the central black hole. Our calculations reproduce the observed time lags and are in accordance with the observed correlation between black hole mass and stellar velocity dispersion. ,1607.06365v1
"['Wolfgang J. Duschl', 'Peter A. Strittmatter']",2006-02-01T09:24:29Z,The Cosmogony of Super-Massive Black Holes,"  We report results of a project investigating the growth of super-massive black holes (BHs) by disk accretion. We find that the BH mass growth is quick enough to account for the inferred masses in the highest-redshift quasars, and the growth time is an inverse function of the final BH mass as seems to be required by recent X-ray surveys. ",astro-ph/0602009v1
"['Wolfgang J. Duschl', 'Markward Britsch']",2006-07-27T09:12:15Z,A Gravitational Instability-Driven Viscosity in Self-Gravitating   Accretion Disks,"  We derive a viscosity from gravitational instability in self-gravitating accretion disks, which has the required properties to account for the observed fast formation of the first super-massive black holes in highly redshifted quasars and for the cosmological evolution of the black hole-mass distribution. ",astro-ph/0607610v2
"['Bernd Vollmer', 'Wolfgang J. Duschl']",1999-04-08T09:24:52Z,The Minispiral in the Galactic Center revisited,"  We present the results of a re-examination of a [Ne II] line emission data cube (\lambda 12.8 \mu m) and discuss the kinematic structure of the inner \sim 3 \times 4 pc of the Galaxy. The quality of [Ne II] as a tracer of ionized gas is examined by comparing it to radio data. A three dimensional representation of the data cube allows us to disentangle features which are projected onto the same location on the sky. A model of gas streams in different planes is fitted to the data. We find that most of the material is located in a main plane which itself is defined by the inner edge of the Circum-Nuclear Disk in the Galactic Center. Finally, we present a possible three dimensional model of the gas streams. ",astro-ph/9904096v1
"['Wolfgang J. Duschl', 'Peter A. Strittmatter']",2004-01-02T18:41:26Z,Formation of Super-massive Black Holes,"  We show that the rapid formation of super-massive black holes in quasars can indeed be understood in terms of major galaxy mergers followed by disk accretion. The necessary short disk evolution time can be achieved provided the disk viscosity is sufficiently large, which, for instance, is the case for hydrodynamic turbulence, unlimited by shock dissipation. We present numerical calculations for a representative case. This general picture can account for (a) the presence of highly luminous quasars at redshifts z > 6; (b) for the peak in quasar activity at z ~ 2; and (c) for a subsequent rapid disappearance of quasars at later epochs. ",astro-ph/0401010v1
"['Meng Xiang-Gruess', 'Yu-Qing Lou', 'Wolfgang J. Duschl']",2009-09-18T18:16:13Z,Dark matter dominated dwarf disc galaxy Segue 1,"  Several observations reveal that dwarf galaxy Segue 1 has a dark matter (DM) halo at least ~ 200 times more massive than its visible baryon mass of only ~ 103 solar masses. The baryon mass is dominated by stars with perhaps an interstellar gas mass of < 13 solar masses. Regarding Segue 1 as a dwarf disc galaxy by its morphological appearance of long stretch, we invoke the dynamic model of Xiang-Gruess, Lou & Duschl (XLD) to estimate its physical parameters for possible equilibria with and without an isopedically magnetized gas disc. We estimate the range of DM mass and compare it with available observational inferences. Due to the relatively high stellar velocity dispersion compared to the stellar surface mass density, we find that a massive DM halo would be necessary to sustain disc equilibria. The required DM halo mass agrees grossly with observational inferences so far. For an isopedic magnetic field in a gas disc, the ratio f between the DM and baryon potentials depends strongly on the magnetic field strength. Therefore, a massive DM halo is needed to counteract either the strong stellar velocity dispersion and rotation of the stellar disc or the magnetic Lorentz force in the gas disc. By the radial force balances, the DM halo mass increases for faster disc rotation. ",0909.3496v1
"['Michael Mayer', 'Wolfgang J. Duschl']",2004-06-23T09:46:47Z,Stationary Population III accretion discs,"  We present stationary models of protostellar population III (Pop III, for short) accretion discs, compare them to Pop I discs, and investigate the influence of the different chemical compositions on the occurence of gravitational, thermal and thermal-viscous instabilities in the discs. In particular in the cooler regions, we find major differences between Pop III and Pop I discs, both in the structure and stability behaviour. This is mainly due to the absence of most molecules and dust in Pop III, which are very efficient absorbers in Pop I discs. ",astro-ph/0406501v3
"['Thomas Beckert', 'Wolfgang J. Duschl', 'Bernd Vollmer']",2004-10-15T08:46:55Z,Torus models for obscuration in type 2 AGN,"  We discuss a clumpy model of obscuring dusty tori around AGN. Cloud-cloud collisions lead to an effective viscosity and a geometrically thick accretion disk, which has the required properties of a torus.   Accretion in the combined gravitational potential of central black hole and stellar cluster generates free energy, which is dissipated in collisions, and maintains the thickness of the torus. A quantitative treatment for the torus in the prototypical Seyfert 2 nucleus of NGC 1068 together with a radiative transfer calculation for NIR re-emission from the torus is presented. ",astro-ph/0410368v1
"['Tobias F. Illenseer', 'Wolfgang J. Duschl']",2008-04-18T09:10:12Z,Two-Dimensional Central-Upwind Schemes for Curvilinear Grids and   Application to Gas Dynamics with Angular Momentum,  In this work we present new second order semi-discrete central schemes for systems of hyperbolic conservation laws on curvilinear grids. Our methods generalise the two-dimensional central-upwind schemes developed by Kurganov and Tadmor. In these schemes we account for area and volume changes in the numerical flux functions due to the non-cartesian geometries. In case of vectorial conservation laws we introduce a general prescription of the geometrical source terms valid for various orthogonal curvilinear coordinate systems. The methods are applied to the two-dimensional Euler equations of inviscid gas dynamics with and without angular momentum transport. In the latter case we introduce a new test problem to examine the detailed conservation of specific angular momentum. ,0804.2979v1
"['Tobias F. Illenseer', 'Wolfgang J. Duschl']",2015-03-17T15:46:33Z,Self-similar Evolution of Self-Gravitating Viscous Accretion Discs,"  A new one-dimensional, dynamical model is proposed for geometrically thin, self-gravitating viscous accretion discs. The vertically integrated equations are simplified using the slow accretion limit and the monopole approximation with a time-dependent central point mass to account for self-gravity and accretion. It is shown that the system of partial differential equations can be reduced to a single non-linear advection diffusion equation which describes the time evolution of angular velocity.   In order to solve the equation three different turbulent viscosity prescriptions are considered. It is shown that for these parametrizations the differential equation allows for similarity transformations depending only on a single non-dimensional parameter. A detailed analysis of the similarity solutions reveals that this parameter is the initial power law exponent of the angular velocity distribution at large radii. The radial dependence of the self-similar solutions is in most cases given by broken power laws. At small radii the rotation law always becomes Keplerian with respect to the current central point mass. In the outer regions the power law exponent of the rotation law deviates from the Keplerian value and approaches asymptotically the value determined by the initial condition. It is shown that accretion discs with flatter rotation laws at large radii yield higher accretion rates.   The methods are applied to self-gravitating accretion discs in active galactic nuclei. Fully self-gravitating discs are found to evolve faster than nearly Keplerian discs. The implications on supermassive black hole formation and Quasar evolution are discussed. ",1503.05099v1
"['Hannes Horst', 'Wolfgang J. Duschl']",2006-02-01T07:29:52Z,A simple model for quasar density evolution,  It is widely agreed upon that AGN and Quasars are driven by gas accretion onto a supermassive black hole. The origin of the latter however still remains an open question. In this work we present the results of an extremely simple cosmological model combined with an evolutionary scenario in which both the formation of the black hole as well as the gas accretion onto it are triggered by major mergers of gas-rich galaxies. Despite its very generous approximations our model reproduces the quasar density evolution in remarkable agreement with observations. ,astro-ph/0602007v1
"['Hannes Horst', 'Alain Smette', 'Poshak Gandhi', 'Wolfgang J. Duschl']",2006-08-16T23:03:46Z,The small dispersion of the mid IR -- hard X-ray correlation in AGN,"  Context: We investigate mid-infrared and X-ray properties of the dusty torus in unification scenarios for active galactic nuclei.   Aims: We use the relation between mid IR and hard X-ray luminosities to constrain AGN unification scenarios.   Methods: With VISIR at the VLT, we have obtained the currently highest angular resolution (0"".35 FWHM) narrow-band mid infrared images of the nuclei of 8 nearby Seyfert galaxies. Combining these observations with X-ray data from the literature we study the correlation between their mid IR and hard X-ray luminosities.   Results: We find that the rest frame 12.3 mircon (L_MIR) and 2-10 keV (L_X) luminosities are correlated at a highly significant level. The best fit power-law to our data is log L_MIR \propto (1.60 \pm 0.22) log L_X, showing a much smaller dispersion than earlier studies.   Conclusions: The similarity in the og L_MIR / log L_X ratio between Sy1s and Sy2s even using high angular resolution MIR data implies that the similarity is intrinsic to AGN and not caused by contamination from extra-nuclear emission. This supports clumpy torus models. The exponent of the correlation constrains the inner geometry of the torus. ",astro-ph/0608358v1
"['Manuel Jung', 'Tobias F. Illenseer', 'Wolfgang J. Duschl']",2015-05-26T14:47:27Z,Resolving vortices with an isothermal HLLC Riemann solver,"  The importance of contact discontinuities in 2D isothermal flows has rarely been discussed, since most Riemann solvers are derived for 1D Euler equations. We present a new contact resolving approximate Riemann solver for the isothermal Euler equations and show its performance for several one- and two-dimensional test problems. The new solver extends the well-known HLL solver, while retaining its computational simplicity. The significant gain in resolution of vortices is displayed by a simulation of the K\'arm\'an vortex street. We discuss the loss of Galilean invariance and its implications for the resolution of contact discontinuities, which is experienced by all modern numerical schemes for hydrodynamics in non-moving grids. ",1505.06974v2
"['Manuel Jung', 'Tobias F. Illenseer', 'Wolfgang J. Duschl']",2018-03-06T08:13:33Z,Multi-scale simulations of black hole accretion in barred galaxies:   Numerical methods and tests,"  Due to the non-axisymmetric potential of the central bar, barred spiral galaxies form, in addition to their characteristic arms and bar, a variety of structures within the thin gas disk, like nuclear rings, inner spirals and dust-lanes. In this first of two papers, we present a method to accurately simulate the gas flow within the galactic plane in the 2D finite volume software package FOSITE, which solves the transport equations for mass, momentum and energy, and apply it to this class of objects. To this extent, we introduced a new transport scheme for angular momentum and a very efficient pseudo-spectral Poisson solver. Moreover, we provide a simple and generally applicable method of how to take care of gravity in the energy equation. ",1803.02055v1
"['Hannes Horst', 'Poshak Gandhi', 'Alain Smette', 'Wolfgang J. Duschl']",2007-11-23T15:03:01Z,The mid IR -- hard X-ray correlation in AGN and its implications for   dusty torus models,"  Context: We investigate mid-infrared and X-ray properties of the dusty torus invoked in the unification scenario for active galactic nuclei.   Aims: We use the relation between mid IR and hard X-ray luminosities to constrain the geometry and physical state of the dusty torus.   Methods: We present new VISIR observations of 17 nearby AGN and combine these with our earlier VISIR sample of 8 Seyfert galaxies. Combining these observations with X-ray data from the literature we study the correlation between their mid IR and hard X-ray luminosities.   Results: A statistically highly significant correlation between the rest frame 12.3 mircon (L_MIR) and 2-10 keV (L_X) luminosities is found. Furthermore, with a probability of 97%, we find that Sy 1 and Sy 2 have the same distribution of L_MIR over L_X.   Conclusions: The high resolution of our MIR imaging allows us to exclude any significant non-torus contribution to the AGN mid IR continuum,thereby implying that the similarity in the L_MIR / L_X ratio between Sy 1s and Sy 2s is intrinsic to AGN. We argue that this is best explained by clumpy torus models. The slope of the correlation is in good agreement with the expectations from the unified scenario and indicates little to no change of the torus geometry with luminosity. In addition, we demonstrate that the high angular resolution is crucial for AGN studies in the IR regime. ",0711.3734v2
"['Jan-Torge Schindler', 'Xiaohui Fan', 'Wolfgang J. Duschl']",2016-04-18T20:04:48Z,Stellar and Black Hole Mass Densities as Empirical Tracers of   Co-evolution Show Lock-step Growth since $z{\sim}3$,"  At redshifts beyond $z{\sim}1$ measuring the black hole galaxy relations proves to be a difficult task. The bright light of the AGN aggravates deconvolution of black hole and galaxy properties. On the other hand high redshift data on these relations is vital to understand in what ways galaxies and black holes co-evolve and in what ways they don't. In this work we use black hole (BHMDs) and stellar mass densities (SMDs) to constrain the possible co-evolution of black holes with their host galaxies since $z{\sim}5$. The BHMDs are calculated from quasar luminosity functions (QLF) using the Soltan argument, while we use integrals over stellar mass functions (SMFs) or the star formation rate density to obtain values for the stellar mass density. We find that both quantities grow in lock-step below redshifts of $z{\sim}3$ with a non-evolving BHMD to SMD ratio. A fit to the data assuming a power law relation between the BHMD and the SMD yields exponents around unity ($1.0{-}1.5$). Up to $z{\sim}5$ the BHMD to SMD ratio doesn't show a strong evolution given the larger uncertainty in the completeness of high-redshift datasets. Our results, always applying the same analysis technique, seem to be consistent across all adopted data sets. ",1604.05333v1
"['Hannes Horst', 'Wolfgang J. Duschl', 'Poshak Gandhi', 'Alain Smette']",2008-12-31T16:16:01Z,Mid-infrared imaging of 25 local AGN with VLT-VISIR,"  Aims. High angular resolution N-band imaging is used to discern the torus of active galactic nuclei (AGN) from its environment in order to allow a comparison of its mid-infrared properties to the expectations of the unified scenario for AGN. Methods. We present VLT-VISIR images of 25 low-redshift AGN of different Seyfert types, as well as N-band SEDs of 20 of them. In addition, we compare our results for 19 of them to Spitzer IRS spectra. Results. We find that at a resolution of ~ 0.35"", all the nuclei of our observed sources are point-like, except for 2 objects whose extension is likely of instrumental origin. For 3 objects, however, we observed additional extended circumnuclear emission, even though our observational strategy was not designed to detect it. Comparison of the VISIR photometry and Spitzer spectrophotometry indicates that the latter is affected by extended emission in at least 7 out of 19 objects and the level of contamination is (0.20 ~ 0.85) * F_IRS. In particular, the 10 um silicate emission feature seen in the Spitzer spectra of 6 type I AGN, possibly 1 type II AGN and 2 LINERs, also probably originates not solely in the torus but also in extended regions. Conclusions. Our results generally agree with the expectations from the unified scenario, while the relative weakness of the silicate feature supports clumpy torus models. Our VISIR data indicate that, for low-redshift AGN, a large fraction of Spitzer IRS spectra are contaminated by extended emission close to the AGN. ",0901.0106v1
"['Manuel Jung', 'Tobias F. Illenseer', 'Wolfgang J. Duschl']",2018-02-19T21:49:59Z,Multi-scale simulations of black hole accretion in barred galaxies:   Self-gravitating disk models,"  Due to the non-axisymmetric potential of the central bar, barred spiral galaxies form, in addition to their characteristic arms and bar, a variety of structures within the thin gas disk, like nuclear rings, inner spirals and dust-lanes. These structures in the inner kiloparsec are most important to explain and understand the rate of black hole feeding. The aim of this work is to investigate the influence of stellar bars in spiral galaxies on the thin self-gravitating gas disk. We focus on the accretion of gas onto the central supermassive black hole and its time-dependent evolution. We conduct multi-scale simulations simultaneously resolving the galactic disk and the accretion disk around the central black-hole. We vary in all simulations the initial gas disk mass. As additional parameter we choose either the gas temperature for isothermal simulations or the cooling timescale in case of non-isothermal simulations. Accretion is either driven by a gravitationally unstable or clumpy accretion disk or by energy dissipation in strong shocks. Most simulations show a strong dependence of the accretion rate at the outer boundary of the central accretion disk ($r< 300~\mathrm{pc}$) on the gas flow at kiloparsec scales. The final black hole masses reach up to $\sim 10^9 M_\odot$ after $1.6~\mathrm{Gyr}$. Our models show the expected influence of the Eddington limit and a decline in growth rate at the corresponding sub-Eddington limit. ",1802.06873v2
"['Wolfgang J. Duschl', 'Peter A. Strittmatter', 'Peter L. Biermann']",2000-04-06T09:26:56Z,A Note on Hydrodynamic Viscosity and Selfgravitation in Accretion Disks,"  We propose a generalized accretion disk viscosity prescription based on hydrodynamically driven turbulence at the critical effective Reynolds number. This approach is consistent with recent re-analysis by Richard & Zahn (1999) of experimental results on turbulent Couette-Taylor flows. This new $\beta$-viscosity formulation is applied to both selfgravitating and non-selfgravitating disks and is shown to yield the standard $\alpha$-disk prescription in the case of shock dissipation limited, non-selfgravitating disks. A specific case of fully selfgravitating $\beta$-disks is analyzed. We suggest that such disks may explain the observed spectra of protoplanetary disks and yield a natural explanation for the radial motions inferred from the observed metallicity gradients in disk galaxies. The $\beta$-mechanism may also account for the rapid mass transport required to power ultra luminous infrared galaxies. ",astro-ph/0004076v1
"['Kerstin Weis', 'Wolfgang J. Duschl', 'Dominik J. Bomans']",2000-12-20T09:59:32Z,"High velocity structures in, and the X-ray emission from the LBV nebula   around Eta Carinae","  The Luminous Blue Variable star Eta Carinae is one of the most massive stars known. It underwent a giant eruption in 1843 in which the Homunculus nebula was created. ROSAT and ASCA data indicate the existence of a hard and a soft X-ray component which appear to be spatially distinct: a softer diffuse shell of the nebula around Eta Carinae and a harder point-like source centered on the star Eta Car. Astonishingly the morphology of the X-ray emission is very different from the optical appearance of the nebula. We present a comparative analysis of optical morphology, the kinematics, and the diffuse soft X-ray structure of the nebula around Eta Carinae. Our kinematic analysis of the nebula shows extremely high expansion velocities. We find a strong correlation between the X-ray emission and the knots in the nebula and the largest velocities, i.e. the X-ray morphology of the nebula around Eta Carinae is determined by the interaction between material streaming away from Eta Car and the ambient medium. ",astro-ph/0012426v1
"['Kerstin Weis', 'Wolfgang J. Duschl', 'Dominik J. Bomans']",2002-11-15T16:05:55Z,An outflow from the nebula around the LBV candidate S 119,"  We present an analysis of the kinematic and morphological structure of the nebula around the LMC LBV candidate S 119. On HST images, we find a predominantly spherical nebula which, however, seems to be much better confined in its eastern hemisphere than in the western one. The filamentary western part of the nebula is indicative of matter flowing out of the nebula's main body. This outflow is even more evidenced by our long-slit echelle spectra. They show that, while most of the nebula has an expansion velocity of 25.5 km/s, the outflowing material reaches velocities of almost 140 km/s, relative to the systemic one. A ROSAT HRI image shows no trace of S 119 and thus no indications of hot or shocked material. ",astro-ph/0211362v1
"['Kerstin Weis', 'Wolfgang J. Duschl']",2002-07-04T09:02:18Z,Outflow from and asymmetries in the nebula around the LBV candidate   Sk-69 279,"  We present and discuss new long-slit Echelle spectra of the LMC LBV candidate Sk-69 279 and put them in context with previous images and spectra. While at first glance a simple spherically expanding symmetric shell, we find a considerably more complex morphology and kinematics. The spectra indicate that morphologically identified deviations from sphericity are outflows of faster material out of the main body of Sk-69 279. The morphological as well as the kinematic similarity with other LBV nebulae makes it likely that Sk-69 279 is an LBV candidate, indeed, and poses the question in how far outflows out of expanding LBV nebulae are a general property of such nebulae--at least during some phases of their evolutions. ",astro-ph/0207109v1
"['Michael Mayer', 'Wolfgang J. Duschl']",2004-11-22T18:29:39Z,Rosseland and Planck mean opacities for primordial matter,"  We present newly calculated low-temperature opacities for gas with a primordial chemical composition. In contrast to earlier calculations which took a pure metal-free Hydrogen/Helium mixture, we take into account the small fractions of Deuterium and Lithium as resulting from Standard Big Bang Nucleosynthesis. Our opacity tables cover the density range -16 < log rho [g cm^{-3}] < -2 and temperature range of 1.8 < T [K] < 4.6, while previous tables were usually restricted to T > 10^3 K. We find that, while the presence of Deuterium does not significantly alter the opacity values, the presence of Lithium gives rise to major modifications of the opacities, at some points increasing it by approximately 2 orders of magnitude relative to pure Hydrogen/Helium opacities. ",astro-ph/0411613v1
"['Stefan Vehoff', 'Dieter E. A. Nuernberger', 'Christian A. Hummel', 'Wolfgang J. Duschl']",2007-11-29T16:15:11Z,VLTI/MIDI Observations of the Massive Protostellar Candidate NGC 3603   IRS 9A,"  We used MIDI, the mid-infrared interferometric instrument of the VLTI, to observe the massive protostellar candidate IRS 9A, located at a distance of about 7 kpc at the periphery of the NGC 3603 star cluster. Our ongoing analysis shows that MIDI almost fully resolves the object on all observed baselines, yet below 9 $\mu$m we detect a steep rise of the visibility. This feature is modelled as a combination of a compact hot component and a resolved warm envelope which lowers the correlated flux at longer wavelengths. The extended envelope can already be seen in both MIDI's acquisition images and in complementary data from aperture masking observations at the Gemini South telescope. Its shape is asymmetric, which could indicate a circumstellar disk inclined against the line of sight. The compact component is possibly related to the inner edge of this (accretion) disk. The uncorrelated mid-infrared spectrum appears featureless and could be caused by optically thick emission without a significant contribution from the disk atmosphere. ",0711.4760v1
"['Stephan C. Deschner', 'Tobias F. Illenseer', 'Wolfgang J. Duschl']",2016-10-20T09:14:09Z,Self-similar solutions to isothermal shock problems,"  We investigate exact solutions for isothermal shock problems in different one-dimensional geometries. These solutions are given as analytical expressions if possible, or are computed using standard numerical methods for solving ordinary differential equations. We test the numerical solutions against the analytical expressions to verify the correctness of all numerical algorithms. We use similarity methods to derive a system of ordinary differential equations (ODE) yielding exact solutions for power law density distributions as initial conditions. Further, the system of ODEs accounts for implosion problems (IP) as well as explosion problems (EP) by changing the initial or boundary conditions, respectively. Taking genuinely isothermal approximations into account leads to additional insights of EPs in contrast to earlier models. We neglect a constant initial energy contribution but introduce a parameter to adjust the initial mass distribution of the system. Moreover, we show that due to this parameter a constant initial density is not allowed for isothermal EPs. Reasonable restrictions for this parameter are given. Both, the (genuinely) isothermal implosion as well as the explosion problem are solved for the first time. ",1610.07879v3
"['Heino Falcke', 'Peter L. Biermann', 'Wolfgang J. Duschl', 'Peter G. Mezger']",1992-12-07T14:59:33Z,A rotating black hole in the Galactic Center,"  Recent observations of Sgr A* give strong constraints for possible models of the physical nature of Sgr A* and suggest the presence of a massive black~hole with M<2 10^6 M_sun surrounded by an accretion disk which we estimate to radiate at a luminosity of <7 10^5 L_sun. We therefore calculate the appearance of a standard accretion disk around a Kerr hole in Sgr A* following from general relativity and a few fundamental assumptions. Effective temperature and luminosity of the disk spectra do not depend on the unknown viscosity mechanism but instead are quite sensitive to variations of intrinsic parameters: the mass, the accretion rate, the angular momentum of the accreting hole and the inclination angle. A radiation field of L~7 10^4 - 7 10^5 L_sun and T_eff ~ 2-4 10^4 K can be ascribed to a rapidly rotating Kerr~hole (a>0.9) accreting 10^-8.5 - 10^-7 M_sun/yr at a black~hole mass of M=2 10^6 M_sunseen almost edge on. A low mass black hole of M<10^3 M_sun seems to be very unlikely. We provide a ``Hertzsprung-Russell diagram for black holes'' together with simple scaling laws to provide an easy-to-handle test for the black hole model. ",astro-ph/9212001v1
"['Jannes Klee', 'Tobias F. Illenseer', 'Manuel Jung', 'Wolfgang J. Duschl']",2017-04-07T11:53:49Z,The impact of numerical oversteepening on the fragmentation boundary in   self-gravitating disks,"  Context. It is still an open issue whether a self-gravitating accretion disk fragments. There are many different physical and numerical explanations for fragmentation, but simulations often show a non-convergent behavior for ever better resolution.   Aims. We investigate the influence of different numerical limiters in Godunov type schemes on the fragmentation boundary in self- gravitating disks.   Methods. We compare the linear and non-linear outcome in two-dimensional shearingsheet simulations using the VANLEER and the SUPERBEE limiter.   Results. We show that choosing inappropriate limiting functions to handle shock-capturing in Godunov type schemes can lead to an overestimation of the surface density in regions with shallow density gradients. The effect amplifies itself on timescales comparable to the dynamical timescale even at high resolutions. This is exactly the environment, where clumps are expected to form. The effect is present without, but scaled up by, self-gravity and also does not depend on cooling. Moreover it can be backtracked to a well known effect called oversteepening. If the effect is also observed in the linear case, the fragmentation limit is shifted to larger values of the critical cooling timescale. ",1704.02193v2
"['Jannes Klee', 'Tobias F. Illenseer', 'Manuel Jung', 'Wolfgang J. Duschl']",2019-09-19T09:29:16Z,Closing the gap to convergence of gravitoturbulence in local simulations,"  Aims. Our goal is to find a converged cooling limit for fragmentation in self-gravitating disks. This is especially interesting for the formation of planets, brown dwarfs or stars and the growth of black holes. While investigating the limit, we want to give a clear criterion for the state of convergence.   Methods. We run two-dimensional shearingsheet simulations with the hydrodynamic package Fosite at high resolutions. Thereby resolution and limiters are altered. Subsequently, we investigate the spectra of important physical quantities at the length scales where fragmentation occurs. In order to avoid prompt fragmentation at high resolutions we start these simulations with a fully developed gravitoturbulent state obtained at a lower resolution.   Results. We show nearly converged results for fragmentation with a critical cooling timescale $t_{\mathrm{crit}} \sim 10\,\Omega^{-1}$ . We can backtrace this claim by investigating the spectra of relevant physical variables at length scales around and below the pressure scale height. We argue that well behaved results cannot be expected if counteracting quantities are varying too much on these critical length scales, either by change of resolution or numerical method. A comparison of fragmentation behaviour with the related spectra reveals that simulations behave similar, if the spectra are converged to the length scales where self-gravity leads to instabilities. Observable deviations in the results obtained with different numerical setup are confined to scales below these critical length scales. ",1909.08883v1
"['Susanne von Linden', 'Peter L. Biermann', 'Wolfgang J. Duschl', 'Harald Lesch', 'Thomas Schmutzler']",1993-08-06T11:21:06Z,Our Galactic Center: A laboratory for the feeding of AGNs?,"  We demonstrate that our Galactic Center, despite little evidence for the presence of a currently active nucleus, provides insight into the feeding of AGN: The observed velocity field of molecular clouds can be interpreted as tracing out the spiralling inwards of gas in a large accretion flow towards the Galactic Center (Linden et al. 1993, Biermann et al. 1993) in the radial distance range from a few parsec to a few hundred pc. The required effective viscosity corresponds well to the observed turbulent velocities and characteristic length scales. The implied mass influx provides indeed all the material needed to maintain the presently observed star formation rate at distances closer than about $100$ pc. We argue that the energy input from supernova explosions due to the high rate of star formation can feed the turbulence of the interstellar medium. This then keeps the effective viscosity high as required to feed the star formation. We suggest that this process leads to limit cycles in star formation, and as a consequence also to limit cycles in the feeding of any activity at the very center. ",astro-ph/9308009v1
"['Kerstin Weis', 'Wolfgang J. Duschl', 'You-Hua Chu']",1999-07-26T18:47:12Z,The nature of strings in the nebula around Eta Carinae,"  Eta Carinae is one of the most extreme cases of a Luminous Blue Variable star. A bipolar nebula of 17"" size surrounds the central object. Even further out, a large amount of filamentary material extends to a distance of 30"" or about 0.3 pc. In this paper we present a detailed kinematic and morphological analysis of some outer filaments in this nebula which we call strings. All strings are extremly long and narrow structures. We identified 5 strings which have sizes of 0.058 to 0.177 pc in length and a width of only 0.002 pc. Using high-resolution long-slit echelle spectroscopy it was found that the strings follow a Hubble law with velocities increasing towards larger distances from the star. With these unique properties, high collimation and linear increase of the radial velocity the strings represent a newly found phenomena in the structure and evolution of nebulae around LBVs. Finally, we show that morphologically similar strings can be found in the planetary nebula NGC 6543, a possible PN-counterpart to this phenomenon. ",astro-ph/9907361v1
"['Marvin Blank', 'Mark R. Morris', 'Adam Frank', 'Jonathan J. Carroll-Nellenback', 'Wolfgang J. Duschl']",2016-03-31T11:07:59Z,The inner cavity of the circumnuclear disc,"  The circumnuclear disc (CND) orbiting the Galaxy's central black hole is a reservoir of material that can ultimately provide energy through accretion, or form stars in the presence of the black hole, as evidenced by the stellar cluster that is presently located at the CND's centre. In this paper, we report the results of a computational study of the dynamics of the CND. The results lead us to question two paradigms that are prevalent in previous research on the Galactic Centre. The first is that the disc's inner cavity is maintained by the interaction of the central stellar cluster's strong winds with the disc's inner rim, and second, that the presence of unstable clumps in the disc implies that the CND is a transient feature. Our simulations show that, in the absence of a magnetic field, the interaction of the wind with the inner disc rim actually leads to a filling of the inner cavity within a few orbital time-scales, contrary to previous expectations. However, including the effects of magnetic fields stabilizes the inner disc rim against rapid inward migration. Furthermore, this interaction causes instabilities that continuously create clumps that are individually unstable against tidal shearing. Thus the occurrence of such unstable clumps does not necessarily mean that the disc is itself a transient phenomenon. The next steps in this investigation are to explore the effect of the magnetorotational instability on the disc evolution and to test whether the results presented here persist for longer time-scales than those considered here. ",1603.09525v1
"['Dominikus Heinzeller', 'Wolfgang J. Duschl', 'Shin Mineshige']",2009-05-03T08:29:15Z,Turbulent viscosity by convection in accretion discs - a self-consistent   approach,"  The source of viscosity in astrophysical accretion flows is still a hotly debated issue. We investigate the contribution of convective turbulence to the total viscosity in a self-consistent approach, where the strength of convection is determined from the vertical disc structure itself. Additional sources of viscosity are parametrized by a beta-viscosity prescription, which also allows an investigation of self-gravitating effects. In the context of accretion discs around stellar mass and intermediate mass black holes, we conclude that convection alone cannot account for the total viscosity in the disc, but significantly adds to it. For accretion rates up to 10% of the Eddington rate, we find that differential rotation provides a sufficiently large underlying viscosity. For higher accretion rates, further support is needed in the inner disc region, which can be provided by an MRI-induced viscosity. We briefly discuss the interplay of MRI, convection and differential rotation. We conduct a detailed parameter study of the effects of central masses and accretion rates on the disc models and find that the threshold value of the supporting viscosity is determined mostly by the Eddington ratio with only little influence from the central black hole mass. ",0905.0252v1
"['Laird M. Close', 'Francois Wildi', 'Michael Lloyd-Hart', 'Guido Brusa', 'Don Fisher', 'Doug Miller', 'Armando Riccardi', 'Piero Salinari', 'Donald W. McCarthy', 'Roger Angel', 'Rich Allen', 'H. M. Martin', 'Richard G. Sosa', 'Manny Montoya', 'Matt Rademacher', 'Mario Rascon', 'Dylan Curley', 'Nick Siegler', 'Wolfgang J. Duschl']",2003-08-29T22:15:55Z,High Resolution Images of Orbital Motion in the Trapezium Cluster: First   Scientific Results from the MMT Deformable Secondary Mirror Adaptive Optics   System,"  We present the first scientific images obtained with a deformable secondary mirror adaptive optics system. We utilized the 6.5m MMT AO system to produce high-resolution (FWHM=0.07'') near infrared (1.6 um) images of the young (~1 Myr) Orion Trapezium theta 1 Ori cluster members. A combination of high spatial resolution and high signal to noise allowed the positions of these stars to be measured to within ~0.003'' accuracies. Including previous speckle data (Weigelt et al. 1999), we analyze a six year baseline of high-resolution observations of this cluster. Over this baseline we are sensitive to relative proper motions of only ~0.002''/yr (4.2 km/s at 450 pc). At such sensitivities we detect orbital motion in the very tight theta 1 Ori B2B3 (52 AU separation) and theta 1 Ori A1A2 (94 AU separation) systems. Such motions are consistent with those independently observed by Schertl et al. (2003) with speckle interferometry, giving us confidence that these very small (~0.002''/yr) orbital motions are real. All five members of the theta 1 Ori B system appear likely gravitationally bound. The very lowest mass member of the theta 1 Ori B system (B4) has K' ~11.66 and an estimated mass of ~0.2 Msun. There was very little motion (4+/-15 km/s) detected of B4 w.r.t B1 or B2, hence B4 is possibly part of the theta 1 Ori B group. We suspect that if this very low mass member is physically associated it most likely is in an unstable (non-hierarchical) orbital position and will soon be ejected from the group. The theta 1 Ori B system appears to be a good example of a star formation ``mini-cluster'' which may eject the lowest mass members of the cluster in the near future. This ``ejection'' process could play a major role in the formation of low mass stars and brown dwarfs. ",astro-ph/0309003v1
"['Sebastian F. Hoenig', 'Makoto Kishimoto', 'Konrad R. W. Tristram', 'M. Almudena Prieto', 'Poshak Gandhi', 'Daniel Asmus', 'Robert Antonucci', 'Leonard Burtscher', 'Wolfgang J. Duschl', 'Gerd Weigelt']",2013-06-18T20:00:00Z,Dust in the polar region as a major contributor to the IR emission of   AGN,"  (abridged) It is generally assumed that the distribution of dust on parsec scales forms a geometrically- and optically-thick entity in the equatorial plane around the accretion disk and broad-line region - dubbed ""dust torus"" - that emits the bulk of the sub-arcsecond-scale IR emission and gives rise to orientation-dependent obscuration. Here we report detailed interferometry observations of the unobscured (type 1) AGN in NGC 3783 that allow us to constrain the size, elongation, and direction of the mid-IR emission with high accuracy. The mid-IR emission is characterized by a strong elongation toward position angle PA -52 deg, closely aligned with the polar axis (PA -45 deg). We determine half-light radii along the major and minor axes at 12.5 {\mu}m of (4.23 +/- 0.63) pc x (1.42 +/- 0.21) pc, which corresponds to intrinsically-scaled sizes of (69.4 +/- 10.8) rin x (23.3 +/- 3.5) rin for the inner dust radius of rin = 0.061 pc as inferred from near-IR reverberation mapping. This implies an axis ratio of 3:1, with about 60-90% of the 8-13 {\mu}m emission associated with the polar-elongated component. These observations are difficult to reconcile with the standard interpretation that most of the parsec-scale mid-IR emission in AGN originates from the torus and challenges the justification of using simple torus models to model the broad-band IR emission. It is quite likely that the hot-dust emission in NGC 3783 as recently resolved by near-IR interferometry is misaligned with the mid-IR emitting source, which also finds a correspondence in the two distinct 3-5 {\mu}m and 20 {\mu}m bumps seen in the high-angular resolution spectral energy distribution (SED). We conclude that these observations support a scenario where the majority of the mid-IR emission in Seyfert AGN originates from a dusty wind in the polar region of the AGN. ",1306.4312v2
"['Lea Kämmerer', 'Gérald Kämmerer', 'Manuel Gruber', 'Jan Grunwald', 'Tobias Lojewski', 'Laurent Mercadier', 'Loïc Le Guyader', 'Robert Carley', 'Cammille Carinan', 'Natalia Gerasimova', 'David Hickin', 'Benjamin E. Van Kuiken', 'Giuseppe Mercurio', 'Martin Teichmann', 'Senthil Kumar Kuppusamy', 'Andreas Scherz', 'Mario Ruben', 'Klaus Sokolowski-Tinten', 'Andrea Eschenlohr', 'Katharina Ollefs', 'Carolin Schmitz-Antoniak', 'Felix Tuczek', 'Peter Kratzer', 'Uwe Bovensiepen', 'Heiko Wende']",2023-12-03T18:59:20Z,Femtosecond spin-state switching dynamics of spin-crossover molecules   condensed in thin films,"  The photoinduced switching of Fe(II)-based spin-crossover complexes from singlet to quintet takes place at ultrafast time scales. This a priori spin-forbidden transition triggered numerous time-resolved experiments of solvated samples to elucidate the mechanism at play. The involved intermediate states remain uncertain. We apply ultrafast x-ray spectroscopy in molecular films as a method sensitive to spin, electronic, and nuclear degrees of freedom. Combining the progress in molecule synthesis and film growth with the opportunities at x-ray free-electron lasers, we analyze the transient evolution of the Fe L3 fine structure at room temperature. Our measurements and calculations indicate the involvement of an Fe triplet intermediate state. The high-spin state saturates at half of the available molecules, limited by molecule-molecule interaction within the film. ",2312.01483v1
"['Jack D. Evans', 'Bikash Garai', 'Helge Reinsch', 'Weijin Li', 'Stefano Dissegna', 'Volodymyr Bon', 'Irena Senkovska', 'Roland A. Fischer', 'Stefan Kaskel', 'Christoph Janiak', 'Norbert Stock', 'Dirk Volkmer']",2019-04-29T07:16:17Z,Metal-Organic Frameworks in Germany: from Synthesis to Function,"  Metal-organic frameworks (MOFs) are constructed from a combination of inorganic and organic units to produce materials which display high porosity, among other unique and exciting properties. MOFs have shown promise in many wide-ranging applications, such as catalysis and gas separations. In this review, we highlight MOF research conducted by Germany-based research groups. Specifically, we feature approaches for the synthesis of new MOFs, high-throughput MOF production, advanced characterization methods and examples of advanced functions and properties. ",1904.12474v1
"['Kolja Them', 'Frowin Ellermann', 'Andrey N. Pravdivtsev', 'Oleg G. Salnikov', 'Ivan V. Skovpin', 'Igor V. Koptyug', 'Rainer Herges', 'Jan-Bernd Hövener']",2020-12-07T12:17:13Z,Parahydrogen-induced polarization relayed via proton exchange,"  The sensitivity of NMR and MRI can be boosted via hyperpolarization of nuclear spins. However, current methods are costly, polarization is relatively low, or applicability is limited. Here, we report a new hyperpolarization method combining the low-cost, high polarization of hydrogenative parahydrogen-induced polarization (PHIP) with the flexibility of polarization transfer via proton exchange. The new method can be used to polarize various molecules, including alcohols, water, lactate, and pyruvate. On average, only $\approx$3 mM of a hyperpolarized transfer agent was sufficient to significantly enhance the signal of $\approx$100 mM of target molecules via proton exchange. Thus, hydrogenative parahydrogen-induced hyperpolarization with proton exchange (PHIP-X) provides a new avenue for NMR applications beyond the limits imposed by thermal polarization. ",2012.03626v1
"['Andreas B. Schmidt', 'Arne Brahms', 'Frowin Ellermann', 'Stephan Berner', 'Jürgen Hennig', 'Dominik von Elverfeldt', 'Rainer Herges', 'Jan-Bernd Hövener', 'Andrey N. Pravdivtsev']",2021-09-10T11:38:45Z,Selectively pulsed spin order transfer increases parahydrogen-induced   NMR amplification of insensitive nuclei and makes polarization transfer more   robust,"  We describe a new method for pulsed spin order transfer (SOT) of parahydrogen induced polarization (PHIP) that enables close to 100 % polarization in incompletely 2H-labeled molecules by exciting only the desired protons in a frequency-selective manner. While a selective pulse (SP) on 1H at the beginning of pulsed SOT had been considered before, using SPs during the SOT suppresses undesired indirect spin-spin interactions. As a result, we achieved a more robust SOT for the SP variants of the phINEPT+ sequence that we refer to as phSPINEPT+. Thereby, for the first time, we report a sequence that is effective for all weakly coupled spin systems. Our simulations show that the method converts close to 100 % of the parahydrogen-derived spin order into 13C hyperpolarization in weakly coupled three-spin systems and partially or fully 2H-labeled molecules if relaxation is neglected. Experimentally we demonstrate high hyperpolarization of 13C with 15.8 % for 1-13C-hydroxyethyl propionate-d3 and 12.6 % for 1-13C-ethyl acetate-d6, which corresponds to ~47 % and ~38 % if the enrichment of parahydrogen had been 100 %. Even in non-2H-labeled molecules, a remarkable 13C polarization is achieved, e.g. up to 20 % were simulated for 100 % pH2, and 1.25 % were obtained experimentally for 1-13C-ethyl pyruvate and 50 % pH2, which can be further improved by faster hydrogenation. As a result, full deuterium labeling may no longer be required e.g., when new PHIP agents are investigated, the synthesis of fully deuterated molecules is too complex, or when a kinetic isotope effect regarding the metabolic conversion rate of an agent is to be avoided. Using SPs during SOT seems very promising and may be extended to other sequences in the context of PHIP and be-yond to make them less prone to experimental imperfections or real molecular environments. ",2109.04799v1
"['Henrik R. Larsson', 'Jens Riedel', 'Jie Wei', 'Friedrich Temps', 'Bernd Hartke']",2018-02-20T10:35:46Z,"Resonance dynamics of DCO ($\widetilde{X}\,{}^2A'$) simulated with the   dynamically pruned discrete variable representation (DP-DVR)","  Selected resonance states of the deuterated formyl radical in the electronic ground state ($\widetilde{X}\,{}^2A'$) are computed using our recently introduced dynamically pruned discrete variable representation (DP-DVR) [H. R. Larsson, B. Hartke and D. J. Tannor, J. Chem. Phys., 145, 204108 (2016)]. Their decay and asymptotic distributions are analyzed and, for selected resonances, compared to experimental results obtained by a combination of stimulated emission pumping (SEP) and velocity-map imaging of the product D atoms. The theoretical results show good agreement with the experimental kinetic energy distributions. The intramolecular vibrational energy redistribution (IVR) is analyzed and compared with previous results from an effective polyad Hamiltonian. Specifically, we analyzed the part of the wavefunction that remains in the interaction region during the decay. The results from the polyad Hamiltonian could mainly be confirmed. The C=O stretch quantum number is typically conserved, while the D-C=O bend quantum number decreases. Differences are due to strong anharmonic coupling such that all resonances have major contributions from several zero-order states. For some of the resonances, the coupling is so strong that no further zero-order states appear during the dynamics in the interaction region, even after propagating for 300 ps. ",1802.07050v2
"['Johannes M. Dieterich', 'Bernd Hartke']",2012-07-18T09:44:22Z,Empirical review of standard benchmark functions using evolutionary   global optimization,"  We have employed a recent implementation of genetic algorithms to study a range of standard benchmark functions for global optimization. It turns out that some of them are not very useful as challenging test functions, since they neither allow for a discrimination between different variants of genetic operators nor exhibit a dimensionality scaling resembling that of real-world problems, for example that of global structure optimization of atomic and molecular clusters. The latter properties seem to be simulated better by two other types of benchmark functions. One type is designed to be deceptive, exemplified here by Lunacek's function. The other type offers additional advantages of markedly increased complexity and of broad tunability in search space characteristics. For the latter type, we use an implementation based on randomly distributed Gaussians. We advocate the use of the latter types of test functions for algorithm development and benchmarking. ",1207.4318v1
['Bernd Hartke'],1995-05-24T13:32:07Z,Global geometry optimization of clusters using a growth strategy   optimized by a genetic algorithm,"  A new strategy for global geometry optimization of clusters is presented. Important features are a restriction of search space to favorable nearest-neighbor distance ranges, a suitable cluster growth representation with diminished correlations, and easy transferability of the results to larger clusters. The strengths and possible limitations of the method are demonstrated for Si10 using an empirical potential. ",chem-ph/9505002v1
"['Florian Spenke', 'Karsten Balzer', 'Sascha Frick', 'Bernd Hartke', 'Johannes M. Dieterich']",2018-01-22T16:27:09Z,Adaptive parallelism with RMI: Idle high-performance computing resources   can be completely avoided,"  In practice, standard scheduling of parallel computing jobs almost always leaves significant portions of the available hardware unused, even with many jobs still waiting in the queue. The simple reason is that the resource requests of these waiting jobs are fixed and do not match the available, unused resources. However, with alternative but existing and well-established techniques it is possible to achieve a fully automated, adaptive parallelism that does not need pre-set, fixed resources. Here, we demonstrate that such an adaptively parallel program can indeed fill in all such scheduling gaps, even in real-life situations on large supercomputers. ",1801.07184v2
"['Henrik R. Larsson', 'Bernd Hartke', 'David J. Tannor']",2016-06-13T15:53:07Z,Efficient molecular quantum dynamics in coordinate and phase space using   pruned bases,"  We present an efficient implementation of dynamically pruned quantum dynamics, both in coordinate space and in phase space. We combine the ideas behind the biorthogonal von Neumann basis (PvB) with the orthogonalized momentum-symmetrized Gaussians (Weylets) to create a new basis, projected Weylets, that takes the best from both methods. We benchmark pruned dynamics using phase-space-localized PvB, projected Weylets, and coordinate-space-localized DVR bases, with real-world examples in up to six dimensions. We show that coordinate-space localization is most important for efficient pruning and that pruned dynamics is much faster compared to unpruned, exact dynamics. Phase-space localization is useful for more demanding dynamics where many basis functions are required. There, projected Weylets offer a more compact representation than pruned DVR bases. ",1606.04004v2
['Michael Braun'],2013-08-08T12:37:32Z,$q$-Analogs of $t$-Wise Balanced Designs from Borel Subgroups,"  A $t\text{-}(n,K,\lambda;q)$ design, also called the $q$-analog of a $t$-wise balanced design, is a set ${\mathcal B}$ of subspaces with dimensions contained in $K$ of the $n$-dimensional vector space ${\mathbb F}_q^n$ over the finite field with $q$ elements such that each $t$-subspace of ${\mathbb F}_q^n$ is contained in exactly $\lambda$ elements of ${\mathcal B}$. In this paper we give a construction of an infinite series of nontrivial $t\text{-}(n,K,\lambda;q)$ designs with $|K|=2$ for all dimensions $t\ge 1$ and all prime powers $q$ admitting the standard Borel subgroup as group of automorphisms. Furthermore, replacing $q=1$ gives an ordinary $t$-wise balanced design defined on sets. ",1308.1829v1
['Michael Braun'],2018-08-08T10:04:24Z,"New lower bounds on the size of (n,r)-arcs in PG(2,q)","  An (n,r)-arc in PG(2,q) is a set of n points such that each line contains at most r of the selected points. It is well-known that (n,r)-arcs in PG(2,q) correspond to projective linear codes. Let m_r(2,q) denote the maximal number n of points for which an (n,r)-arc in PG(2,q) exists. In this paper we obtain improved lower bounds on m_r(2,q) by explicitly constructing (n,r)-arcs. Some of the constructed (n,r)-arcs correspond to linear codes meeting the Griesmer bound. All results are obtained by integer linear programming. ",1808.02702v2
['Michael Braun'],2021-06-10T16:56:06Z,"Update: Some new results on lower bounds on $(n,r)$-arcs in $PG(2,q)$   for $q\le 31$","  An $(n,r)$-arc in $PG(2,q)$ is a set $B$ of points in $PG(2,q)$ such that each line in $PG(2,q)$ contains at most $r$ elements of $B$ and such that there is at least one line containing exactly $r$ elements of $B$. The value $m_r(2,q)$ denotes the maximal number $n$ of points in the projective geometry $PG(2,q)$ for which an $(n,r)$-arc exists. By explicitly constructing $(n,r)$-arcs using prescribed automorphisms and integer linear programming we obtain some improved lower bounds for $m_r(2,q)$: $m_{10}(2,16)\ge 144$, $m_3(2,25)\ge 39$, $m_{18}(2,25)\ge 418$, $m_9(2,27)\ge 201$, $m_{14}(2,29)\ge 364$, $m_{25}(2,29)\ge 697$, $m_{25}(2,31)\ge 734$. Furthermore, we show by systematically excluding possible automorphisms that putative $(44,5)$-arcs, $(90,9)$-arcs in $PG(2,11)$, and $(39,4)$-arcs in $PG(2,13)$ -- in case of their existence -- are rigid, i.e. they all would only admit the trivial automorphism group of order $1$. In addition, putative $(50,5)$-arcs, $(65,6)$-arcs, $(119,10)$-arcs, $(133,11)$-arcs, and $(146,12)$-arcs in $PG(2,13)$ would be rigid or would admit a unique automorphism group (up to conjugation) of order $2$. ",2106.05908v1
"['Michael Braun', 'Alfred Wassermann']",2012-11-06T17:56:12Z,q-Steiner Systems Do Exist,"  In this paper we give the first construction of a q-analog of a Steiner system. Using a computer search we found at least 26 q-Steiner Systems S_2[2,3,13] admitting the normalizer of a singer cycle as a group of automorphisms. ",1211.2758v1
"['Michael Braun', 'Paul Damien']",2014-01-31T17:35:09Z,Scalable Rejection Sampling for Bayesian Hierarchical Models,"  Bayesian hierarchical modeling is a popular approach to capturing unobserved heterogeneity across individual units. However, standard estimation methods such as Markov chain Monte Carlo (MCMC) can be impracticable for modeling outcomes from a large number of units. We develop a new method to sample from posterior distributions of Bayesian models, without using MCMC. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. The algorithm is scalable under the weak assumption that individual units are conditionally independent, making it applicable for large datasets. It can also be used to compute marginal likelihoods. ",1401.8236v3
"['Michael Braun', 'Paul Damien']",2011-08-10T18:54:53Z,Generalized Direct Sampling for Hierarchical Bayesian Models,"  We develop a new method to sample from posterior distributions in hierarchical models without using Markov chain Monte Carlo. This method, which is a variant of importance sampling ideas, is generally applicable to high-dimensional models involving large data sets. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. Additionally, the method can be used to compute marginal likelihoods. ",1108.2245v3
"['Michael Braun', 'André Bonfrer']",2010-12-21T19:18:49Z,Scalable Inference of Customer Similarities from Interactions Data using   Dirichlet Processes,"  Under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. However, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. In this paper we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily, and is flexible enough to account for random variation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. We find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities. ",1012.4769v1
"['Michael Braun', 'Jan Reichelt']",2012-12-19T10:12:06Z,q-Analogs of Packing Designs,"  A $P_q(t,k,n)$ $q$-packing design is a selection of $k$-subspaces of $\F_q^n$ such that each $t$-subspace is contained in at most one element of the collection. A successful approach adopted from the Kramer-Mesner-method of prescribing a group of automorphisms was applied by Kohnert and Kurz to construct some constant dimension codes with moderate parameters which arise by $q$-packing designs. In this paper we recall this approach and give a version of the Kramer-Mesner-method breaking the condition that the whole $q$-packing design must admit the prescribed group of automorphisms. Afterwards, we describe the basic idea of an algorithm to tackle the integer linear optimization problems representing the $q$-packing design construction by means of a metaheuristic approach. Finally, we give some improvements on the size of $P_2(2,3,n)$ $q$-packing designs. ",1212.4614v1
"['Michael Braun', 'Axel Kohnert', 'Patric Östergård', 'Alfred Wassermann']",2013-05-07T10:04:17Z,Large Sets of $t$-Designs over Finite Fields,"  A $t\text{-}(n,k,\lambda;q)$-design is a set of $k$-subspaces, called blocks, of an $n$-dimensional vector space $V$ over the finite field with $q$ elements such that each $t$-subspace is contained in exactly $\lambda$ blocks. A partition of the complete set of $k$-subspaces of $V$ into disjoint $t\text{-}(n,k,\lambda;q)$ designs is called a large set of $t$-designs over finite fields. In this paper we give the first nontrivial construction of such a large set with $t\ge2$. ",1305.1455v1
"['Michael Braun', 'Michael Kiermaier', 'Anamari Nakić']",2015-01-30T14:43:59Z,On the automorphism group of a binary $q$-analog of the Fano plane,"  The smallest set of admissible parameters of a $q$-analog of a Steiner system is $S_2[2,3,7]$. The existence of such a Steiner system -- known as a binary $q$-analog of the Fano plane -- is still open. In this article, the automorphism group of a putative binary $q$-analog of the Fano plane is investigated by a combination of theoretical and computational methods. As a conclusion, it is either rigid or its automorphism group is cyclic of order $2$, $3$ or $4$. Up to conjugacy in $\operatorname{GL}(7,2)$, there remains a single possible group of order $2$ and $4$, respectively, and two possible groups of order $3$. For the automorphisms of order $2$, we give a more general result which is valid for any binary $q$-Steiner triple system. ",1501.07790v2
"['Michael Braun', 'Dean Crnković', 'Maarten De Boeck', 'Vedrana Mikulić Crnković', 'Andrea Švob']",2021-05-30T13:04:30Z,$q$-Analogs of strongly regular graphs,"  We introduce the notion of q-analogs of strongly regular graphs and give several examples of such structures. We prove a necessary condition on the parameters, show the connection to designs over finite fields, and present a classification. ",2105.14525v2
"['Michael Braun', 'Florian Weber', 'Florian Alt']",2020-03-30T18:26:01Z,Affective Automotive User Interfaces -- Reviewing the State of Emotion   Regulation in the Car,"  Affective technology offers exciting opportunities to improve road safety by catering to human emotions. Modern car interiors enable the contactless detection of user states, paving the way for a systematic promotion of safe driver behavior through emotion regulation. We review the current literature regarding the impact of emotions on driver behavior and analyze the state of emotion regulation approaches in the car. We summarize challenges for affective interaction in form of cultural aspects, technological hurdles and methodological considerations, as well as opportunities to improve road safety by reinstating drivers into an emotionally balanced state. The purpose of this review is to outline the community's combined knowledge for interested researchers, to provide a focussed introduction for practitioners and to identify future directions for affective interaction in the car. ",2003.13731v2
"['Michael Braun', 'Jon McAuliffe']",2007-12-15T16:16:18Z,Variational inference for large-scale models of discrete choice,"  Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model. ",0712.2526v3
"['Michael Braun', 'Michael Kiermaier', 'Axel Kohnert', 'Reinhard Laue']",2014-11-26T11:10:50Z,Large sets of subspace designs,"  In this article, three types of joins are introduced for subspaces of a vector space. Decompositions of the Gra{\ss}mannian into joins are discussed. This framework admits a generalization of large set recursion methods for block designs to subspace designs.   We construct a $2$-$(6,3,78)_5$ design by computer, which corresponds to a halving $\operatorname{LS}_5[2](2,3,6)$. The application of the new recursion method to this halving and an already known $\operatorname{LS}_3[2](2,3,6)$ yields two infinite two-parameter series of halvings $\operatorname{LS}_3[2](2,k,v)$ and $\operatorname{LS}_5[2](2,k,v)$ with integers $v\geq 6$, $v\equiv 2\mod 4$ and $3\leq k\leq v-3$, $k\equiv 3\mod 4$.   Thus in particular, two new infinite series of nontrivial subspace designs with $t = 2$ are constructed. Furthermore as a corollary, we get the existence of infinitely many nontrivial large sets of subspace designs with $t = 2$. ",1411.7181v2
"['Anna-Lena Trautmann', 'Felice Manganiello', 'Michael Braun', 'Joachim Rosenthal']",2011-12-06T11:30:21Z,Cyclic Orbit Codes,"  In network coding a constant dimension code consists of a set of k-dimensional subspaces of F_q^n. Orbit codes are constant dimension codes which are defined as orbits of a subgroup of the general linear group, acting on the set of all subspaces of F_q^n. If the acting group is cyclic, the corresponding orbit codes are called cyclic orbit codes. In this paper we give a classification of cyclic orbit codes and propose a decoding procedure for a particular subclass of cyclic orbit codes. ",1112.1238v1
"['Michael Braun', 'Jingyi Li', 'Florian Weber', 'Bastian Pfleging', 'Andreas Butz', 'Florian Alt']",2020-04-06T08:33:16Z,What If Your Car Would Care? Exploring Use Cases For Affective   Automotive User Interfaces,"  In this paper we present use cases for affective user interfaces (UIs) in cars and how they are perceived by potential users in China and Germany. Emotion-aware interaction is enabled by the improvement of ubiquitous sensing methods and provides potential benefits for both traffic safety and personal well-being. To promote the adoption of affective interaction at an international scale, we developed 20 mobile in-car use cases through an inter-cultural design approach and evaluated them with 65 drivers in Germany and China. Our data shows perceived benefits in specific areas of pragmatic quality as well as cultural differences, especially for socially interactive use cases. We also discuss general implications for future affective automotive UI. Our results provide a perspective on cultural peculiarities and a concrete starting point for practitioners and researchers working on emotion-aware interfaces. ",2004.02481v1
"['Michael Braun', 'Steffen J. Glaser']",2014-04-19T10:24:04Z,Cooperative pulses in robust quantum control: Application to broadband   Ramsey-type pulse sequence elements,"  A general approach is introduced for the efficient simultaneous optimization of pulses that compensate each other' s imperfections within the same scan. This is applied to broadband Ramsey-type experiments, resulting in pulses with significantly shorter duration compared to individually optimized broadband pulses. The advantage of the cooperative pulse approach is demonstrated experimentally for the case of two-dimensional nuclear Overhauser enhancement spectroscopy. In addition to the general approach, a symmetry-adapted analysis of the optimization of Ramsey sequences is presented. Furthermore, the numerical results led to the disovery of a powerful class of pulses with a special symmetry property, which results in excellent performance in Ramsey-type experiments. A significantly different scaling of pulse sequence performance as a function of pulse duration is found for characteristic pulse families, which is explained in terms of the different numbers of available degrees of freedom in the offset dependence of the associated Euler angles. ",1404.4943v1
"['Michael Braun', 'Tuvi Etzion', 'Alexander Vardy']",2011-03-16T07:48:11Z,Linearity and Complements in Projective Space,"  The projective space of order $n$ over the finite field $\Fq$, denoted here as $\Ps$, is the set of all subspaces of the vector space $\Fqn$. The projective space can be endowed with distance function $d_S(X,Y) = \dim(X) + \dim(Y) - 2\dim(X\cap Y)$ which turns $\Ps$ into a metric space. With this, \emph{an $(n,M,d)$ code $\C$ in projective space} is a subset of $\Ps$ of size $M$ such that the distance between any two codewords (subspaces) is at least $d$. Koetter and Kschischang recently showed that codes in projective space are precisely what is needed for error-correction in networks: an $(n,M,d)$ code can correct $t$ packet errors and $\rho$ packet erasures introduced (adversarially) anywhere in the network as long as $2t + 2\rho < d$. This motivates new interest in such codes.   In this paper, we examine the two fundamental concepts of \myemph{complements} and \myemph{linear codes} in the context of $\Ps$. These turn out to be considerably more involved than their classical counterparts. These concepts are examined from two different points of view, coding theory and lattice theory. Our discussion reveals some surprised phenomena of these concepts in $\Ps$ and leaves some interesting problems for further research. ",1103.3117v1
"['Michael Braun', 'Tuvi Etzion', 'Patric Ostergard', 'Alexander Vardy', 'Alfred Wassermann']",2013-04-04T18:47:22Z,Existence of $q$-Analogs of Steiner Systems,"  Let $\F_q^n$ be a vector space of dimension $n$ over the finite field $\F_q$. A $q$-analog of a Steiner system (briefly, a $q$-Steiner system), denoted $S_q[t,k,n]$, is a set $S$ of $k$-dimensional subspaces of $\F_q^n$ such that each $t$-dimensional subspace of $\F_q^n$ is contained in exactly one element of $S$. Presently, $q$-Steiner systems are known only for $t=1$, and in the trivial cases $t = k$ and $k = n$. Invthis paper, the first nontrivial $q$-Steiner systems with $t >= 2$ are constructed. Specifically, several nonisomorphic $q$-Steiner systems $S_2[2,3,13]$ are found by requiring that their automorphism groups contain the normalizer of a Singer subgroup of $\GL(13,2)$. This approach leads to an instance of the exact cover problem, which turns out to have many solutions. ",1304.1462v2
"['Abu Md Niamul Taufique', 'Andreas Savakis', 'Michael Braun', 'Daniel Kubacki', 'Ethan Dell', 'Lei Qian', ""Sean M. O'Rourke""]",2021-04-08T04:58:34Z,SiamReID: Confuser Aware Siamese Tracker with Re-identification Feature,"  Siamese deep-network trackers have received significant attention in recent years due to their real-time speed and state-of-the-art performance. However, Siamese trackers suffer from similar looking confusers, that are prevalent in aerial imagery and create challenging conditions due to prolonged occlusions where the tracker object re-appears under different pose and illumination. Our work proposes SiamReID, a novel re-identification framework for Siamese trackers, that incorporates confuser rejection during prolonged occlusions and is well-suited for aerial tracking. The re-identification feature is trained using both triplet loss and a class balanced loss. Our approach achieves state-of-the-art performance in the UAVDT single object tracking benchmark. ",2104.03510v3
"['Timo Baumgärtner', 'Benjamin J. Mittmann', 'Till Malzacher', 'Johannes Roßkopf', 'Michael Braun', 'Bernd Schmitz', 'Alfred M. Franz']",2023-05-23T11:23:38Z,Towards clinical translation of deep-learning based classification of   DSA image sequences for stroke treatment,"  In the event of stroke, a catheter-guided procedure (thrombectomy) is used to remove blood clots. Feasibility of machine learning based automatic classifications for thrombus detection on digital substraction angiography (DSA) sequences has been demonstrated. It was however not used live in the clinic, yet. We present an open-source tool for automatic thrombus classification and test it on three selected clinical cases regarding functionality and classification runtime. With our trained model all large vessel occlusions in the M1 segment were correctly classified. One small remaining M3 thrombus was not detected. Runtime was in the range from 1 to 10 seconds depending on the used hardware. We conclude that our open-source software tool enables clinical staff to classify DSA sequences in (close to) realtime and can be used for further studies in clinics. ",2306.06207v1
"['Michael Braun', 'Jannik Castenow', 'Friedhelm Meyer auf der Heide']",2020-05-13T10:55:33Z,Local Gathering of Mobile Robots in Three Dimensions,"  In this work, we initiate the research about the Gathering problem for robots with limited viewing range in the three-dimensional Euclidean space. In the Gathering problem, a set of initially scattered robots is required to gather at the same position. The robots' capabilities are very restricted -- they do not agree on any coordinate system or compass, have a limited viewing range, have no memory of the past and cannot communicate. We study the problem in two different time models, in FSYNC (fully synchronized discrete rounds) and the continuous time model. For FSYNC, we introduce the 3D-Go-To-The-Center-strategy and prove a runtime of $\Theta(n^2)$ that matches the currently best runtime bound for the same model in the Euclidean plane [SPAA'11]. Our main result is the generalization of contracting strategies (continuous time) from [Algosensors'17] to three dimensions. In contracting strategies, every robot that is located on the global convex hull of all robots' positions moves with full speed towards the inside of the convex hull. We prove a runtime bound of $O(\Delta \cdot n^{3/2})$ for any three-dimensional contracting strategy, where $\Delta$ denotes the diameter of the initial configuration. This comes up to a factor of $\sqrt{n}$ close to the lower bound of $\Omega (\Delta \cdot n)$ which is already true in two dimensions. In general, it might be hard for robots with limited viewing range to decide whether they are located on the global convex hull and which movement maintains the connectivity of the swarm, rendering the design of concrete contracting strategies a challenging task. We prove that the continuous variant of 3D-Go-To-The-Center is contracting and keeps the swarm connected. Moreover, we give a simple design criterion for three-dimensional contracting strategies that maintains the connectivity of the swarm and introduce an exemplary strategy based on this criterion. ",2005.07495v1
"['Thomas E. Skinner', 'Michael Braun', 'Klaus Woelk', 'Naum I. Gershenzon', 'Steffen J. Glaser']",2010-11-29T15:24:49Z,Design and application of robust rf pulses for toroid cavity NMR   spectroscopy,"  We present robust radio frequency (rf) pulses that tolerate a factor of six inhomogeneity in the B1 field, significantly enhancing the potential of toroid cavity resonators for NMR spectroscopic applications. Both point-to-point (PP) and unitary rotation (UR) pulses were optimized for excitation, inversion, and refocusing using the gradient ascent pulse engineering (GRAPE) algorithm based on optimal control theory. In addition, the optimized parameterization (OP) algorithm applied to the adiabatic BIR-4 UR pulse scheme enabled ultra-short (50 microsec) pulses with acceptable performance compared to standard implementations. OP also discovered a new class of non-adiabatic pulse shapes with improved performance within the BIR-4 framework. However, none of the OP-BIR4 pulses are competitive with the more generally optimized UR pulses. The advantages of the new pulses are demonstrated in simulations and experiments. In particular, the DQF COSY result presented here represents the first implementation of 2D NMR spectroscopy using a toroid probe. ",1011.6258v1
['Christian Winter'],2021-04-23T19:08:44Z,Lower Bound on the Size-Ramsey Number of Tight Paths,"  The size-Ramsey number $R^{(k)}(H)$ of a $k$-uniform hypergraph $H$ is the minimum number of edges in a $k$-uniform hypergraph $G$ with the property that every `$2$-edge coloring' of $G$ contains a monochromatic copy of $H$. For $k\ge2$ and $n\in\mathbb{N}$, a $k$-uniform tight path on $n$ vertices $P^{(k)}_{n}$ is defined as a $k$-uniform hypergraph on $n$ vertices for which there is an ordering of its vertices such that the edges are all sets of $k$ consecutive vertices with respect to this order. We prove a lower bound on the size-Ramsey number of $k$-uniform tight paths, which is, considered assymptotically in both the uniformity $k$ and the number of vertices $n$, $R^{(k)}(P^{(k)}_{n})= \Omega\big(\log (k)n\big)$. ",2104.11788v2
['Christian Winter'],2022-04-06T18:00:17Z,"Poset Ramsey number $R(P,Q_n)$. I. Complete multipartite posets","  A poset $(P',\le_{P'})$ contains a copy of some other poset $(P,\le_P)$ if there is an injection $f\colon P'\to P$ where for every $X,Y\in P$, $X\le_P Y$ if and only if $f(X)\le_{P'} f(Y)$. For any posets $P$ and $Q$, the poset Ramsey number $R(P,Q)$ is the smallest integer $N$ such that any blue/red coloring of a Boolean lattice of dimension $N$ contains either a copy of $P$ with all elements blue or a copy of $Q$ with all elements red. We denote by $K_{t_1,\dots,t_\ell}$ a complete $\ell$-partite poset, i.e.\ a poset consisting of $\ell$ pairwise disjoint sets $A^i$ of size $t_i$, $1\le i\le \ell$, such that for any $i,j\in\{1,\dots,\ell\}$ and any two $X\in A^{i}$ and $Y\in A^{j}$, $X<Y$ if and only if $i<j$. In this paper we show that $R(K_{t_1,\dots,t_\ell},Q_n)\le n+\frac{(2+o_n(1))\ell n}{\log n}$. ",2204.03010v1
['Christian Winter'],2022-05-04T18:23:03Z,"Poset Ramsey Number $R(P,Q_n)$. II. Antichains","  For two posets $(P,\le_P)$ and $(P',\le_{P'})$, we say that $P'$ contains a copy of $P$ if there exists an injective function $f\colon P'\to P$ such that for every two $X,Y\in P$, $X\le_P Y$ if and only if $f(X)\le_{P'} f(Y)$. Given two posets $P$ and $Q$, let the poset Ramsey number $R(P,Q)$ be the smallest integer $N$ such that any coloring of the elements of an $N$-dimensional Boolean lattice in blue or red contains either a copy of $P$ where all elements are blue or a copy of $Q$ where all elements are red. We determine the poset Ramsey number $R(A_t,Q_n)$ of an antichain versus a Boolean lattice for small $t$ by showing that $R(A_t,Q_n)=n+3$ for $3\le t\le \log \log n$. ",2205.02275v2
['Christian Winter'],2023-10-04T07:23:25Z,Erdős-Hajnal problems for posets,"  We say that a poset $(Q,\le_{Q})$ contains an induced copy of a poset $(P,\le_P)$, if there is an injective function $\phi\colon P\to Q$ such that for every two $X,Y\in P$, $X\le_P Y$ if and only if $\phi(X)\le_Q \phi(Y)$. Given a fixed $2$-coloring $c$ of a poset $P$, the poset Erd\H{o}s-Hajnal number of this colored poset is the minimum integer $N$ such that every $2$-coloring of the Boolean lattice $Q_N$ contains an induced copy of $P$ colored as in $c$, or a monochromatic induced copy of $Q_n$. We present bounds on the poset Erd\H{o}s-Hajnal number of general colored posets, antichains, chains, and small Boolean lattices.   Let the poset Ramsey number $R(Q_n,Q_n)$ be the minimal $N$ such that every $2$-coloring of $Q_N$ contains a monochromatic induced copy of $Q_n$. As a corollary, we show that $R(Q_n,Q_n)> 2.24n$, improving on the best known lower bound $2n+1$ by Cox and Stolee. ",2310.02621v1
['Christian Winter'],2023-03-08T09:26:46Z,"Poset Ramsey number $R(P,Q_n)$. III. Chain Compositions and Antichains","  An induced subposet $(P_2,\le_2)$ of a poset $(P_1,\le_1)$ is a subset of $P_1$ such that for every two $X,Y\in P_2$, $X\le_2 Y$ if and only if $X\le_1 Y$. The Boolean lattice $Q_n$ of dimension $n$ is the poset consisting of all subsets of $\{1,\dots,n\}$ ordered by inclusion. Given two posets $P_1$ and $P_2$ the poset Ramsey number $R(P_1,P_2)$ is the smallest integer $N$ such that in any blue/red coloring of the elements of $Q_N$ there is either a monochromatically blue induced subposet isomorphic to $P_1$ or a monochromatically red induced subposet isomorphic to $P_2$.   We provide upper bounds on $R(P,Q_n)$ for two classes of $P$: parallel compositions of chains, i.e.\ posets consisting of disjoint chains which are pairwise element-wise incomparable, as well as subdivided $Q_2$, which are posets obtained from two parallel chains by adding a common minimal and a common maximal element. This completes the determination of $R(P,Q_n)$ for posets $P$ with at most $4$ elements. If $P$ is an antichain $A_t$ on $t$ elements, we show that $R(A_t,Q_n)=n+3$ for $3\le t\le \log \log n$. Additionally, we briefly survey proof techniques in the poset Ramsey setting $P$ versus $Q_n$. ",2303.04462v2
"['Dominik Bohnert', 'Christian Winter']",2023-05-02T19:29:23Z,A note on asymmetric hypergraphs,"  A $k$-graph $\mathcal{G}$ is asymmetric if there does not exist an automorphism on $\mathcal{G}$ other than the identity, and $\mathcal{G}$ is called minimal asymmetric if it is asymmetric but every non-trivial induced sub-hypergraph of $\mathcal{G}$ is non-asymmetric. Extending a result of Jiang and Ne\v{s}et\v{r}il, we show that for every $k$-graph, $k\ge3$, there exist infinitely many minimal asymmetric $k$-graphs which have maximum degree $2$ and are linear. Further, we show that there are infinitely many $2$-regular asymmetric $k$-graphs for $k\ge3$. ",2305.01748v1
"['Maria Axenovich', 'Christian Winter']",2021-10-14T18:13:59Z,Poset Ramsey numbers: large Boolean lattice versus a fixed poset,"  Given partially ordered sets (posets) $(P, \leq_P)$ and $(P', \leq_{P'})$, we say that $P'$ contains a copy of $P$ if for some injective function $f: P\rightarrow P'$ and for any $X, Y\in P$, $X\leq _P Y$ if and only of $f(X)\leq_{P'} f(Y)$. For any posets $P$ and $Q$, the poset Ramsey number $R(P,Q)$ is the least positive integer $N$ such that no matter how the elements of an $N$-dimensional Boolean lattice are colored in blue and red, there is either a copy of $P$ with all blue elements or a copy of $Q$ with all red elements. We focus on a poset Ramsey number $R(P, Q_n)$ for a fixed poset $P$ and an $n$-dimensional Boolean lattice $Q_n$, as $n$ grows large. We show a sharp jump in behaviour of this number as a function of $n$ depending on whether or not $P$ contains a copy of either a poset $V$, i.e. a poset on elements $A, B, C$ such that $B>C$, $A>C$, and $A$ and $B$ incomparable, or a poset $\Lambda$, its symmetric counterpart. Specifically, we prove that if $P$ contains a copy of $V$ or $\Lambda$ then $R(P, Q_n) \geq n +\frac{1}{15} \frac{n}{\log n}$. Otherwise $R(P, Q_n) \leq n + c(P)$ for a constant $c(P)$. This gives the first non-marginal improvement of a lower bound on poset Ramsey numbers and as a consequence gives $R(Q_2, Q_n) = n + \Theta (\frac{n}{\log n})$. ",2110.07648v1
"['Maria Axenovich', 'Christian Winter']",2022-11-04T13:28:02Z,"Poset Ramsey number $R(P,Q_n)$. II. N-shaped poset","  Given partially ordered sets (posets) $(P, \leq_P)$ and $(P', \leq_{P'})$, we say that $P'$ contains a copy of $P$ if for some injective function $f\colon P\rightarrow P'$ and for any $A, B\in P$, $A\leq _P B$ if and only if $f(A)\leq_{P'} f(B)$. For any posets $P$ and $Q$, the poset Ramsey number $R(P,Q)$ is the least positive integer $N$ such that no matter how the elements of an $N$-dimensional Boolean lattice are colored in blue and red, there is either a copy of $P$ with all blue elements or a copy of $Q$ with all red elements.   We focus on the poset Ramsey number $R(P, Q_n)$ for a fixed poset $P$ and an $n$-dimensional Boolean lattice $Q_n$, as $n$ grows large. It is known that $n+c_1(P) \leq R(P,Q_n) \leq c_2(P) n$, for positive constants $c_1$ and $c_2$. However, there is no poset $P$ known, for which $R(P, Q_n)> (1+\epsilon)n$, for $\epsilon >0$. This paper is devoted to a new method for finding upper bounds on $R(P, Q_n)$ using a duality between copies of $Q_n$ and sets of elements that cover them, referred to as blockers. We prove several properties of blockers and their direct relation to the Ramsey numbers. Using these properties we show that $R(\mathcal{N},Q_n)=n+\Theta(n/\log n)$, for a poset $\mathcal{N}$ with four elements $A, B, C, $ and $D$, such that $A<C$, $B<D$, $B<C$, and the remaining pairs of elements are incomparable. ",2211.02440v2
"['Maria Axenovich', 'Christian Winter']",2024-02-20T23:27:56Z,Diagonal poset Ramsey numbers,"  A poset $(Q,\le_Q)$ contains an induced copy of a poset $(P,\le_P)$ if there exists an injective mapping $\phi\colon P\to Q$ such that for any two elements $X,Y\in P$, $X\le_P Y$ if and only if $\phi(X)\le_Q \phi(Y)$. By $Q_n$ we denote the Boolean lattice $(2^{[n]},\subseteq)$. The poset Ramsey number $R(P,Q)$ for posets $P$ and $Q$ is the least integer $N$ for which any coloring of the elements of $Q_N$ in blue and red contains either a blue induced copy of $P$ or a red induced copy of $Q$. In this paper, we show that $R(Q_m,Q_n)\le nm-\big(1-o(1)\big)n\log m$ where $n\ge m$ and $m$ is sufficiently large. This improves the best known upper bound on $R(Q_n,Q_n)$ from $n^2-n+2$ to $n^2-\big(1-o(1)\big) n\log n$. Furthermore, we determine $R(P,P)$ where $P$ is an $n$-fork or $n$-diamond up to an additive constant of $2$.   A poset $(Q,\le_Q)$ contains a weak copy of $(P,\le_P)$ if there is an injection $\psi\colon P\to Q$ such that $\psi(X)\le_Q \psi(Y)$ for any $X,Y\in P$ with $X\le_P Y$. The weak poset Ramsey number $R^{\text{w}}(P,Q)$ is the smallest $N$ for which any blue/red-coloring of $Q_N$ contains a blue weak copy of $P$ or a red weak copy of $Q$. We show that $R^{\text{w}}(Q_n,Q_n)\le 0.96n^2$. ",2402.13423v1
"['Maria Axenovich', 'Christian Winter']",2018-10-30T13:49:08Z,A note on saturation for Berge-G hypergraphs,"  For a graph G, a hypergraph H is called Berge-G if there is a hypergraph H', isomorphic to H, containing all vertices of G, so that e is contained in f(e) for each edge e of G, where f is a bijection between E(G) and E(H'). The set of all Berge-G hypergraphs is denoted B(G). A hypergraph H is called Berge-G saturated if it does not contain any subhypergraph from B(G), but adding any new hyperedge of size at least 2 to H creates such a subhypergraph. Each Berge-G saturated hypergraph has at least |E(G)|-1 hyperedges. We show that for each graph G that is not a certain star and for any n at least |V(G)|, there is a Berge-G saturated hypergraph on n vertices and exactly |E(G)|-1 hyperedges. This solves a problem of finding a saturated hypergraph on n vertices with the smallest number of edges exactly. ",1810.12734v1
"['Oren Halvani', 'Christian Winter', 'Lukas Graner']",2017-06-01T22:48:49Z,Authorship Verification based on Compression-Models,"  Compression models represent an interesting approach for different classification tasks and have been used widely across many research fields. We adapt compression models to the field of authorship verification (AV), a branch of digital text forensics. The task in AV is to verify if a questioned document and a reference document of a known author are written by the same person. We propose an intrinsic AV method, which yields competitive results compared to a number of current state-of-the-art approaches, based on support vector machines or neural networks. However, in contrast to these approaches our method does not make use of machine learning algorithms, natural language processing techniques, feature engineering, hyperparameter optimization or external documents (a common strategy to transform AV from a one-class to a multi-class classification problem). Instead, the only three key components of our method are a compressing algorithm, a dissimilarity measure and a threshold, needed to accept or reject the authorship of the questioned document. Due to its compactness, our method performs very fast and can be reimplemented with minimal effort. In addition, the method can handle complicated AV cases where both, the questioned and the reference document, are not related to each other in terms of topic or genre. We evaluated our approach against publicly available datasets, which were used in three international AV competitions. Furthermore, we constructed our own corpora, where we evaluated our method against state-of-the-art approaches and achieved, in both cases, promising results. ",1706.00516v1
"['Oren Halvani', 'Christian Winter', 'Lukas Graner']",2018-12-31T16:04:16Z,Unary and Binary Classification Approaches and their Implications for   Authorship Verification,"  Retrieving indexed documents, not by their topical content but their writing style opens the door for a number of applications in information retrieval (IR). One application is to retrieve textual content of a certain author X, where the queried IR system is provided beforehand with a set of reference texts of X. Authorship verification (AV), which is a research subject in the field of digital text forensics, is suitable for this purpose. The task of AV is to determine if two documents (i.e. an indexed and a reference document) have been written by the same author X. Even though AV represents a unary classification problem, a number of existing approaches consider it as a binary classification task. However, the underlying classification model of an AV method has a number of serious implications regarding its prerequisites, evaluability, and applicability. In our comprehensive literature review, we observed several misunderstandings regarding the differentiation of unary and binary AV approaches that require consideration. The objective of this paper is, therefore, to clarify these by proposing clear criteria and new properties that aim to improve the characterization of existing and future AV approaches. Given both, we investigate the applicability of eleven existing unary and binary AV methods as well as four generic unary classification algorithms on two self-compiled corpora. Furthermore, we highlight an important issue concerning the evaluation of AV methods based on fixed decision criterions, which has not been paid attention in previous AV studies. ",1901.00399v1
"['Oren Halvani', 'Christian Winter', 'Lukas Graner']",2019-06-24T09:44:46Z,Assessing the Applicability of Authorship Verification Methods,"  Authorship verification (AV) is a research subject in the field of digital text forensics that concerns itself with the question, whether two documents have been written by the same person. During the past two decades, an increasing number of proposed AV approaches can be observed. However, a closer look at the respective studies reveals that the underlying characteristics of these methods are rarely addressed, which raises doubts regarding their applicability in real forensic settings. The objective of this paper is to fill this gap by proposing clear criteria and properties that aim to improve the characterization of existing and future AV approaches. Based on these properties, we conduct three experiments using 12 existing AV approaches, including the current state of the art. The examined methods were trained, optimized and evaluated on three self-compiled corpora, where each corpus focuses on a different aspect of applicability. Our results indicate that part of the methods are able to cope with very challenging verification cases such as 250 characters long informal chat conversations (72.7% accuracy) or cases in which two scientific documents were written at different times with an average difference of 15.6 years (> 75% accuracy). However, we also identified that all involved methods are prone to cross-topic verification cases. ",1906.10551v1
"['Maria Axenovich', 'Ryan R. Martin', 'Christian Winter']",2023-03-27T18:11:16Z,On graphs embeddable in a layer of a hypercube and their extremal   numbers,"  A graph is cubical if it is a subgraph of a hypercube. For a cubical graph $H$ and a hypercube $Q_n$, $ex(Q_n, H)$ is the largest number of edges in an $H$-free subgraph of $Q_n$. If $ex(Q_n, H)$ is equal to a positive proportion of the number of edges in $Q_n$, $H$ is said to have positive Tur\'an density in a hypercube; otherwise it has zero Tur\'an density. Determining $ex(Q_n, H)$ and even identifying whether $H$ has positive or zero Tur\'an density remains a widely open question for general $H$.   In this paper we focus on layered graphs, i.e., graphs that are contained in an edge-layer of some hypercube. Graphs $H$ that are not layered have positive Tur\'an density because one can form an $H$-free subgraph of $Q_n$ consisting of edges of every other layer. For example, a $4$-cycle is not layered and has positive Tur\'an density.   However, in general it is not obvious what properties layered graphs have. We give a characterisation of layered graphs in terms of edge-colorings. We show that most non-trivial subdivisions have zero Tur\'an density, extending known results on zero Tur\'an density of even cycles of length at least $12$ and of length $8$. However, we prove that there are cubical graphs of girth $8$ that are not layered and thus having positive Tur\'an density. The cycle of length $10$ remains the only cycle for which it is not known whether its Tur\'an density is positive or not. We prove that $ex(Q_n, C_{10})= \Omega(n2^n/ \log^a n)$, for a constant $a$, showing that the extremal number for a $10$-cycle behaves differently from any other cycle of zero Tur\'an density. ",2303.15529v3
"['Orencio Duran Vinent', 'Bruno Andreotti', 'Philippe Claudin', 'Christian Winter']",2019-11-25T22:07:44Z,A unified model of ripples and dunes in water and planetary environments,"  Subaqueous and aeolian bedforms are ubiquitous on Earth and other planetary environments. However, it is still unclear which hydrodynamical mechanisms lead to the observed variety of morphologies of self-organized natural patterns such as ripples, dunes or compound bedforms. Here we present simulations with a coupled hydrodynamic and sediment transport model that resolve the initial and mature stages of subaqueous and aeolian bedform evolution in the limit of large flow thickness. We identify two types of bedforms consistent with subaqueous ripples and dunes, and separated by a gap in wavelength. This gap is explained in terms of an anomalous hydrodynamic response in the structure of the inner boundary layer that leads to a shift of the position of the maximum shear stress from upstream to downstream of the crest. This anomaly gradually disappears when the bed becomes hydrodynamically rough. By also considering the effect of the spatial relaxation of sediment transport we provide a new unifying framework to compare ripples and dunes in planetary environments to their terrestrial counterparts. ",1911.12300v1
"['Dierck Hillmann', 'Hendrik Spahr', 'Carola Hain', 'Helge Sudkamp', 'Gesa Franke', 'Clara Pfäffle', 'Christian Winter', 'Gereon Hüttmann']",2016-05-12T10:29:29Z,Aberration-free volumetric high-speed imaging of in vivo retina,"  Research and medicine rely on non-invasive optical techniques to image living tissue with high resolution in space and time. But so far a single data acquisition could not provide entirely diffraction-limited tomographic volumes of rapidly moving or changing targets, which additionally becomes increasingly difficult in the presence of aberrations, e.g., when imaging retina in vivo. We show, that a simple interferometric setup based on parallelized optical coherence tomography acquires volumetric data with 10 billion voxels per second, exceeding previous imaging speeds by an order of magnitude. This allows us to computationally obtain and correct defocus and aberrations resulting in entirely diffraction-limited volumes. As demonstration, we imaged living human retina with clearly visible nerve fiber layer, small capillary networks, and photoreceptor cells, but the technique is also applicable to obtain phase-sensitive volumes of other scattering structures at unprecedented acquisition speeds. ",1605.03747v1
['Sebastian Bauer'],2016-04-20T09:27:37Z,A non-relativistic Model of Plasma Physics Containing a Radiation   Reaction Term,  While a fully relativistic collisionless plasma is modeled by the Vlasov-Maxwell system a good approximation in the non-relativistic limit is given by the Vlasov-Poisson system. We modify the Vlasov-Poisson system so that damping due to the relativistic effect of radiation reaction is included. We prove the existence and uniqueness as well as the higher regularity of local classical solutions. These theorems also include the higher regularity of classical solutions of the Vlasov-Poisson system depending on the regularity of the initial datum. ,1604.05869v1
['Sebastian Bauer'],2006-02-13T14:56:59Z,Post-Newtonian dynamics at order 1.5 in the Vlasov-Maxwell system,"  We study the dynamics of many charges interacting with the Maxwell field. The particles are modeled by means of non-negative distribution functions $f^+$ and $f^-$ representing two species of charged matter with positive and negative charge, respectively. If their initial velocities are small compared to the speed of light, $c$, then in lowest order, the Newtonian or classical limit, their motion is governed by the Vlasov-Poisson system. We investigate higher order corrections with an explicit control on the error terms. The Darwin order correction, order $|v/c|^2$, has been proved previously. In this contribution we obtain the dissipative corrections due to radiation damping, which are of order $|v/c|^3$ relative to the Newtonian limit. If all particles have the same charge-to-mass ratio, the dissipation would vanish at that order. ",math-ph/0602031v1
['Sebastian Bauer'],2004-10-23T11:41:14Z,Post-Newtonian approximation of the Vlasov-Nordström system,"  We study the Nordstr\""om-Vlasov system which describes the dynamics of a self-gravitating ensemble of collisionless particles in the framework of the Nordstr\""om scalar theory of gravitation. If the speed of light $c$ is considered as a parameter, it is known that in the Newtonian limit $c\to\infty$ the Vlasov-Poisson system is obtained. In this paper we determine a higher approximation and establish a pointwise error estimate of order $O(c^{-4})$. Such an approximation is usually called a 1.5 post-Newtonian approximation. ",math-ph/0410048v1
"['Sebastian Bauer', 'Dirk Pauly']",2015-12-28T19:09:02Z,On Korn's First Inequality for Mixed Tangential and Normal Boundary   Conditions on Bounded Lipschitz-Domains in $\mathbb{R}^N$,  We prove that for bounded Lipschitz domains in $\mathbb{R}^N$ Korn's first inequality holds for vector fields satisfying homogeneous mixed normal and tangential boundary conditions. ,1512.08483v2
"['Sebastian Bauer', 'Markus Kunze']",2004-01-07T13:29:07Z,The Darwin Approximation of the Relativistic Vlasov-Maxwell System,"  We study the relativistic Vlasov-Maxwell system which describes large systems of particles interacting by means of their collectively generated forces. If the speed of light $c$ is considered as a parameter then it is known that in the Newtonian limit $c\to\infty$ the Vlasov-Poisson system is obtained. In this paper we determine the next order approximate system, which in the case of individual particles usually is called the Darwin approximation. ",math-ph/0401012v1
"['Sebastian Bauer', 'Dirk Pauly']",2015-03-25T15:19:07Z,On Korn's First Inequality for Tangential or Normal Boundary Conditions   with Explicit Constants,  We will prove that for piecewise smooth and concave domains Korn's first inequality holds for vector fields satisfying homogeneous normal or tangential boundary conditions with explicit Korn constant square root of 2. ,1503.07419v5
"['Sebastian Bauer', 'Patrizio Neff', 'Dirk Pauly', 'Gerhard Starke']",2013-07-04T18:04:38Z,Dev-Div- and DevSym-DevCurl-inequalities for incompatible square tensor   fields with mixed boundary conditions,  For an n-dimensional bounded domain we derive some inequalities bounding the norm of a square tensor field. Concerning the Div-Dev-inequality the bound is given by the trace-free part and the divergence and the tensor. In the case of the DevSym-Curl-inequality the bound is given by the trace-free and symmetric part and the curl of the tensor. For n=3 the bound is given by the trace-free symmetric part of the tensor and the trace-free part of the curl of the tensor. Some prototype applications are presented in which the new inequalities may be used to derive the coercivity of the models. ,1307.1434v1
"['Sebastian Bauer', 'Jean-Baptiste Raclet']",2012-07-15T09:03:08Z,Proceedings Fourth Workshop on Foundations of Interface Technologies,"  This volume contains the proceedings of the 4th workshop on Foundations of Interface Technologies (FIT 2012) which was collocated with ETAPS 2012 in Tallinn, Estonia, and took place on March 25, 2012. The aim of this workshop is to bring together researchers who are interested in the formal underpinnings of interface technologies. ",1207.3485v1
"['Sebastian Bauer', 'Dirk Pauly', 'Michael Schomburg']",2015-11-20T17:17:49Z,The Maxwell Compactness Property in Bounded Weak Lipschitz Domains with   Mixed Boundary Conditions,"  For a bounded weak Lipschitz domain we show the so called `Maxwell compactness property', that is, the space of square integrable vector fields having square integrable weak rotation and divergence and satisfying mixed tangential and normal boundary conditions is compactly embedded into the space of square integrable vector fields. We will also prove some canonical applications, such as Maxwell estimates, Helmholtz decompositions and a static solution theory. Furthermore, a Fredholm alternative for the underlying time-harmonic Maxwell problem and all corresponding and related results for exterior domains formulated in weighted Sobolev spaces are straight forward. ",1511.06697v4
"['Sebastian Bauer', 'Dirk Pauly', 'Michael Schomburg']",2018-09-04T18:44:43Z,Weck's Selection Theorem: The Maxwell Compactness Property for Bounded   Weak Lipschitz Domains with Mixed Boundary Conditions in Arbitrary Dimensions,"  It is proved that the space of differential forms with weak exterior and co-derivative, is compactly embedded into the space of square integrable differential forms. Mixed boundary conditions on weak Lipschitz domains are considered. Furthermore, canonical applications such as Maxwell estimates, Helmholtz decompositions and a static solution theory are proved. As a side product and crucial tool for our proofs we show the existence of regular potentials and regular decompositions as well. ",1809.01192v4
"['Syed Azer Reza', 'Sebastian Bauer', 'Andreas Velten']",2020-06-04T00:48:12Z,Phasor field waves: A statistical treatment for the case of a partially   coherent optical carrier,"  This paper presents a statistical treatment of phasor fields (P-fields) - a wave-like quantity denoting the slow temporal variations in time-averaged irradiance (which was recently introduced to model and describe non-line-of-sight (NLoS) imaging as well as imaging through diffuse or scattering apertures) - and quantifies the magnitude of a spurious signal which emerges due to a partial spatial coherence of the underlying optical carrier. This spurious signal is not described by the Huygens-like P-field imaging integral which assumes optical incoherence as a necessary condition to describe P-field imaging completely (as was shown by Reza etal. recently). In this paper, we estimate the relationship between the expected magnitude of this spurious signal and the degree of partial roughness within the P-field imaging system. The treatment allows us to determine the accuracy of the estimate provided by the P-field integral for varying degrees of partial coherence and allows to define a P-field signal-to-noise ratio as a figure-of-merit for the case of a partially coherent optical carrier. The study of partial coherence also enables to better relate aperture roughness to P-field noise. ",2006.02600v2
"['Sebastian Bauer', 'Markus Kunze', 'Gerhard Rein', 'Alan D. Rendall']",2005-08-29T09:58:41Z,Multipole radiation in a collisonless gas coupled to electromagnetism or   scalar gravitation,"  We consider the relativistic Vlasov-Maxwell and Vlasov-Nordstr\""om systems which describe large particle ensembles interacting by either electromagnetic fields or a relativistic scalar gravity model. For both systems we derive a radiation formula analogous to the Einstein quadrupole formula in general relativity. ",math-ph/0508057v1
"['Syed Azer Reza', 'Marco La Manna', 'Sebastian Bauer', 'Andreas Velten']",2019-04-02T17:40:23Z,Wave-like Properties of Phasor Fields: Experimental Demonstrations,"  Recently, an optical meta concept called the Phasor Field (P-Field) was proposed that yields great quality in the reconstruction of hidden objects imaged by non-line-of-sight (NLOS) imaging. It is based on virtual sinusoidal modulation of the light with frequencies in the MHz range. Phasor Field propagation was shown to be described by the Rayleigh-Sommerfeld diffraction integral. We extend this concept and stress the analogy between electric field and Phasor Field. We introduce Phasor Field optical elements and present experiments demonstrating the validity of the approach. Straightforward use of the Phasor Field concept in real-world applications is also discussed. ",1904.01565v1
"['Diyuan Lu', 'Sebastian Bauer', 'Valentin Neubert', 'Lara Sophie Costard', 'Felix Rosenow', 'Jochen Triesch']",2020-06-17T14:08:12Z,Staging Epileptogenesis with Deep Neural Networks,"  Epilepsy is a common neurological disorder characterized by recurrent seizures accompanied by excessive synchronous brain activity. The process of structural and functional brain alterations leading to increased seizure susceptibility and eventually spontaneous seizures is called epileptogenesis (EPG) and can span months or even years. Detecting and monitoring the progression of EPG could allow for targeted early interventions that could slow down disease progression or even halt its development. Here, we propose an approach for staging EPG using deep neural networks and identify potential electroencephalography (EEG) biomarkers to distinguish different phases of EPG. Specifically, continuous intracranial EEG recordings were collected from a rodent model where epilepsy is induced by electrical perforant pathway stimulation (PPS). A deep neural network (DNN) is trained to distinguish EEG signals from before stimulation (baseline), shortly after the PPS and long after the PPS but before the first spontaneous seizure (FSS). Experimental results show that our proposed method can classify EEG signals from the three phases with an average area under the curve (AUC) of 0.93, 0.89, and 0.86. To the best of our knowledge, this represents the first successful attempt to stage EPG prior to the FSS using DNNs. ",2006.09885v1
"['Matthias Opel', 'Karl-Wilhelm Nielsen', 'Sebastian Bauer', 'Sebastian T. B. Goennenwein', 'Rudolf Gross', 'Julio C. Cezar', 'Dieter Schmeisser', 'Juergen Simon', 'Werner Mader']",2008-03-26T17:14:55Z,Nanosized superparamagnetic precipitates in cobalt-doped ZnO,"  The existence of semiconductors exhibiting long-range ferromagnetic ordering at room temperature still is controversial. One particularly important issue is the presence of secondary magnetic phases such as clusters, segregations, etc... These are often tedious to detect, leading to contradictory interpretations. We show that in our cobalt doped ZnO films grown homoepitaxially on single crystalline ZnO substrates the magnetism unambiguously stems from metallic cobalt nano-inclusions. The magnetic behavior was investigated by SQUID magnetometry, x-ray magnetic circular dichroism, and AC susceptibility measurements. The results were correlated to a detailed microstructural analysis based on high resolution x-ray diffraction, transmission electron microscopy, and electron-spectroscopic imaging. No evidence for carrier mediated ferromagnetic exchange between diluted cobalt moments was found. In contrast, the combined data provide clear evidence that the observed room temperature ferromagnetic-like behavior originates from nanometer sized superparamagnetic metallic cobalt precipitates. ",0803.3774v2
"['Diyuan Lu', 'Sebastian Bauer', 'Valentin Neubert', 'Lara Sophie Costard', 'Felix Rosenow', 'Jochen Triesch']",2020-06-11T21:04:54Z,Towards Early Diagnosis of Epilepsy from EEG Data,"  Epilepsy is one of the most common neurological disorders, affecting about 1% of the population at all ages. Detecting the development of epilepsy, i.e., epileptogenesis (EPG), before any seizures occur could allow for early interventions and potentially more effective treatments. Here, we investigate if modern machine learning (ML) techniques can detect EPG from intra-cranial electroencephalography (EEG) recordings prior to the occurrence of any seizures. For this we use a rodent model of epilepsy where EPG is triggered by electrical stimulation of the brain. We propose a ML framework for EPG identification, which combines a deep convolutional neural network (CNN) with a prediction aggregation method to obtain the final classification decision. Specifically, the neural network is trained to distinguish five second segments of EEG recordings taken from either the pre-stimulation period or the post-stimulation period. Due to the gradual development of epilepsy, there is enormous overlap of the EEG patterns before and after the stimulation. Hence, a prediction aggregation process is introduced, which pools predictions over a longer period. By aggregating predictions over one hour, our approach achieves an area under the curve (AUC) of 0.99 on the EPG detection task. This demonstrates the feasibility of EPG prediction from EEG recordings. ",2006.06675v2
"['Ji Hyun Nam', 'Eric Brandt', 'Sebastian Bauer', 'Xiaochun Liu', 'Eftychios Sifakis', 'Andreas Velten']",2020-10-24T01:40:06Z,Real-time Non-line-of-Sight imaging of dynamic scenes,"  Non-Line-of-Sight (NLOS) imaging aims at recovering the 3D geometry of objects that are hidden from the direct line of sight. In the past, this method has suffered from the weak available multibounce signal limiting scene size, capture speed, and reconstruction quality. While algorithms capable of reconstructing scenes at several frames per second have been demonstrated, real-time NLOS video has only been demonstrated for retro-reflective objects where the NLOS signal strength is enhanced by 4 orders of magnitude or more. Furthermore, it has also been noted that the signal-to-noise ratio of reconstructions in NLOS methods drops quickly with distance and past reconstructions, therefore, have been limited to small scenes with depths of few meters. Actual models of noise and resolution in the scene have been simplistic, ignoring many of the complexities of the problem. We show that SPAD (Single-Photon Avalanche Diode) array detectors with a total of just 28 pixels combined with a specifically extended Phasor Field reconstruction algorithm can reconstruct live real-time videos of non-retro-reflective NLOS scenes. We provide an analysis of the Signal-to-Noise-Ratio (SNR) of our reconstructions and show that for our method it is possible to reconstruct the scene such that SNR, motion blur, angular resolution, and depth resolution are all independent of scene size suggesting that reconstruction of very large scenes may be possible. In the future, the light efficiency for NLOS imaging systems can be improved further by adding more pixels to the sensor array. ",2010.12737v1
"['Matthias Opel', 'Sebastian T. B. Goennenwein', 'Matthias Althammer', 'Karl-Wilhelm Nielsen', 'Eva-Maria Karrer-Müller', 'Sebastian Bauer', 'Konrad Senn', 'Christoph Schwark', 'Christian Weier', 'Gernot Güntherodt', 'Bernd Beschoten', 'Rudolf Gross']",2013-09-23T16:02:21Z,Zinc Oxide - From Dilute Magnetic Doping to Spin Transport,"  During the past years there has been renewed interest in the wide-bandgap II-VI semiconductor ZnO, triggered by promising prospects for spintronic applications. First, ferromagnetism was predicted for dilute magnetic doping. In comprehensive investigation of ZnO:Co thin films based on the combined measurement of macroscopic and microscopic properties, we find no evidence for carrier-mediated itinerant ferromagnetism. Phase-pure, crystallographically excellent ZnO:Co is uniformly paramagnetic. Superparamagnetism arises when phase separation or defect formation occurs, due to nanometer-sized metallic precipitates. Other compounds like ZnO:(Li,Ni) and ZnO:Cu do not exhibit indication of ferromagnetism.   Second, its small spin-orbit coupling and correspondingly large spin coherence length makes ZnO suitable for transporting or manipulating spins in spintronic devices. From optical pump/optical probe experiments, we find a spin dephasing time of the order of 15 ns at low temperatures which we attribute to electrons bound to Al donors. In all-electrical magnetotransport measurements, we successfully create and detect a spin-polarized ensemble of electrons and transport this spin information across several nanometers. We derive a spin lifetime of 2.6 ns for these itinerant spins at low temperatures, corresponding well to results from an electrical pump/optical probe experiment. ",1309.5857v1
"['Alexander Preuhs', 'Martin Berger', 'Sebastian Bauer', 'Thomas Redel', 'Mathias Unberath', 'Stephan Achenbach', 'Andreas Maier']",2018-08-02T15:19:21Z,Viewpoint Planning for Quantitative Coronary Angiography,"  In coronary angiography the condition of myocardial blood supply is assessed by analyzing 2-D X-ray projections of contrasted coronary arteries. This is done using a flexible C-arm system. Due to the X-ray immanent dimensionality reduction projecting the 3-D scene onto a 2-D image, the viewpoint is critical to guarantee an appropriate view onto the affected artery and, thus, enable reliable diagnosis. In this work we introduce an algorithm computing optimal viewpoints for the assessment of coronary arteries without the need for 3-D models. We introduce the concept of optimal viewpoint planning solely based on a single angiographic X-ray image. The subsequent viewpoint is computed such that it is rotated precisely around a vessel, while minimizing foreshortening. Our algorithm reduces foreshortening substantially compared to the input view and completely eliminates it for 90 degree rotations. Rotations around iso-centered foreshortening-free vessels passing the isocenter are exact. The precision, however, decreases when the vessel is off-centered or foreshortened. We evaluate worst case boundaries, providing insight in the maximal inaccuracies to be expected. This can be utilized to design viewpoints guaranteeing desired requirements, e.g. a true rotation around the vessel of at minimum 30 degree. ",1808.00853v1
"['Abdul Hannan Mustajab', 'Hao Lyu', 'Zarghaam Rizvi', 'Frank Wuttke']",2024-01-05T13:45:08Z,Physics-Informed Neural Networks for High-Frequency and Multi-Scale   Problems using Transfer Learning,"  Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks for solving more complex problems. ",2401.02810v2
"['Frank Wuttke', 'Hao Lyu', 'Amir S. Sattari', 'Zarghaam H. Rizvi']",2021-03-30T13:31:50Z,Wave based damage detection in solid structures using artificial neural   networks,"  The identification of structural damages takes a more and more important role within the modern economy, where often the monitoring of an infrastructure is the last approach to keep it under public use. Conventional monitoring methods require specialized engineers and are mainly time consuming. This research paper considers the ability of neural networks to recognize the initial or alteration of structural properties based on the training processes. The presented work here is based on Convolutional Neural Networks (CNN) for wave field pattern recognition, or more specifically the wave field change recognition. The CNN model is used to identify the change within propagating wave fields after a crack initiation within the structure. The paper describes the implemented method and the required training procedure to get a successful crack detection accuracy, where the training data are based on the dynamic lattice model. Although the training of the model is still time consuming, the proposed new method has an enormous potential to become a new crack detection or structural health monitoring approach within the conventional monitoring methods. ",2103.16339v1
['Thomas Lux'],2005-06-24T16:46:47Z,Emergent Statistical Wealth Distributions in Simple Monetary Exchange   Models: A Critical Review,"  This paper reviews recent attempts at modelling inequality of wealth as an emergent phenomenon of interacting-agent processes. We point out that recent models of wealth condensation which draw their inspiration from molecular dynamics have, in fact, reinvented a process introduced quite some time ago by Angle (1986) in the sociological literature. We emphasize some problematic aspects of simple wealth exchange models and contrast them with a monetary model based on economic principles of market mediated exchange. The paper also reports new results on the influence of market power on the wealth distribution in statistical equilibrium. As it turns out, inequality increases but market power alone is not sufficient for changing the exponential tails of simple exchange models into Pareto tails. ",cs/0506092v1
"['Thomas Lux', 'D. Sornette']",1999-10-08T22:44:38Z,On Rational Bubbles and Fat Tails,"  This paper addresses the statistical properties of time series driven by rational bubbles a la Blanchard and Watson (1982), corresponding to multiplicative maps, whose study has recently be revived recently in physics as a mechanism of intermittent dynamics generating power law distributions. Using insights on the behavior of multiplicative stochastic processes, we demonstrate that the tails of the unconditional distribution emerging from such bubble processes follow power-laws (exhibit hyperbolic decline). More precisely, we find that rational bubbles predict a 'fat' power tail for both the bubble component and price differences with an exponent smaller than 1, implying absence of convergence of the mean. The distribution of returns is dominated by the same power-law over an extended range of large returns. Although power-law tails are a pervasive feature of empirical data, these numerical predictions are in disagreement with the usual empirical estimates of an exponent between 2 and 4. It, therefore, appears that exogenous rational bubbles are hardly reconcilable with some of the stylized facts of financial data at a very elementary level. ",cond-mat/9910141v1
"['Ruipeng Liu', 'T. Di Matteo', 'Thomas Lux']",2007-04-11T01:16:18Z,True and Apparent Scaling: The Proximity of the Markov-Switching   Multifractal Model to Long-Range Dependence,"  In this paper, we consider daily financial data of a collection of different stock market indices, exchange rates, and interest rates, and we analyze their multi-scaling properties by estimating a simple specification of the Markov-switching multifractal model (MSM). In order to see how well the estimated models capture the temporal dependence of the data, we estimate and compare the scaling exponents $H(q)$ (for $q = 1, 2$) for both empirical data and simulated data of the estimated MSM models. In most cases the multifractal model appears to generate `apparent' long memory in agreement with the empirical scaling laws. ",0704.1338v1
"['Li Xu', 'Thomas Lux', 'Tyler Chang', 'Bo Li', 'Yili Hong', 'Layne Watson', 'Ali Butt', 'Danfeng Yao', 'Kirk Cameron']",2020-12-14T19:56:52Z,Prediction of High-Performance Computing Input/Output Variability and   Its Application to Optimization for System Configurations,"  Performance variability is an important measure for a reliable high performance computing (HPC) system. Performance variability is affected by complicated interactions between numerous factors, such as CPU frequency, the number of input/output (IO) threads, and the IO scheduler. In this paper, we focus on HPC IO variability. The prediction of HPC variability is a challenging problem in the engineering of HPC systems and there is little statistical work on this problem to date. Although there are many methods available in the computer experiment literature, the applicability of existing methods to HPC performance variability needs investigation, especially, when the objective is to predict performance variability both in interpolation and extrapolation settings. A data analytic framework is developed to model data collected from large-scale experiments. Various promising methods are used to build predictive models for the variability of HPC systems. We evaluate the performance of the methods by measuring prediction accuracy at previously unseen system configurations. We also discuss a methodology for optimizing system configurations that uses the estimated variability map. The findings from method comparisons and developed tool sets in this paper yield new insights into existing statistical methods and can be beneficial for the practice of HPC variability management. This paper has supplementary materials online. ",2012.07915v1
"['Yueyao Wang', 'Li Xu', 'Yili Hong', 'Rong Pan', 'Tyler Chang', 'Thomas Lux', 'Jon Bernard', 'Layne Watson', 'Kirk Cameron']",2022-01-24T15:55:01Z,Design Strategies and Approximation Methods for High-Performance   Computing Variability Management,"  Performance variability management is an active research area in high-performance computing (HPC). We focus on input/output (I/O) variability. To study the performance variability, computer scientists often use grid-based designs (GBDs) to collect I/O variability data, and use mathematical approximation methods to build a prediction model. Mathematical approximation models could be biased particularly if extrapolations are needed. Space-filling designs (SFDs) and surrogate models such as Gaussian process (GP) are popular for data collection and building predictive models. The applicability of SFDs and surrogates in the HPC variability needs investigation. We investigate their applicability in the HPC setting in terms of design efficiency, prediction accuracy, and scalability. We first customize the existing SFDs so that they can be applied in the HPC setting. We conduct a comprehensive investigation of design strategies and the prediction ability of approximation methods. We use both synthetic data simulated from three test functions and the real data from the HPC setting. We then compare different methods in terms of design efficiency, prediction accuracy, and scalability. In synthetic and real data analysis, GP with SFDs outperforms in most scenarios. With respect to approximation models, GP is recommended if the data are collected by SFDs. If data are collected using GBDs, both GP and Delaunay can be considered. With the best choice of approximation method, the performance of SFDs and GBD depends on the property of the underlying surface. For the cases in which SFDs perform better, the number of design points needed for SFDs is about half of or less than that of the GBD to achieve the same prediction accuracy. SFDs that can be tailored to high dimension and non-smooth surface are recommended especially when large numbers of input factors need to be considered in the model. ",2201.09766v1
"['Fabian Allmendinger', 'Benjamin Brauneis', 'Werner Heil', 'Ulrich Schmidt']",2023-07-14T08:41:47Z,Degaussing Procedure and Performance Enhancement by Low-Frequency   Shaking of a 3-Layer Magnetically Shielded Room,"  We report on the performance of a Magnetically Shielded Room (MSR) intended for next level $^3$He/$^{129}$Xe co-magnetometer experiments which require improved magnetic conditions. The MSR consists of three layers of Mu-metal with a thickness of 3 mm each, and one additional highly conductive copper-coated aluminum layer with a thickness of 10 mm. It has a cubical shape with an walk-in interior volume with an edge length of 2560 mm. An optimized degaussing (magnetic equilibration) procedure using a frequency sweep with constant amplitude followed by an exponential decay of the amplitude will be presented. The procedure for the whole MSR takes 21 minutes and measurements of the residual magnetic field at the center of the MSR show that $|B|<1$ nT can be reached reliably. The chosen degaussing procedure will be motivated by online hysteresis measurements of the assembled MSR and by Eddy current simulations showing that saturation at the center of the Mu-metal layer is reached. Shielding Factors can be improved by a factor $\approx 4$ in all directions by low frequency (0.2 Hz), low current (1 A) shaking of the outermost Mu-metal layer. ",2307.07225v1
"['Fabian Allmendinger', 'Ulrich Schmidt', 'Werner Heil', 'Sergej Karpuk', 'Yuri Sobolev']",2016-07-22T15:58:31Z,Limit on Lorentz-Invariance- and CPT-Violating Neutron Spin Interactions   Using a $^3$He-$^{129}$Xe Comagnetometer,"  We performed a search for a Lorentz-invariance- and CPT-violating coupling of the $^3$He and $^{129}$Xe nuclear spins to posited background fields. Our experimental approach is to measure the free precession of nuclear spin polarized $^3$He and $^{129}$Xe atoms using SQUID detectors. As the laboratory reference frame rotates with respect to distant stars, we look for a sidereal modulation of the Larmor frequencies of the co-located spin samples. As a result we obtain an upper limit on the equatorial component of the background field $\tilde{b}^n_{\bot}< 8.4 \cdot 10^{-34}$ GeV (68\% C.L.). Furthermore, this technique was modified to search for an electric dipole moment (EDM) of $^{129}$Xe. ",1607.06719v1
"['Hartmut Abele', 'Tobias Jenke', 'Erwin Jericha', 'Gertrud Konrad', 'Bastian Märkisch', 'Christian Plonka', 'Ulrich Schmidt', 'Torsten Soldner']",2014-12-16T14:25:27Z,High Precision Experiments with Cold and Ultra-Cold Neutrons,"  This work presents selected results from the first round of the DFG Priority Programme SPP 1491 ""precision experiments in particle and astroparticle physics with cold and ultra-cold neutrons"". ",1412.5013v1
"['Torsten Soldner', 'Hartmut Abele', 'Gertrud Konrad', 'Bastian Märkisch', 'Florian M. Piegsa', 'Ulrich Schmidt', 'Camille Theroine', 'Pablo Torres Sánchez']",2018-11-28T17:20:01Z,ANNI - A pulsed cold neutron beam facility for particle physics at the   ESS,"  Pulsed beams have tremendous advantages for precision experiments with cold neutrons. In order to minimise and measure systematic effects, they are used at continuous sources in spite of the related substantial decrease in intensity. At the European Spallation Source ESS these experiments will profit from the pulse structure of the source and its 50 times higher peak brightness compared to the most intense reactor facilities, making novel concepts feasible. Therefore, the cold neutron beam facility for particle physics ANNI was proposed as part of the ESS instrument suite. The proposed design has been re-optimised to take into account the present ESS cold moderator layout. We present design considerations, the optimised instrument parameters and performance, and expected gain factors for several reference experiments. ",1811.11692v1
"['Kathlynne Tullney', 'Fabian Allmendinger', 'Martin Burghoff', 'Werner Heil', 'Sergej Karpuk', 'Wolfgang Kilian', 'Silvia Knappe-Grüneberg', 'Wolfgang Müller', 'Ulrich Schmidt', 'Allard Schnabel', 'Frank Seifert', 'Yuri Sobolev', 'Lutz Trahms']",2013-03-26T19:23:49Z,Constraints on Spin-Dependent Short-Range Interaction between Nucleons,"  We report on the search for a new spin-dependent P- and T-violating interaction between nucleons mediated by light, pseudoscalar bosons such as the axion which was invented to solve the strong CP problem. Our experimental approach is to use an ultra-sensitive low-field magnetometer based on the detection of free precession of co-located 3He and 129Xe nuclear spins using SQUIDs as low-noise magnetic flux detectors. In the presence of an unpolarized mass the precession frequency shif ",1303.6612v2
"['Salma Khedr', 'Katrin Rehdanz', 'Roy Brouwer', 'Hanna Dijkstra', 'Sem Duijndam', 'Pieter van Beukering', 'Ikechukwu C. Okoli']",2021-07-08T16:31:34Z,Public preferences for marine plastic litter reductions across Europe,"  Plastic pollution is one of the most challenging problems affecting the marine environment of our time. Based on a unique dataset covering four European seas and eight European countries, this paper adds to the limited empirical evidence base related to the societal welfare effects of marine litter management. We use a discrete choice experiment to elicit public willingness-to-pay (WTP) for macro and micro plastic removal to achieve Good Environmental Status across European seas as required by the European Marine Strategy Framework Directive. Using a common valuation design and following best-practice guidelines, we draw meaningful comparisons between countries, seas and policy contexts. European citizens have strong preferences to improve the environmental status of the marine environment by removing both micro and macro plastic litter favouring a pan-European approach. However, public WTP estimates differ significantly across European countries and seas. We explain why and discuss implications for policymaking. ",2107.03957v1
['Stefan Hoffmann'],2021-03-17T12:08:01Z,Regularity Conditions for Iterated Shuffle on Commutative Regular   Languages,"  We identify a subclass of the regular commutative languages that is closed under the iterated shuffle, or shuffle closure. In particular, it is regularity-preserving on this subclass. This subclass contains the commutative group languages and, for every alphabet $\Sigma$, the class $\textbf{Com}^+(\Sigma^*)$ given by the ordered variety $\textbf{Com}^+$. Then, we state a simple characterization when the iterated shuffle on finite commutative languages gives a regular language again and state partial results for aperiodic commutative languages. We also show that the aperiodic, or star-free, commutative languages and the commutative group languages are closed under projection. ",2103.09587v2
['Stefan Hoffmann'],2021-11-26T14:45:20Z,"State Complexity Investigations on Commutative Languages -- The Upward   and Downward Closure, Commutative Aperiodic and Commutative Group Languages","  We investigate the state complexity of the upward and downward closure and interior operations on commutative regular languages. Then, we systematically study the state complexity of these operations and of the shuffle operation on commutative group languages and commutative aperiodic (or star-free) languages. ",2111.13524v1
['Stefan Hoffmann'],2020-05-08T13:43:23Z,Computational Complexity of Synchronization under Regular Commutative   Constraints,"  Here we study the computational complexity of the constrained synchronization problem for the class of regular commutative constraint languages. Utilizing a vector representation of regular commutative constraint languages, we give a full classification of the computational complexity of the constraint synchronization problem. Depending on the constraint language, our problem becomes PSPACE-complete, NP-complete or polynomial time solvable. In addition, we derive a polynomial time decision procedure for the complexity of the constraint synchronization problem, given some constraint automaton accepting a commutative language as input. ",2005.04042v2
['Stefan Hoffmann'],2020-05-12T16:35:30Z,Ideal Separation and General Theorems for Constrained Synchronization   and their Application to Small Constraint Automata,"  In the constrained synchronization problem we ask if a given automaton admits a synchronizing word coming from a fixed regular constraint language. We show that intersecting a given constraint language with an ideal language decreases the computational complexity. Additionally, we state a theorem giving PSPACE-hardness that broadly generalizes previously used constructions and a result on how to combine languages by concatenation to get polynomial time solvable constrained synchronization problems. We use these results to give a classification of the complexity landscape for small constraint automata of up to three states. ",2005.05907v2
['Stefan Hoffmann'],2020-06-02T19:36:48Z,On a Class of Constrained Synchronization Problems in NP,"  The class of known constraint automata for which the constrained synchronization problem is in NP all admit a special form. In this work, we take a closer look at them. We characterize a wider class of constraint automata that give constrained synchronization problems in NP, which encompasses all known problems in NP. We call these automata polycyclic automata. The corresponding language class of polycyclic languages is introduced. We show various characterizations and closure properties for this new language class. We then give a criterion for NP-completeness and a criterion for polynomial time solvability for polycyclic constraint languages. ",2006.01903v2
['Stefan Hoffmann'],2020-06-26T19:10:22Z,State Complexity of Permutation and Related Decision Problems on   Alphabetical Pattern Constraints,"  We investigate the state complexity of the permutation operation, or the commutative closure, on Alphabetical Pattern Constraints (APC). This class corresponds to level $3/2$ of the Straubing-Th{\'e}rien Hierarchy and includes the finite, the piecewise-testable, or $\mathcal J$-trivial, and the $\mathcal R$-trivial and $\mathcal L$-trivial languages. We give a sharp state complexity bound expressed in terms of the longest strings in the unary projection languages of an associated finite language and which is already sharp for the subclass of finite languages. Additionally, for two subclasses, we give sharp bounds expressed in terms of the size of a recognizing input automaton and the size of the alphabet. Lastly, we investigate the inclusion and universality problem on APCs up to permutational equivalence, two problems known to be PSPACE-complete on APCs even for fixed alphabets in general, and show them to be decidable in polynomial time for fixed alphabets in this case. ",2006.15178v3
['Stefan Hoffmann'],2021-09-06T21:07:27Z,Constrained Synchronization for Commutative Automata and Automata with   Simple Idempotents,"  For general input automata, there exist regular constraint languages such that asking if a given input automaton admits a synchronizing word in the constraint language is PSPACE-complete or NP-complete. Here, we investigate this problem for commutative automata over an arbitrary alphabet and automata with simple idempotents over a binary alphabet as input automata. The latter class contains, for example, the \v{C}ern\'y family of automata. We find that for commutative input automata, the problem is always solvable in polynomial time, for every constraint language. For input automata with simple idempotents over a binary alphabet and with a constraint language given by a partial automaton with up to three states, the constrained synchronization problem is also solvable in polynomial time. ",2109.02743v1
['Stefan Hoffmann'],2020-07-17T16:31:47Z,"Completely Reachable Automata, Primitive Groups and the State Complexity   of the Set of Synchronizing Words","  We give a new characterization of primitive permutation groups tied to the notion of completely reachable automata. Also, we introduce sync-maximal permutation groups tied to the state complexity of the set of synchronizing words of certain associated automata and show that they are contained between the $2$-homogeneous and the primitive groups. Lastly, we define $k$-reachable groups in analogy with synchronizing groups and motivated by our characterization of primitive permutation groups. But the results show that a $k$-reachable permutation group of degree $n$ with $6 \le k \le n - 6$ is either the alternating or the symmetric group. ",2007.09104v2
['Stefan Hoffmann'],2020-08-12T16:22:40Z,The Commutative Closure of Shuffle Expressions over Group Languages is   Regular,"  We show that the commutative closure combined with the iterated shuffle is a regularity-preserving operation on group languages. In particular, for commutative group languages, the iterated shuffle is a regularity-preserving operation. We also give bounds for the size of minimal recognizing automata. Then, we use these results to deduce that the commutative closure of any shuffle expression over group languages, i.e., expressions involving shuffle, iterated shuffle, concatenation, Kleene star and union in any order, starting with the group languages, always yields a regular language. ",2008.05420v3
['Stefan Hoffmann'],2020-11-29T17:56:38Z,State Complexity of the Set of Synchronizing Words for Circular Automata   and Automata over Binary Alphabets,"  Most slowly synchronizing automata over binary alphabets are circular, i.e., containing a letter permuting the states in a single cycle, and their set of synchronizing words has maximal state complexity, which also implies complete reachability.Here, we take a closer look at generalized circular and completely reachable automata. We derive that over a binary alphabet every completely reachable automaton must be circular, a consequence of a structural result stating that completely reachable automata over strictly less letters than states always contain permutational letters. We state sufficient conditions for the state complexity of the set of synchronizing words of a generalized circular automaton to be maximal. We apply our main criteria to the family $\mathscr K_n$ of automata that was previously only conjectured to have this property. ",2011.14404v1
['Stefan Hoffmann'],2021-07-30T21:09:01Z,Computational Complexity of Synchronization under Sparse Regular   Constraints,"  The constrained synchronization problem (CSP) asks for a synchronizing word of a given input automaton contained in a regular set of constraints. It could be viewed as a special case of synchronization of a discrete event system under supervisory control. Here, we study the computational complexity of this problem for the class of sparse regular constraint languages. We give a new characterization of sparse regular sets, which equal the bounded regular sets, and derive a full classification of the computational complexity of CSP for letter-bounded regular constraint languages, which properly contain the strictly bounded regular languages. Then, we introduce strongly self-synchronizing codes and investigate CSP for bounded languages induced by these codes. With our previous result, we deduce a full classification for these languages as well. In both cases, depending on the constraint language, our problem becomes NP-complete or polynomial time solvable. ",2108.00081v1
['Stefan Hoffmann'],2021-08-16T09:07:47Z,State Complexity of Projection on Languages Recognized by Permutation   Automata and Commuting Letters,"  The projected language of a general deterministic automaton with $n$ states is recognizable by a deterministic automaton with $2^{n-1} + 2^{n-m} - 1$ states, where $m$ denotes the number of states incident to unobservable non-loop transitions, and this bound is best possible. Here, we derive the tight bound $2^{n - \lceil \frac{m}{2} \rceil} - 1$ for permutation automata. For a state-partition automaton with $n$ states (also called automata with the observer property) the projected language is recognizable with $n$ states. Up to now, these, and finite languages projected onto unary languages, were the only classes of automata known to possess this property. We show that this is also true for commutative automata and we find commutative automata that are not state-partition automata. ",2108.06976v1
['Stefan Hoffmann'],2021-08-16T09:25:40Z,Constrained Synchronization and Subset Synchronization Problems for   Weakly Acyclic Automata,"  We investigate the constrained synchronization problem for weakly acyclic, or partially ordered, input automata. We show that, for input automata of this type, the problem is always in NP. Furthermore, we give a full classification of the realizable complexities for constraint automata with at most two states and over a ternary alphabet. We find that most constrained problems that are PSPACE-complete in general become NP-complete. However, there also exist constrained problems that are PSPACE-complete in the general setting but become polynomial time solvable when considered for weakly acyclic input automata. We also investigate two problems related to subset synchronization, namely if there exists a word mapping all states into a given target subset of states, and if there exists a word mapping one subset into another. Both problems are PSPACE-complete in general, but in our setting the former is polynomial time solvable and the latter is NP-complete. ",2108.06984v1
['Stefan Hoffmann'],2021-08-20T17:19:39Z,The n-ary Initial Literal and Literal Shuffle,"  The literal and the initial literal shuffle have been introduced to model the behavior of two synchronized processes. However, it is not possible to describe the synchronization of multiple processes. Furthermore, both restricted forms of shuffling are not associative. Here, we extend the literal shuffle and the initial literal shuffle to multiple arguments. We also introduce iterated versions, much different from the iterated ones previously introduced for the binary literal and initial literal shuffle. We investigate formal properties, and show that in terms of expressive power, in a full trio, they coincide with the general shuffle. Furthermore, we look at closure properties with respect to the regular, context-free, context-sensitive, recursive and recursively enumerable languages for all operations introduced. Then, we investigate various decision problems motivated by analogous problems for the (ordinary) shuffle operation. Most problems we look at are tractable, but we also identify one intractable decision problem. ",2108.09282v1
['Stefan Hoffmann'],2021-11-26T14:44:20Z,Commutative Regular Languages with Product-Form Minimal Automata,"  We introduce a subclass of the commutative regular languages that is characterized by the property that the state set of the minimal deterministic automaton can be written as a certain Cartesian product. This class behaves much better with respect to the state complexity of the shuffle, for which we find the bound~$2nm$ if the input languages have state complexities $n$ and $m$, and the upward and downward closure and interior operations, for which we find the bound~$n$. In general, only the bounds $(2nm)^{|\Sigma|}$ and $n^{|\Sigma|}$ are known for these operations in the commutative case. We prove different characterizations of this class and present results to construct languages from this class. Lastly, in a slightly more general setting of partial commutativity, we introduce other, related, language classes and investigate the relations between them. ",2111.13523v1
['Stefan Hoffmann'],2021-11-26T14:47:39Z,Sync-Maximal Permutation Groups Equal Primitive Permutation Groups,"  The set of synchronizing words of a given $n$-state automaton forms a regular language recognizable by an automaton with $2^n - n$ states. The size of a recognizing automaton for the set of synchronizing words is linked to computational problems related to synchronization and to the length of synchronizing words. Hence, it is natural to investigate synchronizing automata extremal with this property, i.e., such that the minimal deterministic automaton for the set of synchronizing words has $2^n - n$ states. The sync-maximal permutation groups have been introduced in [{\sc S. Hoffmann}, Completely Reachable Automata, Primitive Groups and the State Complexity of the Set of Synchronizing Words, LATA 2021] by stipulating that an associated automaton to the group and a non-permutation has this extremal property. The definition is in analogy with the synchronizing groups and analog to a characterization of primitivity obtained in the mentioned work. The precise relation to other classes of groups was mentioned as an open problem. Here, we solve this open problem by showing that the sync-maximal groups are precisely the primitive groups. Our result gives a new characterization of the primitive groups. Lastly, we explore an alternative and stronger definition than sync-maximality. ",2111.13527v1
['Stefan Hoffmann'],2020-04-24T14:33:03Z,State Complexity Bounds for the Commutative Closure of Group Languages,"  In this work we construct an automaton for the commutative closure of a given regular group language. The number of states of the resulting automaton is bounded by the number of states of the original automaton, raised to the power of the alphabet size, times the product of the order of the letters, viewed as permutations of the state set. This gives the asymptotic state bound $O((n\exp(\sqrt{n\ln n}))^{|\Sigma|})$, if the original regular language is accepted by an automaton with $n$ states. Depending on the automaton in question, we label points of $\mathbb N_0^{|\Sigma|}$ by subsets of states and introduce unary automata which decompose the thus labelled grid. Based on these constructions, we give a general regularity condition, which is fulfilled for group languages. ",2004.11772v4
"['Stefan Hoffmann', 'Egon Wanke']",2013-06-10T12:55:16Z,Metric Dimension for Gabriel Unit Disk Graphs is NP-Complete,  We show that finding a minimal number of landmark nodes for a unique virtual addressing by hop-distances in wireless ad-hoc sensor networks is NP-complete even if the networks are unit disk graphs that contain only Gabriel edges. This problem is equivalent to Metric Dimension for Gabriel unit disk graphs. The Gabriel edges of a unit disc graph induce a planar O(\sqrt{n}) distance and an optimal energy spanner. This is one of the most interesting restrictions of Metric Dimension in the context of wireless multi-hop networks. ,1306.2187v1
"['Stefan Hoffmann', 'Egon Wanke']",2016-05-05T20:48:14Z,Minimum Power Range Assignment for Symmetric Connectivity in Sensor   Networks with two Power Levels,"  This paper examines the problem of assigning a transmission power to every node of a wireless sensor network. The goal is to minimize the total power consumption while ensuring that the resulting communication graph is connected. We focus on a restricted version of this Range Assignment (RA) problem in which there are two different power levels. We only consider symmetrical transmission links to allow easy integration with low level wireless protocols that typically require bidirectional communication between two neighboring nodes. We introduce a parameterized polynomial time approximation algorithm with a performance ratio arbitrarily close to $\pi^2/6$. Additionally, we give an almost linear time approximation algorithm with a tight quality bound of $7/4$. ",1605.01752v1
"['Henning Fernau', 'Stefan Hoffmann', 'Michael Wehar']",2021-08-11T14:25:30Z,Finite Automata Intersection Non-Emptiness: Parameterized Complexity   Revisited,"  The problem DFA-Intersection-Nonemptiness asks if a given number of deterministic automata accept a common word. In general, this problem is PSPACE-complete. Here, we investigate this problem for the subclasses of commutative automata and automata recognizing sparse languages. We show that in both cases DFA-Intersection-Nonemptiness is complete for NP and for the parameterized class $W[1]$, where the number of input automata is the parameter, when the alphabet is fixed. Additionally, we establish the same result for Tables Non-Empty Join, a problem that asks if the join of several tables (possibly containing null values) in a database is non-empty. Lastly, we show that Bounded NFA-Intersection-Nonemptiness, parameterized by the length bound, is $\mbox{co-}W[2]$-hard with a variable input alphabet and for nondeterministic automata recognizing finite strictly bounded languages, yielding a variant leaving the realm of $W[1]$. ",2108.05244v1
"['Daniel Gaußmann', 'Stefan Hoffmann', 'Egon Wanke']",2013-07-08T09:18:45Z,Hierarchical Bipartition Routing for delivery guarantee in sparse   wireless ad hoc sensor networks with obstacles,"  We introduce and evaluate a very simple landmark-based network partition technique called Hierarchical Bipartition Routing (HBR) to support routing with delivery guarantee in wireless ad hoc sensor networks. It is a simple routing protocol that can easily be combined with any other greedy routing algorithm to obtain delivery guarantee. The efficiency of HBR increases if the network is sparse and contains obstacles. The space necessary to store the additional routing information at a node u is on average not larger than the size necessary to store the IDs of the neighbors of u. The amount of work to setup the complete data structure is on average proportional to flooding the entire network log(n) times, where n is the total number of sensor nodes. We evaluate the performance of HBR in combination with two simple energy-aware geographic greedy routing algorithms based on physical coordinates and virtual coordinates, respectively. Our simulations show that the difference between using HBR and a weighted shortest path to escape a dead-end is only a few percent in typical cases. ",1307.1994v1
"['Duygu Vietz', 'Stefan Hoffmann', 'Egon Wanke']",2018-06-27T10:12:55Z,Computing the metric dimension by decomposing graphs into extended   biconnected components,"  A vertex set $U \subseteq V$ of an undirected graph $G=(V,E)$ is a $\textit{resolving set}$ for $G$, if for every two distinct vertices $u,v \in V$ there is a vertex $w \in U$ such that the distances between $u$ and $w$ and the distance between $v$ and $w$ are different. The $\textit{Metric Dimension}$ of $G$ is the size of a smallest resolving set for $G$. Deciding whether a given graph $G$ has Metric Dimension at most $k$ for some integer $k$ is well-known to be NP-complete. Many research has been done to understand the complexity of this problem on restricted graph classes. In this paper, we decompose a graph into its so called $\textit{extended biconnected components}$ and present an efficient algorithm for computing the metric dimension for a class of graphs having a minimum resolving set with a bounded number of vertices in every extended biconnected component. Further we show that the decision problem METRIC DIMENSION remains NP-complete when the above limitation is extended to usual biconnected components. ",1806.10389v1
"['Andreas Erlebach', 'Christian Thieme', 'Carolin Müller', 'Stefan Hoffmann', 'Thomas Höche', 'Christian Rüssel', 'Marek Sierka']",2024-05-29T00:00:47Z,Thermomechanical properties of zero thermal expansion materials from   theory and experiments,"  Origin and composition dependence of the anisotropic thermomechanical properties are elucidated for Ba1-xSrxZn2Si2O7 (BZS) solid solutions. The high-temperature phase of BZS shows negative thermal expansion (NTE) along one crystallographic axis and highly anisotropic elastic properties characterized by X-ray diffraction experiments and simulations at the density functional theory level. Ab initio molecular dynamics simulations provide accurate predictions of the anisotropic thermal expansion in excellent agreement with experimental observations. The NTE considerably decreases with increasing Sr content x. This is connected with the composition dependence of the vibrational density of states (VDOS) and the anisotropic Gr\""uneisen parameters. The VDOS shifts to higher frequencies between 0-5 THz due to substitution of Ba with Sr. In the same frequency range, vibrational modes contributing most to the NTE are found. In addition, phonon calculations using the quasi-harmonic approximation revealed that the NTE is mainly connected with deformation of four-membered rings formed by SiO4 and ZnO4 tetrahedra. The thermomechanical and vibrational properties obtained in this work provide the basis for future studies facilitating the targeted design of BZS solid solutions as zero or negative thermal expansion material. ",2405.19378v1
['Carsten Schultz'],2004-11-11T16:56:21Z,The cohomology ring of the complement of a finite family of linear   subspaces in a complex projective space,"  The integral cohomology ring of the complement of an arrangement of linear subspaces of a finite dimensional complex projective space is determined by combinatorial data, i.e. the intersection poset and the dimension function. ",math/0411263v2
['Carsten Schultz'],2005-07-17T16:12:31Z,"A short proof of w_1^n(Hom(C_{2r+1}, K_{n+2}))=0 for all n and a graph   colouring theorem by Babson and Kozlov","  We show that the n-th power of the first Stiefel-Whitney class of the Z_2-operation on the graph complex Hom(C_{2r+1},K_{n+2})$ is zero, confirming a conjecture by Babson and Kozlov. This proves the strong form of their graph colouring theorem, which they had only proven for odd n. Our proof is also considerably simpler than their proof of the weak form of the theorem, which is also known as the Lov\'asz conjecture. ",math/0507346v4
['Carsten Schultz'],2005-10-25T18:50:36Z,"Small models of graph colouring manifolds and the Stiefel manifolds   Hom(C_5, K_n)","  We show P\'eter Csorba's conjecture that the graph homomorphism complex Hom(C_5,K_{n+2}) is homeomorphic to a Stiefel manifold, the space of unit tangent vectors to the n-dimensional sphere. For this a general tool is developed that allows to replace the complexes Hom(G, K_n) by smaller complexes that are homeomorphic to them whenever G is a graph for which those complexes are manifolds. The equivariant version of Csorba's conjecture is proved up to homotopy.   We also study certain subdivisions of simplicial manifolds that are related to the interval poset of their face posets and their connection with geometric approximations to diagonal maps. ",math/0510535v2
['Carsten Schultz'],2010-06-02T19:38:35Z,Paths of homomorphisms from stable Kneser graphs,"  We denote by SG_{n,k} the stable Kneser graph (Schrijver graph) of stable n-subsets of a set of cardinality 2n+k. For k congruent 3 (mod 4) and n\ge2 we show that there is a component of the \chi-colouring graph of SG_{n,k} which is invariant under the action of the automorphism group of SG_{n,k}. We derive that there is a graph G with \chi(G)=\chi(SG_{n,k}) such that the complex Hom(SG_{n,k}, G) is non-empty and connected. In particular, for k congruent 3 (mod 4) and n\ge2 the graph SG_{n,k} is not a test graph. ",1006.0474v1
['Carsten Schultz'],2010-03-29T21:39:37Z,The equivariant topology of stable Kneser graphs,"  The stable Kneser graph $SG_{n,k}$, $n\ge1$, $k\ge0$, introduced by Schrijver \cite{schrijver}, is a vertex critical graph with chromatic number $k+2$, its vertices are certain subsets of a set of cardinality $m=2n+k$. Bj\""orner and de Longueville \cite{anders-mark} have shown that its box complex is homotopy equivalent to a sphere, $\Hom(K_2,SG_{n,k})\homot\Sphere^k$. The dihedral group $D_{2m}$ acts canonically on $SG_{n,k}$, the group $C_2$ with 2 elements acts on $K_2$. We almost determine the $(C_2\times D_{2m})$-homotopy type of $\Hom(K_2,SG_{n,k})$ and use this to prove the following results. The graphs $SG_{2s,4}$ are homotopy test graphs, i.e. for every graph $H$ and $r\ge0$ such that $\Hom(SG_{2s,4},H)$ is $(r-1)$-connected, the chromatic number $\chi(H)$ is at least $r+6$. If $k\notin\set{0,1,2,4,8}$ and $n\ge N(k)$ then $SG_{n,k}$ is not a homotopy test graph, i.e.\ there are a graph $G$ and an $r\ge1$ such that $\Hom(SG_{n,k}, G)$ is $(r-1)$-connected and $\chi(G)<r+k+2$. ",1003.5688v1
['Carsten Schultz'],2006-06-29T14:45:56Z,"Graph colourings, spaces of edges and spaces of circuits","  By Lovasz' proof of the Kneser conjecture, the chromatic number of a graph G is bounded from below by the index of the Z_2-space Hom(K_2,G) plus two. We show that the cohomological index of Hom(K_2,G) is also greater than the cohomological index of the Z_2-space Hom(C_{2r+1}, G) for r>0. This gives a new and simple proof of the strong form of the graph colouring theorem by Babson and Kozlov, which had been conjectured by Lovasz, and at the same time shows that it never gives a stronger bound than can be obtained by Hom(K_2, G). The proof extends ideas introduced by Zivaljevic in a previous elegant proof of a special case. We then generalise the arguments and obtain conditions under which corresponding results hold for other graphs in place of C_{2r+1}. This enables us to find an infinite family of test graphs of chromatic number 4 among the Kneser graphs.   Our main new result is a description of the Z_2-homotopy type of the direct limit of the system of all the spaces Hom(C_{2r+1}, G) in terms of the Z_2-homotopy type of Hom(K_2, G). A corollary is that the coindex of Hom(K_2, G) does not exceed the coindex of Hom(C_{2r+1}, G) by more then one if r is chosen sufficiently large. Thus the graph colouring bound in the theorem by Babson & Kozlov is also never weaker than that from Lovasz' proof of the Kneser conjecture. ",math/0606763v1
"['Anton Dochtermann', 'Carsten Schultz']",2009-07-29T09:37:31Z,Topology of Hom complexes and test graphs for bounding chromatic number,"  We introduce new methods for understanding the topology of $\Hom$ complexes (spaces of homomorphisms between two graphs), mostly in the context of group actions on graphs and posets. We view $\Hom(T,-)$ and $\Hom(-,G)$ as functors from graphs to posets, and introduce a functor $(-)^1$ from posets to graphs obtained by taking atoms as vertices. Our main structural results establish useful interpretations of the equivariant homotopy type of $\Hom$ complexes in terms of spaces of equivariant poset maps and $\Gamma$-twisted products of spaces. When $P = F(X)$ is the face poset of a simplicial complex $X$, this provides a useful way to control the topology of $\Hom$ complexes.   Our foremost application of these results is the construction of new families of `test graphs' with arbitrarily large chromatic number - graphs $T$ with the property that the connectivity of $\Hom(T,G)$ provides the best possible lower bound on the chromatic number of $G$. In particular we focus on two infinite families, which we view as higher dimensional analogues of odd cycles. The family of `spherical graphs' have connections to the notion of homomorphism duality, whereas the family of `twisted toroidal graphs' lead us to establish a weakened version of a conjecture (due to Lov\'{a}sz) relating topological lower bounds on chromatic number to maximum degree. Other structural results allow us to show that any finite simplicial complex $X$ with a free action by the symmetric group $S_n$ can be approximated up to $S_n$-homotopy equivalence as $\Hom(K_n,G)$ for some graph $G$; this is a generalization of a result of Csorba. We conclude the paper with some discussion regarding the underlying categorical notions involved in our study. ",0907.5079v2
"['Bernhard Hanke', 'Raman Sanyal', 'Carsten Schultz', 'Günter M. Ziegler']",2007-09-29T12:36:34Z,Combinatorial Stokes formulas via minimal resolutions,"  We describe an explicit chain map from the standard resolution to the minimal resolution for the finite cyclic group Z_k of order k. We then demonstrate how such a chain map induces a ""Z_k-combinatorial Stokes theorem"", which in turn implies ""Dold's theorem"" that there is no equivariant map from an n-connected to an n-dimensional free Z_k-complex.   Thus we build a combinatorial access road to problems in combinatorics and discrete geometry that have previously been treated with methods from equivariant topology. The special case k=2 for this is classical; it involves Tucker's (1949) combinatorial lemma which implies the Borsuk-Ulam theorem, its proof via chain complexes by Lefschetz (1949), the combinatorial Stokes formula of Fan (1967), and Meunier's work (2006). ",0710.0050v1
"['Yulia Anoshkina', 'Marc Goerigk', 'Frank Meisel']",2020-09-09T15:01:25Z,Robust Optimization Approaches for Routing and Scheduling of   Multi-Skilled Teams under Uncertain Job Skill Requirements,"  We consider a combined problem of teaming and scheduling of multi-skilled employees that have to perform jobs with uncertain qualification requirements. We propose two modeling approaches that generate solutions that are robust to possible data variations. Both approaches use variants of budgeted uncertainty, where deviations in qualification requirements are bounded by a constraint. In the first approach, we aggregate uncertain constraints to ensure that the total number of job qualifications present at a job is not less than a worst-case value. We show that these values can be computed beforehand, resulting in a robust model with little additional complexity compared with the nominal model. In our second approach, we bound the overall qualification deviation over all jobs. While this approach is more complex, we show that it is still possible to derive a compact problem formulation by using a linear programming formulation for the adversarial problem based on a dynamic program. The performance of both approaches is analyzed on a test bed of instances which were originally provided for a deterministic problem version. Our experiments show the effectiveness of the proposed approaches in the presence of data uncertainty and reveal the price and gain of robustness. ",2009.04342v2
"['Ribana Roscher', 'Lukas Roth', 'Cyrill Stachniss', 'Achim Walter']",2023-12-06T11:38:26Z,Data-Centric Digital Agriculture: A Perspective,"  In response to the increasing global demand for food, feed, fiber, and fuel, digital agriculture is rapidly evolving to meet these demands while reducing environmental impact. This evolution involves incorporating data science, machine learning, sensor technologies, robotics, and new management strategies to establish a more sustainable agricultural framework. So far, machine learning research in digital agriculture has predominantly focused on model-centric approaches, focusing on model design and evaluation. These efforts aim to optimize model accuracy and efficiency, often treating data as a static benchmark. Despite the availability of agricultural data and methodological advancements, a saturation point has been reached, with many established machine learning methods achieving comparable levels of accuracy and facing similar limitations. To fully realize the potential of digital agriculture, it is crucial to have a comprehensive understanding of the role of data in the field and to adopt data-centric machine learning. This involves developing strategies to acquire and curate valuable data and implementing effective learning and evaluation strategies that utilize the intrinsic value of data. This approach has the potential to create accurate, generalizable, and adaptable machine learning methods that effectively and sustainably address agricultural tasks such as yield prediction, weed detection, and early disease identification ",2312.03437v1
"['Inkyu Sa', 'Marija Popovic', 'Raghav Khanna', 'Zetao Chen', 'Philipp Lottes', 'Frank Liebisch', 'Juan Nieto', 'Cyrill Stachniss', 'Achim Walter', 'Roland Siegwart']",2018-07-31T22:45:11Z,WeedMap: A large-scale semantic weed mapping framework using aerial   multispectral imaging and deep neural network for precision farming,"  We present a novel weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics. ",1808.00100v2
"['Alberto Pretto', 'Stéphanie Aravecchia', 'Wolfram Burgard', 'Nived Chebrolu', 'Christian Dornhege', 'Tillmann Falck', 'Freya Fleckenstein', 'Alessandra Fontenla', 'Marco Imperoli', 'Raghav Khanna', 'Frank Liebisch', 'Philipp Lottes', 'Andres Milioto', 'Daniele Nardi', 'Sandro Nardi', 'Johannes Pfeifer', 'Marija Popović', 'Ciro Potena', 'Cédric Pradalier', 'Elisa Rothacker-Feder', 'Inkyu Sa', 'Alexander Schaefer', 'Roland Siegwart', 'Cyrill Stachniss', 'Achim Walter', 'Wera Winterhalter', 'Xiaolong Wu', 'Juan Nieto']",2019-11-08T07:19:47Z,Building an Aerial-Ground Robotics System for Precision Farming: An   Adaptable Solution,"  The application of autonomous robots in agriculture is gaining increasing popularity thanks to the high impact it may have on food security, sustainability, resource use efficiency, reduction of chemical treatments, and the optimization of human effort and yield. With this vision, the Flourish research project aimed to develop an adaptable robotic solution for precision farming that combines the aerial survey capabilities of small autonomous unmanned aerial vehicles (UAVs) with targeted intervention performed by multi-purpose unmanned ground vehicles (UGVs). This paper presents an overview of the scientific and technological advances and outcomes obtained in the project. We introduce multi-spectral perception algorithms and aerial and ground-based systems developed for monitoring crop density, weed pressure, crop nitrogen nutrition status, and to accurately classify and locate weeds. We then introduce the navigation and mapping systems tailored to our robots in the agricultural environment, as well as the modules for collaborative mapping. We finally present the ground intervention hardware, software solutions, and interfaces we implemented and tested in different field conditions and with different crops. We describe a real use case in which a UAV collaborates with a UGV to monitor the field and to perform selective spraying without human intervention. ",1911.03098v4
"['Leif Feddersen', 'Catherine Cleophas']",2024-04-05T12:54:09Z,Hierarchical Neural Additive Models for Interpretable Demand Forecasts,"  Demand forecasts are the crucial basis for numerous business decisions, ranging from inventory management to strategic facility planning. While machine learning (ML) approaches offer accuracy gains, their interpretability and acceptance are notoriously lacking. Addressing this dilemma, we introduce Hierarchical Neural Additive Models for time series (HNAM). HNAM expands upon Neural Additive Models (NAM) by introducing a time-series specific additive model with a level and interacting covariate components.   Covariate interactions are only allowed according to a user-specified interaction hierarchy. For example, weekday effects may be estimated independently of other covariates, whereas a holiday effect may depend on the weekday and an additional promotion may depend on both former covariates that are lower in the interaction hierarchy.   Thereby, HNAM yields an intuitive forecasting interface in which analysts can observe the contribution for each known covariate. We evaluate the proposed approach and benchmark its performance against other state-of-the-art machine learning and statistical models extensively on real-world retail data. The results reveal that HNAM offers competitive prediction performance whilst providing plausible explanations. ",2404.04070v1
"['Martin Comis', 'Catherine Cleophas', 'Christina Büsing']",2019-10-24T11:03:23Z,"Patients, Primary Care, and Policy: Simulation Modeling for Health Care   Decision Support","  Demand for health care is constantly increasing due to the ongoing demographic change, while at the same time health service providers face difficulties in finding skilled personnel. This creates pressure on health care systems around the world, such that the efficient, nationwide provision of primary health care has become one of society's greatest challenges. Due to the complexity of health care systems, unforeseen future events, and a frequent lack of data, analyzing and optimizing the performance of health care systems means tackling a wicked problem. To support this task for primary care, this paper introduces the hybrid agent-based simulation model SiM-Care. SiM-Care models the interactions of patients and primary care physicians on an individual level. By tracking agent interactions, it enables modelers to assess multiple key indicators such as patient waiting times and physician utilization. Based on these indicators, primary care systems can be assessed and compared. Moreover, changes in the infrastructure, patient behavior, and service design can be directly evaluated. To showcase the opportunities offered by SiM-Care and aid model validation, we present a case study for a primary care system in Germany. Specifically, we investigate the effects of an aging population, a decrease in the number of physicians, as well as the combined effects. ",1910.11027v2
"['Nicola Rennie', 'Catherine Cleophas', 'Adam M. Sykulski', 'Florian Dost']",2022-04-12T23:09:00Z,Analysing and visualising bike-sharing demand with outliers,"  Bike-sharing is a popular component of sustainable urban mobility. It requires anticipatory planning, e.g. of station locations and inventory, to balance expected demand and capacity. However, external factors such as extreme weather or glitches in public transport, can cause demand to deviate from baseline levels. Identifying such outliers keeps historic data reliable and improves forecasts. In this paper we show how outliers can be identified by clustering stations and applying a functional depth analysis. We apply our analysis techniques to the Washington D.C. Capital Bikeshare data set as the running example throughout the paper, but our methodology is general by design. Furthermore, we offer an array of meaningful visualisations to communicate findings and highlight patterns in demand. Last but not least, we formulate managerial recommendations on how to use both the demand forecast and the identified outliers in the bike-sharing planning process. ",2204.06112v2
"['Claudia Ehrig', 'Benedikt Sonnleitner', 'Ursula Neumann', 'Catherine Cleophas', 'Germain Forestier']",2024-04-09T10:41:59Z,The impact of data set similarity and diversity on transfer learning   success in time series forecasting,"  Pre-trained models have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning. While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures to explain which characteristics of source and target data lead to transfer learning success. Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We investigate these dynamics using pre-trained neural networks across five public source datasets, applied to forecasting five target data sets, including real-world wholesales data. We identify two feature-based similarity and diversity measures, finding that source-target similarity reduces forecasting bias, while source diversity improves forecasting accuracy and uncertainty estimation, but increases the bias. ",2404.06198v2
"['Nicola Rennie', 'Catherine Cleophas', 'Adam M. Sykulski', 'Florian Dost']",2019-12-12T14:20:10Z,Identifying and Responding to Outlier Demand in Revenue Management,"  Revenue management strongly relies on accurate forecasts. Thus, when extraordinary events cause outlier demand, revenue management systems need to recognise this and adapt both forecast and controls. Many passenger transport service providers, such as railways and airlines, control the sale of tickets through revenue management. State-of-the-art systems in these industries rely on analyst expertise to identify outlier demand both online (within the booking horizon) and offline (in hindsight). So far, little research focuses on automating and evaluating the detection of outlier demand in this context. To remedy this, we propose a novel approach, which detects outliers using functional data analysis in combination with time series extrapolation. We evaluate the approach in a simulation framework, which generates outliers by varying the demand model. The results show that functional outlier detection yields better detection rates than alternative approaches for both online and offline analyses. Depending on the category of outliers, extrapolation further increases online detection performance. We also apply the procedure to a set of empirical data to demonstrate its practical implications. By evaluating the full feedback-driven system of forecast and optimisation, we generate insight on the asymmetric effects of positive and negative demand outliers. We show that identifying instances of outlier demand and adjusting the forecast in a timely fashion substantially increases revenue compared to what is earned when ignoring outliers. ",1912.05974v3
"['Nicola Rennie', 'Catherine Cleophas', 'Adam M. Sykulski', 'Florian Dost']",2021-04-09T13:40:45Z,Outlier detection in network revenue management,"  This paper presents an automated approach for providing ranked lists of outliers in observed demand to support analysts in network revenue management. Such network revenue management, e.g. for railway itineraries, needs accurate demand forecasts. However, demand outliers across or in parts of a network complicate accurate demand forecasting, and the network structure makes such demand outliers hard to detect.   We propose a two-step approach combining clustering with functional outlier detection to identify outlying demand from network bookings observed on the leg level. The first step clusters legs to appropriately partition and pool booking patterns. The second step identifies outliers within each cluster and uses a novel aggregation method across legs to create a ranked alert list of affected instances. Our method outperforms analyses that consider leg data without regard for network implications and offers a computationally efficient alternative to storing and analysing all data on the itinerary level, especially in highly-connected networks where most customers book multi-leg products. A simulation study demonstrates the robustness of the approach and quantifies the potential revenue benefits from adjusting demand forecasts for offer optimisation. Finally, we illustrate the applicability based on empirical data obtained from Deutsche Bahn. ",2104.04157v3
"['Florian Becker', 'Katharina Rauthmann', 'Lutz Pauli', 'Philipp Knechtges']",2023-08-18T08:51:31Z,An Eigenvalue-Free Implementation of the Log-Conformation Formulation,"  The log-conformation formulation, although highly successful, was from the beginning formulated as a partial differential equation that contains an, for PDEs unusual, eigenvalue decomposition of the unknown field. To this day, most numerical implementations have been based on this or a similar eigenvalue decomposition, with Knechtges et al. (2014) being the only notable exception for two-dimensional flows.   In this paper, we present an eigenvalue-free algorithm to compute the constitutive equation of the log-conformation formulation that works for two- and three-dimensional flows. Therefore, we first prove that the challenging terms in the constitutive equations are representable as a matrix function of a slightly modified matrix of the log-conformation field. We give a proof of equivalence of this term to the more common log-conformation formulations. Based on this formulation, we develop an eigenvalue-free algorithm to evaluate this matrix function. The resulting full formulation is first discretized using a finite volume method, and then tested on the confined cylinder and sedimenting sphere benchmarks. ",2308.09394v2
"['Florian Becker', 'Age K. Smilde', 'Evrim Acar']",2022-09-01T09:47:27Z,Unsupervised EHR-based Phenotyping via Matrix and Tensor Decompositions,"  Computational phenotyping allows for unsupervised discovery of subgroups of patients as well as corresponding co-occurring medical conditions from electronic health records (EHR). Typically, EHR data contains demographic information, diagnoses and laboratory results. Discovering (novel) phenotypes has the potential to be of prognostic and therapeutic value. Providing medical practitioners with transparent and interpretable results is an important requirement and an essential part for advancing precision medicine. Low-rank data approximation methods such as matrix (e.g., non-negative matrix factorization) and tensor decompositions (e.g., CANDECOMP/PARAFAC) have demonstrated that they can provide such transparent and interpretable insights. Recent developments have adapted low-rank data approximation methods by incorporating different constraints and regularizations that facilitate interpretability further. In addition, they offer solutions for common challenges within EHR data such as high dimensionality, data sparsity and incompleteness. Especially extracting temporal phenotypes from longitudinal EHR has received much attention in recent years. In this paper, we provide a comprehensive review of low-rank approximation-based approaches for computational phenotyping. The existing literature is categorized into temporal vs. static phenotyping approaches based on matrix vs. tensor decompositions. Furthermore, we outline different approaches for the validation of phenotypes, i.e., the assessment of clinical significance. ",2209.00322v1
"['Frank Lenzen', 'Jan Lellmann', 'Florian Becker', 'Christoph Schnörr']",2014-07-03T13:44:34Z,Solving QVIs for Image Restoration with Adaptive Constraint Sets,"  We consider a class of quasi-variational inequalities (QVIs) for adaptive image restoration, where the adaptivity is described via solution-dependent constraint sets. In previous work we studied both theoretical and numerical issues. While we were able to show the existence of solutions for a relatively broad class of problems, we encountered problems concerning uniqueness of the solution as well as convergence of existing algorithms for solving QVIs. In particular, it seemed that with increasing image size the growing condition number of the involved differential operator poses severe problems. In the present paper we prove uniqueness for a larger class of problems and in particular independent of the image size. Moreover, we provide a numerical algorithm with proved convergence. Experimental results support our theoretical findings. ",1407.0921v1
"['Johannes Gasteiger', 'Florian Becker', 'Stephan Günnemann']",2021-06-02T15:44:55Z,GemNet: Universal Directional Graph Neural Networks for Molecules,"  Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with spherical representations are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then discretize such GNNs via directed edge embeddings and two-hop message passing, and incorporate multiple structural improvements to arrive at the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%, respectively, and performs especially well on the most challenging molecules. Our implementation is available online. ",2106.08903v10
"['Johannes Berger', 'Frank Lenzen', 'Florian Becker', 'Andreas Neufeld', 'Christoph Schnörr']",2015-07-24T10:56:36Z,Second-Order Recursive Filtering on the Rigid-Motion Lie Group SE(3)   Based on Nonlinear Observations,"  Camera motion estimation from observed scene features is an important task in image processing to increase the accuracy of many methods, e.g. optical flow and structure-from-motion. Due to the curved geometry of the state space SE(3) and the non-linear relation to the observed optical flow, many recent filtering approaches use a first-order approximation and assume a Gaussian a posteriori distribution or restrict the state to Euclidean geometry. The physical model is usually also limited to uniform motions.   We propose a second-order minimum energy filter with a generalized kinematic model that copes with the full geometry of SE(3) as well as with the nonlinear dependencies between the state space and observations. The derived filter enables reconstructing motions correctly for synthetic and real scenes, e.g. from the KITTI benchmark. Our experiments confirm that the derived minimum energy filter with higher-order state differential equation copes with higher-order kinematics and is also able to minimize model noise. We also show that the proposed filter is superior to state-of-the-art extended Kalman filters on Lie groups in the case of linear observations and that our method reaches the accuracy of modern visual odometry methods. ",1507.06810v1
"['Johannes Berger', 'Andreas Neufeld', 'Florian Becker', 'Frank Lenzen', 'Christoph Schnörr']",2015-02-27T14:14:15Z,Second Order Minimum Energy Filtering on $\operatorname{SE}_3$ with   Nonlinear Measurement Equations,"  Accurate camera motion estimation is a fundamental building block for many Computer Vision algorithms. For improved robustness, temporal consistency of translational and rotational camera velocity is often assumed by propagating motion information forward using stochastic filters. Classical stochastic filters, however, use linear approximations for the non-linear observer model and for the non-linear structure of the underlying Lie Group $\operatorname{SE}_3$ and have to approximate the unknown posteriori distribution. In this paper we employ a non-linear measurement model for the camera motion estimation problem that incorporates multiple observation equations. We solve the underlying filtering problem using a novel Minimum Energy Filter on $\operatorname{SE}_3$ and give explicit expressions for the optimal state variables. Experiments on the challenging KITTI benchmark show that, although a simple motion model is only employed, our approach improves rotational velocity estimation and otherwise is on par with the state-of-the-art. ",1502.07903v1
"['Jan Brabec', 'Filip Šrajer', 'Radek Starosta', 'Tomáš Sixta', 'Marc Dupont', 'Miloš Lenoch', 'Jiří Menšík', 'Florian Becker', 'Jakub Boros', 'Tomáš Pop', 'Pavel Novák']",2023-08-21T15:06:02Z,A Modular and Adaptive System for Business Email Compromise Detection,"  The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide. The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic. Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together. We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years. Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context. This decomposition makes CAPE's verdicts naturally explainable. In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge. Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures. ",2308.10776v1
"['Klaus Jansen', 'Lars Rohwedder']",2017-01-25T08:55:14Z,A Quasi-Polynomial Approximation for the Restricted Assignment Problem,"  The Restricted Assignment Problem is a prominent special case of Scheduling on Parallel Unrelated Machines. For the strongest known linear programming relaxation, the configuration LP, we improve the non-constructive bound on its integrality gap from 1.9142 to 1.8334 and significantly simplify the proof. Then we give a constructive variant, yielding a 1.8334-approximation in quasi-polynomial time. This is the first quasi-polynomial algorithm for this problem improving on the long-standing approximation rate of 2. ",1701.07208v2
"['Klaus Jansen', 'Malin Rau']",2021-07-04T12:58:45Z,Closing the gap for single resource constraint scheduling,"  In the problem called single resource constraint scheduling, we are given $m$ identical machines and a set of jobs, each needing one machine to be processed as well as a share of a limited renewable resource $R$. A schedule of these jobs is feasible if, at each point in the schedule, the number of machines and resources required by jobs processed at this time is not exceeded. It is NP-hard to approximate this problem with a ratio better than $3/2$. On the other hand, the best algorithm so far has an absolute approximation ratio of $2+\varepsilon$. This paper presents an algorithm with absolute approximation ratio~$(3/2+\varepsilon)$, which closes the gap between inapproximability and best algorithm except for a negligible small~$\varepsilon$. ",2107.01613v1
"['Klaus Jansen', 'Lars Rohwedder']",2018-07-10T13:32:12Z,A note on the integrality gap of the configuration LP for restricted   Santa Claus,"  In the restricted Santa Claus problem we are given resources $\mathcal R$ and players $\mathcal P$. Every resource $j\in\mathcal R$ has a value $v_j$ and every player $i$ desires a set $\mathcal R(i)$ of resources. We are interested in distributing the resources to players that desire them. The quality of a solution is measured by the least happy player, i.e., the lowest sum of resource values. This value should be maximized. The local search algorithm by Asadpour et al. and its connection to the configuration LP has proved itself to be a very influential technique for this and related problems. In the original proof, a local search was used to obtain a bound of $4$ for the ratio of the fractional to the integral optimum of the configuration LP (integrality gap). This bound is non-constructive since the local search has not been shown to terminate in polynomial time. On the negative side, the worst instance known has an integrality gap of $2$. Although much progress was made in this area, neither bound has been improved since. We present a better analysis that shows the integrality gap is not worse than $3 + 5/6 \approx 3.8333$. ",1807.03626v1
"['Klaus Jansen', 'Malin Rau']",2016-10-14T12:29:31Z,Improved approximation for two dimensional strip packing with polynomial   bounded width,"  We study the well-known two-dimensional strip packing problem. Given is a set of rectangular axis-parallel items and a strip of width $W$ with infinite height. The objective is to find a packing of these items into the strip, which minimizes the packing height. Lately, it has been shown that the lower bound of $3/2$ of the absolute approximation ratio can be beaten when we allow a pseudo-polynomial running-time of type $(n W)^{f(1/\varepsilon)}$. If $W$ is polynomially bounded by the number of items, this is a polynomial running-time. We present a pseudo-polynomial algorithm with approximation ratio $4/3 +\varepsilon$ and running time $(n W)^{1/\varepsilon^{\mathcal{O}(2^{1/\varepsilon})}}$. ",1610.04430v2
"['Klaus Jansen', 'Felix Land']",2017-10-31T20:47:39Z,Scheduling Monotone Moldable Jobs in Linear Time,"  A moldable job is a job that can be executed on an arbitrary number of processors, and whose processing time depends on the number of processors allotted to it. A moldable job is monotone if its work doesn't decrease for an increasing number of allotted processors. We consider the problem of scheduling monotone moldable jobs to minimize the makespan.   We argue that for certain compact input encodings a polynomial algorithm has a running time polynomial in n and log(m), where n is the number of jobs and m is the number of machines. We describe how monotony of jobs can be used to counteract the increased problem complexity that arises from compact encodings, and give tight bounds on the approximability of the problem with compact encoding: it is NP-hard to solve optimally, but admits a PTAS.   The main focus of this work are efficient approximation algorithms. We describe different techniques to exploit the monotony of the jobs for better running times, and present a (3/2+{\epsilon})-approximate algorithm whose running time is polynomial in log(m) and 1/{\epsilon}, and only linear in the number n of jobs. ",1711.00103v2
"['Klaus Jansen', 'Lars Rohwedder']",2018-03-13T12:12:36Z,"On Integer Programming, Discrepancy, and Convolution","  Integer programs with m constraints are solvable in pseudo-polynomial time in $\Delta$, the largest coefficient in a constraint, when m is a fixed constant. We give a new algorithm with a running time of $O(\sqrt{m}\Delta)^{2m} + O(nm)$, which improves on the state-of-the-art. Moreover, we show that improving on our algorithm for any $m$ is equivalent to improving over the quadratic time algorithm for $(\min,~+)$-convolution. This is a strong evidence that our algorithm's running time is the best possible. We also present a specialized algorithm with running time $O(\sqrt{m} \Delta)^{(1 + o(1))m} + O(nm)$ for testing feasibility of an integer program and also give a tight lower bound, which is based on the SETH in this case. ",1803.04744v4
"['Klaus Jansen', 'Lars Rohwedder']",2016-11-07T08:35:47Z,On the Configuration-LP of the Restricted Assignment Problem,"  We consider the classical problem of Scheduling on Unrelated Machines. In this problem a set of jobs is to be distributed among a set of machines and the maximum load (makespan) is to be minimized. The processing time $p_{ij}$ of a job $j$ depends on the machine $i$ it is assigned to. Lenstra, Shmoys and Tardos gave a polynomial time $2$-approximation for this problem. In this paper we focus on a prominent special case, the Restricted Assignment problem, in which $p_{ij}\in\{p_j,\infty\}$. The configuration-LP is a linear programming relaxation for the Restricted Assignment problem. It was shown by Svensson that the multiplicative gap between integral and fractional solution, the integrality gap, is at most $2 - 1/17 \approx 1.9412$. In this paper we significantly simplify his proof and achieve a bound of $2 - 1/6 \approx 1.8333$. As a direct consequence this provides a polynomial $(2 - 1/6 + \epsilon)$-estimation algorithm for the Restricted Assignment problem by approximating the configuration-LP. The best lower bound known for the integrality gap is $1.5$ and no estimation algorithm with a guarantee better than $1.5$ exists unless $\mathrm{P} = \mathrm{NP}$. ",1611.01934v2
"['Klaus Jansen', 'Malin Rau']",2017-12-13T18:48:39Z,Closing the gap for pseudo-polynomial strip packing,"  The set of 2-dimensional packing problems builds an important class of optimization problems and Strip Packing together with 2-dimensional Bin Packing and 2-dimensional Knapsack is one of the most famous of these problems. Given a set of rectangular axis parallel items and a strip with bounded width and infinite height the objective is to find a packing of the items into the strip which minimizes the packing height. We speak of pseudo-polynomial Strip Packing if we consider algorithms with pseudo-polynomial running time with respect to the width of the strip.   It is known that there is no pseudo-polynomial algorithm for Strip Packing with a ratio better than $5/4$ unless $\mathrm{P} = \mathrm{NP}$. The best algorithm so far has a ratio of $(4/3 + \varepsilon)$. In this paper, we close this gap between inapproximability result and best known algorithm by presenting an algorithm with approximation ratio $(5/4 + \varepsilon)$ and thus categorize the problem accurately. The algorithm uses a structural result which states that each optimal solution can be transformed such that it has one of a polynomial number of different forms. The strength of this structural result is that it applies to other problem settings as well for example to Strip Packing with rotations (90 degrees) and Contiguous Moldable Task Scheduling. This fact enabled us to present algorithms with approximation ratio $(5/4 + \varepsilon)$ for these problems as well. ",1712.04922v2
"['Klaus Jansen', 'Malin Rau']",2019-02-09T14:25:23Z,Linear Time Algorithms for Multiple Cluster Scheduling and Multiple   Strip Packing,"  We study the Multiple Cluster Scheduling problem and the Multiple Strip Packing problem. For both problems, there is no algorithm with approximation ratio better than $2$ unless $P = NP$. In this paper, we present an algorithm with approximation ratio $2$ and running time $O(n)$ for both problems. While a $2$ approximation was known before, the running time of the algorithm is at least $\Omega(n^{256})$ in the worst case. Therefore, an $O(n)$ algorithm is surprising and the best possible. We archive this result by calling an AEPTAS with approximation guarantee $(1+\varepsilon)OPT +p_{\max}$ and running time of the form $O(n\log(1/\varepsilon)+ f(1/\varepsilon))$ with a constant $\varepsilon$ to schedule the jobs on a single cluster. This schedule is then distributed on the $N$ clusters in $O(n)$. Moreover, this distribution technique can be applied to any variant of of Multi Cluster Scheduling for which there exists an AEPTAS with additive term $p_{\max}$.   While the above result is strong from a theoretical point of view, it might not be very practical due to a large hidden constant caused by calling an AEPTAS with a constant $\varepsilon \geq 1/8$ as subroutine. Nevertheless, we point out that the general approach of finding first a schedule on one cluster and then distributing it onto the other clusters might come in handy in practical approaches. We demonstrate this by presenting a practical algorithm with running time $O(n\log(n))$, with out hidden constants, that is a $9/4$-approximation for one third of all possible instances, i.e, all instances where the number of clusters is dividable by $3$, and has an approximation ratio of at most $2.3$ for all instances with at least $9$ clusters. ",1902.03428v1
"['Klaus Jansen', 'Lars Rohwedder']",2018-11-02T16:08:29Z,Local search breaks 1.75 for Graph Balancing,"  Graph Balancing is the problem of orienting the edges of a weighted multigraph so as to minimize the maximum weighted in-degree. Since the introduction of the problem the best algorithm known achieves an approximation ratio of $1.75$ and it is based on rounding a linear program with this exact integrality gap. It is also known that there is no $(1.5 - \epsilon)$-approximation algorithm, unless $\mathrm{P}=\mathrm{NP}$. Can we do better than $1.75$? We prove that a different LP formulation, the configuration LP, has a strictly smaller integrality gap. Graph Balancing was the last one in a group of related problems from literature, for which it was open whether the configuration LP is stronger than previous, simple LP relaxations. We base our proof on a local search approach that has been applied successfully to the more general Restricted Assignment problem, which in turn is a prominent special case of makespan minimization on unrelated machines. With a number of technical novelties we are able to obtain a bound of $1.749$ for the case of Graph Balancing. It is not clear whether the local search algorithm we present terminates in polynomial time, which means that the bound is non-constructive. However, it is a strong evidence that a better approximation algorithm is possible using the configuration LP and it allows the optimum to be estimated within a factor better than $1.75$. A particularly interesting aspect of our techniques is the way we handle small edges in the local search. We manage to exploit the configuration constraints enforced on small edges in the LP. This may be of interest to other problems such as Restricted Assignment as well. ",1811.00955v1
"['Hauke Brinkop', 'Klaus Jansen']",2021-01-03T20:00:51Z,Solving Cut-Problems in Quadratic Time for Graphs With Bounded Treewidth,"  In the problem (Unweighted) Max-Cut we are given a graph $G = (V,E)$ and asked for a set $S \subseteq V$ such that the number of edges from $S$ to $V \setminus S$ is maximal. In this paper we consider an even harder problem: (Weighted) Max-Bisection. Here we are given an undirected graph $G = (V,E)$ and a weight function $w \colon E \to \mathbb Q_{>0}$ and the task is to find a set $S \subseteq V$ such that (i) the sum of the weights of edges from $S$ is maximal; and (ii) $S$ contains $\left\lceil{\frac{n}{2}}\right\rceil$ vertices (where $n = \lvert V\rvert$). We design a framework that allows to solve this problem in time $\mathcal O(2^t n^2)$ if a tree decomposition of width $t$ is given as part of the input. This improves the previously best running time for Max-Bisection of [DBLP:journals/tcs/HanakaKS21] by a factor $t^2$. Under common hardness assumptions, neither the dependence on $t$ in the exponent nor the dependence on $n$ can be reduced [DBLP:journals/tcs/HanakaKS21,DBLP:journals/jcss/EibenLM21,DBLP:journals/talg/LokshtanovMS18]. Our framework can be applied to other cut problems like Min-Edge-Expansion, Sparsest-Cut, Densest-Cut, $\beta$-Balanced-Min-Cut, and Min-Bisection. It also works in the setting with arbitrary weights and directed edges. ",2101.00694v3
"['Hauke Brinkop', 'Klaus Jansen']",2022-03-03T14:33:45Z,High Multiplicity Scheduling on Uniform Machines in FPT-Time,"  In high-multiplicity scheduling, jobs of the same size are encoded in an efficient way, that is, for each size the number of jobs of that size is given instead of a list of jobs. Similarly, machines are encoded. We consider scheduling on uniform machines where a job of size $p_j$ takes time $p_j/s_i$ on a machine of speed $s_i$. Classical (NP-hard) objectives are Makespan minimization ($C_{\max}$) and Santa Claus ($C_{\min}$). We show that both objectives can be solved in time $\mathcal O( p_{\max}^{\mathcal O(d^2)} \operatorname {poly} |I| )$ where $p_{\max}$ is the largest jobs size, $d$ the number of different job sizes and $|I|$ the encoding length of the instance. Our approach incorporates two structural theorems: The first allows us to replace machines of large speed by multiple machines of smaller speed. The second tells us that some fractional assignments can be used to reduce the instance significantly. Using only the first theorem, we show some additional results. For the problem Envy Minimization ($C_{\mathit{envy}}$), we propose an $\mathcal O(s_{\max} \cdot p_{\max}^{\mathcal O(d^3)} \operatorname{poly} |I|)$ time algorithm (where $s_{\max}$ is the largest speed). For $C_{\max}$ and $C_{\min}$ in the Restricted Assignment setting, we give an $\mathcal O( (d p_{\max})^{\mathcal O(d^3)} \operatorname{poly} |I|)$ time algorithm. As far as we know, those running times are better than the running times of the algorithms known until today. ",2203.01741v1
"['Klaus Jansen', 'Marten Maack']",2017-01-12T08:12:36Z,An EPTAS for Scheduling on Unrelated Machines of Few Different Types,"  In the classical problem of scheduling on unrelated parallel machines, a set of jobs has to be assigned to a set of machines. The jobs have a processing time depending on the machine and the goal is to minimize the makespan, that is the maximum machine load. It is well known that this problem is NP-hard and does not allow polynomial time approximation algorithms with approximation guarantees smaller than $1.5$ unless P$=$NP. We consider the case that there are only a constant number $K$ of machine types. Two machines have the same type if all jobs have the same processing time for them. This variant of the problem is strongly NP-hard already for $K=1$. We present an efficient polynomial time approximation scheme (EPTAS) for the problem, that is, for any $\varepsilon > 0$ an assignment with makespan of length at most $(1+\varepsilon)$ times the optimum can be found in polynomial time in the input length and the exponent is independent of $1/\varepsilon$. In particular we achieve a running time of $2^{\mathcal{O}(K\log(K) \frac{1}{\varepsilon}\log^4 \frac{1}{\varepsilon})}+\mathrm{poly}(|I|)$, where $|I|$ denotes the input length. Furthermore, we study three other problem variants and present an EPTAS for each of them: The Santa Claus problem, where the minimum machine load has to be maximized; the case of scheduling on unrelated parallel machines with a constant number of uniform types, where machines of the same type behave like uniformly related machines; and the multidimensional vector scheduling variant of the problem where both the dimension and the number of machine types are constant. For the Santa Claus problem we achieve the same running time. The results are achieved, using mixed integer linear programming and rounding techniques. ",1701.03263v2
"['Klaus Jansen', 'Kai Kahler']",2022-02-16T08:49:50Z,On the Complexity of Scheduling Problems With a Fixed Number of Parallel   Identical Machines,"  In parallel machine scheduling, we are given a set of jobs, together with a number of machines and our goal is to decide for each job, when and on which machine(s) it should be scheduled in order to minimize some objective function. Different machine models, job characteristics and objective functions result in a multitude of scheduling problems and many of them are NP-hard, even for a fixed number of identical machines. In this work, we give conditional running time lower bounds for a large number of scheduling problems, indicating the optimality of some classical algorithms. Most notably, we show that the algorithm by Lawler and Moore for $1||\sum w_jU_j$ and $Pm||C_{max}$, as well as the algorithm by Lee and Uzsoy for $P2||\sum w_jC_j$ are probably optimal. There is still small room for improvement for the $1|Rej\leq Q|\sum w_jU_j$ algorithm by Zhang et al., the algorithm for $1||\sum T_j$ by Lawler and the FPTAS for $1||\sum w_jU_j$ by Gens and Levner. We also give a lower bound for $P2|any|C_{max}$ and improve the dynamic program by Du and Leung from $\mathcal{O}(nP^2)$ to $\mathcal{O}(nP)$, matching this new lower bound. Here, $P$ is the sum of all processing times. The same idea also improves the algorithm for $P3|any|C_{max}$ by Du and Leung from $\mathcal{O}(nP^5)$ to $\mathcal{O}(nP^2)$. While our results suggest the optimality of some classical algorithms, they also motivate future research in cases where the best known algorithms do not quite match the lower bounds. ",2202.07932v3
"['Marten Maack', 'Klaus Jansen']",2019-07-08T11:47:49Z,Inapproximability Results for Scheduling with Interval and Resource   Restrictions,"  In the restricted assignment problem, the input consists of a set of machines and a set of jobs each with a processing time and a subset of eligible machines. The goal is to find an assignment of the jobs to the machines minimizing the makespan, that is, the maximum summed up processing time any machine receives. Herein, jobs should only be assigned to those machines on which they are eligible. It is well-known that there is no polynomial time approximation algorithm with an approximation guarantee of less than 1.5 for the restricted assignment problem unless P=NP. In this work, we show hardness results for variants of the restricted assignment problem with particular types of restrictions.   In the case of interval restrictions the machines can be totally ordered such that jobs are eligible on consecutive machines. We resolve the open question of whether the problem admits a polynomial time approximation scheme (PTAS) in the negative (unless P=NP). There are several special cases of this problem known to admit a PTAS.   Furthermore, we consider a variant with resource restriction where each machine has capacities and each job demands for a fixed number of resources. A job is eligible on a machine if its demand is at most the capacity of the machine for each resource. For one resource, this problem is known to admit a PTAS, for two, the case of interval restrictions is contained, and in general, the problem is closely related to unrelated scheduling with a low rank processing time matrix. We show that there is no polynomial time approximation algorithm with a rate smaller than 48/47 or 1.5 for scheduling with resource restrictions with 2 or 4 resources, respectively, unless P=NP. All our results can be extended to the so called Santa Claus variants of the problems where the goal is to maximize the minimal processing time any machine receives. ",1907.03526v1
"['Kilian Grage', 'Klaus Jansen']",2024-03-24T12:16:51Z,Convolution and Knapsack in Higher Dimensions,"  In the Knapsack problem, one is given the task of packing a knapsack of a given size with items in order to gain a packing with a high profit value. In recent years, a connection to the $(\max,+)$-convolution problem has been established, where knapsack solutions can be combined by building the convolution of two sequences. This observation has been used to give conditional lower bounds but also parameterized algorithms.   In this paper we want to carry these results into higher dimension. We consider Knapsack where items are characterized by multiple properties - given through a vector - and a knapsack that has a capacity vector. The packing must now not exceed any of the given capacity constraints. In order to show a similar sub-quadratic lower bound we introduce a multi-dimensional version of convolution as well. Instead of combining sequences, we will generalize this problem and combine higher dimensional matrices. We will establish a few variants of these problems and prove that they are all equivalent in terms of algorithms that allow for a running time sub-quadratic in the number of entries of the matrix.   We further develop a parameterized algorithm to solve higher dimensional Knapsack. The techniques we apply are inspired by an algorithm introduced by Axiotis and Tzamos. In general, we manage not only to extend their result to higher dimension. We will show that even for higher dimensional Knapsack, we can reduce the problem to convolution on one-dimensional sequences, leading to an $\mathcal{O}(d(n + D \cdot \max\{\Pi_{i=1}^d{t_i}, t_{\max}\log t_{\max}\} ))$ algorithm, where $D$ is the number of different weight vectors, $t$ the capacity vector and $d$ is the dimension of the problem. Finally we also modify this algorithm to handle items with negative weights to cross the bridge from solving not only Knapsack but also Integer Linear Programs (ILPs) in general. ",2403.16117v1
"['Klaus Jansen', 'Marten Maack', 'Roberto Solis-Oba']",2017-01-25T10:24:44Z,Structural Parameters for Scheduling with Assignment Restrictions,"  We consider scheduling on identical and unrelated parallel machines with job assignment restrictions. These problems are NP-hard and they do not admit polynomial time approximation algorithms with approximation ratios smaller than $1.5$ unless P$=$NP. However, if we impose limitations on the set of machines that can process a job, the problem sometimes becomes easier in the sense that algorithms with approximation ratios better than $1.5$ exist. We introduce three graphs, based on the assignment restrictions and study the computational complexity of the scheduling problem with respect to structural properties of these graphs, in particular their tree- and rankwidth. We identify cases that admit polynomial time approximation schemes or FPT algorithms, generalizing and extending previous results in this area. ",1701.07242v1
"['Max A. Deppert', 'Klaus Jansen']",2018-10-02T13:14:46Z,Near-Linear Approximation Algorithms for Scheduling Problems with Batch   Setup Times,"  We investigate the scheduling of $n$ jobs divided into $c$ classes on $m$ identical parallel machines. For every class there is a setup time which is required whenever a machine switches from the processing of one class to another class. The objective is to find a schedule that minimizes the makespan. We give near-linear approximation algorithms for the following problem variants: the non-preemptive context where jobs may not be preempted, the preemptive context where jobs may be preempted but not parallelized, as well as the splittable context where jobs may be preempted and parallelized.   We present the first algorithm improving the previously best approximation ratio of $2$ to a better ratio of $3/2$ in the preemptive case. In more detail, for all three flavors we present an approximation ratio $2$ with running time $\mathcal{O}(n)$, ratio $3/2+\varepsilon$ in time $\mathcal{O}(n\log 1/\varepsilon)$ as well as a ratio of $3/2$. The $(3/2)$-approximate algorithms have different running times. In the non-preemptive case we get time $\mathcal{O}(n\log (n+\Delta))$ where $\Delta$ is the largest value of the input. The splittable approximation runs in time $\mathcal{O}(n+c\log(c+m))$ whereas the preemptive algorithm has a running time $\mathcal{O}(n \log (c+m)) \leq \mathcal{O}(n \log n)$. So far, no PTAS is known for the preemptive problem without restrictions, so we make progress towards that question. Recently Jansen et al. found an EPTAS for the splittable and non-preemptive case but with impractical running times exponential in $1/\varepsilon$. ",1810.01223v2
"['Klaus Jansen', 'Kim-Manuel Klein']",2013-02-18T10:23:00Z,A Robust AFPTAS for Online Bin Packing with Polynomial Migration,"  In this paper we develop general LP and ILP techniques to find an approximate solution with improved objective value close to an existing solution. The task of improving an approximate solution is closely related to a classical theorem of Cook et al. in the sensitivity analysis for LPs and ILPs. This result is often applied in designing robust algorithms for online problems. We apply our new techniques to the online bin packing problem, where it is allowed to reassign a certain number of items, measured by the migration factor. The migration factor is defined by the total size of reassigned items divided by the size of the arriving item. We obtain a robust asymptotic fully polynomial time approximation scheme (AFPTAS) for the online bin packing problem with migration factor bounded by a polynomial in $\frac{1}{\epsilon}$. This answers an open question stated by Epstein and Levin in the affirmative. As a byproduct we prove an approximate variant of the sensitivity theorem by Cook at el. for linear programs. ",1302.4213v1
"['Sören Henning', 'Klaus Jansen', 'Malin Rau', 'Lars Schmarje']",2017-05-12T14:22:53Z,Complexity and Inapproximability Results for Parallel Task Scheduling   and Strip Packing,"  We study the Parallel Task Scheduling problem $Pm|size_j|C_{\max}$ with a constant number of machines. This problem is known to be strongly NP-complete for each $m \geq 5$, while it is solvable in pseudo-polynomial time for each $m \leq 3$. We give a positive answer to the long-standing open question whether this problem is strongly $NP$-complete for $m=4$. As a second result, we improve the lower bound of $\frac{12}{11}$ for approximating pseudo-polynomial Strip Packing to $\frac{5}{4}$. Since the best known approximation algorithm for this problem has a ratio of $\frac{4}{3} + \varepsilon$, this result narrows the gap between approximation ratio and inapproximability result by a significant step. Both results are proven by a reduction from the strongly $NP$-complete problem 3-Partition. ",1705.04587v1
"['Klaus Jansen', 'Marten Maack', 'Alexander Mäcker']",2018-09-27T09:41:39Z,Scheduling on (Un-)Related Machines with Setup Times,"  We consider a natural generalization of scheduling $n$ jobs on $m$ parallel machines so as to minimize the makespan. In our extension the set of jobs is partitioned into several classes and a machine requires a setup whenever it switches from processing jobs of one class to jobs of a different class. During such a setup, a machine cannot process jobs and the duration of a setup may depend on the machine as well as the class of the job to be processed next.   For this problem, we study approximation algorithms for non-identical machines. We develop a polynomial-time approximation scheme for uniformly related machines. For unrelated machines we obtain an $O(\log n + \log m)$-approximation, which we show to be optimal (up to constant factors) unless $NP \subset RP$. We also identify two special cases that admit constant factor approximations. ",1809.10428v1
"['Marin Bougeret', 'Klaus Jansen', 'Michael Poss', 'Lars Rohwedder']",2019-05-21T12:58:12Z,Approximation results for makespan minimization with budgeted   uncertainty,"  We study approximation algorithms for the problem of minimizing the makespan on a set of machines with uncertainty on the processing times of jobs. In the model we consider, which goes back to~\cite{BertsimasS03}, once the schedule is defined an adversary can pick a scenario where deviation is added to some of the jobs' processing times. Given only the maximal cardinality of these jobs, and the magnitude of potential deviation for each job, the goal is to optimize the worst-case scenario. We consider both the cases of identical and unrelated machines. Our main result is an EPTAS for the case of identical machines. We also provide a $3$-approximation algorithm and an inapproximability ratio of $2-\epsilon$ for the case of unrelated machines ",1905.08592v1
"['Lin Chen', 'Klaus Jansen', 'Guochuan Zhang']",2013-10-01T17:25:57Z,On the optimality of approximation schemes for the classical scheduling   problem,"  We consider the classical scheduling problem on parallel identical machines to minimize the makespan, and achieve the following results under the Exponential Time Hypothesis (ETH)   1. The scheduling problem on a constant number $m$ of identical machines, which is denoted as $Pm||C_{max}$, is known to admit a fully polynomial time approximation scheme (FPTAS) of running time $O(n) + (1/\epsilon)^{O(m)}$ (indeed, the algorithm works for an even more general problem where machines are unrelated). We prove this algorithm is essentially the best possible in the sense that a $(1/\epsilon)^{O(m^{1-\delta})}+n^{O(1)}$ time FPTAS for any $\delta>0$ implies that ETH fails.   2. The scheduling problem on an arbitrary number of identical machines, which is denoted as $P||C_{max}$, is known to admit a polynomial time approximation scheme (PTAS) of running time $2^{O(1/\epsilon^2\log^3(1/\epsilon))}+n^{O(1)}$. We prove this algorithm is nearly optimal in the sense that a $2^{O((1/\epsilon)^{1-\delta})}+n^{O(1)}$ time PTAS for any $\delta>0$ implies that ETH fails, leaving a small room for improvement.   To obtain these results we will provide two new reductions from 3SAT, one for $Pm||C_{max}$ and another for $P||C_{max}$. Indeed, the new reductions explore the structure of scheduling problems and can also lead to other interesting results. For example, using the framework of our reduction for $P||C_{max}$, Chen et al. (arXiv:1306.3727) is able to prove the APX-hardness of the scheduling problem in which the matrix of job processing times $P=(p_{ij})_{m\times n}$ is of rank 3, solving the open problem mentioned by Bhaskara et al. (SODA 2013). ",1310.0398v1
"['Klaus Jansen', 'Stefan Erich Julius Kraft']",2015-04-17T22:13:48Z,A Faster FPTAS for the Unbounded Knapsack Problem,"  The Unbounded Knapsack Problem (UKP) is a well-known variant of the famous 0-1 Knapsack Problem (0-1 KP). In contrast to 0-1 KP, an arbitrary number of copies of every item can be taken in UKP. Since UKP is NP-hard, fully polynomial time approximation schemes (FPTAS) are of great interest. Such algorithms find a solution arbitrarily close to the optimum $\mathrm{OPT}(I)$, i.e. of value at least $(1-\varepsilon) \mathrm{OPT}(I)$ for $\varepsilon > 0$, and have a running time polynomial in the input length and $\frac{1}{\varepsilon}$. For over thirty years, the best FPTAS was due to Lawler with a running time in $O(n + \frac{1}{\varepsilon^3})$ and a space complexity in $O(n + \frac{1}{\varepsilon^2})$, where $n$ is the number of knapsack items. We present an improved FPTAS with a running time in $O(n + \frac{1}{\varepsilon^2} \log^3 \frac{1}{\varepsilon})$ and a space bound in $O(n + \frac{1}{\varepsilon} \log^2 \frac{1}{\varepsilon})$. This directly improves the running time of the fastest known approximation schemes for Bin Packing and Strip Packing, which have to approximately solve UKP instances as subproblems. ",1504.04650v2
"['Klaus Jansen', 'Kim-Manuel Klein']",2016-04-25T14:44:13Z,About the Structure of the Integer Cone and its Application to Bin   Packing,"  We consider the bin packing problem with $d$ different item sizes and revisit the structure theorem given by Goemans and Rothvo\ss [6] about solutions of the integer cone. We present new techniques on how solutions can be modified and give a new structure theorem that relies on the set of vertices of the underlying integer polytope. As a result of our new structure theorem, we obtain an algorithm for the bin packing problem with running time $|V|^{2^{O(d)}} \cdot enc(I)^{O(1)}$, where $V$ is the set of vertices of the integer knapsack polytope and $enc(I)$ is the encoding length of the bin packing instance. The algorithm is fixed parameter tractable, parameterized by the number of vertices of the integer knapsack polytope $|V|$. This shows that the bin packing problem can be solved efficiently when the underlying integer knapsack polytope has an easy structure, i.e. has a small number of vertices.   Furthermore, we show that the presented bounds of the structure theorem are asymptotically tight. We give a construction of bin packing instances using new structural insights and classical number theoretical theorems which yield the desired lower bound. ",1604.07286v2
"['Kilian Grage', 'Klaus Jansen', 'Kim Manuel Klein']",2018-10-17T13:00:20Z,An EPTAS for machine scheduling with bag-constraints,"  Machine scheduling is a fundamental optimization problem in computer science. The task of scheduling a set of jobs on a given number of machines and minimizing the makespan is well studied and among other results, we know that EPTAS's for machine scheduling on identical machines exist. Das and Wiese initiated the research on a generalization of makespan minimization, that includes so called bag-constraints. In this variation of machine scheduling the given set of jobs is partitioned into subsets, so called bags. Given this partition a schedule is only considered feasible when on any machine there is at most one job from each bag.   Das and Wiese showed that this variant of machine scheduling admits a PTAS. We will improve on this result by giving the first EPTAS for the machine scheduling problem with bag-constraints. We achieve this result by using new insights on this problem and restrictions given by the bag-constraints. We show that, to gain an approximate solution, we can relax the bag-constraints and ignore some of the restrictions. Our EPTAS uses a new instance transformation that will allow us to schedule large and small jobs independently of each other for a majority of bags. We also show that it is sufficient to respect the bag-constraint only among a constant number of bags, when scheduling large jobs. With these observations our algorithm will allow for some conflicts when computing a schedule and we show how to repair the schedule in polynomial-time by swapping certain jobs around. ",1810.07510v1
"['Klaus Jansen', 'Alexandra Lassota', 'Lars Rohwedder']",2018-11-02T15:59:32Z,Near-Linear Time Algorithm for n-fold ILPs via Color Coding,"  We study an important case of ILPs $\max\{c^Tx \ \vert\ \mathcal Ax = b, l \leq x \leq u,\, x \in \mathbb{Z}^{n t} \} $ with $n\cdot t$ variables and lower and upper bounds $\ell, u\in\mathbb Z^{nt}$. In $n$-fold ILPs non-zero entries only appear in the first $r$ rows of the matrix $\mathcal A$ and in small blocks of size $s\times t$ along the diagonal underneath. Despite this restriction many optimization problems can be expressed in this form. It is known that $n$-fold ILPs can be solved in FPT time regarding the parameters $s, r,$ and $\Delta$, where $\Delta$ is the greatest absolute value of an entry in $\mathcal A$. The state-of-the-art technique is a local search algorithm that subsequently moves in an improving direction. Both, the number of iterations and the search for such an improving direction take time $\Omega(n)$, leading to a quadratic running time in $n$. We introduce a technique based on Color Coding, which allows us to compute these improving directions in logarithmic time after a single initialization step. This leads to the first algorithm for $n$-fold ILPs with a running time that is near-linear in the number $nt$ of variables, namely $(rs\Delta)^{O(r^2s + s^2)} L^2 \cdot nt \log^{O(1)}(nt)$, where $L$ is the encoding length of the largest integer in the input. In contrast to the algorithms in recent literature, we do not need to solve the LP relaxation in order to handle unbounded variables. Instead, we give a structural lemma to introduce appropriate bounds. If, on the other hand, we are given such an LP solution, the running time can be decreased by a factor of $L$. ",1811.00950v1
"['Max A. Deppert', 'Klaus Jansen']",2022-10-05T16:06:07Z,The Power of Duality: Response Time Analysis meets Integer Programming,"  We study a mutually enriching connection between response time analysis in real-time systems and the mixing set problem. Thereby generalizing over known results we present a new approach to the computation of response times in fixed-priority uniprocessor real-time scheduling. We even allow that the tasks are delayed by some period-constrained release jitter. By studying a dual problem formulation of the decision problem as an integer linear program we show that worst-case response times can be computed by algorithmically exploiting a conditional reduction to an instance of the mixing set problem. In the important case of harmonic periods our new technique admits a near-quadratic algorithm to the exact computation of worst-case response times. We show that generally, a smaller utilization leads to more efficient algorithms even in fixed-priority scheduling. Worst-case response times can be understood as least fixed points to non-trivial fixed point equations and as such, our approach may also be used to solve suitable fixed point problems. Furthermore, we show that our technique can be reversed to solve the mixing set problem by computing worst-case response times to associated real-time scheduling task systems. Finally, we also apply our optimization technique to solve 4-block integer programs with simple objective functions. ",2210.02361v2
"['Klaus Jansen', 'Malin Rau', 'Malte Tutas']",2024-04-24T15:12:20Z,Hardness and Tight Approximations of Demand Strip Packing,"  We settle the pseudo-polynomial complexity of the Demand Strip Packing (DSP) problem: Given a strip of fixed width and a set of items with widths and heights, the items must be placed inside the strip with the objective of minimizing the peak height. This problem has gained significant scientific interest due to its relevance in smart grids[Deppert et al.\ APPROX'21, G\'alvez et al.\ APPROX'21]. Smart Grids are a modern form of electrical grid that provide opportunities for optimization. They are forecast to impact the future of energy provision significantly. Algorithms running in pseudo-polynomial time lend themselves to these applications as considered time intervals, such as days, are small. Moreover, such algorithms can provide superior approximation guarantees over those running in polynomial time. Consequently, they evoke scientific interest in related problems.   We prove that Demand Strip Packing is strongly NP-hard for approximation ratios below $5/4$. Through this proof, we provide novel insights into the relation of packing and scheduling problems. Using these insights, we show a series of frameworks that solve both Demand Strip Packing and Parallel Task Scheduling optimally when increasing the strip's width or number of machines. Such alterations to problems are known as resource augmentation. Applications are found when penalty costs are prohibitively large. Finally, we provide a pseudo-polynomial time approximation algorithm for DSP with an approximation ratio of $(5/4+\varepsilon)$, which is nearly optimal assuming $P\neq NP$. The construction of this algorithm provides several insights into the structure of DSP solutions and uses novel techniques to restructure optimal solutions. ",2404.15917v1
"['Klaus Jansen', 'Arindam Khan', 'Marvin Lira', 'K. V. N. Sreenivas']",2022-02-24T05:03:43Z,A PTAS for Packing Hypercubes into a Knapsack,"  We study the d-dimensional hypercube knapsack problem where we are given a set of d-dimensional hypercubes with associated profits, and a knapsack which is a unit d-dimensional hypercube. The goal is to find an axis-aligned non-overlapping packing of a subset of hypercubes such that the profit of the packed hypercubes is maximized. For this problem, Harren (ICALP'06) gave an algorithm with an approximation ratio of (1+1/2^d+epsilon). For d=2, Jansen and Solis-Oba (IPCO'08) showed that the problem admits a polynomial-time approximation scheme (PTAS); Heydrich and Wiese (SODA'17) further improved the running time and gave an efficient polynomial-time approximation scheme (EPTAS). Both the results use structural properties of 2-D packing, which do not generalize to higher dimensions. For d>2, it remains open to obtain a PTAS, and in fact, there has been no improvement since Harren's result.   We settle the problem by providing a PTAS. Our main technical contribution is a structural lemma which shows that any packing of hypercubes can be converted into another structured packing such that a high profitable subset of hypercubes is packed into a constant number of special hypercuboids, called V-Boxes and N-Boxes. As a side result, we give an almost optimal algorithm for a variant of the strip packing problem in higher dimensions. This might have applications for other multidimensional geometric packing problems. ",2202.11902v2
"['Klaus Jansen', 'Kim-Manuel Klein', 'José Verschae']",2016-04-25T07:47:34Z,Closing the Gap for Makespan Scheduling via Sparsification Techniques,"  Makespan scheduling on identical machines is one of the most basic and fundamental packing problems studied in the discrete optimization literature. It asks for an assignment of $n$ jobs to a set of $m$ identical machines that minimizes the makespan. The problem is strongly NP-hard, and thus we do not expect a $(1+\epsilon)$-approximation algorithm with a running time that depends polynomially on $1/\epsilon$. Furthermore, Chen et al. [3] recently showed that a running time of $2^{(1/\epsilon)^{1-\delta}}+\text{poly}(n)$ for any $\delta>0$ would imply that the Exponential Time Hypothesis (ETH) fails. A long sequence of algorithms have been developed that try to obtain low dependencies on $1/\epsilon$, the better of which achieves a running time of $2^{\tilde{O}(1/\epsilon^2)}+O(n\log n)$ [11]. In this paper we obtain an algorithm with a running time of $2^{\tilde{O}(1/\epsilon)}+O(n\log n)$, which is tight under ETH up to logarithmic factors on the exponent.   Our main technical contribution is a new structural result on the configuration-IP. More precisely, we show the existence of a highly symmetric and sparse optimal solution, in which all but a constant number of machines are assigned a configuration with small support. This structure can then be exploited by integer programming techniques and enumeration. We believe that our structural result is of independent interest and should find applications to other settings. In particular, we show how the structure can be applied to the minimum makespan problem on related machines and to a larger class of objective functions on parallel machines. For all these cases we obtain an efficient PTAS with running time $2^{\tilde{O}(1/\epsilon)} + \text{poly}(n)$. ",1604.07153v1
"['Klaus Jansen', 'Alexandra Lassota', 'Marten Maack']",2019-09-26T08:21:08Z,Approximation Algorithms for Scheduling with Class Constraints,"  Assigning jobs onto identical machines with the objective to minimize the maximal load is one of the most basic problems in combinatorial optimization. Motivated by product planing and data placement, we study a natural extension called Class Constrainted Scheduling (CCS). In this problem, each job additionally belongs to a class and each machine can only schedule jobs from at most $c$ different classes. Even though this problem is closely related to the Class Constraint Bin Packing, the Class Constraint Knapsack and the Cardinality Constraint variants, CCS lacks results regarding approximation algorithms, even though it is also well-known to be NP-hard. We fill this gap by analyzing the problem considering three different ways to feasibly allot the jobs: The splittable case, where we can split and allot the jobs arbitrarily; the preemptive case, where jobs pieces belonging to the same job are not allowed to be scheduled in parallel; and finally the non-preemptive case, where no splitting is allowed at all. For each case we introduce the first PTAS where neither $c$ nor the number of all classes have to be a constant. In order to achieve this goal, we give new insights about the structure of optimal solutions. This allows us to preprocess the instance appropriately and by additionally grouping variables to set up a configuration Integer Linear Program (ILP) with N-fold structure. This N-fold structure allows us to solve the ILP efficiently. Further we developed the first simple approximation algorithms with a constant approximation ratio running in strongly polynomial time. The splittable and the preemptive case admit algorithms with ratio $2$ and a running time of $O(n^2 \log(n))$. The algorithm for the non-preemptive case has a ratio of $7/3$ and a running time of $O(n^2 \log^2(n))$. All results even hold if the number of machines cannot be bounded by a polynomial in $n$. ",1909.11970v1
"['Klaus Jansen', 'Kim-Manuel Klein', 'Alexandra Lassota']",2020-08-29T07:35:24Z,The Double Exponential Runtime is Tight for 2-Stage Stochastic ILPs,"  We consider fundamental algorithmic number theoretic problems and their relation to a class of block structured Integer Linear Programs (ILPs) called $2$-stage stochastic. A $2$-stage stochastic ILP is an integer program of the form $\min \{c^T x \mid \mathcal{A} x = b, \ell \leq x \leq u, x \in \mathbb{Z}^{r + ns} \}$ where the constraint matrix $\mathcal{A} \in \mathbb{Z}^{nt \times r +ns}$ consists of $n$ matrices $A_i \in \mathbb{Z}^{t \times r}$ on the vertical line and $n$ matrices $B_i \in \mathbb{Z}^{t \times s}$ on the diagonal line aside.   First, we show a stronger hardness result for a number theoretic problem called Quadratic Congruences where the objective is to compute a number $z \leq \gamma$ satisfying $z^2 \equiv \alpha \bmod \beta$ for given $\alpha, \beta, \gamma \in \mathbb{Z}$. This problem was proven to be NP-hard already in 1978 by Manders and Adleman. However, this hardness only applies for instances where the prime factorization of $\beta$ admits large multiplicities of each prime number. We circumvent this necessity proving that the problem remains NP-hard, even if each prime number only occurs constantly often.   Then, using this new hardness result for the Quadratic Congruences problem, we prove a lower bound of $2^{2^{\delta(s+t)}} |I|^{O(1)}$ for some $\delta > 0$ for the running time of any algorithm solving $2$-stage stochastic ILPs assuming the Exponential Time Hypothesis (ETH). Here, $|I|$ is the encoding length of the instance. This result even holds if $r$, $||b||_{\infty}$, $||c||_{\infty}, ||\ell||_{\infty}$ and the largest absolute value $\Delta$ in the constraint matrix $\mathcal{A}$ are constant. This shows that the state-of-the-art algorithms are nearly tight. Further, it proves the suspicion that these ILPs are indeed harder to solve than the closely related $n$-fold ILPs where the contraint matrix is the transpose of $\mathcal A$. ",2008.12928v3
"['Sebastian Berndt', 'Klaus Jansen', 'Alexandra Lassota']",2020-10-19T06:48:56Z,Tightness of Sensitivity and Proximity Bounds for Integer Linear   Programs,"  We consider ILPs, where each variable corresponds to an integral point within a polytope $\mathcal{P}$, i. e., ILPs of the form $\min\{c^{\top}x\mid \sum_{p\in\mathcal P\cap \mathbb Z^d} x_p p = b, x\in\mathbb Z^{|\mathcal P\cap \mathbb Z^d|}_{\ge 0}\}$.   The distance between an optimal fractional solution and an optimal integral solution (called proximity) is an important measure. A classical result by Cook et al.~(Math. Program., 1986) shows that it is at most $\Delta^{\Theta(d)}$ where $\Delta$ is the largest coefficient in the constraint matrix.   Another important measure studies the change in an optimal solution if the right-hand side $b$ is replaced by another right-hand side $b'$. The distance between an optimal solution $x$ w.r.t.~$b$ and an optimal solution $x'$ w.r.t.~$b'$ (called sensitivity) is similarly bounded, i. e., $\lVert b-b' \rVert_{1}\cdot \Delta^{\Theta(d)}$, also shown by Cook et al.   Even after more than thirty years, these bounds are essentially the best known bounds for these measures.   While some lower bounds are known for these measures, they either only work for very small values of $\Delta$, require negative entries in the constraint matrix, or have fractional right-hand sides.   Hence, these lower bounds often do not correspond to instances from algorithmic problems.   This work presents for each $\Delta > 0$ and each $d > 0$ ILPs of the above type with non-negative constraint matrices such that their proximity and sensitivity is at least $\Delta^{\Theta(d)}$.   Furthermore, these instances are closely related to instances of the Bin Packing problem as they form a subset of columns of the configuration ILP.   We thereby show that the results of Cook et al. are indeed tight, even for instances arising naturally from problems in combinatorial optimization. ",2010.09255v1
"['Klaus Jansen', 'Alexandra Lassota', 'Marten Maack', 'Tytus Pikies']",2020-11-12T01:20:34Z,Total Completion Time Minimization for Scheduling with Incompatibility   Cliques,"  This paper considers parallel machine scheduling with incompatibilities between jobs. The jobs form a graph and no two jobs connected by an edge are allowed to be assigned to the same machine. In particular, we study the case where the graph is a collection of disjoint cliques. Scheduling with incompatibilities between jobs represents a well-established line of research in scheduling theory and the case of disjoint cliques has received increasing attention in recent years. While the research up to this point has been focused on the makespan objective, we broaden the scope and study the classical total completion time criterion. In the setting without incompatibilities, this objective is well known to admit polynomial time algorithms even for unrelated machines via matching techniques. We show that the introduction of incompatibility cliques results in a richer, more interesting picture. Scheduling on identical machines remains solvable in polynomial time, while scheduling on unrelated machines becomes APX-hard. Furthermore, we study the problem under the paradigm of fixed-parameter tractable algorithms (FPT). In particular, we consider a problem variant with assignment restrictions for the cliques rather than the jobs. We prove that it is NP-hard and can be solved in FPT time with respect to the number of cliques. Moreover, we show that the problem on unrelated machines can be solved in FPT time for reasonable parameters, e.g., the parameter pair: number of machines and maximum processing time. The latter result is a natural extension of known results for the case without incompatibilities and can even be extended to the case of total weighted completion time. All of the FPT results make use of n-fold Integer Programs that recently have received great attention by proving their usefulness for scheduling problems. ",2011.06150v2
"['Kilian Grage', 'Klaus Jansen', 'Felix Ohnesorge']",2023-03-02T17:04:33Z,Improved Algorithms for Monotone Moldable Job Scheduling using   Compression and Convolution,"  In the moldable job scheduling problem one has to assign a set of $n$ jobs to $m$ machines, in order to minimize the time it takes to process all jobs. Each job is moldable, so it can be assigned not only to one but any number of the equal machines. We assume that the work of each job is monotone and that jobs can be placed non-contiguously. In this work we present a $(\frac 3 2 + \epsilon)$-approximation algorithm with a worst-case runtime of ${O(n \log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon) + \frac{n}{\epsilon} \log(\frac 1 \epsilon) {\log (\epsilon m)})}$ when $m\le 16n$. This is an improvement over the best known algorithm of the same quality by a factor of $\frac 1 \epsilon$ and several logarithmic dependencies. We complement this result with an improved FPTAS with running time $O(n \log^2(\frac 1 \epsilon + \frac {\log (\epsilon m)} \epsilon))$ for instances with many machines $m> 8\frac n \epsilon$. This yields a $\frac 3 2$-approximation with runtime $O(n \log^2(\log m))$ when $m>16n$.   We achieve these results through one new core observation: In an approximation setting one does not need to consider all $m$ possible allotments for each job. We will show that we can reduce the number of relevant allotments for each job from $m$ to $O(\frac 1 \epsilon + \frac {\log (\epsilon m)}{\epsilon})$. Using this observation immediately yields the improved FPTAS. For the other result we use a reduction to the knapsack problem first introduced by Mouni\'e, Rapine and Trystram. We use the reduced number of machines to give a new elaborate rounding scheme and define a modified version of this this knapsack instance. This in turn allows for the application of a convolution based algorithm by Axiotis and Tzamos. We further back our theoretical results through a practical implementation and compare our algorithm to the previously known best result. ",2303.01414v1
"['Klaus Jansen', 'Kai Kahler', 'Esther Zwanger']",2024-04-26T09:26:32Z,Exact and Approximate High-Multiplicity Scheduling on Identical Machines,"  Goemans and Rothvoss (SODA'14) gave a framework for solving problems in time $enc(P)^{2^{O(N)}}enc(Q)^{O(1)}$ that can be described as finding a point in $\text{int.cone}(P\cap\mathbb{Z}^N)\cap Q$, where $P,Q\subset\mathbb{R}^N$ are (bounded) polyhedra. This framework can be used to solve various scheduling problems, but the encoding length $enc(P)$ usually involves large parameters like the makespan. We describe three tools to improve the framework by Goemans and Rothvoss: Problem-specific preprocessing, LP relaxation techniques and a new bound for the number of vertices of the integer hull.   In particular, applied to the classical scheduling problem $P||C_{\max}$, these tools each improve the running time from $(\log(C_{\max}))^{2^{O(d)}} enc(I)^{O(1)}$ to the possibly much better $(\log(p_{\max}))^{2^{O(d)}}enc(I)^{O(1)}$. Here, $p_{\max}$ is the largest processing time, $d$ is the number of different processing times, $C_{\max}$ is the makespan and $enc(I)$ is the encoding length of the instance. This running time is FPT w.r.t. parameter $d$ if $p_{\max}$ is given in unary. We obtain similar results for various other problems. Moreover, we show how a balancing result by Govzmann et al. can be used to speed up an additive approximation scheme by Buchem et al. (ICALP'21) in the high-multiplicity setting.   On the complexity side, we use reductions from the literature to provide new parameterized lower bounds for $P||C_{\max}$ and to show that the improved running time of the additive approximation algorithm is probably optimal. Finally, we show that the big open question asked by Mnich and van Bevern (Comput. Oper. Res. '18) whether $P||C_{\max}$ is FPT w.r.t. the number of job types $d$ has the same answer as the question whether $Q||C_{\max}$ is FPT w.r.t. the number of job and machine types $d+\tau$ (all in high-multiplicity encoding). The same holds for objective $C_{\min}$. ",2404.17274v1
"['Thomas Erlebach', 'Torben Hagerup', 'Klaus Jansen', 'Moritz Minzlaff', 'Alexander Wolff']",2008-02-20T14:23:38Z,"Trimming of Graphs, with Application to Point Labeling","  For $t,g>0$, a vertex-weighted graph of total weight $W$ is $(t,g)$-trimmable if it contains a vertex-induced subgraph of total weight at least $(1-1/t)W$ and with no simple path of more than $g$ edges. A family of graphs is trimmable if for each constant $t>0$, there is a constant $g=g(t)$ such that every vertex-weighted graph in the family is $(t,g)$-trimmable. We show that every family of graphs of bounded domino treewidth is trimmable. This implies that every family of graphs of bounded degree is trimmable if the graphs in the family have bounded treewidth or are planar. Based on this result, we derive a polynomial-time approximation scheme for the problem of labeling weighted points with nonoverlapping sliding labels of unit height and given lengths so as to maximize the total weight of the labeled points. This settles one of the last major open questions in the theory of map labeling. ",0802.2854v1
"['Klaus Jansen', 'Kim-Manuel Klein', 'Maria Kosche', 'Leon Ladewig']",2017-06-15T15:57:36Z,Online Strip Packing with Polynomial Migration,"  We consider the relaxed online strip packing problem: Rectangular items arrive online and have to be packed without rotations into a strip of fixed width such that the packing height is minimized. Thereby, repacking of previously packed items is allowed. The amount of repacking is measured by the migration factor, defined as the total size of repacked items divided by the size of the arriving item. First, we show that no algorithm with constant migration factor can produce solutions with asymptotic ratio better than 4/3. Against this background, we allow amortized migration, i.e. to save migration for a later time step. As a main result, we present an AFPTAS with asymptotic ratio $1 + \mathcal{O}(\epsilon)$ for any $\epsilon > 0$ and amortized migration factor polynomial in $1 / \epsilon$. To our best knowledge, this is the first algorithm for online strip packing considered in a repacking model. ",1706.04939v2
"['Sebastian Berndt', 'Klaus Jansen', 'Kim-Manuel Klein']",2020-06-18T20:47:45Z,New Bounds for the Vertices of the Integer Hull,"  The vertices of the integer hull are the integral equivalent to the well-studied basic feasible solutions of linear programs. In this paper we give new bounds on the number of non-zero components -- their support -- of these vertices matching either the best known bounds or improving upon them. While the best known bounds make use of deep techniques, we only use basic results from probability theory to make use of the concentration of measure effect. To show the versatility of our techniques, we use our results to give the best known bounds on the number of such vertices and an algorithm to enumerate them. We also improve upon the known lower bounds to show that our results are nearly optimal. One of the main ingredients of our work is a generalization of the famous Hoeffding bound to vector-valued random variables that might be of general interest. ",2006.10847v1
"['Max A. Deppert', 'Klaus Jansen', 'Kim-Manuel Klein']",2020-02-18T17:26:14Z,Fuzzy Simultaneous Congruences,"  We introduce a very natural generalization of the well-known problem of simultaneous congruences. Instead of searching for a positive integer $s$ that is specified by $n$ fixed remainders modulo integer divisors $a_1,\dots,a_n$ we consider remainder intervals $R_1,\dots,R_n$ such that $s$ is feasible if and only if $s$ is congruent to $r_i$ modulo $a_i$ for some remainder $r_i$ in interval $R_i$ for all $i$.   This problem is a special case of a 2-stage integer program with only two variables per constraint which is is closely related to directed Diophantine approximation as well as the mixing set problem. We give a hardness result showing that the problem is NP-hard in general.   By investigating the case of harmonic divisors, i.e. $a_{i+1}/a_i$ is an integer for all $i<n$, which was heavily studied for the mixing set problem as well, we also answer a recent algorithmic question from the field of real-time systems. We present an algorithm to decide the feasibility of an instance in time $\mathcal{O}(n^2)$ and we show that if it exists even the smallest feasible solution can be computed in strongly polynomial time $\mathcal{O}(n^3)$. ",2002.07746v2
"['Thi Huyen Chau Nguyen', 'Werner Grass', 'Klaus Jansen']",2019-11-30T09:12:12Z,Exact Polynomial Time Algorithm for the Response Time Analysis of   Harmonic Tasks with Constrained Release Jitter,"  In some important application areas of hard real-time systems, preemptive sporadic tasks with harmonic periods and constraint deadlines running upon a uni-processor platform play an important role. We propose a new algorithm for determining the exact worst-case response time for a task that has a lower computational complexity (linear in the number of tasks) than the known algorithm developed for the same system class. We also allow the task executions to start delayed due to release jitter if they are within certain value ranges. For checking if these constraints are met we define a constraint programming problem that has a special structure and can be solved with heuristic components in a time that is linear in the task number. If the check determines the admissibility of the jitter values, the linear time algorithm can be used to determine the worst-case response time also for jitter-aware systems. ",1912.01161v1
"['Sebastian Berndt', 'Kilian Grage', 'Klaus Jansen', 'Lukas Johannsen', 'Maria Kosche']",2021-04-20T07:32:01Z,Robust Online Algorithms for Dynamic Choosing Problems,"  Semi-online algorithms that are allowed to perform a bounded amount of repacking achieve guaranteed good worst-case behaviour in a more realistic setting. Most of the previous works focused on minimization problems that aim to minimize some costs. In this work, we study maximization problems that aim to maximize their profit.   We mostly focus on a class of problems that we call choosing problems, where a maximum profit subset of a set objects has to be maintained. Many known problems, such as Knapsack, MaximumIndependentSet and variations of these, are part of this class. We present a framework for choosing problems that allows us to transfer offline $\alpha$-approximation algorithms into $(\alpha-epsilon)$-competitive semi-online algorithms with amortized migration $O(1/\epsilon)$. Moreover we complement these positive results with lower bounds that show that our results are tight in the sense that no amortized migration of $o(1/\epsilon)$ is possible. ",2104.09803v1
"['Sebastian Berndt', 'Klaus Jansen', 'Kim-Manuel Klein']",2014-11-04T16:47:56Z,Fully Dynamic Bin Packing Revisited,"  We consider the fully dynamic bin packing problem, where items arrive and depart in an online fashion and repacking of previously packed items is allowed. The goal is, of course, to minimize both the number of bins used as well as the amount of repacking. A recently introduced way of measuring the repacking costs at each timestep is the migration factor, defined as the total size of repacked items divided by the size of an arriving or departing item. Concerning the trade-off between number of bins and migration factor, if we wish to achieve an asymptotic competitive ration of $1 + \epsilon$ for the number of bins, a relatively simple argument proves a lower bound of $\Omega(\frac{1}{\epsilon})$ for the migration factor. We establish a nearly matching upper bound of $O(\frac{1}{\epsilon}^4 \log \frac{1}{\epsilon})$ using a new dynamic rounding technique and new ideas to handle small items in a dynamic setting such that no amortization is needed. The running time of our algorithm is polynomial in the number of items $n$ and in $\frac{1}{\epsilon}$. The previous best trade-off was for an asymptotic competitive ratio of $\frac{5}{4}$ for the bins (rather than $1+\epsilon$) and needed an amortized number of $O(\log n)$ repackings (while in our scheme the number of repackings is independent of $n$ and non-amortized). ",1411.0960v2
"['Klaus Jansen', 'Kim-Manuel Klein', 'Marten Maack', 'Malin Rau']",2018-01-19T15:27:46Z,Empowering the Configuration-IP $-$ New PTAS Results for Scheduling with   Setups Times,"  Integer linear programs of configurations, or configuration IPs, are a classical tool in the design of algorithms for scheduling and packing problems, where a set of items has to be placed in multiple target locations. Herein a configuration describes a possible placement on one of the target locations, and the IP is used to chose suitable configurations covering the items. We give an augmented IP formulation, which we call the module configuration IP. It can be described within the framework of n-fold integer programming and therefore be solved efficiently. As an application, we consider scheduling problems with setup times, in which a set of jobs has to be scheduled on a set of identical machines, with the objective of minimizing the makespan. For instance, we investigate the case that jobs can be split and scheduled on multiple machines. However, before a part of a job can be processed an uninterrupted setup depending on the job has to be paid. For both of the variants that jobs can be executed in parallel or not, we obtain an efficient polynomial time approximation scheme (EPTAS) of running time $f(1/\varepsilon)\times \mathrm{poly}(|I|)$ with a single exponential term in $f$ for the first and a double exponential one for the second case. Previously, only constant factor approximations of $5/3$ and $4/3 + \varepsilon$ respectively were known. Furthermore, we present an EPTAS for a problem where classes of (non-splittable) jobs are given, and a setup has to be paid for each class of jobs being executed on one machine. ",1801.06460v2
"['Sebastian Berndt', 'Max A. Deppert', 'Klaus Jansen', 'Lars Rohwedder']",2021-07-28T20:51:06Z,Load Balancing: The Long Road from Theory to Practice,"  There is a long history of approximation schemes for the problem of scheduling jobs on identical machines to minimize the makespan. Such a scheme grants a $(1+\epsilon)$-approximation solution for every $\epsilon > 0$, but the running time grows exponentially in $1/\epsilon$. For a long time, these schemes seemed like a purely theoretical concept. Even solving instances for moderate values of $\epsilon$ seemed completely illusional. In an effort to bridge theory and practice, we refine recent ILP techniques to develop the fastest known approximation scheme for this problem. An implementation of this algorithm reaches values of $\epsilon$ lower than $2/11\approx 18.2\%$ within a reasonable timespan. This is the approximation guarantee of MULTIFIT, which, to the best of our knowledge, has the best proven guarantee of any non-scheme algorithm. ",2107.13638v1
"['Sebastian Berndt', 'Valentin Dreismann', 'Kilian Grage', 'Klaus Jansen', 'Ingmar Knof']",2019-05-20T10:50:13Z,Robust Online Algorithms for Dynamic Problems,"  Online algorithms that allow a small amount of migration or recourse have been intensively studied in the last years. They are essential in the design of competitive algorithms for dynamic problems, where objects can also depart from the instance. In this work, we give a general framework to obtain so called robust online algorithms for these dynamic problems: these online algorithm achieve an asymptotic competitive ratio of $\gamma+\epsilon$ with migration $O(1/\epsilon)$, where $\gamma$ is the best known offline asymptotic approximation ratio. In order to use our framework, one only needs to construct a suitable online algorithm for the static online case, where items never depart. To show the usefulness of our approach, we improve upon the best known robust algorithms for the dynamic versions of generalizations of Strip Packing and Bin Packing, including the first robust algorithm for general $d$-dimensional Bin Packing and Vector Packing. ",1905.07986v1
"['Sebastian Berndt', 'Leah Epstein', 'Klaus Jansen', 'Asaf Levin', 'Marten Maack', 'Lars Rohwedder']",2019-04-13T13:10:16Z,Online Bin Covering with Limited Migration,"  Semi-online models where decisions may be revoked in a limited way have been studied extensively in the last years.   This is motivated by the fact that the pure online model is often too restrictive to model real-world applications, where some changes might be allowed. A well-studied measure of the amount of decisions that can be revoked is the migration factor $\beta$: When an object $o$ of size $s(o)$ arrives, the decisions for objects of total size at most $\beta\cdot s(o)$ may be revoked. Usually $\beta$ should be a constant. This means that a small object only leads to small changes. This measure has been successfully investigated for different, classic problems such as bin packing or makespan minimization. The dual of makespan minimization - the Santa Claus or machine covering problem - has also been studied, whereas the dual of bin packing - the bin covering problem - has not been looked at from such a perspective.   In this work, we extensively study the bin covering problem with migration in different scenarios. We develop algorithms both for the static case - where only insertions are allowed - and for the dynamic case, where items may also depart. We also develop lower bounds for these scenarios both for amortized migration and for worst-case migration showing that our algorithms have nearly optimal migration factor and asymptotic competitive ratio (up to an arbitrary small $\eps$). We therefore resolve the competitiveness of the bin covering problem with migration. ",1904.06543v1
"['Max A. Deppert', 'Klaus Jansen', 'Marten Maack', 'Simon Pukrop', 'Malin Rau']",2022-10-04T11:02:31Z,Scheduling with Many Shared Resources,"  Consider the many shared resource scheduling problem where jobs have to be scheduled on identical parallel machines with the goal of minimizing the makespan. However, each job needs exactly one additional shared resource in order to be executed and hence prevents the execution of jobs that need the same resource while being processed. Previously a $(2m/(m+1))$-approximation was the best known result for this problem. Furthermore, a $6/5$-approximation for the case with only two machines was known as well as a PTAS for the case with a constant number of machines. We present a simple and fast 5/3-approximation and a much more involved but still reasonable 1.5-approximation. Furthermore, we provide a PTAS for the case with only a constant number of machines, which is arguably simpler and faster than the previously known one, as well as a PTAS with resource augmentation for the general case. The approximation schemes make use of the N-fold integer programming machinery, which has found more and more applications in the field of scheduling recently. It is plausible that the latter results can be improved and extended to more general cases. Lastly, we give a $5/4 - \varepsilon$ inapproximability result for the natural problem extension where each job may need up to a constant number (in particular $3$) of different resources. ",2210.01523v1
"['Max A. Deppert', 'Klaus Jansen', 'Arindam Khan', 'Malin Rau', 'Malte Tutas']",2021-05-15T13:13:05Z,Peak Demand Minimization via Sliced Strip Packing,"  We study Nonpreemptive Peak Demand Minimization (NPDM) problem, where we are given a set of jobs, specified by their processing times and energy requirements. The goal is to schedule all jobs within a fixed time period such that the peak load (the maximum total energy requirement at any time) is minimized. This problem has recently received significant attention due to its relevance in smart-grids. Theoretically, the problem is related to the classical strip packing problem (SP). In SP, a given set of axis-aligned rectangles must be packed into a fixed-width strip, such that the height of the strip is minimized. NPDM can be modeled as strip packing with slicing and stacking constraint: each rectangle may be cut vertically into multiple slices and the slices may be packed into the strip as individual pieces. The stacking constraint forbids solutions where two slices of the same rectangle are intersected by the same vertical line. Nonpreemption enforces the slices to be placed in contiguous horizontal locations (but may be placed at different vertical locations).   We obtain a $(5/3+\epsilon)$-approximation algorithm for the problem. We also provide an asymptotic efficient polynomial-time approximation scheme (AEPTAS) which generates a schedule for almost all jobs with energy consumption $(1+\epsilon)OPT$. The remaining jobs fit into a thin container of height $1$. The previous best for NPDM was 2.7 approximation based on FFDH [Ranjan et al. 2015]. One of our key ideas is providing several new lower bounds on the optimal solution of a geometric packing, which could be useful in other related problems. These lower bounds help us to obtain approximative solutions based on Steinberg's algorithm in many cases. In addition, we show how to split schedules generated by the AEPTAS into few segments and to rearrange the corresponding jobs to insert the thin container mentioned above. ",2105.07219v1
"['Cristina Bazgan', 'Ljiljana Brankovic', 'Katrin Casel', 'Henning Fernau', 'Klaus Jansen', 'Michael Lampis', 'Mathieu Liedloff', 'Jérôme Monnot', 'Vangelis Th. Paschos']",2015-06-24T07:19:16Z,Algorithmic Aspects of Upper Domination,"  In this paper we study combinatorial and algorithmic resp. complexity questions of upper domination, i.e., the maximum cardinality of a minimal dominating set in a graph. We give a full classification of the related maximisation and minimisation problems, as well as the related parameterised problems, on general graphs and on graphs of bounded degree, and we also study planar graphs. ",1506.07260v1
"['Sebastian Berndt', 'Hauke Brinkop', 'Klaus Jansen', 'Matthias Mnich', 'Tobias Stamm']",2023-05-15T08:20:57Z,"New Support Size Bounds for Integer Programming, Applied to Makespan   Minimization on Uniformly Related Machines","  Mixed-integer linear programming (MILP) is at the core of many advanced algorithms for solving fundamental problems in combinatorial optimization. The complexity of solving MILPs directly correlates with their support size, which is the minimum number of non-zero integer variables in an optimal solution. A hallmark result by Eisenbrand and Shmonin (Oper. Res. Lett., 2006) shows that any feasible integer linear program (ILP) has a solution with support size $s\leq 2m\cdot\log(4m\Delta)$, where $m$ is the number of constraints, and $\Delta$ is the largest coefficient in any constraint.   Our main combinatorial result are improved support size bounds for ILPs.   To improve granularity, we analyze for the largest $1$-norm $A_{\max}$ of any column of the constraint matrix, instead of $\Delta$. We show a support size upper bound of $s\leq m\cdot(\log(3A_{\max})+\sqrt{\log(A_{\max})})$, by deriving a new bound on the -1 branch of the Lambert $\mathcal{W}$ function. Additionally, we provide a lower bound of $m\log(A_{\max})$, proving our result asymptotically optimal. Furthermore, we give support bounds of the form $s\leq 2m\cdot\log(1.46A_{\max})$. These improve upon the previously best constants by Aliev. et. al. (SIAM J. Optim., 2018), because all our upper bounds hold equally with $A_{\max}$ replaced by $\sqrt{m}\Delta$.   Using our combinatorial result, we obtain the fastest known approximation schemes (EPTAS) for the fundamental scheduling problem of makespan minimization of uniformly related machines ($Q\mid\mid C_{\max}$). ",2305.08432v1
"['Jaroslaw Piwonski', 'Thomas Slawig']",2014-10-01T12:42:31Z,The Idea and Concept of Metos3D: A Marine Ecosystem Toolkit for   Optimization and Simulation in 3D,"  The simulation and parameter optimization of coupled ocean circulation and ecosystem models in three space dimensions is one of the most challenging tasks in numerical climate research. Here we present a scientific toolkit that aims at supporting researchers by defining clear coupling interfaces, providing state-of-the-art numerical methods for simulation, parallelization and optimization while using only freely available and (to a great extend) platform-independent software. Besides defining a user-friendly coupling interface (API) for marine ecosystem or biogeochemical models, we heavily rely on the Portable, Extensible Toolkit for Scientific computation (PETSc) developed at Argonne Nat. Lab. for a wide variety of parallel linear and non-linear solvers and optimizers. We specifically focus on the usage of matrix-free Newton-Krylov methods for the fast computation of steady periodic solutions, and make use of the Transport Matrix Method (TMM) introduced by Khatiwala et al. ",1410.0204v1
"['Alessandro Cotronei', 'Thomas Slawig']",2020-01-05T11:28:36Z,Single precision arithmetic in ECHAM radiation reduces runtime and   energy consumption,"  We converted the radiation part of the atmospheric model ECHAM to single precision arithmetic. We analyzed different conversion strategies and finally used a step by step change of all modules, subroutines and functions. We found out that a small code portion still requires higher precision arithmetic. We generated code that can be easily changed from double to single precision and vice versa, basically using a simple switch in one module. We compared the output of the single precision version in the coarse resolution with observational data and with the original double precision code. The results of both versions are comparable. We extensively tested different parallelization options with respect to the possible performance gain, in both coarse and low resolution. The single precision radiation itself was accelerated by about 40%, whereas the speed-up for the whole ECHAM model using the converted radiation achieved 18% in the best configuration. We further measured the energy consumption, which could also be reduced. ",2001.01214v1
"['Giovanni Samaey', 'Thomas Slawig']",2018-06-12T11:21:37Z,A micro/macro parallel-in-time (parareal) algorithm applied to a climate   model with discontinuous non-monotone coefficients and oscillatory forcing,"  We present the application of a micro/macro parareal algorithm for a 1-D energy balance climate model with discontinuous and non-monotone coefficients and forcing terms. The micro/macro parareal method uses a coarse propagator, based on a (macroscopic) 0-D approximation of the underlying (microscopic) 1-D model. We compare the performance of the method using different versions of the macro model, as well as different numerical schemes for the micro propagator, namely an explicit Euler method with constant stepsize and an adaptive library routine. We study convergence of the method and the theoretical gain in computational time in a realization on parallel processors. We show that, in this example and for all settings, the micro/macro parareal method converges in fewer iterations than the number of used parareal subintervals, and that a theoretical gain in performance of up to 10 is possible. ",1806.04442v2
"['Markus Pfeil', 'Thomas Slawig']",2022-07-28T07:18:43Z,Adaptive Time Step Algorithms for the Simulation of marine Ecosystem   Models using the Transport Matrix Method Implementation Metos3D,"  The reduction of the computational effort is desirable for the simulation of marine ecosystem models. Using a marine ecosystem model, the assessment and the validation of annual periodic solutions (i.e., steady annual cycles) against observational data are crucial to identify biogeochemical processes, which, for example, influence the global carbon cycle. For marine ecosystem models, the transport matrix method (TMM) already lowers the runtime of the simulation significantly and enables the application of larger time steps straightforwardly. However, the selection of an appropriate time step is a challenging compromise between accuracy and shortening the runtime. Using an automatic time step adjustment during the computation of a steady annual cycle with the TMM, we present in this paper different algorithms applying either an adaptive step size control or decreasing time steps in order to use the time step always as large as possible without any manual selection. For these methods and a variety of marine ecosystem models of different complexity, the accuracy of the computed steady annual cycle achieved the same accuracy as solutions obtained with a fixed time step. Depending on the complexity of the marine ecosystem model, the application of the methods shortened the runtime significantly. Due to the certain overhead of the adaptive method, the computational effort may be higher in special cases using the adaptive step size control. The presented methods represent computational efficient methods for the simulation of marine ecosystem models using the TMM but without any manual selection of the time step. ",2207.13918v1
"['Christina Roschat', 'Thomas Slawig']",2014-03-18T14:10:32Z,Mathematical analysis of a marine ecosystem model with nonlinear   coupling terms and non-local boundary conditions,"  We investigate the weak solvability of initial boundary value problems associated with an ecosystem model of the marine phosphorus cycle. The analysis covers the model equations themselves as well as their linearization which is important in the model calibration via parameter identification. We treat both cases simultaneously by investigating a system of advection-diffusion-reaction equations coupled by general reaction terms and boundary conditions. We derive a weak formulation of the generalized equations and prove two theorems about its unique solvability provided that the reaction terms consist of Lipschitz continuous and monotone operators. In the proofs, we adapt different techniques (Galerkin approximation, Banach's Fixed Point Theorem) to the multi-dimensional model equation. By applying the general theorems to the problems associated with the phosphorus model we obtain results about existence and uniqueness of their solutions. Actually, by assuming a generalized setting the theorems establish the basis for the mathematical analysis of the whole model class to which the investigated phosphorus model belongs. ",1403.4461v3
"['Christina Roschat', 'Thomas Slawig']",2014-09-26T11:34:33Z,Nontrivial Periodic Solutions of Marine Ecosystem Models of N-DOP type,"  We investigate marine ecosystem models of N-DOP type with regard to nontrivial periodic solutions. The elements of this important, widely-used model class typically consist of two coupled advection-diffusion-reaction equations. The corresponding reaction terms are divided into a linear part, describing the transformation of one model variable into the other, and a bounded nonlinear part. Additionally, the model equations conserve the mass contained in the system, i.e. the masses of both variables add up to a constant total mass. In particular, the trivial function is a periodic solution. In this paper, we prove that there is at least one periodic solution for every prescribed total mass. The proof makes use of the typical properties of N-DOP type models by combining results from monotone operator theory and a fixed point argument. In the end, we apply the theorem to the PO4-DOP model, an N-DOP type model which is well-known and often used. ",1409.7540v1
"['Markus Pfeil', 'Thomas Slawig']",2021-11-17T13:58:31Z,Shortening the runtime using larger time steps for the simulation of   marine ecosystem models,"  The reduction of computational costs for marine ecosystem models is important for the investigation and detection of the relevant biogeochemical processes because such models are computationally expensive. In order to lower these computational costs by means of larger time steps we investigated the accuracy of steady annual cycles (i.e., an annual periodic solution) calculated with different time steps. We compared the accuracy for a hierarchy of biogeochemical models showing an increasing complexity and computed the steady annual cycles with offline simulations that are based on the transport matrix approach. For each of these biogeochemical models, we obtained practically the same solution even though larger time steps. This indicates that larger time steps shortened the runtime with an acceptable loss of accuracy. ",2111.09122v1
"['Markus Pfeil', 'Thomas Slawig']",2021-11-30T14:15:37Z,Unique steady annual cycle in marine ecosystem model simulations,"  Marine ecosystem models are an important tool to assess the role of the ocean biota in climate change and to identify relevant biogeochemical processes by validating the model outputs against observational data. For the assessment of the marine ecosystem models, the existence and uniqueness of an annual periodic solution (i.e., a steady annual cycle) is desirable. To analyze the uniqueness of a steady annual cycle, we performed a larger number of simulations starting from different initial concentrations for a hierarchy of biogeochemical models with an increasing complexity. The numerical results suggested that the simulations finished always with the same steady annual cycle regardless of the initial concentration. Due to numerical instabilities, some inadmissible approximations of the steady annual cycle, however, occurred in some cases for the three most complex biogeochemical models. Our numerical results indicate a unique steady annual cycle for practical applications. ",2111.15424v1
"['Markus Pfeil', 'Thomas Slawig']",2021-11-30T17:43:32Z,Surrogate-based optimization using an artificial neural network for a   parameter identification in a 3D marine ecosystem model,"  Parameter identification for marine ecosystem models is important for the assessment and validation of marine ecosystem models against observational data. The surrogate-based optimization (SBO) is a computationally efficient method to optimize complex models. SBO replaces the computationally expensive (high-fidelity) model by a surrogate constructed from a less accurate but computationally cheaper (low-fidelity) model in combination with an appropriate correction approach, which improves the accuracy of the low-fidelity model. To construct a computationally cheap low-fidelity model, we tested three different approaches to compute an approximation of the annual periodic solution (i.e., a steady annual cycle) of a marine ecosystem model: firstly, a reduced number of spin-up iterations (several decades instead of millennia), secondly, an artificial neural network (ANN) approximating the steady annual cycle and, finally, a combination of both approaches. Except for the low-fidelity model using only the ANN, the SBO yielded a solution close to the target and reduced the computational effort significantly. If an ANN approximating appropriately a marine ecosystem model is available, the SBO using this ANN as low-fidelity model presents a promising and computational efficient method for the validation. ",2111.15597v1
"['Benedict Philippi', 'Thomas Slawig']",2022-08-16T08:21:16Z,The Parareal Algorithm Applied to the FESOM 2 Ocean Circulation Model,"  In this work the parallel-in-time algorithm Parareal was applied to the ocean-circulation and sea-ice model FESOM2 developed by the Alfred-Wegener Institut (AWI). The climate model provides one time integration method and hence, the coarse and fine propagators were defined by time step width. The coarse method was executed at the CFL condition limit, while fine step-sizes where gradually refined. As a first assessment of the performance of Parareal a low-resolution test mesh was used with default settings provided by the AWI. An introduction to FESOM2 and the straightforward implementation of Parareal on the DKRZ cluster is given. The evaluation of numerical results for different simulation intervals and fine propagator configurations shows strong dependence on the simulation time and time step-size of the fine propagator. Increasing the latter leads to stagnation and eventually divergence of the Parareal algorithm. ",2208.07598v1
"['Christina Roschat', 'Thomas Slawig']",2015-01-22T08:52:13Z,Mathematical analysis of the $PO_4$-$DOP$-$Fe$ marine ecosystem model   driven by 3-D ocean transport,"  Marine ecosystem models are developed to understand and simulate the biogeochemical processes involved in marine ecosystems. Parekh, Follows and Boyle introduced the $PO_4$-$DOP$-$Fe$ model of the coupled phosphorus and iron cycles in 2005. Especially the part describing the phosphorus cycle ($PO_4$-$DOP$ model) is often applied in the context of parameter identification. The mathematical analysis presented in this study is concerned with the existence of solutions and the reconstruction of parameters from given data. Both are important questions in the numerical model's assessment and validation not answered so far. In this study, we obtain transient, stationary and periodic solutions (steady annual cycles) of the $PO_4$-$DOP$-$Fe$ model equations after a slight change in the equation modeling iron. This result confirms the validity of the solutions computed numerically. Furthermore, we present a calculation showing that four of the $PO_4$-$DOP$ model's parameters are possibly dependent, i.e. different parameter values might be associated with the same model output. Thereby, we identify a relevant source of uncertainty in parameter identification. On the basis of the results, possible ways to overcome this deficit can be proposed. In addition, the stated mathematical conditions for solvability are universal and thus applicable to the analysis of other ecosystem models as well. ",1501.05428v1
"['Benedict Philippi', 'Mahfuz Sarker Miraz', 'Thomas Slawig']",2023-09-06T14:29:32Z,A Micro-Macro parallel-in-time Implementation for the 2D Navier-Stokes   Equations,"  In this paper the Micro-Macro Parareal algorithm was adapted to PDEs. The parallel-in-time approach requires two meshes of different spatial resolution in order to compute approximations in an iterative way to a predefined reference solution. When fast convergence in few iterations can be accomplished the algorithm is able to generate wall-time reduction in comparison to the serial computation. We chose the laminar flow around a cylinder benchmark on 2-dimensional domain which was simulated with the open-source software OpenFoam. The numerical experiments presented in this work aim to approximate states local in time and space and the diagnostic lift coefficient. The Reynolds number is gradually increased from 100 to 1,000, before the transition to turbulent flows sets in. After the results are presented the convergence behavior is discussed with respect to the Reynolds number and the applied interpolation schemes. ",2309.03037v2
"['Janek Meyer', 'Kai Graf', 'Thomas Slawig']",2020-12-05T15:24:10Z,Simulation of scour around arbitrary offshore foundations based on the   Volume-of-Fluid method combined with a Bingham model,"  This paper presents a method for the simulation of scour around arbitrary offshore structures. It is based on the solution of the Reynolds-Averaged-Navier-Stokes equations implemented in the OpenFOAM framework. The sediment is simulated with the help of a Bingham model, which basically models a solid sediment behavior by introducing a very high viscosity. The relative pressure used by the Bingham model is estimated with a new approach based on the solution of a Poisson equation. The position of the sediment surface is calculated with the Volume-of-Fluid approach using a high-resolution scheme. To keep the typical wall characteristics without demanding a fine grid, the common wall functions are transferred to the domain internal sediment walls. Furthermore, additional modifications are applied to model a solid sediment wall inside the solution domain. The new internal wall function implementation is validated with a 2D test case. The results show a very good agreement to common wall functions and a significant improvement compared to its negligence. Furthermore the solver is used to simulate the scour downstream of an apron and the scour around a vertical cylinder in current. The results are compared to experiments presented in the literature and show good agreement. The applicability onto arbitrary structures is demonstrated by applying the solver onto a vertical cylinder with a mudplate. The current development state is able to resolve all important physical flow and scour phenomena. The results also unveil that modeling of the suspension and the treatment of the internal wall need additional attention. ",2012.03051v2
"['Deepak Sharma', 'Matthias Renz', 'Philipp Hövel']",2024-08-09T13:16:01Z,Discovering Motifs to Fingerprint Multi-Layer Networks: a Case Study on   the Connectome of C. Elegans,"  Motif discovery is a powerful approach to understanding network structures and their function. We present a comprehensive analysis of regulatory motifs in the connectome of the model organism Caenorhabditis elegans (C. elegans). Leveraging the Efficient Subgraph Counting Algorithmic PackagE (ESCAPE) algorithm, we identify network motifs in the multi-layer nervous system of C. elegans and link them to functional circuits. We further investigate motif enrichment within signal pathways and benchmark our findings with random networks of similar size and link density. Our findings provide valuable insights into the organization of the nerve net of this well documented organism and can be easily transferred to other species and disciplines alike. ",2408.13263v1
"['Gregor Jossé', 'Ying Lu', 'Tobias Emrich', 'Matthias Renz', 'Cyrus Shahabi', 'Ugur Demiryurek', 'Matthias Schubert']",2016-09-27T14:50:15Z,Scenic Routes Now: Efficiently Solving the Time-Dependent Arc   Orienteering Problem,"  This paper extends the Arc Orienteering Problem (AOP) to large road networks with time-dependent travel times and time-dependent value gain, termed Twofold Time-Dependent AOP or 2TD-AOP for short. In its original definition, the NP-hard Orienteering Problem (OP) asks to find a path from a source to a destination maximizing the accumulated value while not exceeding a cost budget. Variations of the OP and AOP have many practical applications such as mobile crowdsourcing tasks (e.g., repairing and maintenance or dispatching field workers), diverse logistics problems (e.g., crowd control or controlling wildfires) as well as several tourist guidance problems (e.g., generating trip recommendations or navigating through theme parks). In the proposed 2TD-AOP, travel times and value functions are assumed to be time-dependent. The dynamic values model, for instance, varying rewards in crowdsourcing tasks or varying urgency levels in damage control tasks. We discuss this novel problem, prove the benefit of time-dependence empirically and present an efficient approximative solution, optimized for fast response systems. Our approach is the first time-dependent variant of the AOP to be evaluated on a large scale, fine-grained, real-world road network. We show that optimal solutions are infeasible and solutions to the static problem are often invalid. We propose an approximate dynamic programming solution which produces valid paths and is orders of magnitude faster than any optimal solution. ",1609.08484v1
"['Christian Frey', 'Andreas Züfle', 'Tobias Emrich', 'Matthias Renz']",2017-01-19T12:44:17Z,Efficient Information Flow Maximization in Probabilistic Graphs,"  Reliable propagation of information through large networks, e.g., communication networks, social networks or sensor networks is very important in many applications concerning marketing, social networks, and wireless sensor networks. However, social ties of friendship may be obsolete, and communication links may fail, inducing the notion of uncertainty in such networks. In this paper, we address the problem of optimizing information propagation in uncertain networks given a constrained budget of edges. We show that this problem requires to solve two NP-hard subproblems: the computation of expected information flow, and the optimal choice of edges. To compute the expected information flow to a source vertex, we propose the F-tree as a specialized data structure, that identifies independent components of the graph for which the information flow can either be computed analytically and efficiently, or for which traditional Monte-Carlo sampling can be applied independently of the remaining network. For the problem of finding the optimal edges, we propose a series of heuristics that exploit properties of this data structure. Our evaluation shows that these heuristics lead to high quality solutions, thus yielding high information flow, while maintaining low running time. ",1701.05395v4
"['Thomas Bernecker', 'Hans-Peter Kriegel', 'Nikos Mamoulis', 'Matthias Renz', 'Andreas Zuefle']",2009-07-16T15:25:50Z,Scalable Probabilistic Similarity Ranking in Uncertain Databases   (Technical Report),"  This paper introduces a scalable approach for probabilistic top-k similarity ranking on uncertain vector data. Each uncertain object is represented by a set of vector instances that are assumed to be mutually-exclusive. The objective is to rank the uncertain data according to their distance to a reference object. We propose a framework that incrementally computes for each object instance and ranking position, the probability of the object falling at that ranking position. The resulting rank probability distribution can serve as input for several state-of-the-art probabilistic ranking models. Existing approaches compute this probability distribution by applying a dynamic programming approach of quadratic complexity. In this paper we theoretically as well as experimentally show that our framework reduces this to a linear-time complexity while having the same memory requirements, facilitated by incremental accessing of the uncertain vector instances in increasing order of their distance to the reference object. Furthermore, we show how the output of our method can be used to apply probabilistic top-k ranking for the objects, according to different state-of-the-art definitions. We conduct an experimental evaluation on synthetic and real data, which demonstrates the efficiency of our approach. ",0907.2868v1
"['Thomas Bernecker', 'Hans-Peter Kriegel', 'Matthias Renz', 'Florian Verhein', 'Andreas Züfle']",2010-08-13T12:06:29Z,Probabilistic Frequent Pattern Growth for Itemset Mining in Uncertain   Databases (Technical Report),"  Frequent itemset mining in uncertain transaction databases semantically and computationally differs from traditional techniques applied on standard (certain) transaction databases. Uncertain transaction databases consist of sets of existentially uncertain items. The uncertainty of items in transactions makes traditional techniques inapplicable. In this paper, we tackle the problem of finding probabilistic frequent itemsets based on possible world semantics. In this context, an itemset X is called frequent if the probability that X occurs in at least minSup transactions is above a given threshold. We make the following contributions: We propose the first probabilistic FP-Growth algorithm (ProFP-Growth) and associated probabilistic FP-Tree (ProFP-Tree), which we use to mine all probabilistic frequent itemsets in uncertain transaction databases without candidate generation. In addition, we propose an efficient technique to compute the support probability distribution of an itemset in linear time using the concept of generating functions. An extensive experimental section evaluates the our proposed techniques and shows that our ProFP-Growth approach is significantly faster than the current state-of-the-art algorithm. ",1008.2300v1
"['Thomas Bernecker', 'Tobias Emrich', 'Hans-Peter Kriegel', 'Nikos Mamoulis', 'Matthias Renz', 'Andreas Zuefle']",2011-01-13T17:32:01Z,A Novel Probabilistic Pruning Approach to Speed Up Similarity Queries in   Uncertain Databases,"  In this paper, we propose a novel, effective and efficient probabilistic pruning criterion for probabilistic similarity queries on uncertain data. Our approach supports a general uncertainty model using continuous probabilistic density functions to describe the (possibly correlated) uncertain attributes of objects. In a nutshell, the problem to be solved is to compute the PDF of the random variable denoted by the probabilistic domination count: Given an uncertain database object B, an uncertain reference object R and a set D of uncertain database objects in a multi-dimensional space, the probabilistic domination count denotes the number of uncertain objects in D that are closer to R than B. This domination count can be used to answer a wide range of probabilistic similarity queries. Specifically, we propose a novel geometric pruning filter and introduce an iterative filter-refinement strategy for conservatively and progressively estimating the probabilistic domination count in an efficient way while keeping correctness according to the possible world semantics. In an experimental evaluation, we show that our proposed technique allows to acquire tight probability bounds for the probabilistic domination count quickly, even for large uncertain databases. ",1101.2613v2
"['Tobias Emrich', 'Hans-Peter Kriegel', 'Andreas Züfle', 'Peer Kröger', 'Matthias Renz']",2020-01-15T22:24:40Z,Complete and Sufficient Spatial Domination of Multidimensional   Rectangles,"  Rectangles are used to approximate objects, or sets of objects, in a plethora of applications, systems and index structures. Many tasks, such as nearest neighbor search and similarity ranking, require to decide if objects in one rectangle A may, must, or must not be closer to objects in a second rectangle B, than objects in a third rectangle R. To decide this relation of ""Spatial Domination"" it can be shown that using minimum and maximum distances it is often impossible to detect spatial domination. This spatial gem provides a necessary and sufficient decision criterion for spatial domination that can be computed efficiently even in higher dimensional space. In addition, this spatial gem provides an example, pseudocode and an implementation in Python. ",2001.05581v1
"['Johannes Niedermayer', 'Andreas Züfle', 'Tobias Emrich', 'Matthias Renz', 'Nikos Mamoulis', 'Lei Chen', 'Hans-Peter Kriegel']",2013-05-15T09:54:58Z,Probabilistic Nearest Neighbor Queries on Uncertain Moving Object   Trajectories,"  Nearest neighbor (NN) queries in trajectory databases have received significant attention in the past, due to their application in spatio-temporal data analysis. Recent work has considered the realistic case where the trajectories are uncertain; however, only simple uncertainty models have been proposed, which do not allow for accurate probabilistic search. In this paper, we fill this gap by addressing probabilistic nearest neighbor queries in databases with uncertain trajectories modeled by stochastic processes, specifically the Markov chain model. We study three nearest neighbor query semantics that take as input a query state or trajectory $q$ and a time interval. For some queries, we show that no polynomial time solution can be found. For problems that can be solved in PTIME, we present exact query evaluation algorithms, while for the general case, we propose a sophisticated sampling approach, which uses Bayesian inference to guarantee that sampled trajectories conform to the observation data stored in the database. This sampling approach can be used in Monte-Carlo based approximation solutions. We include an extensive experimental study to support our theoretical results. ",1305.3407v2
"['Thomas Bernecker', 'Tobias Emrich', 'Hans-Peter Kriegel', 'Nikos Mamoulis', 'Matthias Renz', 'Shiming Zhang', 'Andreas Züfle']",2011-03-01T14:07:04Z,Inverse Queries For Multidimensional Spaces,"  Traditional spatial queries return, for a given query object $q$, all database objects that satisfy a given predicate, such as epsilon range and $k$-nearest neighbors. This paper defines and studies {\em inverse} spatial queries, which, given a subset of database objects $Q$ and a query predicate, return all objects which, if used as query objects with the predicate, contain $Q$ in their result. We first show a straightforward solution for answering inverse spatial queries for any query predicate. Then, we propose a filter-and-refinement framework that can be used to improve efficiency. We show how to apply this framework on a variety of inverse queries, using appropriate space pruning strategies. In particular, we propose solutions for inverse epsilon range queries, inverse $k$-nearest neighbor queries, and inverse skyline queries. Our experiments show that our framework is significantly more efficient than naive approaches. ",1103.0172v2
"['Georgios Skoumas', 'Klaus Arthur Schmid', 'Gregor Jossé', 'Andreas Züfle', 'Mario A. Nascimento', 'Matthias Renz', 'Dieter Pfoser']",2014-09-09T09:51:01Z,Towards Knowledge-Enriched Path Computation,"  Directions and paths, as commonly provided by navigation systems, are usually derived considering absolute metrics, e.g., finding the shortest path within an underlying road network. With the aid of crowdsourced geospatial data we aim at obtaining paths that do not only minimize distance but also lead through more popular areas using knowledge generated by users. We extract spatial relations such as ""nearby"" or ""next to"" from travel blogs, that define closeness between pairs of points of interest (PoIs) and quantify each of these relations using a probabilistic model. Subsequently, we create a relationship graph where each node corresponds to a PoI and each edge describes the spatial connection between the respective PoIs. Using Bayesian inference we obtain a probabilistic measure of spatial closeness according to the crowd. Applying this measure to the corresponding road network, we obtain an altered cost function which does not exclusively rely on distance, and enriches an actual road networks taking crowdsourced spatial relations into account. Finally, we propose two routing algorithms on the enriched road networks. To evaluate our approach, we use Flickr photo data as a ground truth for popularity. Our experimental results -- based on real world datasets -- show that the paths computed w.r.t.\ our alternative cost function yield competitive solutions in terms of path length while also providing more ""popular"" paths, making routing easier and more informative for the user. ",1409.2585v1
"['Julian Risch', 'Ralf Krestel']",2019-11-25T21:29:59Z,My Approach = Your Apparatus? Entropy-Based Topic Modeling on Multiple   Domain-Specific Text Collections,"  Comparative text mining extends from genre analysis and political bias detection to the revelation of cultural and geographic differences, through to the search for prior art across patents and scientific papers. These applications use cross-collection topic modeling for the exploration, clustering, and comparison of large sets of documents, such as digital libraries. However, topic modeling on documents from different collections is challenging because of domain-specific vocabulary. We present a cross-collection topic model combined with automatic domain term extraction and phrase segmentation. This model distinguishes collection-specific and collection-independent words based on information entropy and reveals commonalities and differences of multiple text collections. We evaluate our model on patents, scientific papers, newspaper articles, forum posts, and Wikipedia articles. In comparison to state-of-the-art cross-collection topic modeling, our model achieves up to 13% higher topic coherence, up to 4% lower perplexity, and up to 31% higher document classification accuracy. More importantly, our approach is the first topic model that ensures disjunct general and specific word distributions, resulting in clear-cut topic representations. ",1911.11240v1
"['Nitisha Jain', 'Ralf Krestel']",2022-02-17T22:05:41Z,Discovering Fine-Grained Semantics in Knowledge Graph Relations,"  When it comes to comprehending and analyzing multi-relational data, the semantics of relations are crucial. Polysemous relations between different types of entities, that represent multiple semantics, are common in real-world relational datasets represented by knowledge graphs. For numerous use cases, such as entity type classification, question answering and knowledge graph completion, the correct semantic interpretation of these relations is necessary. In this work, we provide a strategy for discovering the different semantics associated with abstract relations and deriving many sub-relations with fine-grained meaning. To do this, we leverage the types of the entities associated with the relations and cluster the vector representations of entities and relations. The suggested method is able to automatically discover the best number of sub-relations for a polysemous relation and determine their semantic interpretation, according to our empirical evaluation. ",2202.08917v1
"['Julian Risch', 'Ralf Krestel']",2020-03-26T14:48:25Z,Top Comment or Flop Comment? Predicting and Explaining User Engagement   in Online News Discussions,"  Comment sections below online news articles enjoy growing popularity among readers. However, the overwhelming number of comments makes it infeasible for the average news consumer to read all of them and hinders engaging discussions. Most platforms display comments in chronological order, which neglects that some of them are more relevant to users and are better conversation starters. In this paper, we systematically analyze user engagement in the form of the upvotes and replies that a comment receives. Based on comment texts, we train a model to distinguish comments that have either a high or low chance of receiving many upvotes and replies. Our evaluation on user comments from TheGuardian.com compares recurrent and convolutional neural network models, and a traditional feature-based classifier. Further, we investigate what makes some comments more engaging than others. To this end, we identify engagement triggers and arrange them in a taxonomy. Explanation methods for neural networks reveal which input words have the strongest influence on our model's predictions. In addition, we evaluate on a dataset of product reviews, which exhibit similar properties as user comments, such as featuring upvotes for helpfulness. ",2003.11949v1
"['Julian Risch', 'Nicolas Alder', 'Christoph Hewel', 'Ralf Krestel']",2020-12-27T11:22:25Z,PatentMatch: A Dataset for Matching Patent Claims & Prior Art,"  Patent examiners need to solve a complex information retrieval task when they assess the novelty and inventive step of claims made in a patent application. Given a claim, they search for prior art, which comprises all relevant publicly available information. This time-consuming task requires a deep understanding of the respective technical domain and the patent-domain-specific language. For these reasons, we address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not. Preliminary experiments using a baseline system show that PatentMatch can indeed be used for training a binary text pair classifier on this challenging information retrieval task. The dataset is available online: https://hpi.de/naumann/s/patentmatch. ",2012.13919v1
"['Mohamed Karim Belaid', 'Eyke Hüllermeier', 'Maximilian Rabus', 'Ralf Krestel']",2022-06-08T06:13:39Z,Do We Need Another Explainable AI Method? Toward Unifying Post-hoc XAI   Evaluation Methods into an Interactive and Multi-dimensional Benchmark,"  In recent years, Explainable AI (xAI) attracted a lot of attention as various countries turned explanations into a legal right. xAI allows for improving models beyond the accuracy metric by, e.g., debugging the learned pattern and demystifying the AI's behavior. The widespread use of xAI brought new challenges. On the one hand, the number of published xAI algorithms underwent a boom, and it became difficult for practitioners to select the right tool. On the other hand, some experiments did highlight how easy data scientists could misuse xAI algorithms and misinterpret their results. To tackle the issue of comparing and correctly using feature importance xAI algorithms, we propose Compare-xAI, a benchmark that unifies all exclusive functional testing methods applied to xAI algorithms. We propose a selection protocol to shortlist non-redundant functional tests from the literature, i.e., each targeting a specific end-user requirement in explaining a model. The benchmark encapsulates the complexity of evaluating xAI methods into a hierarchical scoring of three levels, namely, targeting three end-user groups: researchers, practitioners, and laymen in xAI. The most detailed level provides one score per test. The second level regroups tests into five categories (fidelity, fragility, stability, simplicity, and stress tests). The last level is the aggregated comprehensibility score, which encapsulates the ease of correctly interpreting the algorithm's output in one easy to compare value. Compare-xAI's interactive user interface helps mitigate errors in interpreting xAI results by quickly listing the recommended xAI solutions for each ML task and their current limitations. The benchmark is made available at https://karim-53.github.io/cxai/ ",2207.14160v2
"['Konstantin Dobler', 'Florian Hübscher', 'Jan Westphal', 'Alejandro Sierra-Múnera', 'Gerard de Melo', 'Ralf Krestel']",2022-02-23T20:45:41Z,Art Creation with Multi-Conditional StyleGANs,"  Creating meaningful art is often viewed as a uniquely human endeavor. A human artist needs a combination of unique skills, understanding, and genuine intention to create artworks that evoke deep feelings and emotions. In this paper, we introduce a multi-conditional Generative Adversarial Network (GAN) approach trained on large amounts of human paintings to synthesize realistic-looking paintings that emulate human art. Our approach is based on the StyleGAN neural network architecture, but incorporates a custom multi-conditional control mechanism that provides fine-granular control over characteristics of the generated paintings, e.g., with regard to the perceived emotion evoked in a spectator. For better control, we introduce the conditional truncation trick, which adapts the standard truncation trick for the conditional setting and diverse datasets. Finally, we develop a diverse set of evaluation techniques tailored to multi-conditional generation. ",2202.11777v1
"['Betty van Aken', 'Julian Risch', 'Ralf Krestel', 'Alexander Löser']",2018-09-20T11:11:42Z,Challenges for Toxic Comment Classification: An In-Depth Error Analysis,"  Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task's challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our findings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels. ",1809.07572v1
"['Martin Goller', 'Sven Tomforde']",2023-01-30T13:05:47Z,A Quantification Approach for Transferability in Lifelike Computing   Systems,"  The basic idea of lifelike computing systems is the transfer of concepts in living systems to technical use that goes even beyond existing concepts of self-adaptation and self-organisation (SASO). As a result, these systems become even more autonomous and changeable - up to a runtime transfer of the actual target function. Maintaining controllability requires a complete and dynamic (self-)quantification of the system behaviour with regard to aspects of SASO but also, in particular, lifelike properties. In this article, we discuss possible approaches for such metrics and establish a first metric for transferability. We analyse the behaviour of the metric using example applications and show that it is suitable for describing the system's behaviour at runtime. ",2301.12854v1
"['Sven Tomforde', 'Bernhard Sick', 'Christian Müller-Schloer']",2017-01-27T17:35:56Z,Organic Computing in the Spotlight,"  Organic Computing is an initiative in the field of systems engineering that proposed to make use of concepts such as self-adaptation and self-organisation to increase the robustness of technical systems. Based on the observation that traditional design and operation concepts reach their limits, transferring more autonomy to the systems themselves should result in a reduction of complexity for users, administrators, and developers. However, there seems to be a need for an updated definition of the term ""Organic Computing"", of desired properties of technical, organic systems, and the objectives of the Organic Computing initiative. With this article, we will address these points. ",1701.08125v1
"['Simon Reichhuber', 'Sven Tomforde']",2022-01-11T13:50:26Z,Active Reinforcement Learning -- A Roadmap Towards Curious Classifier   Systems for Self-Adaptation,"  Intelligent systems have the ability to improve their behaviour over time taking observations, experiences or explicit feedback into account. Traditional approaches separate the learning problem and make isolated use of techniques from different field of machine learning such as reinforcement learning, active learning, anomaly detection or transfer learning, for instance. In this context, the fundamental reinforcement learning approaches come with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reactive behaviour or isolated problem handling. The idea of this article is to present a concept for alleviating these drawbacks by setting up a research agenda towards what we call ""active reinforcement learning"" in intelligent systems. ",2201.03947v1
"['Nikita Smirnov', 'Sven Tomforde']",2024-04-25T11:02:54Z,Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual   Analysis,"  This conceptual analysis examines the dynamics of data transmission in 5G networks. It addresses various aspects of sending data from cameras and LiDARs installed on a remote-controlled ferry to a land-based control center. The range of topics includes all stages of video and LiDAR data processing from acquisition and encoding to final decoding, all aspects of their transmission and reception via the WebRTC protocol, and all possible types of network problems such as handovers or congestion that could affect the quality of experience for end-users. A series of experiments were conducted to evaluate the key aspects of the data transmission. These include simulation-based reproducible runs and real-world experiments conducted using open-source solutions we developed: ""Gymir5G"" - an OMNeT++-based 5G simulation and ""GstWebRTCApp"" - a GStreamer-based application for adaptive control of media streams over the WebRTC protocol. One of the goals of this study is to formulate the bandwidth and latency requirements for reliable real-time communication and to estimate their approximate values. This goal was achieved through simulation-based experiments involving docking maneuvers in the Bay of Kiel, Germany. The final latency for the entire data processing pipeline was also estimated during the real tests. In addition, a series of simulation-based experiments showed the impact of key WebRTC features and demonstrated the effectiveness of the WebRTC protocol, while the conducted video codec comparison showed that the hardware-accelerated H.264 codec is the best. Finally, the research addresses the topic of adaptive communication, where the traditional congestion avoidance and deep reinforcement learning approaches were analyzed. The comparison in a sandbox scenario shows that the AI-based solution outperforms the WebRTC baseline GCC algorithm in terms of data rates, latency, and packet loss. ",2404.16508v1
"['Stefan Rudolph', 'Sven Tomforde', 'Jörg Hähner']",2019-05-10T15:04:48Z,On the Detection of Mutual Influences and Their Consideration in   Reinforcement Learning Processes,"  Self-adaptation has been proposed as a mechanism to counter complexity in control problems of technical systems. A major driver behind self-adaptation is the idea to transfer traditional design-time decisions to runtime and into the responsibility of systems themselves. In order to deal with unforeseen events and conditions, systems need creativity -- typically realized by means of machine learning capabilities. Such learning mechanisms are based on different sources of knowledge. Feedback from the environment used for reinforcement purposes is probably the most prominent one within the self-adapting and self-organizing (SASO) systems community. However, the impact of other (sub-)systems on the success of the individual system's learning performance has mostly been neglected in this context. In this article, we propose a novel methodology to identify effects of actions performed by other systems in a shared environment on the utility achievement of an autonomous system. Consider smart cameras (SC) as illustrating example: For goals such as 3D reconstruction of objects, the most promising configuration of one SC in terms of pan/tilt/zoom parameters depends largely on the configuration of other SCs in the vicinity. Since such mutual influences cannot be pre-defined for dynamic systems, they have to be learned at runtime. Furthermore, they have to be taken into consideration when self-improving the own configuration decisions based on a feedback loop concept, e.g., known from the SASO domain or the Autonomic and Organic Computing initiatives. We define a methodology to detect such influences at runtime, present an approach to consider this information in a reinforcement learning technique, and analyze the behavior in artificial as well as real-world SASO system settings. ",1905.04205v1
"['Malte Lehna', 'Clara Holzhüter', 'Sven Tomforde', 'Christoph Scholz']",2024-05-01T16:54:12Z,HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement   Learning with a Heuristic Target Topology Approach,"  With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex. One solution could be automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges. However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms. In contrast, we propose a more holistic approach by proposing specific Target Topologies (TTs) as actions. These topologies are selected based on their robustness. As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent. We compare the upgrade to the previous CAgent and can increase their L2RPN score significantly by 10%. Further, we achieve a 25% better median survival time with our TTs included. Later analysis shows that almost all TTs are close to the base topology, explaining their robustness ",2405.00629v2
"['David Bannach', 'Martin Jänicke', 'Vitor F. Rey', 'Sven Tomforde', 'Bernhard Sick', 'Paul Lukowicz']",2017-01-30T10:01:38Z,Self-Adaptation of Activity Recognition Systems to New Sensors,"  Traditional activity recognition systems work on the basis of training, taking a fixed set of sensors into account. In this article, we focus on the question how pattern recognition can leverage new information sources without any, or with minimal user input. Thus, we present an approach for opportunistic activity recognition, where ubiquitous sensors lead to dynamically changing input spaces. Our method is a variation of well-established principles of machine learning, relying on unsupervised clustering to discover structure in data and inferring cluster labels from a small number of labeled dates in a semi-supervised manner. Elaborating the challenges, evaluations of over 3000 sensor combinations from three multi-user experiments are presented in detail and show the potential benefit of our approach. ",1701.08528v1
"[""Mirko D'Angelo"", 'Sona Ghahremani', 'Simos Gerasimou', 'Johannes Grohmann', 'Ingrid Nunes', 'Sven Tomforde', 'Evangelos Pournaras']",2020-08-10T09:50:17Z,Learning to Learn in Collective Adaptive Systems: Mining Design Patterns   for Data-driven Reasoning,"  Engineering collective adaptive systems (CAS) with learning capabilities is a challenging task due to their multi-dimensional and complex design space. Data-driven approaches for CAS design could introduce new insights enabling system engineers to manage the CAS complexity more cost-effectively at the design-phase. This paper introduces a systematic approach to reason about design choices and patterns of learning-based CAS. Using data from a systematic literature review, reasoning is performed with a novel application of data-driven methodologies such as clustering, multiple correspondence analysis and decision trees. The reasoning based on past experience as well as supporting novel and innovative design choices are demonstrated. ",2008.03995v1
"['Lukas Rauch', 'Raphael Schwinger', 'Moritz Wirth', 'Bernhard Sick', 'Sven Tomforde', 'Christoph Scholz']",2023-08-14T13:06:10Z,Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with   Transformers,"  We propose a shift towards end-to-end learning in bird sound monitoring by combining self-supervised (SSL) and deep active learning (DAL). Leveraging transformer models, we aim to bypass traditional spectrogram conversions, enabling direct raw audio processing. ActiveBird2Vec is set to generate high-quality bird sound representations through SSL, potentially accelerating the assessment of environmental changes and decision-making processes for wind farms. Additionally, we seek to utilize the wide variety of bird vocalizations through DAL, reducing the reliance on extensively labeled datasets by human experts. We plan to curate a comprehensive set of tasks through Huggingface Datasets, enhancing future comparability and reproducibility of bioacoustic research. A comparative analysis between various transformer models will be conducted to evaluate their proficiency in bird sound recognition tasks. We aim to accelerate the progression of avian bioacoustic research and contribute to more effective conservation strategies. ",2308.07121v2
"['Inga Loeser', 'Martin Braun', 'Christian Gruhl', 'Jan-Hendrik Menke', 'Bernhard Sick', 'Sven Tomforde']",2021-12-14T16:11:55Z,"Towards Organic Distribution Systems -- The Vision of Self-Configuring,   Self-Organising, Self-Healing, and Self-Optimising Power Distribution   Management","  Due to the decarbonisation of energy use, the power system is expected to become the backbone of all energy sectors and thus the basic critical infrastructure. High penetration with distributed energy resources demands the coordination of a large number of prosumers, partly controlled by home energy management systems (HEMS), to be designed in such a way that the power system's operational limits are not violated. On the grid level, distribution management systems (DMS) try to keep the power system in the normal operational state. On the prosumer level, distributed HEMS optimise the internal power flows by using batteries, photovoltaic generators, or flexible loads optimally. The vision of the ODiS (Organic Distribution System) initiative is to develop an architecture to operate a distribution grid reliably, with high resiliency, and fully autonomously by developing ""organic"" HEMS and DMS which possess multiple self-* capabilities. Thus, ODiS seeks answers to the following question: How can we create the most appropriate models, techniques, and algorithms to develop novel kinds of self-configuring, self-organising, self-healing, and self-optimising DMS that are integrally coupled with the distributed HEMS? In this article, the vision of ODiS is presented in detail based on a thorough review of the state of the art. ",2112.07507v1
"['Malte Lehna', 'Jan Viebahn', 'Christoph Scholz', 'Antoine Marot', 'Sven Tomforde']",2023-04-03T07:34:43Z,Managing power grids through topology actions: A comparative study   between advanced rule-based and reinforcement learning agents,"  The operation of electricity grids has become increasingly complex due to the current upheaval and the increase in renewable energy production. As a consequence, active grid management is reaching its limits with conventional approaches. In the context of the Learning to Run a Power Network challenge, it has been shown that Reinforcement Learning (RL) is an efficient and reliable approach with considerable potential for automatic grid operation. In this article, we analyse the submitted agent from Binbinchen and provide novel strategies to improve the agent, both for the RL and the rule-based approach. The main improvement is a N-1 strategy, where we consider topology actions that keep the grid stable, even if one line is disconnected. More, we also propose a topology reversion to the original grid, which proved to be beneficial. The improvements are tested against reference approaches on the challenge test sets and are able to increase the performance of the rule-based agent by 27%. In direct comparison between rule-based and RL agent we find similar performance. However, the RL agent has a clear computational advantage. We also analyse the behaviour in an exemplary case in more detail to provide additional insights. Here, we observe that through the N-1 strategy, the actions of the agents become more diversified. ",2304.00765v2
"['Malte Lehna', 'Mohamed Hassouna', 'Dmitry Degtyar', 'Sven Tomforde', 'Christoph Scholz']",2024-06-24T08:20:43Z,Fault Detection for agents on power grid topology optimization: A   Comprehensive analysis,"  The topology optimization of transmission networks using Deep Reinforcement Learning (DRL) has increasingly come into focus. Various researchers have proposed different DRL agents, which are often benchmarked on the Grid2Op environment from the Learning to Run a Power Network (L2RPN) challenges. The environments have many advantages with their realistic chronics and underlying power flow backends. However, the interpretation of agent survival or failure is not always clear, as there are a variety of potential causes. In this work, we focus on the failures of the power grid to identify patterns and detect them a priori. We collect the failed chronics of three different agents on the WCCI 2022 L2RPN environment, totaling about 40k data points. By clustering, we are able to detect five distinct clusters, identifying different failure types. Further, we propose a multi-class prediction approach to detect failures beforehand and evaluate five different models. Here, the Light Gradient-Boosting Machine (LightGBM) shows the best performance, with an accuracy of 86%. It also correctly identifies in 91% of the time failure and survival observations. Finally, we provide a detailed feature importance analysis that identifies critical features and regions in the grid. ",2406.16426v2
"['Lukas Rauch', 'Raphael Schwinger', 'Moritz Wirth', 'René Heinrich', 'Denis Huseljic', 'Jonas Lange', 'Stefan Kahl', 'Bernhard Sick', 'Sven Tomforde', 'Christoph Scholz']",2024-03-15T15:10:40Z,BirdSet: A Dataset and Benchmark for Classification in Avian   Bioacoustics,"  Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to assess environmental health. To maximize the potential of cost-effective and minimal-invasive passive acoustic monitoring (PAM), DL models must analyze bird vocalizations across a wide range of species and environmental conditions. However, data fragmentation challenges a comprehensive evaluation of generalization performance. Therefore, we introduce the BirdSet dataset, comprising approximately 520,000 global bird recordings for training and over 400 hours of PAM recordings for testing. Our benchmark offers baselines for several DL models to enhance comparability and consolidate research across studies, along with code implementations that include comprehensive training and evaluation protocols. ",2403.10380v3
"['Tom Hanika', 'Marek Herde', 'Jochen Kuhn', 'Jan Marco Leimeister', 'Paul Lukowicz', 'Sarah Oeste-Reiß', 'Albrecht Schmidt', 'Bernhard Sick', 'Gerd Stumme', 'Sven Tomforde', 'Katharina Anna Zweig']",2019-05-16T14:59:53Z,Collaborative Interactive Learning -- A clarification of terms and a   differentiation from other research fields,"  The field of collaborative interactive learning (CIL) aims at developing and investigating the technological foundations for a new generation of smart systems that support humans in their everyday life. While the concept of CIL has already been carved out in detail (including the fields of dedicated CIL and opportunistic CIL) and many research objectives have been stated, there is still the need to clarify some terms such as information, knowledge, and experience in the context of CIL and to differentiate CIL from recent and ongoing research in related fields such as active learning, collaborative learning, and others. Both aspects are addressed in this paper. ",1905.07264v1
"['Claudius Zelenka', 'Reinhard Koch']",2017-04-02T17:19:02Z,Restoration of Images with Wavefront Aberrations,"  This contribution deals with image restoration in optical systems with coherent illumination, which is an important topic in astronomy, coherent microscopy and radar imaging. Such optical systems suffer from wavefront distortions, which are caused by imperfect imaging components and conditions. Known image restoration algorithms work well for incoherent imaging, they fail in case of coherent images. In this paper a novel wavefront correction algorithm is presented, which allows image restoration under coherent conditions. In most coherent imaging systems, especially in astronomy, the wavefront deformation is known. Using this information, the proposed algorithm allows a high quality restoration even in case of severe wavefront distortions. We present two versions of this algorithm, which are an evolution of the Gerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is verified on simulated and real microscopic images. ",1704.00331v1
"['Lars Schmarje', 'Reinhard Koch']",2021-10-13T09:20:41Z,Life is not black and white -- Combining Semi-Supervised Learning with   fuzzy labels,"  The required amount of labeled data is one of the biggest issues in deep learning. Semi-Supervised Learning can potentially solve this issue by using additional unlabeled data. However, many datasets suffer from variability in the annotations. The aggregated labels from these annotation are not consistent between different annotators and thus are considered fuzzy. These fuzzy labels are often not considered by Semi-Supervised Learning. This leads either to an inferior performance or to higher initial annotation costs in the complete machine learning development cycle. We envision the incorporation of fuzzy labels into Semi-Supervised Learning and give a proof-of-concept of the potential lower costs and higher consistency in the complete development cycle. As part of our concept, we discuss current limitations, futures research opportunities and potential broad impacts. ",2110.06592v1
"['Tim Michels', 'Reinhard Koch']",2022-03-09T11:57:00Z,Ray Tracing-Guided Design of Plenoptic Cameras,"  The design of a plenoptic camera requires the combination of two dissimilar optical systems, namely a main lens and an array of microlenses. And while the construction process of a conventional camera is mainly concerned with focusing the image onto a single plane, in the case of plenoptic cameras there can be additional requirements such as a predefined depth of field or a desired range of disparities in neighboring microlens images. Due to this complexity, the manual creation of multiple plenoptic camera setups is often a time-consuming task. In this work we assume a simulation framework as well as the main lens data given and present a method to calculate the remaining aperture, sensor and microlens array parameters under different sets of constraints. Our ray tracing-based approach is shown to result in models outperforming their pendants generated with the commonly used paraxial approximations in terms of image quality, while still meeting the desired constraints. Both the implementation and evaluation setup including 30 plenoptic camera designs are made publicly available. ",2203.04660v1
"['Vasco Grossmann', 'Lars Schmarje', 'Reinhard Koch']",2022-07-13T14:25:30Z,Beyond Hard Labels: Investigating data label distributions,"  High-quality data is a key aspect of modern machine learning. However, labels generated by humans suffer from issues like label noise and class ambiguities. We raise the question of whether hard labels are sufficient to represent the underlying ground truth distribution in the presence of these inherent imprecision. Therefore, we compare the disparity of learning with hard and soft labels quantitatively and qualitatively for a synthetic and a real-world dataset. We show that the application of soft labels leads to improved performance and yields a more regular structure of the internal feature space. ",2207.06224v2
"['Simon-Martin Schröder', 'Rainer Kiko', 'Reinhard Koch']",2020-05-04T16:08:03Z,MorphoCluster: Efficient Annotation of Plankton images by Clustering,"  In this work, we present MorphoCluster, a software tool for data-driven, fast and accurate annotation of large image data sets. While already having surpassed the annotation rate of human experts, volume and complexity of marine data will continue to increase in the coming years. Still, this data requires interpretation. MorphoCluster augments the human ability to discover patterns and perform object classification in large amounts of data by embedding unsupervised clustering in an interactive process. By aggregating similar images into clusters, our novel approach to image annotation increases consistency, multiplies the throughput of an annotator and allows experts to adapt the granularity of their sorting scheme to the structure in the data. By sorting a set of 1.2M objects into 280 data-driven classes in 71 hours (16k objects per hour), with 90% of these classes having a precision of 0.889 or higher. This shows that MorphoCluster is at the same time fast, accurate and consistent, provides a fine-grained and data-driven classification and enables novelty detection. MorphoCluster is available as open-source software at https://github.com/morphocluster. ",2005.01595v1
"['Tim Michels', 'Arne Petersen', 'Reinhard Koch']",2022-03-09T11:58:00Z,Creating Realistic Ground Truth Data for the Evaluation of Calibration   Methods for Plenoptic and Conventional Cameras,"  Camera calibration methods usually consist of capturing images of known calibration patterns and using the detected correspondences to optimize the parameters of the assumed camera model. A meaningful evaluation of these methods relies on the availability of realistic synthetic data. In previous works concerned with conventional cameras the synthetic data was mainly created by rendering perfect images with a pinhole camera and subsequently adding distortions and aberrations to the renderings and correspondences according to the assumed camera model. This method can bias the evaluation since not every camera perfectly complies with an assumed model. Furthermore, in the field of plenoptic camera calibration there is no synthetic ground truth data available at all. We address these problems by proposing a method based on backward ray tracing to create realistic ground truth data that can be used for an unbiased evaluation of calibration methods for both types of cameras. ",2203.04661v1
"['Tim Michels', 'Arne Petersen', 'Luca Palmieri', 'Reinhard Koch']",2022-03-09T11:58:31Z,Simulation of Plenoptic Cameras,"  Plenoptic cameras enable the capturing of spatial as well as angular color information which can be used for various applications among which are image refocusing and depth calculations. However, these cameras are expensive and research in this area currently lacks data for ground truth comparisons. In this work we describe a flexible, easy-to-use Blender model for the different plenoptic camera types which is on the one hand able to provide the ground truth data for research and on the other hand allows an inexpensive assessment of the cameras usefulness for the desired applications. Furthermore we show that the rendering results exhibit the same image degradation effects as real cameras and make our simulation publicly available. ",2203.04662v1
"['Tim Michels', 'Daniel Mäckelmann', 'Reinhard Koch']",2024-02-20T10:35:51Z,Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard   Plenoptic Camera,"  Among the common applications of plenoptic cameras are depth reconstruction and post-shot refocusing. These require a calibration relating the camera-side light field to that of the scene. Numerous methods with this goal have been developed based on thin lens models for the plenoptic camera's main lens and microlenses. Our work addresses the often-overlooked role of the main lens exit pupil in these models and specifically in the decoding process of standard plenoptic camera (SPC) images. We formally deduce the connection between the refocusing distance and the resampling parameter for the decoded light field and provide an analysis of the errors that arise when the exit pupil is not considered. In addition, previous work is revisited with respect to the exit pupil's role and all theoretical results are validated through a ray-tracing-based simulation. With the public release of the evaluated SPC designs alongside our simulation and experimental data we aim to contribute to a more accurate and nuanced understanding of plenoptic camera optics. ",2402.12891v2
"['Stefan Reinhold. Timo Damm', 'Lukas Huber', 'Reimer Andresen', 'Reinhard Barkmann', 'Claus-C. Glüer', 'Reinhard Koch']",2018-12-03T12:07:01Z,An Analysis by Synthesis Approach for Automatic Vertebral Shape   Identification in Clinical QCT,"  Quantitative computed tomography (QCT) is a widely used tool for osteoporosis diagnosis and monitoring. The assessment of cortical markers like cortical bone mineral density (BMD) and thickness is a demanding task, mainly because of the limited spatial resolution of QCT. We propose a direct model based method to automatically identify the surface through the center of the cortex of human vertebra. We develop a statistical bone model and analyze its probability distribution after the imaging process. Using an as-rigid-as-possible deformation we find the cortical surface that maximizes the likelihood of our model given the input volume. Using the European Spine Phantom (ESP) and a high resolution \mu CT scan of a cadaveric vertebra, we show that the proposed method is able to accurately identify the real center of cortex ex-vivo. To demonstrate the in-vivo applicability of our method we use manually obtained surfaces for comparison. ",1812.00693v1
"['Yuan Gao', 'Robert Bregovic', 'Reinhard Koch', 'Atanas Gotchev']",2020-03-19T15:28:08Z,DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field   Reconstruction,"  The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one of the most effective methods for Densely-Sampled Light Field (DSLF) reconstruction. The ST-based DSLF reconstruction typically relies on an iterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse regularization in shearlet domain, involving dozens of transformations between image domain and shearlet domain, which are in general time-consuming. To overcome this limitation, a novel learning-based ST approach, referred to as Deep Residual Shearlet Transform (DRST), is proposed in this paper. Specifically, for an input sparsely-sampled EPI, DRST employs a deep fully Convolutional Neural Network (CNN) to predict the residuals of the shearlet coefficients in shearlet domain in order to reconstruct a densely-sampled EPI in image domain. The DRST network is trained on synthetic Sparsely-Sampled Light Field (SSLF) data only by leveraging elaborately-designed masks. Experimental results on three challenging real-world light field evaluation datasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the superiority of the proposed learning-based DRST approach over the non-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a 2.4x speedup over ST, at least. ",2003.08865v1
"['Lars Schmarje', 'Vasco Grossmann', 'Claudius Zelenka', 'Johannes Brünger', 'Reinhard Koch']",2023-06-21T11:35:37Z,Annotating Ambiguous Images: General Annotation Strategy for   High-Quality Data with Real-World Biomedical Validation,"  In the field of image classification, existing methods often struggle with biased or ambiguous data, a prevalent issue in real-world scenarios. Current strategies, including semi-supervised learning and class blending, offer partial solutions but lack a definitive resolution. Addressing this gap, our paper introduces a novel strategy for generating high-quality labels in challenging datasets. Central to our approach is a clearly designed flowchart, based on a broad literature review, which enables the creation of reliable labels. We validate our methodology through a rigorous real-world test case in the biomedical field, specifically in deducing height reduction from vertebral imaging. Our empirical study, leveraging over 250,000 annotations, demonstrates the effectiveness of our strategies decisions compared to their alternatives. ",2306.12189v2
"['Lars Schmarje', 'Monty Santarossa', 'Simon-Martin Schröder', 'Reinhard Koch']",2020-02-20T13:29:27Z,"A survey on Semi-, Self- and Unsupervised Learning for Image   Classification","  While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains: The current strategies rely heavily on a huge amount of labeled data. In many real-world problems, it is not feasible to create such an amount of labeled training data. Therefore, it is common to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey, we provide an overview of often used ideas and methods in image classification with fewer labels. We compare 34 methods in detail based on their performance and their commonly used ideas rather than a fine-grained taxonomy. In our analysis, we identify three major trends that lead to future research opportunities. 1. State-of-the-art methods are scaleable to real-world applications in theory but issues like class imbalance, robustness, or fuzzy labels are not considered. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing and therefore methods need to be extended to settings with a variable number of classes. 3. All methods share some common ideas but we identify clusters of methods that do not share many ideas. We show that combining ideas from different clusters can lead to better performance. ",2002.08721v5
"['Monty Santarossa', 'Lukas Schneider', 'Claudius Zelenka', 'Lars Schmarje', 'Reinhard Koch', 'Uwe Franke']",2021-07-07T08:16:42Z,Learning Stixel-based Instance Segmentation,"  Stixels have been successfully applied to a wide range of vision tasks in autonomous driving, recently including instance segmentation. However, due to their sparse occurrence in the image, until now Stixels seldomly served as input for Deep Learning algorithms, restricting their utility for such approaches. In this work we present StixelPointNet, a novel method to perform fast instance segmentation directly on Stixels. By regarding the Stixel representation as unstructured data similar to point clouds, architectures like PointNet are able to learn features from Stixels. We use a bounding box detector to propose candidate instances, for which the relevant Stixels are extracted from the input image. On these Stixels, a PointNet models learns binary segmentations, which we then unify throughout the whole image in a final selection step. StixelPointNet achieves state-of-the-art performance on Stixel-level, is considerably faster than pixel-based segmentation methods, and shows that with our approach the Stixel domain can be introduced to many new 3D Deep Learning tasks. ",2107.03070v1
"['Lars Schmarje', 'Claudius Zelenka', 'Ulf Geisen', 'Claus-C. Glüer', 'Reinhard Koch']",2019-07-30T12:56:01Z,2D and 3D Segmentation of uncertain local collagen fiber orientations in   SHG microscopy,"  Collagen fiber orientations in bones, visible with Second Harmonic Generation (SHG) microscopy, represent the inner structure and its alteration due to influences like cancer. While analyses of these orientations are valuable for medical research, it is not feasible to analyze the needed large amounts of local orientations manually. Since we have uncertain borders for these local orientations only rough regions can be segmented instead of a pixel-wise segmentation. We analyze the effect of these uncertain borders on human performance by a user study. Furthermore, we compare a variety of 2D and 3D methods such as classical approaches like Fourier analysis with state-of-the-art deep neural networks for the classification of local fiber orientations. We present a general way to use pretrained 2D weights in 3D neural networks, such as Inception-ResNet-3D a 3D extension of Inception-ResNet-v2. In a 10 fold cross-validation our two stage segmentation based on Inception-ResNet-3D and transferred 2D ImageNet weights achieves a human comparable accuracy. ",1907.12868v1
"['Claudius Zelenka', 'Marius Kamp', 'Kolja Strohm', 'Akram Kadoura', 'Jacob Johny', 'Reinhard Koch', 'Lorenz Kienle']",2022-07-28T11:31:43Z,Automated Classification of Nanoparticles with Various Ultrastructures   and Sizes,"  Accurately measuring the size, morphology, and structure of nanoparticles is very important, because they are strongly dependent on their properties for many applications. In this paper, we present a deep-learning based method for nanoparticle measurement and classification trained from a small data set of scanning transmission electron microscopy images. Our approach is comprised of two stages: localization, i.e., detection of nanoparticles, and classification, i.e., categorization of their ultrastructure. For each stage, we optimize the segmentation and classification by analysis of the different state-of-the-art neural networks. We show how the generation of synthetic images, either using image processing or using various image generation neural networks, can be used to improve the results in both stages. Finally, the application of the algorithm to bimetallic nanoparticles demonstrates the automated data collection of size distributions including classification of complex ultrastructures. The developed method can be easily transferred to other material systems and nanoparticle structures. ",2207.14023v1
"['Lars Schmarje', 'Vasco Grossmann', 'Tim Michels', 'Jakob Nazarenus', 'Monty Santarossa', 'Claudius Zelenka', 'Reinhard Koch']",2023-05-22T08:12:25Z,"Label Smarter, Not Harder: CleverLabel for Faster Annotation of   Ambiguous Image Classification with Higher Quality","  High-quality data is crucial for the success of machine learning, but labeling large datasets is often a time-consuming and costly process. While semi-supervised learning can help mitigate the need for labeled data, label quality remains an open issue due to ambiguity and disagreement among annotators. Thus, we use proposal-guided annotations as one option which leads to more consistency between annotators. However, proposing a label increases the probability of the annotators deciding in favor of this specific label. This introduces a bias which we can simulate and remove. We propose a new method CleverLabel for Cost-effective LabEling using Validated proposal-guidEd annotations and Repaired LABELs. CleverLabel can reduce labeling costs by up to 30.0%, while achieving a relative improvement in Kullback-Leibler divergence of up to 29.8% compared to the previous state-of-the-art on a multi-domain real-world image classification benchmark. CleverLabel offers a novel solution to the challenge of efficiently labeling large datasets while also improving the label quality. ",2305.12811v1
"['Lars Schmarje', 'Stefan Reinhold', 'Timo Damm', 'Eric Orwoll', 'Claus-C. Glüer', 'Reinhard Koch']",2022-07-22T09:35:48Z,Opportunistic hip fracture risk prediction in Men from X-ray: Findings   from the Osteoporosis in Men (MrOS) Study,"  Osteoporosis is a common disease that increases fracture risk. Hip fractures, especially in elderly people, lead to increased morbidity, decreased quality of life and increased mortality. Being a silent disease before fracture, osteoporosis often remains undiagnosed and untreated. Areal bone mineral density (aBMD) assessed by dual-energy X-ray absorptiometry (DXA) is the gold-standard method for osteoporosis diagnosis and hence also for future fracture prediction (prognostic). However, the required special equipment is not broadly available everywhere, in particular not to patients in developing countries. We propose a deep learning classification model (FORM) that can directly predict hip fracture risk from either plain radiographs (X-ray) or 2D projection images of computed tomography (CT) data. Our method is fully automated and therefore well suited for opportunistic screening settings, identifying high risk patients in a broader population without additional screening. FORM was trained and evaluated on X-rays and CT projections from the Osteoporosis in Men (MrOS) study. 3108 X-rays (89 incident hip fractures) or 2150 CTs (80 incident hip fractures) with a 80/20 split were used. We show that FORM can correctly predict the 10-year hip fracture risk with a validation AUC of 81.44 +- 3.11% / 81.04 +- 5.54% (mean +- STD) including additional information like age, BMI, fall history and health background across a 5-fold cross validation on the X-ray and CT cohort, respectively. Our approach significantly (p < 0.01) outperforms previous methods like Cox Proportional-Hazards Model and \frax with 70.19 +- 6.58 and 74.72 +- 7.21 respectively on the X-ray cohort. Our model outperform on both cohorts hip aBMD based predictions. We are confident that FORM can contribute on improving osteoporosis diagnosis at an early stage. ",2207.10970v2
"['Stefan Reinhold', 'Timo Damm', 'Sebastian Büsse', 'Stanislav N. Gorb', 'Claus-C. Glüer', 'Reinhard Koch']",2020-09-18T07:30:18Z,An Analysis by Synthesis Method that Allows Accurate Spatial Modeling of   Thickness of Cortical Bone from Clinical QCT,"  Osteoporosis is a skeletal disorder that leads to increased fracture risk due to decreased strength of cortical and trabecular bone. Even with state-of-the-art non-invasive assessment methods there is still a high underdiagnosis rate. Quantitative computed tomography (QCT) permits the selective analysis of cortical bone, however the low spatial resolution of clinical QCT leads to an overestimation of the thickness of cortical bone (Ct.Th) and bone strength.   We propose a novel, model based, fully automatic image analysis method that allows accurate spatial modeling of the thickness distribution of cortical bone from clinical QCT. In an analysis-by-synthesis (AbS) fashion a stochastic scan is synthesized from a probabilistic bone model, the optimal model parameters are estimated using a maximum a-posteriori approach. By exploiting the different characteristics of in-plane and out-of-plane point spread functions of CT scanners the proposed method is able assess the spatial distribution of cortical thickness.   The method was evaluated on eleven cadaveric human vertebrae, scanned by clinical QCT and analyzed using standard methods and AbS, both compared to high resolution peripheral QCT (HR-pQCT) as gold standard. While standard QCT based measurements overestimated Ct.Th. by 560% and did not show significant correlation with the gold standard ($r^2 = 0.20,\, p = 0.169$) the proposed method eliminated the overestimation and showed a significant tight correlation with the gold standard ($r^2 = 0.98,\, p < 0.0001$) a root mean square error below 10%. ",2009.08664v1
"['Lars Schmarje', 'Johannes Brünger', 'Monty Santarossa', 'Simon-Martin Schröder', 'Rainer Kiko', 'Reinhard Koch']",2020-12-03T08:54:25Z,Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels   with overclustering,"  A long-standing issue with deep learning is the need for large and consistently labeled datasets. Although the current research in semi-supervised learning can decrease the required amount of annotated data by a factor of 10 or even more, this line of research still uses distinct classes like cats and dogs. However, in the real-world we often encounter problems where different experts have different opinions, thus producing fuzzy labels. We propose a novel framework for handling semi-supervised classifications of such fuzzy labels. Our framework is based on the idea of overclustering to detect substructures in these fuzzy labels. We propose a novel loss to improve the overclustering capability of our framework and show on the common image classification dataset STL-10 that it is faster and has better overclustering performance than previous work. On a real-world plankton dataset, we illustrate the benefit of overclustering for fuzzy labels and show that we beat previous state-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more consistent predictions of substructures. ",2012.01768v2
"['Lars Schmarje', 'Johannes Brünger', 'Monty Santarossa', 'Simon-Martin Schröder', 'Rainer Kiko', 'Reinhard Koch']",2021-10-13T10:50:50Z,Fuzzy Overclustering: Semi-Supervised Classification of Fuzzy Labels   with Overclustering and Inverse Cross-Entropy,"  Deep learning has been successfully applied to many classification problems including underwater challenges. However, a long-standing issue with deep learning is the need for large and consistently labeled datasets. Although current approaches in semi-supervised learning can decrease the required amount of annotated data by a factor of 10 or even more, this line of research still uses distinct classes. For underwater classification, and uncurated real-world datasets in general, clean class boundaries can often not be given due to a limited information content in the images and transitional stages of the depicted objects. This leads to different experts having different opinions and thus producing fuzzy labels which could also be considered ambiguous or divergent. We propose a novel framework for handling semi-supervised classifications of such fuzzy labels. It is based on the idea of overclustering to detect substructures in these fuzzy labels. We propose a novel loss to improve the overclustering capability of our framework and show the benefit of overclustering for fuzzy labels. We show that our framework is superior to previous state-of-the-art semi-supervised methods when applied to real-world plankton data with fuzzy labels. Moreover, we acquire 5 to 10\% more consistent predictions of substructures. ",2110.06630v1
"['Lars Schmarje', 'Monty Santarossa', 'Simon-Martin Schröder', 'Claudius Zelenka', 'Rainer Kiko', 'Jenny Stracke', 'Nina Volkmann', 'Reinhard Koch']",2021-06-30T17:00:47Z,A data-centric approach for improving ambiguous labels with combined   semi-supervised classification and clustering,"  Consistently high data quality is essential for the development of novel loss functions and architectures in the field of deep learning. The existence of such data and labels is usually presumed, while acquiring high-quality datasets is still a major issue in many cases. In real-world datasets we often encounter ambiguous labels due to subjective annotations by annotators. In our data-centric approach, we propose a method to relabel such ambiguous labels instead of implementing the handling of this issue in a neural network. A hard classification is by definition not enough to capture the real-world ambiguity of the data. Therefore, we propose our method ""Data-Centric Classification & Clustering (DC3)"" which combines semi-supervised classification and clustering. It automatically estimates the ambiguity of an image and performs a classification or clustering depending on that ambiguity. DC3 is general in nature so that it can be used in addition to many Semi-Supervised Learning (SSL) algorithms. On average, this results in a 7.6% better F1-Score for classifications and 7.9% lower inner distance of clusters across multiple evaluated SSL algorithms and datasets. Most importantly, we give a proof-of-concept that the classifications and clusterings from DC3 are beneficial as proposals for the manual refinement of such ambiguous labels. Overall, a combination of SSL with our method DC3 can lead to better handling of ambiguous labels during the annotation process. ",2106.16209v4
"['Lars Schmarje', 'Vasco Grossmann', 'Claudius Zelenka', 'Sabine Dippel', 'Rainer Kiko', 'Mariusz Oszust', 'Matti Pastell', 'Jenny Stracke', 'Anna Valros', 'Nina Volkmann', 'Reinhard Koch']",2022-07-13T14:17:21Z,Is one annotation enough? A data-centric image classification benchmark   for noisy and ambiguous label estimation,"  High-quality data is necessary for modern machine learning. However, the acquisition of such data is difficult due to noisy and ambiguous annotations of humans. The aggregation of such annotations to determine the label of an image leads to a lower data quality. We propose a data-centric image classification benchmark with ten real-world datasets and multiple annotations per image to allow researchers to investigate and quantify the impact of such data quality issues. With the benchmark we can study the impact of annotation costs and (semi-)supervised methods on the data quality for image classification by applying a novel methodology to a range of different algorithms and diverse datasets. Our benchmark uses a two-phase approach via a data label improvement method in the first phase and a fixed evaluation model in the second phase. Thereby, we give a measure for the relation between the input labeling effort and the performance of (semi-)supervised algorithms to enable a deeper insight into how labels should be created for effective model training. Across thousands of experiments, we show that one annotation is not enough and that the inclusion of multiple annotations allows for a better approximation of the real underlying class distribution. We identify that hard labels can not capture the ambiguity of the data and this might lead to the common issue of overconfident models. Based on the presented datasets, benchmarked methods, and analysis, we create multiple research opportunities for the future directed at the improvement of label noise estimation approaches, data annotation schemes, realistic (semi-)supervised learning, or more reliable image collection. ",2207.06214v3
['Michael Hanus'],2022-05-13T18:20:50Z,From Logic to Functional Logic Programs,"  Logic programming is a flexible programming paradigm due to the use of predicates without a fixed data flow. To extend logic languages with the compact notation of functional programming, there are various proposals to map evaluable functions into predicates in order to stay in the logic programming framework. Since amalgamated functional logic languages offer flexible as well as efficient evaluation strategies, we propose an opposite approach in this paper. By mapping logic programs into functional logic programs with a transformation based on inferring functional dependencies, we develop a fully automatic transformation which keeps the flexibility of logic programming but can improve computations by reducing infinite search spaces to finite ones. ",2205.06841v1
['Michael Hanus'],2016-08-19T14:40:02Z,CurryCheck: Checking Properties of Curry Programs,"  We present CurryCheck, a tool to automate the testing of programs written in the functional logic programming language Curry. CurryCheck executes unit tests as well as property tests which are parameterized over one or more arguments. In the latter case, CurryCheck tests these properties by systematically enumerating test cases so that, for smaller finite domains, CurryCheck can actually prove properties. Unit tests and properties can be defined in a Curry module without being exported. Thus, they are also useful to document the intended semantics of the source code. Furthermore, CurryCheck also supports the automated checking of specifications and contracts occurring in source programs. Hence, CurryCheck is a useful tool that contributes to the property- and specification-based development of reliable and well tested declarative programs. ",1608.05617v1
['Michael Hanus'],2017-09-14T14:35:50Z,Combining Static and Dynamic Contract Checking for Curry,"  Static type systems are usually not sufficient to express all requirements on function calls. Hence, contracts with pre- and postconditions can be used to express more complex constraints on operations. Contracts can be checked at run time to ensure that operations are only invoked with reasonable arguments and return intended results. Although such dynamic contract checking provides more reliable program execution, it requires execution time and could lead to program crashes that might be detected with more advanced methods at compile time. To improve this situation for declarative languages, we present an approach to combine static and dynamic contract checking for the functional logic language Curry. Based on a formal model of contract checking for functional logic programming, we propose an automatic method to verify contracts at compile time. If a contract is successfully verified, dynamic checking of it can be omitted. This method decreases execution time without degrading reliable program execution. In the best case, when all contracts are statically verified, it provides trust in the software since crashes due to contract violations cannot occur during program execution. ",1709.04816v1
['Michael Hanus'],2024-02-20T12:25:36Z,Inferring Non-Failure Conditions for Declarative Programs,"  Unintended failures during a computation are painful but frequent during software development. Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. This paper presents an approach to verify such assumptions. For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. In the positive case, the absence of such failures is ensured. In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. Our method is fully automatic and can be applied to larger declarative programs. The results of an implementation for functional logic Curry programs are presented. ",2402.12960v1
['Michael Hanus'],2007-01-24T06:55:34Z,A Generic Analysis Environment for Curry Programs,"  We present CurryBrowser, a generic analysis environment for the declarative multi-paradigm language Curry. CurryBrowser supports browsing through the program code of an application written in Curry, i.e., the main module and all directly or indirectly imported modules. Each module can be shown in different formats (e.g., source code, interface, intermediate code) and, inside each module, various properties of functions defined in this module can be analyzed. In order to support the integration of various program analyses, CurryBrowser has a generic interface to connect local and global analyses implemented in Curry. CurryBrowser is completely implemented in Curry using libraries for GUI programming and meta-programming. ",cs/0701147v1
"['Sergio Antoy', 'Michael Hanus']",2019-09-20T15:37:21Z,Equivalence Checking of Non-deterministic Operations,"  Checking the semantic equivalence of operations is an important task in software development. For instance, regression testing is a routine task performed when software systems are developed and improved, and software package managers require the equivalence of operations in different versions of a package within the same major number version. A solid foundation is required to support a good automation of this process. It has been shown that the notion of equivalence is not obvious when non-deterministic features are present. In this paper, we discuss a general notion of equivalence in functional logic programs and develop a practical method to check it. Our method is integrated in a property-based testing tool which is used in a software package manager to check the semantic versioning of software packages. ",1909.09562v1
"['Sergio Antoy', 'Michael Hanus', 'Finn Teegen']",2018-08-22T15:13:56Z,Synthesizing Set Functions,"  Set functions are a feature of functional logic programming to encapsulate all results of a non-deterministic computation in a single data structure. Given a function $f$ of a functional logic program written in Curry, we describe a technique to synthesize the definition of the set function of $f$. The definition produced by our technique is based on standard Curry constructs. Our approach is interesting for three reasons. It allows reasoning about set functions, it offers an implementation of set functions which can be added to any Curry system, and it has the potential of changing our thinking about the implementation of non-determinism, a notoriously difficult problem. ",1808.07401v1
"['Sergio Antoy', 'Michael Hanus', 'Andy Jost', 'Steven Libby']",2019-08-29T08:47:52Z,ICurry,"  FlatCurry is a well-established intermediate representation of Curry programs used in compilers that translate Curry code into Prolog and Haskell code. Some FlatCurry constructs have no direct translation into imperative code. These constructs must be each handled differently when translating Curry code into C, C++ and Python code. We introduce a new representation of Curry programs, called ICurry, and derive a translation from all FlatCurry constructs into ICurry. We present the syntax of ICurry and the translation from FlatCurry to ICurry. We present a model of functional logic computations as graph rewriting, show how this model can be implemented in a low-level imperative language, and describe the translation from ICurry to this model. ",1908.11101v1
"['Michael Hanus', 'Claudio Sacerdoti Coen']",2020-09-02T12:33:02Z,Pre-Proceedings of the 28th International Workshop on Functional and   Logic Programming (WFLP 2020),"  This volume constitutes the pre-proceedings of the 28th International Workshop on Functional and Logic Programming (WFLP 2020), organized by the University of Bologna, Italy, as part of Bologna Federated Conference on Programming Languages 2020. The international Workshop on Functional and (constraint) Logic Programming (WFLP) aims at bringing together researchers, students, and practitioners interested in functional programming, logic programming, and their integration. WFLP has a reputation for being a lively and friendly forum, and it is open for presenting and discussing work in progress, technical contributions, experience reports, experiments, reviews, and system descriptions. ",2009.01001v2
"['Michael Hanus', 'Johannes Koj']",2001-11-14T12:50:17Z,An Integrated Development Environment for Declarative Multi-Paradigm   Programming,"  In this paper we present CIDER (Curry Integrated Development EnviRonment), an analysis and programming environment for the declarative multi-paradigm language Curry. CIDER is a graphical environment to support the development of Curry programs by providing integrated tools for the analysis and visualization of programs. CIDER is completely implemented in Curry using libraries for GUI programming (based on Tcl/Tk) and meta-programming. An important aspect of our environment is the possible adaptation of the development environment to other declarative source languages (e.g., Prolog or Haskell) and the extensibility w.r.t. new analysis methods. To support the latter feature, the lazy evaluation strategy of the underlying implementation language Curry becomes quite useful. ",cs/0111039v2
"['Michael Hanus', 'Sven Koschnicke']",2011-03-21T20:01:02Z,An ER-based Framework for Declarative Web Programming,"  We describe a framework to support the implementation of web-based systems intended to manipulate data stored in relational databases. Since the conceptual model of a relational database is often specified as an entity-relationship (ER) model, we propose to use the ER model to generate a complete implementation in the declarative programming language Curry. This implementation contains operations to create and manipulate entities of the data model, supports authentication, authorization, session handling, and the composition of individual operations to user processes. Furthermore, the implementation ensures the consistency of the database w.r.t. the data dependencies specified in the ER model, i.e., updates initiated by the user cannot lead to an inconsistent state of the database. In order to generate a high-level declarative implementation that can be easily adapted to individual customer requirements, the framework exploits previous works on declarative database programming and web user interface construction in Curry. ",1103.4133v2
"['Michael Hanus', 'Finn Teegen']",2019-08-28T09:34:50Z,Adding Data to Curry,"  Functional logic languages can solve equations over user-defined data and functions. Thus, the definition of an appropriate meaning of equality has a long history in these languages, ranging from reflexive equality in early equational logic languages to strict equality in contemporary functional logic languages like Curry. With the introduction of type classes, where the equality operation ""=="" is overloaded and user-defined, the meaning became more complex. Moreover, logic variables appearing in equations require a different typing than pattern variables, since the latter might be instantiated with functional values or non-terminating operations. In this paper, we present a solution to these problems by introducing a new type class ""Data"" which is associated with specific algebraic data types, logic variables, and strict equality. We discuss the ideas of this class and its implications on various concepts of Curry, like unification, functional patterns, and program optimization. ",1908.10607v1
"['Sergio Antoy', 'Michael Hanus']",2016-05-04T17:18:05Z,Default Rules for Curry,"  In functional logic programs, rules are applicable independently of textual order, i.e., any rule can potentially be used to evaluate an expression. This is similar to logic languages and contrary to functional languages, e.g., Haskell enforces a strict sequential interpretation of rules. However, in some situations it is convenient to express alternatives by means of compact default rules. Although default rules are often used in functional programs, the non-deterministic nature of functional logic programs does not allow to directly transfer this concept from functional to functional logic languages in a meaningful way. In this paper we propose a new concept of default rules for Curry that supports a programming style similar to functional programming while preserving the core properties of functional logic programming, i.e., completeness, non-determinism, and logic-oriented use of functions. We discuss the basic concept and propose an implementation which exploits advanced features of functional logic languages. ",1605.01352v1
"['Michael Hanus', 'Finn Teegen']",2020-08-27T09:07:36Z,Memoized Pull-Tabbing for Functional Logic Programming,"  Pull-tabbing is an evaluation technique for functional logic programs which computes all non-deterministic results in a single graph structure. Pull-tab steps are local graph transformations to move non-deterministic choices towards the root of an expression. Pull-tabbing is independent of a search strategy so that different strategies (depth-first, breadth-first, parallel) can be used to extract the results of a computation. It has been used to compile functional logic languages into imperative or purely functional target languages. Pull-tab steps might duplicate choices in case of shared subexpressions. This could result in a dramatic increase of execution time compared to a backtracking implementation. In this paper we propose a refinement which avoids this efficiency problem while keeping all the good properties of pull-tabbing. We evaluate a first implementation of this improved technique in the Julia programming language. ",2008.11999v1
"['Bernd Braßel', 'Michael Hanus', 'Björn Peemöller', 'Fabian Reck']",2011-08-29T15:38:31Z,Implementing Equational Constraints in a Functional Language,"  KiCS2 is a new system to compile functional logic programs of the source language Curry into purely functional Haskell programs. The implementation is based on the idea to represent the search space as a data structure and logic variables as operations that generate their values. This has the advantage that one can apply various, and in particular, complete search strategies to compute solutions. However, the generation of all values for logic variables might be inefficient for applications that exploit constraints on partially known values. To overcome this drawback, we propose new techniques to implement equational constraints in this framework. In particular, we show how unification modulo function evaluation and functional patterns can be added without sacrificing the efficiency of the kernel implementation. ",1108.5609v1
"['Bernd Braßel', 'Michael Hanus', 'Marion Muller']",2007-11-02T16:49:30Z,Compiling ER Specifications into Declarative Programs,"  This paper proposes an environment to support high-level database programming in a declarative programming language. In order to ensure safe database updates, all access and update operations related to the database are generated from high-level descriptions in the entity- relationship (ER) model. We propose a representation of ER diagrams in the declarative language Curry so that they can be constructed by various tools and then translated into this representation. Furthermore, we have implemented a compiler from this representation into a Curry program that provides access and update operations based on a high-level API for database programming. ",0711.0348v1
"['Maria Alpuente', 'Michael Hanus', 'Salvador Lucas', 'German Vidal']",2004-03-09T15:43:00Z,Specialization of Functional Logic Programs Based on Needed Narrowing,"  Many functional logic languages are based on narrowing, a unification-based goal-solving mechanism which subsumes the reduction mechanism of functional languages and the resolution principle of logic languages. Needed narrowing is an optimal evaluation strategy which constitutes the basis of modern (narrowing-based) lazy functional logic languages. In this work, we present the fundamentals of partial evaluation in such languages. We provide correctness results for partial evaluation based on needed narrowing and show that the nice properties of this strategy are essential for the specialization process. In particular, the structure of the original program is preserved by partial evaluation and, thus, the same evaluation strategy can be applied for the execution of specialized programs. This is in contrast to other partial evaluation schemes for lazy functional logic programs which may change the program structure in a negative way. Recent proposals for the partial evaluation of declarative multi-paradigm programs use (some form of) needed narrowing to perform computations at partial evaluation time. Therefore, our results constitute the basis for the correctness of such partial evaluators. ",cs/0403011v1
"['Michael Hanus', 'Fabian Reck']",2013-07-17T14:30:17Z,A Generic Analysis Server System for Functional Logic Programs,"  We present a system, called CASS, for the analysis of functional logic programs. The system is generic so that various kinds of analyses (e.g., groundness, non-determinism, demanded arguments) can be easily integrated. In order to analyze larger applications consisting of dozens or hundreds of modules, CASS supports a modular and incremental analysis of programs. Moreover, it can be used by different programming tools, like documentation generators, analysis environments, program optimizers, as well as Eclipse-based development environments. For this purpose, CASS can also be invoked as a server system to get a language-independent access to its functionality. CASS is completely implemented in the functional logic language Curry as a master/worker architecture to exploit parallel or distributed execution environments. ",1307.4648v1
"['Michael Hanus', 'Julia Krone']",2017-01-03T10:32:32Z,A Typeful Integration of SQL into Curry,"  We present an extension of the declarative programming language Curry to support the access to data stored in relational databases via SQL. Since Curry is statically typed, our emphasis on this SQL integration is on type safety. Our extension respects the type system of Curry so that run-time errors due to ill-typed data are avoided. This is obtained by preprocessing SQL statements at compile time and translating them into type-safe database access operations. As a consequence, the type checker of the Curry system can spot type errors in SQL statements at compile time. To generate appropriately typed access operations, the preprocessor uses an entity-relationship (ER) model describing the structure of the relational data. In addition to standard SQL, SQL statements embedded in Curry can include program expressions and also relationships specified in the ER model. The latter feature is useful to avoid the error-prone use of foreign keys. As a result, our SQL integration supports a high-level and type-safe access to databases in Curry programs. ",1701.00631v1
"['Sergio Antoy', 'Michael Hanus', 'Steven Libby']",2017-01-03T10:34:39Z,Proving Non-Deterministic Computations in Agda,"  We investigate proving properties of Curry programs using Agda. First, we address the functional correctness of Curry functions that, apart from some syntactic and semantic differences, are in the intersection of the two languages. Second, we use Agda to model non-deterministic functions with two distinct and competitive approaches incorporating the non-determinism. The first approach eliminates non-determinism by considering the set of all non-deterministic values produced by an application. The second approach encodes every non-deterministic choice that the application could perform. We consider our initial experiment a success. Although proving properties of programs is a notoriously difficult task, the functional logic paradigm does not seem to add any significant layer of difficulty or complexity to the task. ",1701.00636v1
['Wilhelm Hasselbring'],2021-05-01T14:54:56Z,Benchmarking as Empirical Standard in Software Engineering Research,"  In empirical software engineering, benchmarks can be used for comparing different methods, techniques and tools. However, the recent ACM SIGSOFT Empirical Standards for Software Engineering Research do not include an explicit checklist for benchmarking. In this paper, we discuss benchmarks for software performance and scalability evaluation as example research areas in software engineering, relate benchmarks to some other empirical research methods, and discuss the requirements on benchmarks that may constitute the basis for a checklist of a benchmarking standard for empirical software engineering research. ",2105.00272v1
"['Holger Knoche', 'Wilhelm Hasselbring']",2021-03-21T13:46:43Z,Continuous API Evolution in Heterogenous Enterprise Software Systems,"  The ability to independently deploy parts of a software system is one of the cornerstones of modern software development, and allows for these parts to evolve independently and at different speeds.   A major challenge of such independent deployment, however, is to ensure that despite their individual evolution, the interfaces between interacting parts remain compatible. This is especially important for enterprise software systems, which are often highly integrated and based on heterogenous IT infrastructures.   Although several approaches for interface evolution have been proposed, many of these rely on the developer to adhere to certain rules, but provide little guidance for doing so. In this paper, we present an approach for interface evolution that is easy to use for developers, and also addresses typical challenges of heterogenous enterprise software, especially legacy system integration. ",2103.11397v1
"['Arne N. Johanson', 'Andreas Oschlies', 'Wilhelm Hasselbring', 'Wilhelm Hasselbring', 'Boris Worm']",2022-09-30T10:38:18Z,SPRAT: A Spatially-Explicit Marine Ecosystem Model Based on Population   Balance Equations,"  To successfully manage marine fisheries using an ecosystem-based approach, long-term predictions of fish stock development considering changing environmental conditions are necessary. Such predictions can be provided by end-to-end ecosystem models, which couple existing physical and biogeochemical ocean models with newly developed spatially-explicit fish stock models. Typically, Individual-Based Models (IBMs) and models based on Advection-Diffusion-Reaction (ADR) equations are employed for the fish stock models. In this paper, we present a novel fish stock model called SPRAT for end-to\hyp{}end ecosystem modeling based on Population Balance Equations (PBEs) that combines the advantages of IBMs and ADR models while avoiding their main drawbacks. SPRAT accomplishes this by describing the modeled ecosystem processes from the perspective of individuals while still being based on partial differential equations. We apply the SPRAT model to explore a well-documented regime shift observed on the eastern Scotian Shelf in the 1990s from a cod-dominated to a herring-dominated ecosystem. Model simulations are able to reconcile the observed multitrophic dynamics with documented changes in both fishing pressure and water temperature, followed by a predator-prey reversal that may have impeded recovery of depleted cod stocks. We conclude that our model can be used to generate new hypotheses and test ideas about spatially interacting fish populations, and their joint responses to both environmental and fisheries forcing. ",2210.01100v1
"['Henning Schnoor', 'Wilhelm Hasselbring']",2019-09-27T07:06:02Z,Comparing Static and Dynamic Weighted Software Coupling Metrics,"  Coupling metrics are an established way to measure software architecture quality with respect to modularity. Static coupling metrics are obtained from the source or compiled code of a program, while dynamic metrics use runtime data gathered e.g., by monitoring a system in production. We study \emph{weighted} dynamic coupling that takes into account how often a connection is executed during a system's run. We investigate the correlation between dynamic weighted metrics and their static counterparts. We use data collected from four different experiments, each monitoring production use of a commercial software system over a period of four weeks. We observe an unexpected level of correlation between the static and the weighted dynamic case as well as revealing differences between class- and package-level analyses. ",1909.12521v1
"['Alexander Barbie', 'Wilhelm Hasselbring']",2023-09-17T08:56:36Z,Embedded Software Development with Digital Twins: Specific Requirements   for Small and Medium-Sized Enterprises,"  The transformation to Industry 4.0 changes the way embedded software systems are developed. Digital twins have the potential for cost-effective software development and maintenance strategies. With reduced costs and faster development cycles, small and medium-sized enterprises (SME) have the chance to grow with new smart products. We interviewed SMEs about their current development processes. In this paper, we present the first results of these interviews. First results show that real-time requirements prevent, to date, a Software-in-the-Loop development approach, due to a lack of proper tooling. Security/safety concerns, and the accessibility of hardware are the main impediments. Only temporary access to the hardware leads to Software-in-the-Loop development approaches based on simulations/emulators. Yet, this is not in all use cases possible. All interviewees see the potential of Software-in-the-Loop approaches and digital twins with regard to quality and customization. One reason it will take some effort to convince engineers, is the conservative nature of the embedded community, particularly in SMEs. ",2309.09216v1
"['Alexander Barbie', 'Wilhelm Hasselbring']",2024-08-25T15:34:00Z,Toward Reproducibility of Digital Twin Research: Exemplified with the   PiCar-X,"  Digital twins are becoming increasingly relevant in the Industrial Internet of Things and Industry 4.0, enhancing the capabilities and quality of various applications. However, the concept of \dts lacks a unified definition and faces validation challenges, partly due to the scarcity of reproducible modules or source codes in existing studies. While many applications are described in case studies, they often lack detailed, re-usable specifications for researchers and engineers. In previous research, we defined and formalized the \dt concept. This paper presents a reproducible laboratory experiment that demonstrates various \dt concepts. Our formalized concept encompasses the \pt, the digital model, the digital template, the digital thread, the digital shadow, the \dt, and the \dtp. We illustrate this series of concepts by using a PiCar-X, showcasing the progression from a \pt to its \dtp. The entire code base is published as open source, and for each concept, Docker-compose files are provided to facilitate independent exploration, understanding, and extension. ",2408.13866v1
"['Alexander Krause', 'Malte Hansen', 'Wilhelm Hasselbring']",2021-09-29T06:49:09Z,Live Visualization of Dynamic Software Cities with Heat Map Overlays,"  The 3D city metaphor in software visualization is a well-explored rendering method. Numerous tools use their custom variation to visualize offline-analyzed data. Heat map overlays are one of these variants. They introduce a separate information layer in addition to the software city's own semantics. Results show that their usage facilitates program comprehension.   In this paper, we present our heat map approach for the city metaphor visualization based on live trace analysis. In comparison to previous approaches, our implementation uses live dynamic analysis of a software system's runtime behavior. At any time, users can toggle the heat map feature and choose which runtime-dependent metric the heat map should visualize. Our approach continuously and automatically renders both software cities and heat maps. It does not require a manual or semi-automatic generation of heat maps and seamlessly blends into the overall software visualization. We implemented this approach in our web-based tool ExplorViz, such that the heat map overlay is also available in our augmented reality environment. ExplorViz is developed as open source software and is continuously published via Docker images. A live demo of ExplorViz is publicly available. ",2109.14217v1
"['Reiner Jung', 'Sven Gundlach', 'Wilhelm Hasselbring']",2022-02-01T20:42:02Z,Thematic Domain Analysis for Ocean Modeling,"  Ocean science is a discipline that employs ocean models as an essential research asset. Such scientific modeling provides mathematical abstractions of real-world systems, e.g., the oceans. These models are then coded as implementations of the mathematical abstractions. The developed software systems are called models of the real-world system.   To advance the state in engineering such ocean models, we intend to better understand how ocean models are developed and maintained in ocean science. In this paper, we present the results of semi-structured interviews and the Thematic Analysis~(TA) of the interview results to analyze the domain of ocean modeling. Thereby, we identified developer requirements and impediments to model development and evolution, and related themes. This analysis can help to understand where methods from software engineering should be introduced and which challenges need to be addressed.   We suggest that other researchers extend and repeat our TA with model developers and research software engineers working in related domains to further advance our knowledge and skills in scientific modeling. ",2202.00747v1
"['Christian Zirkelbach', 'Alexander Krause', 'Wilhelm Hasselbring']",2019-07-12T10:27:42Z,Modularization of Research Software for Collaborative Open Source   Development,"  Software systems evolve over their lifetime. Changing conditions, such as requirements or customer requests make it inevitable for developers to perform adjustments to the underlying code base. Especially in the context of open source software where everybody can contribute, requirements can change over time and new user groups may be addressed. In particular, research software is often not structured with a maintainable and extensible architecture. In combination with obsolescent technologies, this is a challenging task for new developers, especially, when students are involved.   In this paper, we report on the modularization process and architecture of our open source research project ExplorViz towards a microservice architecture. The new architecture facilitates a collaborative development process for both researchers and students. We describe the modularization measures and present how we solved occurring issues and enhanced our development process. Afterwards, we illustrate our modularization approach with our modernized, extensible software system architecture and highlight the improved collaborative development process. Finally, we present a proof-of-concept implementation featuring several developed extensions in terms of architecture and extensibility. ",1907.05663v1
"['Reiner Jung', 'Sven Gundlach', 'Wilhelm Hasselbring']",2021-08-19T09:52:19Z,Software Development Processes in Ocean System Modeling,"  Scientific modeling provides mathematical abstractions of real-world systems and builds software as implementations of these mathematical abstractions. Ocean science is a multidisciplinary discipline developing scientific models and simulations as ocean system models that are an essential research asset.   In software engineering and information systems research, modeling is also an essential activity. In particular, business process modeling for business process management and systems engineering is the activity of representing processes of an enterprise, so that the current process may be analyzed, improved, and automated.   In this paper, we employ process modeling for analyzing scientific software development in ocean science to advance the state in engineering of ocean system models and to better understand how ocean system models are developed and maintained in ocean science. We interviewed domain experts in semi-structured interviews, analyzed the results via thematic analysis, and modeled the results via the business process modeling notation BPMN.   The processes modeled as a result describe an aspired state of software development in the domain, which are often not (yet) implemented. This enables existing processes in simulation-based system engineering to be improved with the help of these process models. ",2108.08589v1
"['David Georg Reichelt', 'Stefan Kühne', 'Wilhelm Hasselbring']",2023-03-24T19:51:52Z,Automated Identification of Performance Changes at Code Level,"  To develop software with optimal performance, even small performance changes need to be identified. Identifying performance changes is challenging since the performance of software is influenced by non-deterministic factors. Therefore, not every performance change is measurable with reasonable effort. In this work, we discuss which performance changes are measurable at code level with reasonable measurement effort and how to identify them. We present (1) an analysis of the boundaries of measuring performance changes, (2) an approach for determining a configuration for reproducible performance change identification, and (3) an evaluation comparing of how well our approach is able to identify performance changes in the application server Jetty compared with the usage of Jetty's own performance regression benchmarks. Thereby, we find (1) that small performance differences are only measurable by fine-grained measurement workloads, (2) that performance changes caused by the change of one operation can be identified using a unit-test-sized workload definition and a suitable configuration, and (3) that using our approach identifies small performance regressions more efficiently than using Jetty's performance regression benchmarks. ",2303.14256v1
"['David Georg Reichelt', 'Stefan Kühne', 'Wilhelm Hasselbring']",2023-04-12T08:26:05Z,Towards Solving the Challenge of Minimal Overhead Monitoring,"  The examination of performance changes or the performance behavior of a software requires the measurement of the performance. This is done via probes, i.e., pieces of code which obtain and process measurement data, and which are inserted into the examined application. The execution of those probes in a singular method creates overhead, which deteriorates performance measurements of calling methods and slows down the measurement process. Therefore, an important challenge for performance measurement is the reduction of the measurement overhead.   To address this challenge, the overhead should be minimized. Based on an analysis of the sources of performance overhead, we derive the following four optimization options: (1) Source instrumentation instead of AspectJ instrumentation, (2) reduction of measurement data, (3) change of the queue and (4) aggregation of measurement data. We evaluate the effect of these optimization options using the MooBench benchmark. Thereby, we show that these optimizations options reduce the monitoring overhead of the monitoring framework Kieker. For MooBench, the execution duration could be reduced from 4.77 ms to 0.39 ms per method invocation on average. ",2304.05688v1
"['Sören Henning', 'Wilhelm Hasselbring']",2019-11-15T09:05:16Z,Scalable and Reliable Multi-Dimensional Aggregation of Sensor Data   Streams,"  Ever-increasing amounts of data and requirements to process them in real time lead to more and more analytics platforms and software systems being designed according to the concept of stream processing. A common area of application is the processing of continuous data streams from sensors, for example, IoT devices or performance monitoring tools. In addition to analyzing pure sensor data, analyses of data for groups of sensors often need to be performed as well. Therefore, data streams of the individual sensors have to be continuously aggregated to a data stream for a group. Motivated by a real-world application scenario, we propose that such a stream aggregation approach has to allow for aggregating sensors in hierarchical groups, support multiple such hierarchies in parallel, provide reconfiguration at runtime, and preserve the scalability and reliability qualities induced by applying stream processing techniques. We propose a stream processing architecture fulfilling these requirements, which can be integrated into existing big data architectures. We present a pilot implementation of such an extended architecture and show how it is used in industry. Furthermore, in experimental evaluations we show that our solution scales linearly with the amount of sensors and provides adequate reliability in the case of faults. ",1911.06525v1
"['Sören Henning', 'Wilhelm Hasselbring']",2020-09-01T09:13:16Z,Theodolite: Scalability Benchmarking of Distributed Stream Processing   Engines in Microservice Architectures,"  Distributed stream processing engines are designed with a focus on scalability to process big data volumes in a continuous manner. We present the Theodolite method for benchmarking the scalability of distributed stream processing engines. Core of this method is the definition of use cases that microservices implementing stream processing have to fulfill. For each use case, our method identifies relevant workload dimensions that might affect the scalability of a use case. We propose to design one benchmark per use case and relevant workload dimension. We present a general benchmarking framework, which can be applied to execute the individual benchmarks for a given use case and workload dimension. Our framework executes an implementation of the use case's dataflow architecture for different workloads of the given dimension and various numbers of processing instances. This way, it identifies how resources demand evolves with increasing workloads. Within the scope of this paper, we present 4 identified use cases, derived from processing Industrial Internet of Things data, and 7 corresponding workload dimensions. We provide implementations of 4 benchmarks with Kafka Streams and Apache Flink as well as an implementation of our benchmarking framework to execute scalability benchmarks in cloud environments. We use both for evaluating the Theodolite method and for benchmarking Kafka Streams' and Flink's scalability for different deployment options. ",2009.00304v3
"['Alexander Krause-Glau', 'Wilhelm Hasselbring']",2023-08-30T06:35:40Z,"Collaborative, Code-Proximal Dynamic Software Visualization within Code   Editors","  Software visualizations are usually realized as standalone and isolated tools that use embedded code viewers within the visualization. In the context of program comprehension, only few approaches integrate visualizations into code editors, such as integrated development environments. This is surprising since professional developers consider reading source code as one of the most important ways to understand software, therefore spend a lot of time with code editors. In this paper, we introduce the design and proof-of-concept implementation for a software visualization approach that can be embedded into code editors. Our contribution differs from related work in that we use dynamic analysis of a software system's runtime behavior. Additionally, we incorporate distributed tracing. This enables developers to understand how, for example, the currently handled source code behaves as a fully deployed, distributed software system. Our visualization approach enhances common remote pair programming tools and is collaboratively usable by employing shared code cities. As a result, user interactions are synchronized between code editor and visualization, as well as broadcasted to collaborators. To the best of our knowledge, this is the first approach that combines code editors with collaboratively usable code cities. Therefore, we conducted a user study to collect first-time feedback regarding the perceived usefulness and perceived usability of our approach. We additionally collected logging information to provide more data regarding time spent in code cities that are embedded in code editors. Seven teams with two students each participated in that study. The results show that the majority of participants find our approach useful and would employ it for their own use. We provide each participant's video recording, raw results, and all steps to reproduce our experiment as supplementary package. ",2308.15785v1
"['Alexander Barbie', 'Wilhelm Hasselbring']",2024-01-15T22:13:48Z,"From Digital Twins to Digital Twin Prototypes: Concepts, Formalization,   and Applications","  The transformation to Industry 4.0 also transforms the processes of how we develop intelligent manufacturing production systems. To advance the software development of these new (embedded) software systems, digital twins may be employed. However, there is no consensual definition of what a digital twin is. In this paper, we give an overview of the current state of the digital twin concept and formalize the digital twin concept using the Object-Z notation. This formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. The relationships between all these concepts are visualized as UML class diagrams.   Our digital twin prototype (DTP) approach supports engineers during the development and automated testing of complex embedded software systems. This approach enable engineers to test embedded software systems in a virtual context, without the need of a connection to a physical object. In continuous integration / continuous deployment pipelines such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process.   In this paper, we demonstrate and report on how to apply and implement a digital twin by the example of two real-world field studies (ocean observation systems and smart farming). For independent replication and extension of our approach by other researchers, we provide a lab study published open source on GitHub. ",2401.07985v1
"['Sören Henning', 'Wilhelm Hasselbring']",2023-03-20T13:22:03Z,Benchmarking scalability of stream processing frameworks deployed as   microservices in the cloud,"  Context: The combination of distributed stream processing with microservice architectures is an emerging pattern for building data-intensive software systems. In such systems, stream processing frameworks such as Apache Flink, Apache Kafka Streams, Apache Samza, Hazelcast Jet, or the Apache Beam SDK are used inside microservices to continuously process massive amounts of data in a distributed fashion. While all of these frameworks promote scalability as a core feature, there is only little empirical research evaluating and comparing their scalability. Objective: The goal of this study to obtain evidence about the scalability of state-of-the-art stream processing framework in different execution environments and regarding different scalability dimensions. Method: We benchmark five modern stream processing frameworks regarding their scalability using a systematic method. We conduct over 740 hours of experiments on Kubernetes clusters in the Google cloud and in a private cloud, where we deploy up to 110 simultaneously running microservice instances, which process up to one million messages per second. Results: All benchmarked frameworks exhibit approximately linear scalability as long as sufficient cloud resources are provisioned. However, the frameworks show considerable differences in the rate at which resources have to be added to cope with increasing load. There is no clear superior framework, but the ranking of the frameworks depends on the use case. Using Apache Beam as an abstraction layer still comes at the cost of significantly higher resource requirements regardless of the use case. We observe our results regardless of scaling load on a microservice, scaling the computational work performed inside the microservice, and the selected cloud environment. Moreover, vertical scaling can be a complementary measure to achieve scalability of stream processing frameworks. ",2303.11088v2
"['Jasminka Matevska-Meyer', 'Sascha Olliges', 'Wilhelm Hasselbring']",2004-11-17T13:34:06Z,Runtime Reconfiguration of J2EE Applications,"  Runtime reconfiguration considered as ""applying required changes to a running system"" plays an important role for providing high availability not only of safety- and mission-critical systems, but also for commercial web-applications offering professional services. Hereby, the main concerns are maintaining the consistency of the running system during reconfiguration and minimizing its down-time caused by the reconfiguration. This paper focuses on the platform independent subsystem that realises deployment and redeployment of J2EE modules based on the new J2EE Deployment API as a part of the implementation of our proposed system architecture enabling runtime reconfiguration of component-based systems. Our ""controlled runtime redeployment"" comprises an extension of hot deployment and dynamic reloading, complemented by allowing for structural change ",cs/0411057v1
"['Sören Henning', 'Wilhelm Hasselbring', 'Armin Möbius']",2019-07-01T20:06:50Z,A Scalable Architecture for Power Consumption Monitoring in Industrial   Production Environments,"  Detailed knowledge about the electrical power consumption in industrial production environments is a prerequisite to reduce and optimize their power consumption. Today's industrial production sites are equipped with a variety of sensors that, inter alia, monitor electrical power consumption in detail. However, these environments often lack an automated data collation and analysis.   We present a system architecture that integrates different sensors and analyzes and visualizes the power consumption of devices, machines, and production plants. It is designed with a focus on scalability to support production environments of various sizes and to handle varying loads. We argue that a scalable architecture in this context must meet requirements for fault tolerance, extensibility, real-time data processing, and resource efficiency. As a solution, we propose a microservice-based architecture augmented by big data and stream processing techniques. Applying the fog computing paradigm, parts of it are deployed in an elastic, central cloud while other parts run directly, decentralized in the production environment.   A prototype implementation of this architecture presents solutions how different kinds of sensors can be integrated and their measurements can be continuously aggregated. In order to make analyzed data comprehensible, it features a single-page web application that provides different forms of data visualization. We deploy this pilot implementation in the data center of a medium-sized enterprise, where we successfully monitor the power consumption of 16~servers. Furthermore, we show the scalability of our architecture with 20,000~simulated sensors. ",1907.01046v1
"['Wilhelm Hasselbring', 'Maik Wojcieszak', 'Schahram Dustdar']",2021-08-18T10:25:01Z,Control Flow Versus Data Flow in Distributed Systems Integration:   Revival of Flow-Based Programming for the Industrial Internet of Things,"  When we consider the application layer of networked infrastructures, data and control flow are important concerns in distributed systems integration. Modularity is a fundamental principle in software design, in particular for distributed system architectures. Modularity emphasizes high cohesion of individual modules and low coupling between modules. Microservices are a recent modularization approach with the specific requirements of independent deployability and, in particular, decentralized data management. Cohesiveness of microservices goes hand-in-hand with loose coupling, making the development, deployment, and evolution of microservice architectures flexible and scalable. However, in our experience with microservice architectures, interactions and flows among microservices are usually more complex than in traditional, monolithic enterprise systems, since services tend to be smaller and only have one responsibility, causing collaboration needs. We suggest that for loose coupling among microservices, explicit control-flow modeling and execution with central workflow engines should be avoided on the application integration level. On the level of integrating microservices, data-flow modeling should be dominant. Control-flow should be secondary and preferably delegated to the microservices. We discuss coupling in distributed systems integration and reflect the history of business process modeling with respect to data and control flow. To illustrate our recommendations, we present some results for flow-based programming in our Industrial DevOps project Titan, where we employ flow-based programming for the Industrial Internet of Things. ",2108.08081v1
"['Nelson Tavares de Sousa', 'Wilhelm Hasselbring']",2021-10-20T06:49:41Z,JavaBERT: Training a transformer-based model for the Java programming   language,"  Code quality is and will be a crucial factor while developing new software code, requiring appropriate tools to ensure functional and reliable code. Machine learning techniques are still rarely used for software engineering tools, missing out the potential benefits of its application. Natural language processing has shown the potential to process text data regarding a variety of tasks. We argue, that such models can also show similar benefits for software code processing. In this paper, we investigate how models used for natural language processing can be trained upon software code. We introduce a data retrieval pipeline for software code and train a model upon Java software code. The resulting model, JavaBERT, shows a high accuracy on the masked language modeling task showing its potential for software engineering tools. ",2110.10404v1
"['Wilhelm Hasselbring', 'Stephan Druskat', 'Jan Bernoth', 'Philine Betker', 'Michael Felderer', 'Stephan Ferenz', 'Anna-Lena Lamprecht', 'Jan Linxweiler', 'Bernhard Rumpe']",2024-04-22T17:17:17Z,Toward Research Software Categories,"  Research software has been categorized in different contexts to serve different goals. We start with a look at what research software is, before we discuss the purpose of research software categories. We propose a multi-dimensional categorization of research software. We present a template for characterizing such categories. As selected dimensions, we present our proposed role-based, developer-based, and maturity-based categories. Since our work has been inspired by various previous efforts to categorize research software, we discuss them as related works. We characterize all these categories via the previously introduced template, to enable a systematic comparison. ",2404.14364v1
"['Wilhelm Hasselbring', 'Leslie Carr', 'Simon Hettrick', 'Heather Packer', 'Thanassis Tiropanis']",2019-08-16T14:26:08Z,FAIR and Open Computer Science Research Software,"  In computational science and in computer science, research software is a central asset for research. Computational science is the application of computer science and software engineering principles to solving scientific problems, whereas computer science is the study of computer hardware and software design.   The Open Science agenda holds that science advances faster when we can build on existing results. Therefore, research software has to be reusable for advancing science. Thus, we need proper research software engineering for obtaining reusable and sustainable research software. This way, software engineering methods may improve research in other disciplines. However, research in software engineering and computer science itself will also benefit from reuse when research software is involved.   For good scientific practice, the resulting research software should be open and adhere to the FAIR principles (findable, accessible, interoperable and repeatable) to allow repeatability, reproducibility, and reuse. Compared to research data, research software should be both archived for reproducibility and actively maintained for reusability. The FAIR data principles do not require openness, but research software should be open source software. Established open source software licenses provide sufficient licensing options, such that it should be the rare exception to keep research software closed.   We review and analyze the current state in this area in order to give recommendations for making computer science research software FAIR and open. We observe that research software publishing practices in computer science and in computational science show significant differences. ",1908.05986v1
"['Alexander Barbie', 'Wilhelm Hasselbring', 'Malte Hansen']",2023-11-09T21:24:12Z,Enabling Automated Integration Testing of Smart Farming Applications via   Digital Twin Prototypes,"  Industry 4.0 represents a major technological shift that has the potential to transform the manufacturing industry, making it more efficient, productive, and sustainable. Smart farming is a concept that involves the use of advanced technologies to improve the efficiency and sustainability of agricultural practices. Industry 4.0 and smart farming are closely related, as many of the technologies used in smart farming are also used in Industry 4.0. Digital twins have the potential for cost-effective software development of such applications. With our Digital Twin Prototype approach, all sensor interfaces are integrated into the development process, and their inputs and outputs of the emulated hardware match those of the real hardware. The emulators respond to the same commands and return identically formatted data packages as their real counterparts, making the Digital Twin Prototype a valid source of a digital shadow, i.e. the Digital Twin Prototype is a prototype of the physical twin and can replace it for automated testing of the digital twin software. In this paper, we present a case study for employing our Digital Twin Prototype approach to automated testing of software for improving the making of silage with a smart farming application. Besides automated testing with continuous integration, we also discuss continuous deployment of modular Docker containers in this context. ",2311.05748v1
"['Alexander Krause-Glau', 'Lukas Damerau', 'Malte Hansen', 'Wilhelm Hasselbring']",2024-08-15T13:19:55Z,Visual Integration of Static and Dynamic Software Analysis in Code   Reviews via Software City Visualization,"  Software visualization approaches for code reviews are often implemented as standalone applications, which use static code analysis. The goal is to visualize the structural changes introduced by a pull / merge request to facilitate the review process. In this way, for example, structural changes that hinder code evolution can be more easily identified, but understanding the changed program behavior is still mainly done by reading the code. For software visualization to be successful in code review, tools must be provided that go beyond an alternative representation of code changes and integrate well into the developers' daily workflow. In this paper, we report on the novel and in-progress design and implementation of a web-based approach capable of combining static and dynamic analysis data in software city visualizations. Our architectural tool design incorporates modern web technologies such as the integration into common Git hosting services. As a result, code reviewers can explore how the modified software evolves and execute its use cases, which is especially helpful for distributed software systems. In this context, developers can be directly linked from the Git hosting service's issue tracking system to the corresponding software city visualization. This approach eliminates the recurring action of manual data collection and setup. We implement our design by extending the web-based software visualization tool ExplorViz. We invite other researchers to extend our open source software and jointly research this approach. Video URL: https://youtu.be/DYxijdCEdrY ",2408.08141v1
"['Alexander Krause', 'Christian Zirkelbach', 'Wilhelm Hasselbring', 'Stephan Lenga', 'Dan Kröger']",2020-03-05T13:32:10Z,Microservice Decomposition via Static and Dynamic Analysis of the   Monolith,"  Migrating monolithic software systems into microservices requires the application of decomposition techniquesto find and select appropriate service boundaries. These techniques are often based on domain knowledge, static code analysis, and non-functional requirements such as maintainability. In this paper, we present our experience with an approach that extends static analysis with dynamic analysis of a legacy software system's runtime behavior, including the live trace visualization to support the decomposition into microservices. Overall, our approach combines established analysis techniques for microservice decomposition, such as the bounded context pattern of domain-driven design, and enriches the collected information via dynamic software visualization to identify appropriate microservice boundaries. In collaboration with the German IT service provider adesso SE, we applied our approach to their real-word, legacy lottery application in|FOCUS to identify good microservice decompositions for this layered monolithic Enterprise Java system. ",2003.02603v1
"['Dilip Jagadeeshwarswamy Hiremath', 'Martin Claus', 'Wilhelm Hasselbring', 'Willi Rath']",2021-03-17T17:11:15Z,Towards Automated Metamorphic Test Identification for Ocean System   Models,"  Metamorphic testing seeks to verify software in the absence of test oracles. Our application domain is ocean system modeling, where test oracles rarely exist, but where symmetries of the simulated physical systems are known. The input data set is large owing to the requirements of the application domain. This paper presents work in progress for the automated generation of metamorphic test scenarios using machine learning. We extended our previously proposed method [1] to identify metamorphic relations with reduced computational complexity. Initially, we represent metamorphic relations as identity maps. We construct a cost function that minimizes for identifying a metamorphic relation orthogonal to previously found metamorphic relations and penalize for the identity map. A machine learning algorithm is used to identify all possible metamorphic relations minimizing the defined cost function. We propose applying dimensionality reduction techniques to identify attributes in the input which have high variance among the identified metamorphic relations. We apply mutation on these selected attributes to identify distinct metamorphic relations with reduced computational complexity. For experimental evaluation, we subject the two implementations of an ocean-modeling application to the proposed method to present the use of metamorphic relations to test the two implementations of this application. ",2103.09782v1
"['Arne Johanson', 'Sascha Flögel', 'Wolf-Christian Dullo', 'Peter Linke', 'Wilhelm Hasselbring']",2022-09-26T12:46:11Z,Modeling Polyp Activity of Paragorgia arborea Using Supervised Learning,"  While the distribution patterns of cold-water corals, such as Paragorgia arborea, have received increasing attention in recent studies, little is known about their in situ activity patterns. In this paper, we examine polyp activity in P. arborea using machine learning techniques to analyze high-resolution time series data and photographs obtained from an autonomous lander cluster deployed in the Stjernsund, Norway. An interactive illustration of the models derived in this paper is provided online as supplementary material. We find that the best predictor of the degree of extension of the coral polyps is current direction with a lag of three hours. Other variables that are not directly associated with water currents, such as temperature and salinity, offer much less information concerning polyp activity. Interestingly, the degree of polyp extension can be predicted more reliably by sampling the laminar flows in the water column above the measurement site than by sampling the more turbulent flows in the direct vicinity of the corals. Our results show that the activity patterns of the P. arborea polyps are governed by the strong tidal current regime of the Stjernsund. It appears that P. arborea does not react to shorter changes in the ambient current regime but instead adjusts its behavior in accordance with the large-scale pattern of the tidal cycle itself in order to optimize nutrient uptake. ",2209.13644v1
"['Dilip J. Hiremath', 'Martin Claus', 'Wilhelm Hasselbring', 'Willi Rath']",2020-09-03T10:03:56Z,Automated identification of metamorphic test scenarios for an   ocean-modeling application,"  Metamorphic testing seeks to validate software in the absence of test oracles. Our application domain is ocean modeling, where test oracles often do not exist, but where symmetries of the simulated physical systems are known. In this short paper we present work in progress for automated generation of metamorphic test scenarios using machine learning. Metamorphic testing may be expressed as f(g(X))=h(f(X)) with f being the application under test, with input data X, and with the metamorphic relation (g, h). Automatically generated metamorphic relations can be used for constructing regression tests, and for comparing different versions of the same software application. Here, we restrict to h being the identity map. Then, the task of constructing tests means finding different g which we tackle using machine learning algorithms. These algorithms typically minimize a cost function. As one possible g is already known to be the identity map, for finding a second possible g, we construct the cost function to minimize for g being a metamorphic relation and to penalize for g being the identity map. After identifying the first metamorphic relation, the procedure is repeated with a cost function rewarding g that are orthogonal to previously found metamorphic relations. For experimental evaluation, two implementations of an ocean-modeling application will be subjected to the proposed method with the objective of presenting the use of metamorphic relations to test the implementations of the applications. ",2009.01554v1
"['Sören Henning', 'Wilhelm Hasselbring', 'Heinz Burmester', 'Armin Möbius', 'Maik Wojcieszak']",2020-09-22T07:55:03Z,Goals and Measures for Analyzing Power Consumption Data in Manufacturing   Enterprises,"  The Internet of Things adoption in the manufacturing industry allows enterprises to monitor their electrical power consumption in real time and at machine level. In this paper, we follow up on such emerging opportunities for data acquisition and show that analyzing power consumption in manufacturing enterprises can serve a variety of purposes. Apart from the prevalent goal of reducing overall power consumption for economical and ecological reasons, such data can, for example, be used to improve production processes.   Based on a literature review and expert interviews, we discuss how analyzing power consumption data can serve the goals reporting, optimization, fault detection, and predictive maintenance. To tackle these goals, we propose to implement the measures real-time data processing, multi-level monitoring, temporal aggregation, correlation, anomaly detection, forecasting, visualization, and alerting in software.   We transfer our findings to two manufacturing enterprises and show how the presented goals reflect in these enterprises. In a pilot implementation of a power consumption analytics platform, we show how our proposed measures can be implemented with a microservice-based architecture, stream processing techniques, and the fog computing paradigm. We provide the implementations as open source as well as a public demo allowing to reproduce and extend our research. ",2009.10369v1
"['Wilhelm Hasselbring', 'Sören Henning', 'Björn Latte', 'Armin Möbius', 'Thomas Richter', 'Stefan Schalk', 'Maik Wojcieszak']",2019-07-03T12:11:43Z,Industrial DevOps,"  The visions and ideas of Industry 4.0 require a profound interconnection of machines, plants, and IT systems in industrial production environments. This significantly increases the importance of software, which is coincidentally one of the main obstacles to the introduction of Industry 4.0. Lack of experience and knowledge, high investment and maintenance costs, as well as uncertainty about future developments cause many small and medium-sized enterprises hesitating to adopt Industry 4.0 solutions. We propose Industrial DevOps as an approach to introduce methods and culture of DevOps into industrial production environments. The fundamental concept of this approach is a continuous process of operation, observation, and development of the entire production environment. This way, all stakeholders, systems, and data can thus be integrated via incremental steps and adjustments can be made quickly. Furthermore, we present the Titan software platform accompanied by a role model for integrating production environments with Industrial DevOps. In two initial industrial application scenarios, we address the challenges of energy management and predictive maintenance with the methods, organizational structures, and tools of Industrial DevOps. ",1907.01875v1
"['Alexander Barbie', 'Wilhelm Hasselbring', 'Niklas Pech', 'Stefan Sommer', 'Sascha Flögel', 'Frank Wenzhöfer']",2021-03-16T23:17:16Z,Prototyping Autonomous Robotic Networks on Different Layers of RAMI 4.0   with Digital Twins,"  In this decade, the amount of (industrial) Internet of Things devices will increase tremendously. Today, there exist no common standards for interconnection, observation, or the monitoring of these devices. In context of the German ""Industrie 4.0"" strategy the Reference Architectural Model Industry 4.0 (RAMI 4.0) was introduced to connect different aspects of this rapid development. The idea is to let different stakeholders of these products speak and understand the same terminology. In this paper, we present an approach using Digital Twins to prototype different layers along the axis of the RAMI 4.0, by the example of an autonomous ocean observation system developed in the project ARCHES. ",2103.09363v1
"['Tobias Pfandzelter', 'Sören Henning', 'Trever Schirmer', 'Wilhelm Hasselbring', 'David Bermbach']",2022-04-25T08:42:39Z,Streaming vs. Functions: A Cost Perspective on Cloud Event Processing,"  In cloud event processing, data generated at the edge is processed in real-time by cloud resources. Both distributed stream processing (DSP) and Function-as-a-Service (FaaS) have been proposed to implement such event processing applications. FaaS emphasizes fast development and easy operation, while DSP emphasizes efficient handling of large data volumes. Despite their architectural differences, both can be used to model and implement loosely-coupled job graphs.   In this paper, we consider the selection of FaaS and DSP from a cost perspective. We implement stateless and stateful workflows from the Theodolite benchmarking suite using cloud FaaS and DSP. In an extensive evaluation, we show how application type, cloud service provider, and runtime environment can influence the cost of application deployments and derive decision guidelines for cloud engineers. ",2204.11509v2
"['Alexander Barbie', 'Niklas Pech', 'Wilhelm Hasselbring', 'Sascha Flögel', 'Frank Wenzhöfer', 'Michael Walter', 'Elena Shchekinova', 'Marc Busse', 'Matthias Türk', 'Michael Hofbauer', 'Stefan Sommer']",2021-03-15T15:45:49Z,Developing an Underwater Network of Ocean Observation Systems with   Digital Twin Prototypes -- A Field Report from the Baltic Sea,"  During the research cruise AL547 with RV ALKOR (October 20-31, 2020), a collaborative underwater network of ocean observation systems was deployed in Boknis Eck (SW Baltic Sea, German exclusive economic zone (EEZ)) in the context of the project ARCHES (Autonomous Robotic Networks to Help Modern Societies). This network was realized via a Digital Twin Prototype approach. During that period different scenarios were executed to demonstrate the feasibility of Digital Twins in an extreme environment such as underwater. One of the scenarios showed the collaboration of stage IV Digital Twins with their physical counterparts on the seafloor. This way, we address the research question, whether Digital Twins represent a feasible approach to operate mobile ad hoc networks for ocean and coastal observation. ",2103.08469v1
"['Andreas Brunnert', 'Andre van Hoorn', 'Felix Willnecker', 'Alexandru Danciu', 'Wilhelm Hasselbring', 'Christoph Heger', 'Nikolas Herbst', 'Pooyan Jamshidi', 'Reiner Jung', 'Joakim von Kistowski', 'Anne Koziolek', 'Johannes Kroß', 'Simon Spinner', 'Christian Vögele', 'Jürgen Walter', 'Alexander Wert']",2015-08-18T12:39:05Z,Performance-oriented DevOps: A Research Agenda,"  DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance.   Performance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area. ",1508.04752v1
['Thomas Wilke'],2016-09-10T14:50:42Z,ω-Automata,"  This paper gives a concise introduction into the basic theory of {\omega}-automata (as of March 2014). The starting point are the different types of recurrence conditions, modes of operation (deterministic, nondeterministic, alternating automata), and directions (forward or backward automata). The main focus is on fundamental automata constructions, for instance, for boolean operations, determinization, disambiguation, and removing alternation. It also covers some algebraic aspects such as congruences for {\omega}-automata (and {\omega}-languages), basic structure theory (loops), and applications in mathematical logic. This paper may eventually become a chapter in a handbook of automata theory. ",1609.03062v1
"['Sebastian Preugschat', 'Thomas Wilke']",2017-01-08T15:25:53Z,Backward deterministic and weak alternating $ω$-automata,"  We present a direct transformation of weak alternating $\omega$-automata into equivalent backward deterministic $\omega$-automata and show (1) how it can be used to obtain a transformation of non-deterministic B\""uchi automata into equivalent backward deterministic automata and (2) that it yields optimal equivalent backward deterministic automata when applied to linear-time temporal logic formulas. (1) uses the alternation-free fragment of the linear-time $\mu$-calculus as an intermediate step; (2) is based on the straightforward translation of linear-time temporal logic into weak alternating $\omega$-automata. ",1701.01971v1
"['Sebastian Eggert', 'Henning Schnoor', 'Thomas Wilke']",2012-08-28T08:00:48Z,Noninterference with Local Policies,"  We develop a theory for state-based noninterference in a setting where different security policies---we call them local policies---apply in different parts of a given system. Our theory comprises appropriate security definitions, characterizations of these definitions, for instance in terms of unwindings, algorithms for analyzing the security of systems with local policies, and corresponding complexity results. ",1208.5580v2
"['Preugschat Sebastian', 'Thomas Wilke']",2013-03-24T15:05:19Z,Effective Characterizations of Simple Fragments of Temporal Logic Using   Carton--Michel Automata,"  We present a framework for obtaining effective characterizations of simple fragments of future temporal logic (LTL) with the natural numbers as time domain. The framework is based on a form of strongly unambiguous automata, also known as prophetic automata or complete unambiguous B\""uchi automata and referred to as Carton-Michel automata in this paper. These automata enjoy strong structural properties, in particular, they separate the ""finitary fraction"" of a regular language of infinite words from its ""infinitary fraction"" in a natural fashion. Within our framework, we provide characterizations of several natural fragments of temporal logic, where, in some cases, no effective characterization had been known previously, and give lower and upper bounds for their computational complexity. ",1303.5956v2
"['Jonathan Lenaghan', 'Thomas Wilke']",2001-08-22T15:03:16Z,Mesoscopic QCD and the Theta Vacua,"  The partition function of QCD is analyzed for an arbitrary number of flavors, N_f, and arbitrary quark masses including the contributions from all topological sectors in the Leutwyler--Smilga regime. For given N_f and arbitrary vacuum angle, \theta, the partition function can be reduced to N_f-2 angular integrations of single Bessel functions. For two and three flavors, the \theta dependence of the QCD vacuum is studied in detail. For N_f= 2 and 3, the chiral condensate decreases monotonically as \theta increases from zero to \pi and the chiral condensate develops a cusp at \theta=\pi for degenerate quark masses in the macroscopic limit. We find a discontinuity at \theta=\pi in the first derivative of the energy density with respect to \theta for degenerate quark masses. This corresponds to the first--order phase transition in which CP is spontaneously broken, known as Dashen's phenomena. ",hep-th/0108166v2
"['Dennis Vetter', 'Muhammad Ahsan', 'Diana Delicado', 'Thomas A. Neubauer', 'Thomas Wilke', 'Gemma Roig']",2024-07-29T13:45:23Z,Classification of freshwater snails of the genus Radomaniola with   multimodal triplet networks,"  In this paper, we present our first proposal of a machine learning system for the classification of freshwater snails of the genus Radomaniola. We elaborate on the specific challenges encountered during system design, and how we tackled them; namely a small, very imbalanced dataset with a high number of classes and high visual similarity between classes. We then show how we employed triplet networks and the multiple input modalities of images, measurements, and genetic information to overcome these challenges and reach a performance comparable to that of a trained domain expert. ",2407.20013v2
"['Sebastian Eggert', 'Ron van der Meyden', 'Henning Schnoor', 'Thomas Wilke']",2013-08-06T08:31:44Z,Complexity and Unwinding for Intransitive Noninterference,"  The paper considers several definitions of information flow security for intransitive policies from the point of view of the complexity of verifying whether a finite-state system is secure. The results are as follows. Checking (i) P-security (Goguen and Meseguer), (ii) IP-security (Haigh and Young), and (iii) TA-security (van der Meyden) are all in PTIME, while checking TO-security (van der Meyden) is undecidable, as is checking ITO-security (van der Meyden). The most important ingredients in the proofs of the PTIME upper bounds are new characterizations of the respective security notions, which also lead to new unwinding proof techniques that are shown to be sound and complete for these notions of security, and enable the algorithms to return simple counter-examples demonstrating insecurity. Our results for IP-security improve a previous doubly exponential bound of Hadj-Alouane et al. ",1308.1204v1
"['Seth J. Fogarty', 'Orna Kupferman', 'Thomas Wilke', 'Moshe Y. Vardi']",2013-02-12T00:29:00Z,Unifying Büchi Complementation Constructions,"  Complementation of B\""uchi automata, required for checking automata containment, is of major theoretical and practical interest in formal verification. We consider two recent approaches to complementation. The first is the rank-based approach of Kupferman and Vardi, which operates over a DAG that embodies all runs of the automaton. This approach is based on the observation that the vertices of this DAG can be ranked in a certain way, termed an odd ranking, iff all runs are rejecting. The second is the slice-based approach of K\""ahler and Wilke. This approach tracks levels of ""split trees"" - run trees in which only essential information about the history of each run is maintained. While the slice-based construction is conceptually simple, the complementing automata it generates are exponentially larger than those of the recent rank-based construction of Schewe, and it suffers from the difficulty of symbolically encoding levels of split trees. In this work we reformulate the slice-based approach in terms of run DAGs and preorders over states. In doing so, we begin to draw parallels between the rank-based and slice-based approaches. Through deeper analysis of the slice-based approach, we strongly restrict the nondeterminism it generates. We are then able to employ the slice-based approach to provide a new odd ranking, called a retrospective ranking, that is different from the one provided by Kupferman and Vardi. This new ranking allows us to construct a deterministic-in-the-limit rank-based automaton with a highly restricted transition function. Further, by phrasing the slice-based approach in terms of ranks, our approach affords a simple symbolic encoding and achieves the tight bound of Schewe's construction ",1302.2675v2
"['Seth Fogarty', 'Orna Kupferman', 'Moshe Y. Vardi', 'Thomas Wilke']",2013-07-17T01:42:06Z,"Profile Trees for Büchi Word Automata, with Application to   Determinization","  The determinization of Buchi automata is a celebrated problem, with applications in synthesis, probabilistic verification, and multi-agent systems. Since the 1960s, there has been a steady progress of constructions: by McNaughton, Safra, Piterman, Schewe, and others. Despite the proliferation of solutions, they are all essentially ad-hoc constructions, with little theory behind them other than proofs of correctness. Since Safra, all optimal constructions employ trees as states of the deterministic automaton, and transitions between states are defined operationally over these trees. The operational nature of these constructions complicates understanding, implementing, and reasoning about them, and should be contrasted with complementation, where a solid theory in terms of automata run DAGs underlies modern constructions.   In 2010, we described a profile-based approach to Buchi complementation, where a profile is simply the history of visits to accepting states. We developed a structural theory of profiles and used it to describe a complementation construction that is deterministic in the limit. Here we extend the theory of profiles to prove that every run DAG contains a profile tree with at most a finite number of infinite branches. We then show that this property provides a theoretical grounding for a new determinization construction where macrostates are doubly preordered sets of states. In contrast to extant determinization constructions, transitions in the new construction are described declaratively rather than operationally. ",1307.4471v1
"['Valentin Poirot', 'Olaf Landsiedel']",2021-12-06T13:46:55Z,eAFH: Informed Exploration for Adaptive Frequency Hopping in Bluetooth   Low Energy,"  With more than 4 billion devices produced in 2020, Bluetooth and Bluetooth Low Energy (BLE) have become the backbone of the Internet of Things. Bluetooth and BLE mitigate interference in the crowded 2.4 GHz band via Adaptive Frequency Hopping (AFH), spreading communication over the entire spectrum, and further allows the exclusion of interfered channels. However, exclusion is challenging in dynamic environments or with user mobility: as a user moves around, interference affects new channels, forcing AFH to deprive itself of new frequencies, while some other excluded channels are now free of losses but remain excluded. Channel re-inclusion is a primordial, yet often left out, aspect of AFH, as it is non-trivial to assess the new situation of excluded frequencies. We present eAFH, a mechanism for channel exclusion and inclusion. eAFH introduces informed exploration to AFH: using only past measurements, eAFH assesses which frequencies we are most likely to benefit from re-including in the hopping sequence. As a result, eAFH adapts in dynamic scenarios where interference varies over time. We show that eAFH achieves 98-99.5% link-layer reliability in the presence of dynamic Wifi interference with 1% control overhead and 40% higher channel diversity than state-of-the-art approaches. ",2112.03046v1
"['Patrick Rathje', 'Olaf Landsiedel']",2022-04-12T11:23:28Z,Time Difference of Arrival Extraction from Two-Way Ranging,"  Two-Way Ranging enables the distance estimation between two active parties and allows time of flight measurements despite relative clock offset and drift. Limited by the number of messages, scalable solutions build on Time Difference of Arrival to infer timing information at passive listeners. However, the demand for accurate distance estimates dictates a tight bound on the time synchronization, thus limiting scalability to the localization of passive tags relative to static, synchronized anchors. This work describes the extraction of Time Difference of Arrival information from a Two-Way Ranging process, enabling the extraction of distance information on passive listeners and further allowing scalable tag localization without the need for static or synchronized anchors. The expected error is formally deducted. The extension allows the extraction of the timing difference despite relative clock offset and drift for the Double-Sided Two-Way Ranging and Single-Sided Two-Way Ranging with additional carrier frequency offset estimation. ",2204.08996v3
"['Valentin Poirot', 'Olaf Landsiedel']",2020-12-07T14:19:33Z,Dimmer: Self-Adaptive Network-Wide Flooding with Reinforcement Learning,"  The last decade saw an emergence of Synchronous Transmissions (ST) as an effective communication paradigm in low-power wireless networks. Numerous ST protocols provide high reliability and energy efficiency in normal wireless conditions, for a large variety of traffic requirements. Recently, with the EWSN dependability competitions, the community pushed ST to harsher and highly-interfered environments, improving upon classical ST protocols through the use of custom rules, hand-tailored parameters, and additional retransmissions. The results are sophisticated protocols, that require prior expert knowledge and extensive testing, often tuned for a specific deployment and envisioned scenario. In this paper, we explore how ST protocols can benefit from self-adaptivity; a self-adaptive ST protocol selects itself its best parameters to (1) tackle external environment dynamics and (2) adapt to its topology over time. We introduce Dimmer as a self-adaptive ST protocol. Dimmer builds on LWB and uses Reinforcement Learning to tune its parameters and match the current properties of the wireless medium. By learning how to behave from an unlabeled dataset, Dimmer adapts to different interference types and patterns, and is able to tackle previously unseen interference. With Dimmer, we explore how to efficiently design AI-based systems for constrained devices, and outline the benefits and downfalls of AI-based low-power networking. We evaluate our protocol on two deployments of resource-constrained nodes achieving 95.8% reliability against strong, unknown WiFi interference. Our results outperform baselines such as non-adaptive ST protocols (27%) and PID controllers, and show a performance close to hand-crafted and more sophisticated solutions, such as Crystal (99%). ",2012.03719v2
"['Ali Hojjat', 'Janek Haberer', 'Olaf Landsiedel']",2023-05-03T14:23:37Z,ProgDTD: Progressive Learned Image Compression with Double-Tail-Drop   Training,"  Progressive compression allows images to start loading as low-resolution versions, becoming clearer as more data is received. This increases user experience when, for example, network connections are slow. Today, most approaches for image compression, both classical and learned ones, are designed to be non-progressive. This paper introduces ProgDTD, a training method that transforms learned, non-progressive image compression approaches into progressive ones. The design of ProgDTD is based on the observation that the information stored within the bottleneck of a compression model commonly varies in importance. To create a progressive compression model, ProgDTD modifies the training steps to enforce the model to store the data in the bottleneck sorted by priority. We achieve progressive compression by transmitting the data in order of its sorted index. ProgDTD is designed for CNN-based learned image compression models, does not need additional parameters, and has a customizable range of progressiveness. For evaluation, we apply ProgDTDto the hyperprior model, one of the most common structures in learned image compression. Our experimental results show that ProgDTD performs comparably to its non-progressive counterparts and other state-of-the-art progressive models in terms of MS-SSIM and accuracy. ",2305.02145v2
"['Martina Brachmann', 'Olaf Landsiedel', 'Diana Göhringer', 'Silvia Santini']",2018-09-11T06:56:03Z,Whisper: Fast Flooding for Low-Power Wireless Networks,"  This paper presents Whisper, a fast and reliable protocol to flood small amounts of data into a multi-hop network. Whisper makes use of synchronous transmissions, a technique first introduced by the Glossy flooding protocol. In contrast to Glossy, Whisper does not let the radio switch from receive to transmit mode between messages. Instead, it makes nodes continuously transmit identical copies of the message and eliminates the gaps between subsequent transmissions. To this end, Whisper embeds the message to be flooded into a signaling packet that is composed of multiple packlets -- where a packlet is a portion of the message payload that mimics the structure of an actual packet. A node must intercept only one of the packlets to detect that there is an ongoing transmission and that it should start forwarding the message. This allows Whisper to speed up the propagation of the flood, and thus, to reduce the overall radio-on time of the nodes. Our evaluation on the FlockLab testbed shows that Whisper achieves comparable reliability but 2x lower radio-on time than Glossy. We further show that by embedding Whisper in an existing data collection application, we can more than double the lifetime of the network. ",1809.03699v2
"['Julia Andersen', 'Patrick Rathje', 'Olaf Landsiedel']",2024-05-06T12:46:43Z,EdgeAlpha: Distributed Process Discovery at the Data Sources,"  Process Mining is moving beyond mining traditional event logs and nowadays includes, for example, data sourced from sensors in the Internet of Things (IoT) in a smart home, factory or hospital setting. The volume and velocity of data generated by such sensors makes it increasingly challenging for traditional process discovery algorithms to store and mine such data in traditional event logs. Further, privacy considerations often prevent data collection at a central location in the first place. To address this challenge, this paper introduces EdgeAlpha, an algorithm for distributed process discovery operating directly on sensor nodes on a stream of real-time event data. Based on the Alpha Miner, EdgeAlpha tracks each event and its predecessor and successor events directly on the sensor node where the event is sensed and recorded. From this local view, each node in EdgeAlpha derives a partial footprint matrix, which we then merge at a central location, whenever we query the system to compute a process model. Our analysis shows that even with over 600 activities, like in the Hospital Log, the average number of predecessors per activity remains below 7. This enables efficient determination of predecessors for each event, reducing the communication overhead by up to 96% compared to querying all nodes and enhancing scalability. Additionally, our analysis demonstrates that the number of queried nodes stabilizes after relatively few events, and batching predecessor queries in groups of 40 reduces the average queried nodes per event to less than 2.5%. EdgeAlpha ensures that the resulting process model is identical to the original Alpha Miner. ",2405.03426v3
"['Beshr Al Nahas', 'Antonio Escobar-Molero', 'Jirka Klaue', 'Simon Duquennoy', 'Olaf Landsiedel']",2020-02-28T18:21:29Z,BlueFlood: Concurrent Transmissions for Multi-Hop Bluetooth 5 --   Modeling and Evaluation,"  Bluetooth is an omnipresent technology, available on billions of devices today. While it has been traditionally limited to peer-to-peer communication and star networks, the recent Bluetooth Mesh standard extends it to multi-hop networking. In addition, the Bluetooth 5 standard introduces new modes to allow for increased reliability. In this paper, we evaluate the feasibility of concurrent transmissions (CT) in Bluetooth via modeling and controlled experiments and then devise an efficient network-wide data dissemination protocol, BlueFlood, based on CT for multi-hop Bluetooth networks.   First, we model and analyze how CT distorts the received waveform and characterize the Bit Error Rate of a Frequency-Shift Keying receiver to show that CT is feasible over Bluetooth. Second, we verify our analytic results with a controlled experimental study of CT over Bluetooth PHY. Third, we present BlueFlood, a fast and efficient network-wide data dissemination in multi-hop Bluetooth networks.   In our experimental evaluation, in two testbeds deployed in university buildings, we show that BlueFlood achieves 99.9% end-to-end delivery ratio with a duty-cycle of 0.4% for periodic dissemination of advertising packets of 38 bytes with 200 milliseconds intervals at 2 Mbps. Moreover, we show that BlueFlood can be received by off-the-shelf devices such as smartphones, paving a seamless integration with existing technologies. ",2002.12906v4
"['Sören Striewski', 'Olga Zagovora', 'Isabella Peters']",2024-05-21T13:50:02Z,Scientific discourse on YouTube: Motivations for citing research in   comments,"  YouTube is a valuable source of user-generated content on a wide range of topics, and it encourages user participation through the use of a comment system. Video content is increasingly addressing scientific topics, and there is evidence that both academics and consumers use video descriptions and video comments to refer to academic research and scientific publications. Because commenting is a discursive behavior, this study will provide insights on why individuals post links to research publications in comments. For this, a qualitative content analysis and iterative coding approach were applied. Furthermore, the reasons for mentioning academic publications in comments were contrasted with the reasons for citing in scholarly works and with reasons for commenting on YouTube. We discovered that the primary motives for sharing research links were (1) providing more insights into the topic and (2) challenging information offered by other commentators. ",2405.12798v1
"['Fakhri Momeni', 'Philipp Mayr', 'Nicholas Fraser', 'Isabella Peters']",2021-03-26T15:20:29Z,What happens when a journal converts to Open Access? A bibliometric   analysis,"  In recent years, increased stakeholder pressure to transition research to Open Access has led to many journals converting, or 'flipping', from a closed access (CA) to an open access (OA) publishing model. Changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. In this paper we aimed to understand how flipping a journal to an OA model influences the journal's future publication volumes and citation impact. We analysed two independent sets of journals that had flipped to an OA model, one from the Directory of Open Access Journals (DOAJ) and one from the Open Access Directory (OAD), and compared their development with two respective control groups of similar journals. For bibliometric analyses, journals were matched to the Scopus database. We assessed changes in the number of articles published over time, as well as two citation metrics at the journal and article level: the normalised impact factor (IF) and the average relative citations (ARC), respectively. Our results show that overall, journals that flipped to an OA model increased their publication output compared to journals that remained closed. Mean normalised IF and ARC also generally increased following the flip to an OA model, at a greater rate than was observed in the control groups. However, the changes appear to vary largely by scientific discipline. Overall, these results indicate that flipping to an OA publishing model can bring positive changes to a journal. ",2103.14522v1
"['Kristin Biesenbender', 'Nina Smirnova', 'Philipp Mayr', 'Isabella Peters']",2023-08-08T11:13:58Z,The Emergence of Preprints: Comparing Publishing Behaviour in the Global   South and the Global North,"  Purpose: The recent proliferation of preprints could be a way for researchers worldwide to increase the availability and visibility of their research findings. Against the background of rising publication costs caused by the increasing prevalence of article processing fees, the search for other ways to publish research results besides traditional journal publication may increase. This could be especially true for lower-income countries. Design/methodology/approach: Therefore, we are interested in the experiences and attitudes towards posting and using preprints in the Global South as opposed to the Global North. To explore whether motivations and concerns about posting preprints differ, we adopted a mixed-methods approach, combining a quantitative survey of researchers with focus group interviews. Findings: We found that respondents from the Global South were more likely to agree to adhere to policies and to emphasise that mandates could change publishing behaviour towards open access. They were also more likely to agree posting preprints has a positive impact. Respondents from the Global South and the Global North emphasised the importance of peer-reviewed research for career advancement. Originality: The study has identified a wide range of experiences with and attitudes towards posting preprints among researchers in the Global South and the Global North. To our knowledge, this has hardly been studied before, which is also because preprints only have emerged lately in many disciplines and countries. ",2308.04186v1
"['Peter Kraker', 'Elisabeth Lex', 'Juan Gorraiz', 'Christian Gumpenberger', 'Isabella Peters']",2015-03-04T12:45:38Z,Research Data Explored II: the Anatomy and Reception of figshare,"  This is the second paper in a series of bibliometric studies of research data. In this paper, we present an analysis of figshare, one of the largest multidisciplinary repositories for research materials to date. We analysed the structure of items archived in figshare, their usage, and their reception in two altmetrics sources (PlumX and ImpactStory). We found that figshare acts (1) as a personal repository for yet unpublished materials, (2) as a platform for newly published research materials, and (3) as an archive for PLOS. Depending on the function, we found different bibliometric characteristics. Items archived from PLOS tend to be coming from the natural sciences and are often unviewed and non-downloaded. Self-archived items, however, come from a variety of disciplines and exhibit some patterns of higher usage. In the altmetrics analysis, we found that Twitter was the social media service where research data gained most attention; generally, research data published in 2014 were most popular across social media services. PlumX detects considerably more items in social media and also finds higher altmetric scores than ImpactStory. ",1503.01298v2
"['Agnes Mainka', 'Sarah Hartmann', 'Wolfgang G. Stock', 'Isabella Peters']",2014-01-18T11:45:44Z,Government and Social Media: A Case Study of 31 Informational World   Cities,"  Social media platforms are increasingly being used by governments to foster user interaction. Particularly in cities with enhanced ICT infrastructures (i.e., Informational World Cities) and high internet penetration rates, social media platforms are valuable tools for reaching high numbers of citizens. This empirical investigation of 31 Informational World Cities will provide an overview of social media services used for governmental purposes, of their popularity among governments, and of their usage intensity in broadcasting information online. ",1401.4533v2
"['Armin Bernstetter', 'Tom Kwasnitschka', 'Jens Karstens', 'Markus Schlüter', 'Isabella Peters']",2024-08-29T08:31:26Z,Virtual Fieldwork in Immersive Environments using Game Engines,"  Fieldwork still is the first and foremost source of insight in many disciplines of the geosciences. Virtual fieldwork is an approach meant to enable scientists trained in fieldwork to apply these skills to a virtual representation of outcrops that are inaccessible to humans e.g. due to being located on the seafloor. For this purpose we develop a virtual fieldwork software in the game engine and 3D creation tool Unreal Engine. This software is developed specifically for a large, spatially immersive environment as well as virtual reality using head-mounted displays. It contains multiple options for quantitative measurements of visualized 3D model data. We visualize three distinct real-world datasets gathered by different photogrammetric and bathymetric methods as use cases and gather initial feedback from domain experts. ",2408.16346v1
"['Fakhri Momeni', 'Nicholas Fraser', 'Isabella Peters', 'Philipp Mayr']",2019-03-27T20:08:58Z,From closed to open access: A case study of flipped journals,"  In recent years, increased stakeholder pressure to transition research to Open Access has led to many journals ""flipping"" from a toll access to an open access publishing model. Changing the publishing model can influence the decision of authors to submit their papers to a journal, and increased article accessibility may influence citation behaviour. The aim of this paper is to show changes in the number of published articles and citations after the flipping of a journal. We analysed a set of 171 journals in the Web of Science (WoS) which flipped to open access. In addition to comparing the number of articles, average relative citation (ARC) and normalized impact factor (IF) are applied, respectively, as bibliometric indicators at the article and journal level, to trace the transformation of flipped journals covered. Our results show that flipping mostly has had positive effects on journal's IF. But it has had no obvious citation advantage for the articles. We also see a decline in the number of published articles after flipping. We can conclude that flipping to open access can improve the performance of journals, despite decreasing the tendency of authors to submit their articles and no better citation advantages for articles. ",1903.11682v3
"['Fakhri Momeni', 'Fariba Karimi', 'Philipp Mayr', 'Isabella Peters', 'Stefan Dietze']",2022-03-14T10:58:29Z,The many facets of academic mobility and its impact on scholars' career,"  International mobility in academia can enhance the human and social capital of researchers and consequently their scientific outcome. However, there is still a very limited understanding of the different mobility patterns among scholars with various socio-demographic characteristics. The aim of this study is twofold. First, we investigate to what extent individual factors associate with the mobility of researchers. Second, we explore the relationship between mobility and scientific activity and impact. For this purpose, we used a bibliometric approach to track the mobility of authors. To compare the scientific outcomes of researchers, we considered the number of publications and received citations as indicators, as well as the number of unique co-authors in all their publications. We also analysed the co-authorship network of researchers and compared centrality measures of mobile and non-mobile researchers. Results show that researchers from North America and Sub-Saharan Africa, particularly female ones, have the lowest, respectively, highest tendency towards international mobility. Having international co-authors increases the probability of international movement. Our findings uncover gender inequality in international mobility across scientific fields and countries. Across genders, researchers in the Physical sciences have the most and in the Social sciences the least rate of mobility. We observed more mobility for Social scientists at the advanced career stage, while researchers in other fields prefer to move at earlier career stages. Also, we found a positive correlation between mobility and scientific outcomes, but no apparent difference between females and males. Comparing the centrality of mobile and non-mobile researchers in the co-authorship networks reveals a higher social capital advantage for mobile researchers. ",2203.06995v3
"['Isabella Peters', 'Peter Kraker', 'Elisabeth Lex', 'Christian Gumpenberger', 'Juan Gorraiz']",2015-01-14T13:42:59Z,Research Data Explored: Citations versus Altmetrics,"  The study explores the citedness of research data, its distribution over time and how it is related to the availability of a DOI (Digital Object Identifier) in Thomson Reuters' DCI (Data Citation Index). We investigate if cited research data ""impact"" the (social) web, reflected by altmetrics scores, and if there is any relationship between the number of citations and the sum of altmetrics scores from various social media-platforms. Three tools are used to collect and compare altmetrics scores, i.e. PlumX, ImpactStory, and Altmetric.com. In terms of coverage, PlumX is the most helpful altmetrics tool. While research data remain mostly uncited (about 85%), there has been a growing trend in citing data sets published since 2007. Surprisingly, the percentage of the number of cited research data with a DOI in DCI has decreased in the last years. Only nine repositories account for research data with DOIs and two or more citations. The number of cited research data with altmetrics scores is even lower (4 to 9%) but shows a higher coverage of research data from the last decade. However, no correlation between the number of citations and the total number of altmetrics scores is observable. Certain data types (i.e. survey, aggregate data, and sequence data) are more often cited and receive higher altmetrics scores. ",1501.03342v2
"['Stefanie Haustein', 'Vincent Larivière', 'Mike Thelwall', 'Didier Amyot', 'Isabella Peters']",2014-10-02T14:34:36Z,Tweets vs. Mendeley readers: How do these two social media metrics   differ?,"  A set of 1.4 million biomedical papers was analyzed with regards to how often articles are mentioned on Twitter or saved by users on Mendeley. While Twitter is a microblogging platform used by a general audience to distribute information, Mendeley is a reference manager targeted at an academic user group to organize scholarly literature. Both platforms are used as sources for so-called altmetrics to measure a new kind of research impact. This analysis shows in how far they differ and compare to traditional citation impact metrics based on a large set of PubMed papers. ",1410.0569v1
"['Maryam Mehrazar', 'Christoph Carl Kling', 'Steffen Lemke', 'Athanasios Mazarakis', 'Isabella Peters']",2018-04-08T20:13:59Z,Can We Count on Social Media Metrics? First Insights into the Active   Scholarly Use of Social Media,"  Measuring research impact is important for ranking publications in academic search engines and for research evaluation. Social media metrics or altmetrics measure the impact of scientific work based on social media activity. Altmetrics are complementary to traditional, citation-based metrics, e.g. allowing the assessment of new publications for which citations are not yet available. Despite the increasing importance of altmetrics, their characteristics are not well understood: Until now it has not been researched what kind of researchers are actively using which social media services and why - important questions for scientific impact prediction. Based on a survey among 3,430 scientists, we uncover previously unknown and significant differences between social media services: We identify services which attract young and experienced researchers, respectively, and detect differences in usage motivations. Our findings have direct implications for the future design of altmetrics for scientific impact prediction. ",1804.02751v1
"['Olga Zagovora', 'Katrin Weller', 'Milan Janosov', 'Claudia Wagner', 'Isabella Peters']",2018-09-17T16:04:45Z,"What increases (social) media attention: Research impact, author   prominence or title attractiveness?","  Do only major scientific breakthroughs hit the news and social media, or does a 'catchy' title help to attract public attention? How strong is the connection between the importance of a scientific paper and the (social) media attention it receives? In this study we investigate these questions by analysing the relationship between the observed attention and certain characteristics of scientific papers from two major multidisciplinary journals: Nature Communication (NC) and Proceedings of the National Academy of Sciences (PNAS). We describe papers by features based on the linguistic properties of their titles and centrality measures of their authors in their co-authorship network. We identify linguistic features and collaboration patterns that might be indicators for future attention, and are characteristic to different journals, research disciplines, and media sources. ",1809.06299v1
"['Stefanie Haustein', 'Isabella Peters', 'Cassidy R. Sugimoto', 'Mike Thelwall', 'Vincent Larivière']",2013-08-08T13:09:13Z,Tweeting biomedicine: an analysis of tweets and citations in the   biomedical literature,"  Data collected by social media platforms have recently been introduced as a new source for indicators to help measure the impact of scholarly research in ways that are complementary to traditional citation-based indicators. Data generated from social media activities related to scholarly content can be used to reflect broad types of impact. This paper aims to provide systematic evidence regarding how often Twitter is used to diffuse journal articles in the biomedical and life sciences. The analysis is based on a set of 1.4 million documents covered by both PubMed and Web of Science (WoS) and published between 2010 and 2012. The number of tweets containing links to these documents was analyzed to evaluate the degree to which certain journals, disciplines, and specialties were represented on Twitter. It is shown that, with less than 10% of PubMed articles mentioned on Twitter, its uptake is low in general. The relationship between tweets and WoS citations was examined for each document at the level of journals and specialties. The results show that tweeting behavior varies between journals and specialties and correlations between tweets and citations are low, implying that impact metrics based on tweets are different from those based on citations. A framework utilizing the coverage of articles and the correlation between Twitter mentions and citations is proposed to facilitate the evaluation of novel social-media based metrics and to shed light on the question in how far the number of tweets is a valid metric to measure research impact. ",1308.1838v1
"['Stefanie Haustein', 'Timothy D. Bowman', 'Kim Holmberg', 'Isabella Peters', 'Vincent Larivière']",2014-10-07T14:26:58Z,Astrophysicists on Twitter: An in-depth analysis of tweeting and   scientific publication behavior,"  This paper analyzes the tweeting behavior of 37 astrophysicists on Twitter and compares their tweeting behavior with their publication behavior and citation impact to show whether they tweet research-related topics or not. Astrophysicists on Twitter are selected to compare their tweets with their publications from Web of Science. Different user groups are identified based on tweeting and publication frequency. A moderate negative correlation (p=-0.390*) is found between the number of publications and tweets per day, while retweet and citation rates do not correlate. The similarity between tweets and abstracts is very low (cos=0.081). User groups show different tweeting behavior such as retweeting and including hashtags, usernames and URLs. The study is limited in terms of the small set of astrophysicists. Results are not necessarily representative of the entire astrophysicist community on Twitter and they most certainly do not apply to scientists in general. Future research should apply the methods to a larger set of researchers and other scientific disciplines. To a certain extent, this study helps to understand how researchers use Twitter. The results hint at the fact that impact on Twitter can neither be equated with nor replace traditional research impact metrics. However, tweets and other so-called altmetrics might be able to reflect other impact of scientists such as public outreach and science communication. To the best of our knowledge, this is the first in-depth study comparing researchers' tweeting activity and behavior with scientific publication output in terms of quantity, content and impact. ",1410.1740v1
"['Fakhri Momeni', 'Stefan Dietze', 'Philipp Mayr', 'Kristin Biesenbender', 'Isabella Peters']",2022-08-17T11:06:44Z,Which Factors are associated with Open Access Publishing? A Springer   Nature Case Study,"  Open Access (OA) facilitates access to articles. But, authors or funders often must pay the publishing costs preventing authors who do not receive financial support from participating in OA publishing and citation advantage for OA articles. OA may exacerbate existing inequalities in the publication system rather than overcome them. To investigate this, we studied 522,411 articles published by Springer Nature. Employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. A machine learning classification method helped us to explore the importance of different features in predicting the publishing model. The results show that authors eligible for APC waivers publish more in gold-OA journals than others. In contrast, authors eligible for an APC discount have the lowest ratio of OA publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold-OA journals. We found a strong correlation between the journal rank and the publishing model in gold-OA journals, whereas the OA option is mostly avoided in hybrid journals. Also, results show that the countries' income level, seniority, and experience with OA publications are the most predictive factors for OA publishing in hybrid journals. ",2208.08221v4
"['Nicholas Fraser', 'Anne Hobert', 'Najko Jahn', 'Philipp Mayr', 'Isabella Peters']",2021-05-25T16:58:16Z,No Deal: Investigating the Influence of Restricted Access to Elsevier   Journals on German Researchers' Publishing and Citing Behaviours,"  In 2014, a union of German research organisations established Projekt DEAL, a national-level project to negotiate licensing agreements with large scientific publishers. Negotiations between DEAL and Elsevier began in 2016, and broke down without a successful agreement in 2018; in this time, around 200 German research institutions cancelled their license agreements with Elsevier, leading Elsevier to restrict journal access at those institutions from July 2018 onwards. We investigated the effect of these access restrictions on researchers' publishing and citing behaviours from a bibliometric perspective, using a dataset of ~410,000 articles published by researchers at the affected DEAL institutions between 2012-2020. We further investigated these effects with respect to the timing of contract cancellations with Elsevier, research disciplines, collaboration patterns, and article open-access status. We find evidence for a decrease in Elsevier's market share of articles from DEAL institutions, from a peak of 25.3% in 2015 to 20.6% in 2020, with the largest year-on-year market share decreases occurring in 2019 (-1.1%) and 2020 (-1.6%) following the implementation of access restrictions. We also observe year-on-year decreases in the proportion of citations made from articles published by authors at DEAL institutions to articles in Elsevier journals post-2018, although the decrease is smaller (-0.4% in 2019 and -0.6% in 2020) than changes in publishing volume. We conclude that Elsevier access restrictions have led to some reduced willingness of researchers at DEAL institutions to publish their research in Elsevier journals, but that researchers are not strongly affected in their ability to cite Elsevier articles, with the implication that researchers use a variety of other methods (e.g. interlibrary loans, sharing between colleagues, or ""shadow libraries"") to access scientific literature. ",2105.12078v1
"['Judit Bar-Ilan', 'Stefanie Haustein', 'Isabella Peters', 'Jason Priem', 'Hadas Shema', 'Jens Terliesner']",2012-05-25T04:41:40Z,Beyond citations: Scholars' visibility on the social Web,"  Traditionally, scholarly impact and visibility have been measured by counting publications and citations in the scholarly literature. However, increasingly scholars are also visible on the Web, establishing presences in a growing variety of social ecosystems. But how wide and established is this presence, and how do measures of social Web impact relate to their more traditional counterparts? To answer this, we sampled 57 presenters from the 2010 Leiden STI Conference, gathering publication and citations counts as well as data from the presenters' Web ""footprints."" We found Web presence widespread and diverse: 84% of scholars had homepages, 70% were on LinkedIn, 23% had public Google Scholar profiles, and 16% were on Twitter. For sampled scholars' publications, social reference manager bookmarks were compared to Scopus and Web of Science citations; we found that Mendeley covers more than 80% of sampled articles, and that Mendeley bookmarks are significantly correlated (r=.45) to Scopus citation counts. ",1205.5611v1
"['Stefanie Haustein', 'Isabella Peters', 'Judit Bar-Ilan', 'Jason Priem', 'Hadas Shema', 'Jens Terliesner']",2013-04-26T22:34:40Z,Coverage and adoption of altmetrics sources in the bibliometric   community,"  Altmetrics, indices based on social media platforms and tools, have recently emerged as alternative means of measuring scholarly impact. Such indices assume that scholars in fact populate online social environments, and interact with scholarly products there. We tested this assumption by examining the use and coverage of social media environments amongst a sample of bibliometricians. As expected, coverage varied: 82% of articles published by sampled bibliometricians were included in Mendeley libraries, while only 28% were included in CiteULike. Mendeley bookmarking was moderately correlated (.45) with Scopus citation. Over half of respondents asserted that social media tools were affecting their professional lives, although uptake of online tools varied widely. 68% of those surveyed had LinkedIn accounts, while Academia.edu, Mendeley, and ResearchGate each claimed a fifth of respondents. Nearly half of those responding had Twitter accounts, which they used both personally and professionally. Surveyed bibliometricians had mixed opinions on altmetrics' potential; 72% valued download counts, while a third saw potential in tracking articles' influence in blogs, Wikipedia, reference managers, and social media. Altogether, these findings suggest that some online tools are seeing substantial use by bibliometricians, and that they present a potentially valuable source of impact data. ",1304.7300v1
"['Tero Harju', 'Dirk Nowotka']",2003-05-23T11:28:00Z,Periodicity and Unbordered Words: A Proof of the Extended Duval   Conjecture,"  The relationship between the length of a word and the maximum length of its unbordered factors is investigated in this paper. Consider a finite word w of length n. We call a word bordered, if it has a proper prefix which is also a suffix of that word. Let f(w) denote the maximum length of all unbordered factors of w, and let p(w) denote the period of w. Clearly, f(w) < p(w)+1.   We establish that f(w) = p(w), if w has an unbordered prefix of length f(w) and n > 2f(w)-2. This bound is tight and solves the stronger version of a 21 years old conjecture by Duval. It follows from this result that, in general, n > 3f(w)-3 implies f(w) = p(w) which gives an improved bound for the question asked by Ehrenfeucht and Silberger in 1979. ",cs/0305039v2
"['Axel Legay', 'Dirk Nowotka', 'Danny Bøgsted Poulsen']",2020-06-04T07:08:45Z,Automatic Verification of LLVM Code,"  In this work we present our work in developing a software verification tool for llvm-code - Lodin - that incorporates both explicit-state model checking, statistical model checking and symbolic state model checking algorithms. ",2006.02670v1
"['Robert Mercaş', 'Dirk Nowotka']",2016-01-11T14:14:23Z,A note on Thue games,"  In this work we improve on a result from~\cite{GryKosZma15}. In particular, we investigate the situation where a word is constructed jointly by two players who alternately append letters to the end of an existing word. One of the players (Ann) tries to avoid (non-trivial) repetitions, while the other one (Ben) tries to enforce them. We show a construction that is closer to the lower bound showed in~\cite{GryKozMic13} using entropy compression, and building on the probabilistic arguments based on a version of the Lov\'asz Local Lemma from~\cite{Peg11}. We provide an explicit strategy for Ann to avoid (non-trivial) repetitions over a $7$-letter alphabet. ",1601.02453v1
"['Dirk Nowotka', 'Aleksi Saarela']",2018-05-24T07:49:35Z,An optimal bound on the solution sets of one-variable word equations and   its consequences,"  We solve two long-standing open problems on word equations. Firstly, we prove that a one-variable word equation with constants has either at most three or an infinite number of solutions. The existence of such a bound had been conjectured, and the bound three is optimal. Secondly, we consider independent systems of three-variable word equations without constants. If such a system has a nonperiodic solution, then this system of equations is at most of size 17. Although probably not optimal, this is the first finite bound found. However, the conjecture of that bound being actually two still remains open. ",1805.09535v1
"['Benjamin Hoffmann', 'Mikhail Lifshits', 'Yury Lifshits', 'Dirk Nowotka']",2010-04-01T09:34:55Z,Maximal Intersection Queries in Randomized Input Models,"  Consider a family of sets and a single set, called the query set. How can one quickly find a member of the family which has a maximal intersection with the query set? Time constraints on the query and on a possible preprocessing of the set family make this problem challenging. Such maximal intersection queries arise in a wide range of applications, including web search, recommendation systems, and distributing on-line advertisements. In general, maximal intersection queries are computationally expensive. We investigate two well-motivated distributions over all families of sets and propose an algorithm for each of them. We show that with very high probability an almost optimal solution is found in time which is logarithmic in the size of the family. Moreover, we point out a threshold phenomenon on the probabilities of intersecting sets in each of our two input models which leads to the efficient algorithms mentioned above. ",1004.0092v1
"['Stefan Göller', 'Dirk Nowotka']",2007-07-04T09:33:21Z,On a Non-Context-Free Extension of PDL,"  Over the last 25 years, a lot of work has been done on seeking for decidable non-regular extensions of Propositional Dynamic Logic (PDL). Only recently, an expressive extension of PDL, allowing visibly pushdown automata (VPAs) as a formalism to describe programs, was introduced and proven to have a satisfiability problem complete for deterministic double exponential time. Lately, the VPA formalism was extended to so called k-phase multi-stack visibly pushdown automata (k-MVPAs). Similarly to VPAs, it has been shown that the language of k-MVPAs have desirable effective closure properties and that the emptiness problem is decidable. On the occasion of introducing k-MVPAs, it has been asked whether the extension of PDL with k-MVPAs still leads to a decidable logic. This question is answered negatively here. We prove that already for the extension of PDL with 2-phase MVPAs with two stacks satisfiability becomes \Sigma_1^1-complete. ",0707.0562v2
"['Dmitry Kosolobov', 'Florin Manea', 'Dirk Nowotka']",2016-03-31T21:25:42Z,Detecting One-variable Patterns,"  Given a pattern $p = s_1x_1s_2x_2\cdots s_{r-1}x_{r-1}s_r$ such that $x_1,x_2,\ldots,x_{r-1}\in\{x,\overset{{}_{\leftarrow}}{x}\}$, where $x$ is a variable and $\overset{{}_{\leftarrow}}{x}$ its reversal, and $s_1,s_2,\ldots,s_r$ are strings that contain no variables, we describe an algorithm that constructs in $O(rn)$ time a compact representation of all $P$ instances of $p$ in an input string of length $n$ over a polynomially bounded integer alphabet, so that one can report those instances in $O(P)$ time. ",1604.00054v5
"['Pamela Fleischmann', 'Mitja Kulczynski', 'Dirk Nowotka']",2019-05-28T14:28:12Z,On Collapsing Prefix Normal Words,"  Prefix normal words are binary words in which each prefix has at least the same number of $\so$s as any factor of the same length. Firstly introduced by Fici and Lipt\'ak in 2011, the problem of determining the index of the prefix equivalence relation is still open. In this paper, we investigate two aspects of the problem, namely prefix normal palindromes and so-called collapsing words (extending the notion of critical words). We prove characterizations for both the palindromes and the collapsing words and show their connection. Based on this, we show that still open problems regarding prefix normal words can be split into certain subproblems. ",1905.11847v2
"['Vesa Halava', 'Tero Harju', 'Dirk Nowotka', 'Esa Sahla']",2023-02-13T10:37:48Z,Decision Problems on Copying and Shuffling,"  We study decision problems of the form: given a regular or linear context-free language $L$, is there a word of a given fixed form in $L$, where given fixed forms are based on word operations copy, marked copy, shuffle and their combinations. ",2302.06248v4
"['Joel D. Day', 'Florin Manea', 'Dirk Nowotka']",2017-02-25T17:05:35Z,The Hardness of Solving Simple Word Equations,"  We investigate the class of regular-ordered word equations. In such equations, each variable occurs at most once in each side and the order of the variables occurring in both sides is the preserved (the variables can be, however, separated by potentially distinct constant factors). Surprisingly, we obtain that solving such simple equations, even when the sides contain exactly the same variables, is NP-hard. By considerations regarding the combinatorial structure of the minimal solutions of the more general quadratic equations we obtain that the satisfiability problem for regular-ordered equations is in NP. Finally, we also show that a related class of simple word equations, that generalises one-variable equations, is in P. ",1702.07922v2
"['Parthasarathy Madhusudan', 'Dirk Nowotka', 'Aayush Rajasekaran', 'Jeffrey Shallit']",2017-10-11T18:32:57Z,Lagrange's Theorem for Binary Squares,"  We show how to prove theorems in additive number theory using a decision procedure based on finite automata. Among other things, we obtain the following analogue of Lagrange's theorem: every natural number > 686 is the sum of at most 4 natural numbers whose canonical base-2 representation is a binary square, that is, a string of the form xx for some block of bits x. Here the number 4 is optimal. While we cannot embed this theorem itself in a decidable theory, we show that stronger lemmas that imply that the theorem can be embedded in decidable theories, and show how automated methods can be used to search for these stronger lemmas. ",1710.04247v2
"['Yannik Potdevin', 'Dirk Nowotka', 'Vijay Ganesh']",2019-09-12T11:44:33Z,An Empirical Investigation of Randomized Defenses against Adversarial   Attacks,"  In recent years, Deep Neural Networks (DNNs) have had a dramatic impact on a variety of problems that were long considered very difficult, e. g., image classification and automatic language translation to name just a few. The accuracy of modern DNNs in classification tasks is remarkable indeed. At the same time, attackers have devised powerful methods to construct specially-crafted malicious inputs (often referred to as adversarial examples) that can trick DNNs into mis-classifying them. What is worse is that despite the many defense mechanisms proposed to protect DNNs against adversarial attacks, attackers are often able to circumvent these defenses, rendering them useless. This state of affairs is extremely worrying, especially since machine learning systems get adopted at scale.   In this paper, we propose a scientific evaluation methodology aimed at assessing the quality, efficacy, robustness and efficiency of randomized defenses to protect DNNs against adversarial examples. Using this methodology, we evaluate a variety of defense mechanisms. In addition, we also propose a defense mechanism we call Randomly Perturbed Ensemble Neural Networks (RPENNs). We provide a thorough and comprehensive evaluation of the considered defense mechanisms against a white-box attacker model, six different adversarial attack methods and using the ILSVRC2012 validation data set. ",1909.05580v1
"['Pamela Fleischmann', 'Mitja Kulczynski', 'Dirk Nowotka']",2021-06-25T09:22:52Z,The Show Must Go On -- Examination During a Pandemic,"  When unexpected incidents occur, new innovative and flexible solutions are required. If this event is something such radical and dramatic like the COVID-19 pandemic, these solutions must aim to guarantee as much normality as possible while protecting lives. After a moment of shock our university decided that the students have to be able to pursue their studies for guaranteeing a degree in the expected time since most of them faced immediate financial problems due to the loss of their student jobs. This implied, for us as teachers, that we had to reorganise not only the teaching methods from nearly one day to the next, but we also had to come up with an adjusted way of examinations which had to take place in person with pen and paper under strict hygiene rules. On the other hand the correction should avoid personal contacts. We developed a framework which allowed us to correct the digitalised exams safely at home while providing the high standards given by the general data protection regulation of our country. Moreover, the time spent in the offices could be reduced to a minimum thanks to automatically generated exam sheets, automatically re-digitalised and sorted worked-on exams. ",2107.04014v1
"['Thorsten Ehlers', 'Florin Manea', 'Dirk Nowotka', 'Kamellia Reshadi']",2019-06-03T11:32:34Z,On Modelling the Avoidability of Patterns as CSP,"  Solving avoidability problems in the area of string combinatorics often requires, in an initial step, the construction, via a computer program, of a very long word that does not contain any word that matches a given pattern. It is well known that this is a computationally hard task. Despite being rather straightforward that, ultimately, all such tasks can be formalized as constraints satisfaction problems, no unified approach to solving them was proposed so far, and very diverse ad-hoc methods were used. We aim to fill this gap: we show how several relevant avoidability problems can be modelled, and consequently solved, in an uniform way as constraint satisfaction problems, using the framework of MiniZinc. The main advantage of this approach is that one is now required only to formulate the avoidability problem in the MiniZinc language, and then the actual search for a solution does not have to be implemented ad-hoc, being instead carried out by a standard CSP-solver. ",1906.00715v1
"['Pamela Fleischmann', 'Jonas Höfer', 'Annika Huch', 'Dirk Nowotka']",2023-06-25T10:16:49Z,$α$-$β$-Factorization and the Binary Case of Simon's Congruence,"  In 1991 H\'ebrard introduced a factorization of words that turned out to be a powerful tool for the investigation of a word's scattered factors (also known as (scattered) subwords or subsequences). Based on this, first Karandikar and Schnoebelen introduced the notion of $k$-richness and later on Barker et al. the notion of $k$-universality. In 2022 Fleischmann et al. presented a generalization of the arch factorization by intersecting the arch factorization of a word and its reverse. While the authors merely used this factorization for the investigation of shortest absent scattered factors, in this work we investigate this new $\alpha$-$\beta$-factorization as such. We characterize the famous Simon congruence of $k$-universal words in terms of $1$-universal words. Moreover, we apply these results to binary words. In this special case, we obtain a full characterization of the classes and calculate the index of the congruence. Lastly, we start investigating the ternary case, present a full list of possibilities for $\alpha\beta\alpha$-factors, and characterize their congruence. ",2306.14192v3
"['Pamela Fleischmann', 'Sungmin Kim', 'Tore Koß', 'Florin Manea', 'Dirk Nowotka', 'Stefan Siemer', 'Max Wiedenhöft']",2023-08-16T13:56:45Z,Matching Patterns with Variables Under Simon's Congruence,  We introduce and investigate a series of matching problems for patterns with variables under Simon's congruence. Our results provide a thorough picture of these problems' computational complexity. ,2308.08374v1
"['Bastian Bischoff', 'Dirk Nowotka']",2011-08-18T03:52:46Z,Pattern Avoidability with Involution,"  An infinte word w avoids a pattern p with the involution t if there is no substitution for the variables in p and no involution t such that the resulting word is a factor of w. We investigate the avoidance of patterns with respect to the size of the alphabet. For example, it is shown that the pattern a t(a) a can be avoided over three letters but not two letters, whereas it is well known that a a a is avoidable over two letters. ",1108.3622v1
"['Max J. Friese', 'Thorsten Ehlers', 'Dirk Nowotka']",2018-04-04T16:43:31Z,Estimating Latencies of Task Sequences in Multi-Core Automotive ECUs,"  The computation of a cyber-physical system's reaction to a stimulus typically involves the execution of several tasks. The delay between stimulus and reaction thus depends on the interaction of these tasks and is subject to timing constraints. Such constraints exist for a number of reasons and range from possible impacts on customer experiences to safety requirements. We present a technique to determine end-to-end latencies of such task sequences. The technique is demonstrated on the example of electronic control units (ECUs) in automotive embedded real-time systems. Our approach is able to deal with multi-core architectures and supports four different activation patterns, including interrupts. It is the first formal analysis approach making use of load assumptions in order to exclude infeasible data propagation paths without the knowledge of worst-case execution times or worst-case response times. We employ a constraint programming solver to compute bounds on end-to-end latencies. ",1804.07647v1
"['Yannik Eikmeier', 'Pamela Fleischmann', 'Mitja Kulczynski', 'Dirk Nowotka']",2020-05-19T08:28:03Z,Weighted Prefix Normal Words: Mind the Gap,"  A prefix normal word is a binary word whose prefixes contain at least as many 1s as any of its factors of the same length. Introduced by Fici and Lipt\'ak in 2011 the notion of prefix normality is so far only defined for words over the binary alphabet. In this work we investigate a generalisation for finite words over arbitrary finite alphabets, namely weighted prefix normality. We prove that weighted prefix normality is more expressive than binary prefix normality. Furthermore, we investigate the existence of a weighted prefix normal form since weighted prefix normality comes with several new peculiarities that did not already occur in the binary case. We characterise these issues and finally present a standard technique to obtain a generalised prefix normal form for all words overarbitrary, finite alphabets. ",2005.09281v3
"['Laura Barker', 'Pamela Fleischmann', 'Katharina Harwardt', 'Florin Manea', 'Dirk Nowotka']",2020-03-10T10:51:05Z,Scattered Factor-Universality of Words,"  A word $u=u_1\dots u_n$ is a scattered factor of a word $w$ if $u$ can be obtained from $w$ by deleting some of its letters: there exist the (potentially empty) words $v_0,v_1,..,v_n$ such that $w = v_0u_1v_1...u_nv_n$. The set of all scattered factors up to length $k$ of a word is called its full $k$-spectrum. Firstly, we show an algorithm deciding whether the $k$-spectra for given $k$ of two words are equal or not, running in optimal time. Secondly, we consider a notion of scattered-factors universality: the word $w$, with $\letters(w)=\Sigma$, is called $k$-universal if its $k$-spectrum includes all words of length $k$ over the alphabet $\Sigma$; we extend this notion to $k$-circular universality. After a series of preliminary combinatorial results, we present an algorithm computing, for a given $k'$-universal word $w$ the minimal $i$ such that $w^i$ is $k$-universal for some $k>k'$. Several other connected problems~are~also~considered. ",2003.04629v1
"['Joel Day', 'Vijay Ganesh', 'Paul He', 'Florin Manea', 'Dirk Nowotka']",2018-02-02T00:46:36Z,The Satisfiability of Extended Word Equations: The Boundary Between   Decidability and Undecidability,"  The study of word equations (or the existential theory of equations over free monoids) is a central topic in mathematics and theoretical computer science. The problem of deciding whether a given word equation has a solution was shown to be decidable by Makanin in the late 1970s, and since then considerable work has been done on this topic. In recent years, this decidability question has gained critical importance in the context of string SMT solvers for security analysis. Further, many extensions (e.g., quantifier-free word equations with linear arithmetic over the length function) and fragments (e.g., restrictions on the number of variables) of this theory are important from a theoretical point of view, as well as for program analysis applications. Motivated by these considerations, we prove several new results and thus shed light on the boundary between decidability and undecidability for many fragments and extensions of the first order theory of word equations. ",1802.00523v1
"['Pamela Fleischmann', 'Marie Lejeune', 'Florin Manea', 'Dirk Nowotka', 'Michel Rigo']",2020-01-30T09:08:41Z,Reconstructing Words from Right-Bounded-Block Words,"  A reconstruction problem of words from scattered factors asks for the minimal information, like multisets of scattered factors of a given length or the number of occurrences of scattered factors from a given set, necessary to uniquely determine a word. We show that a word $w \in \{a, b\}^{*}$ can be reconstructed from the number of occurrences of at most $\min(|w|_a, |w|_b)+ 1$ scattered factors of the form $a^{i} b$. Moreover, we generalize the result to alphabets of the form $\{1,\ldots,q\}$ by showing that at most $ \sum^{q-1}_{i=1} |w|_i (q-i+1)$ scattered factors suffices to reconstruct $w$. Both results improve on the upper bounds known so far. Complexity time bounds on reconstruction algorithms are also considered here. ",2001.11218v2
"['Joel D. Day', 'Pamela Fleischmann', 'Florin Manea', 'Dirk Nowotka']",2019-04-19T09:09:49Z,k-Spectra of weakly-c-Balanced Words,"  A word $u$ is a scattered factor of $w$ if $u$ can be obtained from $w$ by deleting some of its letters. That is, there exist the (potentially empty) words $u_1,u_2,..., u_n$, and $v_0,v_1,..,v_n$ such that $u = u_1u_2...u_n$ and $w = v_0u_1v_1u_2v_2...u_nv_n$. We consider the set of length-$k$ scattered factors of a given word w, called here $k$-spectrum and denoted $\ScatFact_k(w)$. We prove a series of properties of the sets $\ScatFact_k(w)$ for binary strictly balanced and, respectively, $c$-balanced words $w$, i.e., words over a two-letter alphabet where the number of occurrences of each letter is the same, or, respectively, one letter has $c$-more occurrences than the other. In particular, we consider the question which cardinalities $n= |\ScatFact_k(w)|$ are obtainable, for a positive integer $k$, when $w$ is either a strictly balanced binary word of length $2k$, or a $c$-balanced binary word of length $2k-c$. We also consider the problem of reconstructing words from their $k$-spectra. ",1904.09125v2
"['Max J. Friese', 'Dirk Nowotka']",2020-04-28T04:23:29Z,Estimating End-to-End Latencies in Automotive Cyber-physical Systems,"  Controller networks in today's automotive systems consist of more than 100 ECUs connected by various bus protocols. Seamless operation of the entire system requires a well-orchestrated interaction of these ECUs. Consequently, to ensure safety and comfort, a performance analysis is an inherent part of the engineering process. Conducting such an analysis manually is expensive, slow, and error prone. Tool support is therefore crucial, and a number of approaches have been presented. However, most work is limited to either network latencies or software latencies which results in an analysis gap at the transition between different layers of the communication stack. The work presented here introduces an approach to close this gap. Furthermore, we discuss the integration of different methods to obtain an end-to-end latency analysis. ",2004.13288v1
"['Pamela Fleischmann', 'Sebastian Bernhard Germann', 'Dirk Nowotka']",2021-04-19T05:45:44Z,Scattered Factor Universality -- The Power of the Remainder,"  Scattered factor (circular) universality was firstly introduced by Barker et al. in 2020. A word $w$ is called $k$-universal for some natural number $k$, if every word of length $k$ of $w$'s alphabet occurs as a scattered factor in $w$; it is called circular $k$-universal if a conjugate of $w$ is $k$-universal. Here, a word $u=u_1\cdots u_n$ is called a scattered factor of $w$ if $u$ is obtained from $w$ by deleting parts of $w$, i.e. there exists (possibly empty) words $v_1,\dots,v_{n+1}$ with $w=v_1u_1v_2\cdots v_nu_nv_{n+1}$. In this work, we prove two problems, left open in the aforementioned paper, namely a generalisation of one of their main theorems to arbitrary alphabets and a slight modification of another theorem such that we characterise the circular universality by the universality. On the way, we present deep insights into the behaviour of the remainder of the so called arch factorisation by Hebrard when repetitions of words are considered. ",2104.09063v1
"['Joel D. Day', 'Thorsten Ehlers', 'Mitja Kulczynski', 'Florin Manea', 'Dirk Nowotka', 'Danny Bøgsted Poulsen']",2019-06-27T15:03:20Z,On Solving Word Equations Using SAT,"  We present Woorpje, a string solver for bounded word equations (i.e., equations where the length of each variable is upper bounded by a given integer). Our algorithm works by reformulating the satisfiability of bounded word equations as a reachability problem for nondeterministic finite automata, and then carefully encoding this as a propositional satisfiability problem, which we then solve using the well-known Glucose SAT-solver. This approach has the advantage of allowing for the natural inclusion of additional linear length constraints. Our solver obtains reliable and competitive results and, remarkably, discovered several cases where state-of-the-art solvers exhibit a faulty behaviour. ",1906.11718v1
"['Pamela Fleischmann', 'Lukas Haschke', 'Annika Huch', 'Annika Mayrock', 'Dirk Nowotka']",2022-02-16T10:45:10Z,m-Nearly k-Universal Words -- Investigating Simon Congruence,"  Determining the index of the Simon congruence is a long outstanding open problem. Two words $u$ and $v$ are called Simon congruent if they have the same set of scattered factors, which are parts of the word in the correct order but not necessarily consecutive, e.g., $\mathtt{oath}$ is a scattered factor of $\mathtt{logarithm}$. Following the idea of scattered factor $k$-universality, we investigate $m$-nearly $k$-universality, i.e., words where $m$ scattered factors of length $k$ are absent, w.r.t. Simon congruence. We present a full characterisation as well as the index of the congruence for $m=1$. For $m\neq 1$, we show some results if in addition $w$ is $(k-1)$-universal as well as some further insights for different $m$. ",2202.07981v1
"['Pamela Fleischmann', 'Tero Harju', 'Lukas Haschke', 'Jonas Höfer', 'Dirk Nowotka']",2022-02-16T10:59:13Z,On the Self Shuffle Language,"  The shuffle product \(u\shuffle v\) of two words \(u\) and \(v\) is the set of all words which can be obtained by interleaving \(u\) and \(v\). Motivated by the paper \emph{The Shuffle Product: New Research Directions} by Restivo (2015) we investigate a special case of the shuffle product. In this work we consider the shuffle of a word with itself called the \emph{self shuffle} or \emph{shuffle square}, showing first that the self shuffle language and the shuffle of the language are in general different sets. We prove that the language of all words arising as a self shuffle of some word is context sensitive but not context free. Furthermore, we show that the self shuffle \(w \shuffle w\) uniquely determines \(w\). ",2202.07988v2
"['Murphy Berzish', 'Joel D. Day', 'Vijay Ganesh', 'Mitja Kulczynski', 'Florin Manea', 'Federico Mora', 'Dirk Nowotka']",2021-05-15T13:13:50Z,String Theories involving Regular Membership Predicates: From Practice   to Theory and Back,"  Widespread use of string solvers in formal analysis of string-heavy programs has led to a growing demand for more efficient and reliable techniques which can be applied in this context, especially for real-world cases. Designing an algorithm for the (generally undecidable) satisfiability problem for systems of string constraints requires a thorough understanding of the structure of constraints present in the targeted cases. In this paper, we investigate benchmarks presented in the literature containing regular expression membership predicates, extract different first order logic theories, and prove their decidability, resp. undecidability. Notably, the most common theories in real-world benchmarks are PSPACE-complete and directly lead to the implementation of a more efficient algorithm to solving string constraints. ",2105.07220v1
"['Mitja Kulczynski', 'Axel Legay', 'Dirk Nowotka', 'Danny Bøgsted Poulsen']",2021-08-06T06:43:56Z,Analysis of Source Code Using UPPAAL,"  In recent years there has been a considerable effort in optimising formal methods for application to code. This has been driven by tools such as CPAChecker, DIVINE, and CBMC. At the same time tools such as Uppaal have been massively expanding the realm of more traditional model checking technologies to include strategy synthesis algorithms - an aspect becoming more and more needed as software becomes increasingly parallel. Instead of reimplementing the advances made by Uppaal in this area, we suggest in this paper to develop a bridge between the source code and the engine of Uppaal. Our approach uses the widespread intermediate language LLVM and makes recent advances of the Uppaal ecosystem readily available to analysis of source code. ",2108.02963v1
"['Joel D. Day', 'Adrian Kröger', 'Mitja Kulczynski', 'Florin Manea', 'Dirk Nowotka', 'Danny Bøgsted Poulsen']",2022-08-18T12:56:12Z,A Generic Information Extraction System for String Constraints,"  String constraint solving, and the underlying theory of word equations, are highly interesting research topics both for practitioners and theoreticians working in the wide area of satisfiability modulo theories. As string constraint solving algorithms, a.k.a. string solvers, gained a more prominent role in the formal analysis of string-heavy programs, especially in connection to symbolic code execution and security protocol verification, we can witness an ever-growing number of benchmarks collecting string solving instances from real-world applications as well as an ever-growing need for more efficient and reliable solvers, especially for the aforementioned real-world instances. Thus, it seems that the string solving area (and the developers, theoreticians, and end-users active in it) could greatly benefit from a better understanding and processing of the existing string solving benchmarks. In this context, we propose SMTQUERY: an SMT-LIB benchmark analysis tool for string constraints. SMTQUERY is implemented in Python 3, and offers a collection of analysis and information extraction tools for a comprehensive data base of string benchmarks (presented in SMT-LIB format), based on an SQL-centred language called QLANG. ",2208.08806v1
"['Duncan Adamson', 'Pamela Fleischmann', 'Annika Huch', 'Tore Koß', 'Florin Manea', 'Dirk Nowotka']",2023-11-17T17:23:38Z,$k$-Universality of Regular Languages,"  A subsequence of a word $w$ is a word $u$ such that $u = w[i_1] w[i_2] \dots w[i_{k}]$, for some set of indices $1 \leq i_1 < i_2 < \dots < i_k \leq \lvert w\rvert$. A word $w$ is $k$-subsequence universal over an alphabet $\Sigma$ if every word in $\Sigma^k$ appears in $w$ as a subsequence. In this paper, we study the intersection between the set of $k$-subsequence universal words over some alphabet $\Sigma$ and regular languages over $\Sigma$. We call a regular language $L$ \emph{$k$-$\exists$-subsequence universal} if there exists a $k$-subsequence universal word in $L$, and \emph{$k$-$\forall$-subsequence universal} if every word of $L$ is $k$-subsequence universal. We give algorithms solving the problems of deciding if a given regular language, represented by a finite automaton recognising it, is \emph{$k$-$\exists$-subsequence universal} and, respectively, if it is \emph{$k$-$\forall$-subsequence universal}, for a given $k$. The algorithms are FPT w.r.t.~the size of the input alphabet, and their run-time does not depend on $k$; they run in polynomial time in the number $n$ of states of the input automaton when the size of the input alphabet is $O(\log n)$. Moreover, we show that the problem of deciding if a given regular language is \emph{$k$-$\exists$-subsequence universal} is NP-complete, when the language is over a large alphabet. Further, we provide algorithms for counting the number of $k$-subsequence universal words (paths) accepted by a given deterministic (respectively, nondeterministic) finite automaton, and ranking an input word (path) within the set of $k$-subsequence universal words accepted by a given finite automaton. ",2311.10658v1
"['Pamela Fleischmann', 'Lukas Haschke', 'Florin Manea', 'Dirk Nowotka', 'Cedric Tsatia Tsida', 'Judith Wiedenbeck']",2020-08-08T13:13:37Z,Blocksequences of k-local Words,"  The locality of words is a relatively young structural complexity measure, introduced by Day et al. in 2017 in order to define classes of patterns with variables which can be matched in polynomial time. The main tool used to compute the locality of a word is called marking sequence: an ordering of the distinct letters occurring in the respective order. Once a marking sequence is defined, the letters of the word are marked in steps: in the ith marking step, all occurrences of the ith letter of the marking sequence are marked. As such, after each marking step, the word can be seen as a sequence of blocks of marked letters separated by blocks of non-marked letters. By keeping track of the evolution of the marked blocks of the word through the marking defined by a marking sequence, one defines the blocksequence of the respective marking sequence. We first show that the words sharing the same blocksequence are only loosely connected, so we consider the stronger notion of extended blocksequence, which stores additional information on the form of each single marked block. In this context, we present a series of combinatorial results for words sharing the extended blocksequence. ",2008.03516v2
"['Murphy Berzish', 'Mitja Kulczynski', 'Federico Mora', 'Florin Manea', 'Joel D. Day', 'Dirk Nowotka', 'Vijay Ganesh']",2020-10-14T17:18:02Z,An SMT Solver for Regular Expressions and Linear Arithmetic over String   Length,"  We present a novel length-aware solving algorithm for the quantifier-free first-order theory over regex membership predicate and linear arithmetic over string length. We implement and evaluate this algorithm and related heuristics in the Z3 theorem prover. A crucial insight that underpins our algorithm is that real-world instances contain a wealth of information about upper and lower bounds on lengths of strings under constraints, and such information can be used very effectively to simplify operations on automata representing regular expressions. Additionally, we present a number of novel general heuristics, such as the prefix/suffix method, that can be used in conjunction with a variety of regex solving algorithms, making them more efficient. We showcase the power of our algorithm and heuristics via an extensive empirical evaluation over a large and diverse benchmark of 57256 regex-heavy instances, almost 75% of which are derived from industrial applications or contributed by other solver developers. Our solver outperforms five other state-of-the-art string solvers, namely, CVC4, OSTRICH, Z3seq, Z3str3, and Z3-Trau, over this benchmark, in particular achieving a 2.4x speedup over CVC4, 4.4x speedup over Z3seq, 6.4x speedup over Z3-Trau, 9.1x speedup over Z3str3, and 13x speedup over OSTRICH. ",2010.07253v3
"['Therese Biedl', 'Ahmad Biniaz', 'Robert Cummings', 'Anna Lubiw', 'Florin Manea', 'Dirk Nowotka', 'Jeffrey Shallit']",2018-01-25T19:14:37Z,Rollercoasters and Caterpillars,"  A rollercoaster is a sequence of real numbers for which every maximal contiguous subsequence, that is increasing or decreasing, has length at least three. By translating this sequence to a set of points in the plane, a rollercoaster can be defined as a polygonal path for which every maximal sub-path, with positive- or negative-slope edges, has at least three points. Given a sequence of distinct real numbers, the rollercoaster problem asks for a maximum-length subsequence that is a rollercoaster. It was conjectured that every sequence of $n$ distinct real numbers contains a rollercoaster of length at least $\lceil n/2\rceil$ for $n>7$, while the best known lower bound is $\Omega(n/\log n)$. In this paper we prove this conjecture. Our proof is constructive and implies a linear-time algorithm for computing a rollercoaster of this length. Extending the $O(n\log n)$-time algorithm for computing a longest increasing subsequence, we show how to compute a maximum-length rollercoaster within the same time bound. A maximum-length rollercoaster in a permutation of $\{1,\dots,n\}$ can be computed in $O(n \log \log n)$ time.   The search for rollercoasters was motivated by orthogeodesic point-set embedding of caterpillars. A caterpillar is a tree such that deleting the leaves gives a path, called the spine. A top-view caterpillar is one of degree 4 such that the two leaves adjacent to each vertex lie on opposite sides of the spine. As an application of our result on rollercoasters, we are able to find a planar drawing of every $n$-node top-view caterpillar on every set of $\frac{25}{3}n$ points in the plane, such that each edge is an orthogonal path with one bend. This improves the previous best known upper bound on the number of required points, which is $O(n \log n)$. We also show that such a drawing can be obtained in linear time, provided that the points are given in sorted order. ",1801.08565v1
"['Eike Petersen', 'Yannik Potdevin', 'Esfandiar Mohammadi', 'Stephan Zidowitz', 'Sabrina Breyer', 'Dirk Nowotka', 'Sandra Henn', 'Ludwig Pechmann', 'Martin Leucker', 'Philipp Rostalski', 'Christian Herzog']",2021-07-20T15:03:05Z,Responsible and Regulatory Conform Machine Learning for Medicine: A   Survey of Challenges and Solutions,"  Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning systems must be developed responsibly. Many high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. This survey provides an overview of the technical and procedural challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. First, a brief review of existing regulations affecting medical machine learning is provided, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations - albeit, in many cases, to an uncertain degree. Next, the key technical obstacles to achieving these desirable properties are discussed, as well as important techniques to overcome these obstacles in the medical context. We notice that distribution shift, spurious correlations, model underspecification, uncertainty quantification, and data scarcity represent severe challenges in the medical context. Promising solution approaches include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge, the use of inherently transparent models, comprehensive out-of-distribution model testing and verification, as well as algorithmic impact assessments. ",2107.09546v2
"['Masoud Taleb', 'Mohsen Samadi', 'Fatemeh Davoodi', 'Maximilian Black', 'Janek Buhl', 'Hannes Lüder', 'Martina Gerken', 'Nahid Talebi']",2022-08-19T09:32:48Z,Spin-orbit interactions in noncentrosymmetric plasmonic crystals probed   by site-selective cathodoluminescence spectroscopy,"  The study of spin-orbit coupling (SOC) of light is crucial to explore the light-matter interactions in sub-wavelength nanostructures with broken symmetries. In noncentrosymmetric photonic crystals, the SOC results in the splitting of the otherwise degenerate energy bands. Herein, we explore the SOC in a noncentrosymmetric plasmonic crystal, both theoretically and experimentally. Cathodoluminescence (CL) spectroscopy combined with the numerically calculated photonic band structure reveals an energy band splitting that is ascribed to the broken symmetries in the noncentrosymmetric plasmonic crystal. By shifting the impact position of the electron beam throughout a unit cell of the plasmonic crystal, we show that the emergence of the energy band splitting strongly depends on the excitation position of the surface plasmon (SP) waves on the crystal. Moreover, we exploit angle-resolved CL and dark-field polarimetry to demonstrate polarization-dependent scattering of SP waves interacting with the plasmonic crystal. The scattering direction of a given polarization is determined by the transverse spin angular momentum inherently carried by the SP wave, which is in turn locked to the direction of SP propagation. Our study gives insight into the design of novel plasmonic devices with polarization-dependent directionality of the Bloch plasmons. We expect spin-orbit plasmonics will find much more scientific interests and potential applications with the continuous development of nanofabrication methodologies and uncovering new aspects of spin-orbit interactions. ",2208.09238v1
"['Andreas Martin Kettner', 'Lorenzo Reyes-Chamorro', 'Johanna Kristin Maria Becker', 'Zhixiang Zou', 'Marco Liserre', 'Mario Paolone']",2021-06-23T09:18:14Z,"Harmonic Power-Flow Study of Polyphase Grids with Converter-Interfaced   Distributed Energy Resources, Part I: Modelling Framework and Algorithm","  Power distribution systems are experiencing a large-scale integration of Converter-Interfaced Distributed Energy Resources (CIDERs). This complicates the analysis and mitigation of harmonics, whose creation and propagation are facilitated by the interactions of converters and their controllers through the grid. In this paper, a method for the calculation of the so-called Harmonic Power-Flow (HPF) in three-phase grids with CIDERs is proposed. The distinguishing feature of this HPF method is the generic and modular representation of the system components. Notably, as opposed to most of the existing approaches, the coupling between harmonics is explicitly considered. The HPF problem is formulated by combining the hybrid nodal equations of the grid with the closed-loop transfer functions of the CIDERs, and solved using the Newton-Raphson method. The grid components are characterized by compound electrical parameters, which allow to represent both transposed or non-transposed lines. The CIDERs are represented by modular linear time-periodic systems, which allows to treat both grid-forming and grid-following control laws. The method's accuracy and computational efficiency are confirmed via time-domain simulations of the CIGR\'E low-voltage benchmark microgrid. This paper is divided in two parts, which focus on the development (Part I) and the validation (Part II) of the proposed method. ",2106.12253v4
"['Johanna Kristin Maria Becker', 'Andreas Martin Kettner', 'Lorenzo Reyes-Chamorro', 'Zhixiang Zou', 'Marco Liserre', 'Mario Paolone']",2021-06-23T09:25:32Z,"Harmonic Power-Flow Study of Polyphase Grids with Converter-Interfaced   Distributed Energy Resources, Part II: Model Library and Validation","  In Part I, a method for the Harmonic Power-Flow (HPF) study of three-phase power grids with Converter-Interfaced Distributed Energy Resources (CIDERs) is proposed. The method is based on generic and modular representations of the grid and the CIDERs, and explicitly accounts for coupling between harmonics. In Part II, the HPF method is validated. First, the applicability of the modeling framework is demonstrated on typical grid-forming and grid-following CIDERs. Then, the HPF method is implemented in Matlab and compared against time-domain simulations with Simulink. The accuracy of the models and the performance of the solution algorithm are assessed for individual resources and a modified version of the CIGR\'E low-voltage benchmark microgrid (i.e., with additional unbalanced components). The observed maximum errors are 6.3E-5 p.u. w.r.t. voltage magnitude, 1.3E-3 p.u. w.r.t. current magnitude, and 0.9 deg w.r.t. phase. Moreover, the scalability of the method is assessed w.r.t. the number of CIDERs and the maximum harmonic order ($\leqslant$25). For the maximum problem size, the execution time of the HPF method is 6.52 sec, which is 5 times faster than the time-domain simulation. The convergence of the method is robust w.r.t. the choice of the initial point, and multiplicity of solutions has not been observed. ",2106.12255v4
"['Johanna Kristin Maria Becker', 'Andreas Martin Kettner', 'Yihui Zuo', 'Federico Cecati', 'Sante Pugliese', 'Marco Liserre', 'Mario Paolone']",2022-06-15T07:29:46Z,Modelling of AC/DC Interactions of Converter-Interfaced Resources for   Harmonic Power-Flow Studies in Microgrids,"  Power distribution systems experience a large-scale integration of Converter-Interfaced Distributed Energy Resources (CIDERs). As acknowledged by recent literature, the interaction of individual CIDER components and different CIDERs through the grid can lead to undesirable amplification of harmonic frequencies and, ultimately, compromise the distribution system stability. In this context, the interaction of the DC and AC sides of CIDERs has been shown to have a significant impact. In order to analyze and support the mitigation of such phenomena, the authors of this paper recently proposed a Harmonic Power-Flow (HPF) framework for polyphase grids with a high share of CIDERs. The framework considers the coupling between harmonics, but ignores the DC-side response of the CIDERs. Modelling the DC side and AC/DC converter introduces a nonlinearity into the CIDER model that needs to be approximated for the numerical solution of the HPF. This paper extends the CIDER model and HPF framework to address this aspect, whose inclusion is non-trivial. The extended HPF method is applied to a modified version of the CIGRE low-voltage benchmark microgrid. The results are compared to (i) time-domain simulations with Simulink, (ii) the predecessor of the extended HPF which neglects the DC side, and (iii) a classical decoupled HPF. ",2206.07332v3
"['Shuyu Ou', 'Mahyar Hassanifar', 'Martin Votava', 'Marius Langwasser', 'Marco Liserre', 'Ariya Sangwongwanich', 'Subham Sahoo', 'Frede Blaabjerg']",2024-04-20T14:56:34Z,A Data-Driven Condition Monitoring Method for Capacitor in Modular   Multilevel Converter (MMC),"  The modular multilevel converter (MMC) is a topology that consists of a high number of capacitors, and degradation of capacitors can lead to converter malfunction, limiting the overall system lifetime. Condition monitoring methods can be applied to assess the health status of capacitors and realize predictive maintenance to improve reliability. Current research works for condition monitoring of capacitors in an MMC mainly monitor either capacitance or equivalent series resistance (ESR), while these two health indicators can shift at different speeds and lead to different end-of-life times. Hence, monitoring only one of these parameters may lead to unreliable health status evaluation. This paper proposes a data-driven method to estimate capacitance and ESR at the same time, in which particle swarm optimization (PSO) is leveraged to update the obtained estimations. Then, the results of the estimations are used to predict the sub-module voltage, which is based on a capacitor voltage equation. Furthermore, minimizing the mean square error between the predicted and actual measured voltage makes the estimations closer to the actual values. The effectiveness and feasibility of the proposed method are validated through simulations and experiments. ",2404.13399v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Stephan Pachnicke']",2022-03-19T09:40:48Z,Machine Learning based Data Driven Diagnostic and Prognostic Approach   for Laser Reliability Enhancement,"  In this paper, a data-driven diagnostic and prognostic approach based on machine learning is proposed to detect laser failure modes and to predict the remaining useful life (RUL) of a laser during its operation. We present an architecture of the proposed cognitive predictive maintenance framework and demonstrate its effectiveness using synthetic data. ",2203.11728v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Stephan Pachnicke']",2022-03-19T09:02:26Z,Federated Learning Approach for Lifetime Prediction of Semiconductor   Lasers,"  A new privacy-preserving federated learning framework allowing laser manufacturers to collaboratively build a robust ML-based laser lifetime prediction model, is proposed. It achieves a mean absolute error of 0.1 years and a significant performance improvement ",2203.12414v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Stephan Pachnicke']",2022-03-19T09:08:14Z,A Hybrid CNN-LSTM Approach for Laser Remaining Useful Life Prediction,  A hybrid prognostic model based on convolutional neural networks (CNN) and long short-term memory (LSTM) is proposed to predict the laser remaining useful life (RUL). The experimental results show that it outperforms the conventional methods. ,2203.12415v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Carsten Tropschug', 'Stephan Pachnicke']",2022-02-16T13:01:45Z,A BiLSTM-CNN based Multitask Learning Approach for Fiber Fault Diagnosis,"  A novel multitask learning approach based on stacked bidirectional long short-term memory (BiLSTM) networks and convolutional neural networks (CNN) for detecting, locating, characterizing, and identifying fiber faults is proposed. It outperforms conventionally employed techniques. ",2202.08034v1
"['Khouloud Abdelli', 'Florian Azendorf', 'Helmut Griesser', 'Carsten Tropschug', 'Stephan Pachnicke']",2022-03-19T09:26:07Z,Gated Recurrent Unit based Autoencoder for Optical Link Fault Diagnosis   in Passive Optical Networks,"  We propose a deep learning approach based on an autoencoder for identifying and localizing fiber faults in passive optical networks. The experimental results show that the proposed method detects faults with 97% accuracy, pinpoints them with an RMSE of 0.18 m and outperforms conventional techniques. ",2203.11727v1
"['Khouloud Abdelli', 'Danish Rafique', 'Helmut Griesser', 'Stephan Pachnicke']",2022-03-19T09:36:25Z,Lifetime Prediction of 1550 nm DFB Laser using Machine learning   Techniques,  A novel approach based on an artificial neural network (ANN) for lifetime prediction of 1.55 um InGaAsP MQW-DFB laser diodes is presented. It outperforms the conventional lifetime projection using accelerated aging tests. ,2203.14762v1
"['Khouloud Abdelli', 'Danish Rafique', 'Stephan Pachnicke']",2022-03-19T09:46:19Z,Machine Learning based Laser Failure Mode Detection,"  Laser degradation analysis is a crucial process for the enhancement of laser reliability. Here, we propose a data-driven fault detection approach based on Long Short-Term Memory (LSTM) recurrent neural networks to detect the different laser degradation modes based on synthetic historical failure data. In comparison to typical threshold-based systems, attaining 24.41% classification accuracy, the LSTM-based model achieves 95.52% accuracy, and also outperforms classical machine learning (ML) models namely Random Forest (RF), K-Nearest Neighbours (KNN) and Logistic Regression (LR). ",2203.11729v1
"['Jonas Koch', 'Alvaro Moscoso-Mártir', 'Juliana Müller', 'Florian Merget', 'Stephan Pachnicke', 'Jeremy Witzens']",2020-04-07T08:05:31Z,Silicon Photonics DWDM NLFT Soliton Transmitter,"  We investigate the transmission of densely multiplexed solitons using a photonic integrated chip and the nonlinear Fourier-transform and analyze required launch conditions, the effect of (de-)multiplexing and noise on the nonlinear spectrum, and equalization techniques that can be used to enhance the transmission performance. ",2004.03183v1
"['khouloud Abdelli', 'Carsten Tropschug', 'Helmut Griesser', 'Sander Jansen', 'Stephan Pachnicke']",2023-04-01T10:26:16Z,Branch Identification in Passive Optical Networks using Machine Learning,  A machine learning approach for improving monitoring in passive optical networks with almost equidistant branches is proposed and experimentally validated. It achieves a high diagnostic accuracy of 98.7% and an event localization error of 0.5m ,2304.00285v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Stephan Pachnicke']",2022-03-19T09:20:40Z,Convolutional Neural Networks for Reflective Event Detection and   Characterization in Fiber Optical Links Given Noisy OTDR Signals,"  Fast and accurate fault detection and localization in fiber optic cables is extremely important to ensure the optical network survivability and reliability. Hence there exists a crucial need to develop an automatic and reliable algorithm for real time optical fiber fault detection and diagnosis leveraging the telemetry data obtained by an optical time domain reflectometry (OTDR) instrument. In this paper, we propose a novel data driven approach based on convolutional neural networks (CNNs) to detect and characterize the fiber reflective faults given noisy simulated OTDR data, whose SNR (signal-to-noise ratio) values vary from 0 dB to 30 dB, incorporating reflective event patterns. In our simulations, we achieved a higher detection capability with low false alarm rate and greater localization accuracy even for low SNR values compared to conventionally employed techniques. ",2203.14820v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Stephan Pachnicke']",2022-11-05T07:53:02Z,A Machine Learning-based Framework for Predictive Maintenance of   Semiconductor Laser for Optical Communication,"  Semiconductor lasers, one of the key components for optical communication systems, have been rapidly evolving to meet the requirements of next generation optical networks with respect to high speed, low power consumption, small form factor etc. However, these demands have brought severe challenges to the semiconductor laser reliability. Therefore, a great deal of attention has been devoted to improving it and thereby ensuring reliable transmission. In this paper, a predictive maintenance framework using machine learning techniques is proposed for real-time heath monitoring and prognosis of semiconductor laser and thus enhancing its reliability. The proposed approach is composed of three stages: i) real-time performance degradation prediction, ii) degradation detection, and iii) remaining useful life (RUL) prediction. First of all, an attention based gated recurrent unit (GRU) model is adopted for real-time prediction of performance degradation. Then, a convolutional autoencoder is used to detect the degradation or abnormal behavior of a laser, given the predicted degradation performance values. Once an abnormal state is detected, a RUL prediction model based on attention-based deep learning is utilized. Afterwards, the estimated RUL is input for decision making and maintenance planning. The proposed framework is validated using experimental data derived from accelerated aging tests conducted for semiconductor tunable lasers. The proposed approach achieves a very good degradation performance prediction capability with a small root mean square error (RMSE) of 0.01, a good anomaly detection accuracy of 94.24% and a better RUL estimation capability compared to the existing ML-based laser RUL prediction models. ",2211.02842v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Carsten Tropschug', 'Stephan Pachnicke']",2022-03-19T08:37:40Z,Optical Fiber Fault Detection and Localization in a Noisy OTDR Trace   Based on Denoising Convolutional Autoencoder and Bidirectional Long   Short-Term Memory,"  Optical time-domain reflectometry (OTDR) has been widely used for characterizing fiber optical links and for detecting and locating fiber faults. OTDR traces are prone to be distorted by different kinds of noise, causing blurring of the backscattered signals, and thereby leading to a misleading interpretation and a more cumbersome event detection task. To address this problem, a novel method combining a denoising convolutional autoencoder (DCAE) and a bidirectional long short-term memory (BiLSTM) is proposed, whereby the former is used for noise removal of OTDR signals and the latter for fault detection, localization, and diagnosis with the denoised signal as input. The proposed approach is applied to noisy OTDR signals of different levels of input SNR ranging from -5 dB to 15 dB. The experimental results demonstrate that: (i) the DCAE is efficient in denoising the OTDR traces and it outperforms other deep learning techniques and the conventional denoising methods; and (ii) the BiLSTM achieves a high detection and diagnostic accuracy of 96.7% with an improvement of 13.74% compared to the performance of the same model trained with noisy OTDR signals. ",2203.12604v1
"['Khouloud Abdelli', 'Carsten Tropschug', 'Helmut Griesser', 'Stephan Pachnicke']",2023-07-08T09:59:19Z,Fault Monitoring in Passive Optical Networks using Machine Learning   Techniques,"  Passive optical network (PON) systems are vulnerable to a variety of failures, including fiber cuts and optical network unit (ONU) transmitter/receiver failures. Any service interruption caused by a fiber cut can result in huge financial losses for service providers or operators. Identifying the faulty ONU becomes difficult in the case of nearly equidistant branch terminations because the reflections from the branches overlap, making it difficult to distinguish the faulty branch given the global backscattering signal. With increasing network size, the complexity of fault monitoring in PON systems increases, resulting in less reliable monitoring. To address these challenges, we propose in this paper various machine learning (ML) approaches for fault monitoring in PON systems, and we validate them using experimental optical time domain reflectometry (OTDR) data. ",2307.03945v1
"['Lars E. Kruse', 'Sebastian Kühl', 'Annika Dochhan', 'Stephan Pachnicke']",2023-12-12T12:21:08Z,Experimental Investigation of Machine Learning based Soft-Failure   Management using the Optical Spectrum,"  The demand for high-speed data is exponentially growing. To conquer this, optical networks underwent significant changes getting more complex and versatile. The increasing complexity necessitates the fault management to be more adaptive to enhance network assurance. In this paper, we experimentally compare the performance of soft-failure management of different machine learning algorithms. We further introduce a machine-learning based soft-failure management framework. It utilizes a variational autoencoder based generative adversarial network (VAE-GAN) running on optical spectral data obtained by optical spectrum analyzers. The framework is able to reliably run on a fraction of available training data as well as identifying unknown failure types. The investigations show, that the VAE-GAN outperforms the other machine learning algorithms when up to 10\% of the total training data is available in identification tasks. Furthermore, the advanced training mechanism for the GAN shows a high F1-score for unknown spectrum identification. The failure localization comparison shows the advantage of a low complexity neural network in combination with a VAE over established machine learning algorithms. ",2312.07208v1
"['Khouloud Abdelli', 'Helmut Griesser', 'Peter Ehrle', 'Carsten Tropschug', 'Stephan Pachnicke']",2022-03-19T08:45:45Z,Reflective Fiber Faults Detection and Characterization Using   Long-Short-Term Memory,"  To reduce operation-and-maintenance expenses (OPEX) and to ensure optical network survivability, optical network operators need to detect and diagnose faults in a timely manner and with high accuracy. With the rapid advancement of telemetry technology and data analysis techniques, data-driven approaches leveraging telemetry data to tackle the fault diagnosis problem have been gaining popularity due to their quick implementation and deployment. In this paper, we propose a novel multi-task learning model based on long short-term memory (LSTM) to detect, locate, and estimate the reflectance of fiber reflective faults (events) including the connectors and the mechanical splices by extracting insights from monitored data obtained by the optical time domain reflectometry (OTDR) principle commonly used for troubleshooting of fiber optic cables or links. The experimental results prove that the proposed method: (i) achieves a good detection capability and high localization accuracy within short measurement time even for low SNR values; and (ii) outperforms conventionally employed techniques. ",2204.07058v1
"['Md Sabbir-Bin Hossain', 'Georg Böcherer', 'Youxi Lin', 'Shuangxu Li', 'Stefano Calabrò', 'Andrei Nedelcu', 'Talha Rahman', 'Tom Wettlin', 'Jinlong Wei', 'Nebojša Stojanović', 'Changsong Xie', 'Maxim Kuschnerov', 'Stephan Pachnicke']",2022-06-14T19:58:28Z,Experimental Comparison of PAM-8 Probabilistic Shaping with Different   Gaussian Orders at 200 Gb/s Net Rate in IM/DD System with O-Band TOSA,"  For 200Gb/s net rates, cap probabilistic shaped PAM-8 with different Gaussian orders are experimentally compared against uniform PAM-8. In back-to-back and 5km measurements, cap-shaped 85-GBd PAM-8 with Gaussian order of 5 outperforms 71-GBd uniform PAM-8 by up to 2.90dB and 3.80dB in receiver sensitivity, respectively. ",2206.07142v1
"['Khouloud Abdelli', 'Carsten Tropschug', 'Helmut Griesser', 'Stephan Pachnicke']",2023-04-03T20:59:16Z,Faulty Branch Identification in Passive Optical Networks using Machine   Learning,"  Passive optical networks (PONs) have become a promising broadband access network solution. To ensure a reliable transmission, and to meet service level agreements, PON systems have to be monitored constantly in order to quickly identify and localize networks faults. Typically, a service disruption in a PON system is mainly due to fiber cuts and optical network unit (ONU) transmitter/receiver failures. When the ONUs are located at different distances from the optical line terminal (OLT), the faulty ONU or branch can be identified by analyzing the recorded optical time domain reflectometry (OTDR) traces. However, faulty branch isolation becomes very challenging when the reflections originating from two or more branches with similar length overlap, which makes it very hard to discriminate the faulty branches given the global backscattered signal. Recently, machine learning (ML) based approaches have shown great potential for managing optical faults in PON systems. Such techniques perform well when trained and tested with data derived from the same PON system. But their performance may severely degrade, if the PON system (adopted for the generation of the training data) has changed, e.g. by adding more branches or varying the length difference between two neighboring branches. etc. A re-training of the ML models has to be conducted for each network change, which can be time consuming. In this paper, to overcome the aforementioned issues, we propose a generic ML approach trained independently of the network architecture for identifying the faulty branch in PON systems given OTDR signals for the cases of branches with close lengths. Such an approach can be applied to an arbitrary PON system without requiring to be re-trained for each change of the network. The proposed approach is validated using experimental data derived from PON system. ",2304.01376v1
"['Tom Wettlin', 'Talha Rahman', 'Jinlong Wei', 'Stefano Calabrò', 'Nebojsa Stojanovic', 'Stephan Pachnicke']",2020-05-29T08:51:26Z,Complexity Reduction of Volterra Nonlinear Equalization for Optical   Short-Reach IM/DD Systems,"  We investigate approaches to reduce the computational complexity of Volterra nonlinear equalizers (VNLEs) for short-reach optical transmission systems using intensity modulation and direct detection (IM/DD). In this contribution we focus on a structural reduction of the number of kernels, i.e. we define rules to decide which terms need to be implemented and which can be neglected before the kernels are calculated. This static complexity reduction is to be distinguished from other approaches like pruning or L1 regularization, that are applied after the adaptation of the full Volterra equalizer e.g. by thresholding. We investigate the impact of the complexity reduction on 90 GBd PAM6 IM/DD experimental data acquired in a back-to-back setup as well as in case of transmission over 1 km SSMF. First, we show, that the third-order VNLE terms have a significant impact on the overall performance of the system and that a high number of coefficients is necessary for optimal performance. Afterwards, we show that restrictions, for example on the tap spacing among samples participating in the same kernel, can lead to an improved tradeoff between performance and complexity compared to a full third-order VNLE. We show an example, in which the number of third-order kernels is halved without any appreciable performance degradation. ",2005.14453v1
"['Irene Estébanez', 'Shi Li', 'Janek Schwind', 'Ingo Fischer', 'Stephan Pachnicke', 'Apostolos Argyris']",2021-05-17T16:14:06Z,56 GBaud PAM-4 100 km Transmission System with Photonic Processing   Schemes,"  Analog photonic computing has been proposed and tested in recent years as an alternative approach for data recovery in fiber transmission systems. Photonic reservoir computing, performing nonlinear transformations of the transmitted signals and exhibiting internal fading memory, has been found advantageous for this kind of processing. In this work, we show that the effectiveness of the internal fading memory depends significantly on the properties of the signal to be processed. Specifically, we demonstrate two experimental photonic post-processing schemes for a 56 GBaud PAM-4 experimental transmission system, with 100 km uncompensated standard single-mode fiber and direct detection. We show that, for transmission systems with significant chromatic dispersion, the contribution of a photonic reservoir's fading memory to the computational performance is limited. In a comparison between the data recovery performances between a reservoir computing and an extreme learning machine fiber-based configuration, we find that both offer equivalent data recovery. The extreme learning machine approach eliminates the necessity of external recurrent connectivity, which simplifies the system and increases the computation speed. Above 31 dB OSNR, the photonics-based equalization exhibits a lower BER than the respective offline DSP-based KK receiver. ",2105.07990v3
"['Khouloud Abdelli', 'Helmut Griesser', 'Christian Neumeyr', 'Robert Hohenleitner', 'Stephan Pachnicke']",2022-11-05T08:10:11Z,Degradation Prediction of Semiconductor Lasers using Conditional   Variational Autoencoder,"  Semiconductor lasers have been rapidly evolving to meet the demands of next-generation optical networks. This imposes much more stringent requirements on the laser reliability, which are dominated by degradation mechanisms (e.g., sudden degradation) limiting the semiconductor laser lifetime. Physics-based approaches are often used to characterize the degradation behavior analytically, yet explicit domain knowledge and accurate mathematical models are required. Building such models can be very challenging due to a lack of a full understanding of the complex physical processes inducing the degradation under various operating conditions. To overcome the aforementioned limitations, we propose a new data-driven approach, extracting useful insights from the operational monitored data to predict the degradation trend without requiring any specific knowledge or using any physical model. The proposed approach is based on an unsupervised technique, a conditional variational autoencoder, and validated using vertical-cavity surface-emitting laser (VCSEL) and tunable edge emitting laser reliability data. The experimental results confirm that our model (i) achieves a good degradation prediction and generalization performance by yielding an F1 score of 95.3%, (ii) outperforms several baseline ML based anomaly detection techniques, and (iii) helps to shorten the aging tests by early predicting the failed devices before the end of the test and thereby saving costs ",2211.02847v1
"['Md Sabbir-Bin Hossain', 'Georg Bocherer', 'Talha Rahman', 'Tom Wettlin', 'Nebojsa Stojanovic', 'Stefano Calabro', 'Stephan Pachnicke']",2023-03-30T09:26:08Z,Probabilistic Shaping for High-Speed Unamplified IM/DD Systems with an   O-Band EML,"  Probabilistic constellation shaping has been used in long-haul optically amplified coherent systems for its capability to approach the Shannon limit and realize fine rate granularity. The availability of high-bandwidth optical-electronic components and the previously mentioned advantages have invigorated researchers to explore probabilistic shaping (PS) in intensity-modulation and direct-detection (IM/DD) systems. This article presents an extensive comparison of uniform 8-ary pulse amplitude modulation (PAM) with PS PAM-8 using cap and cup Maxwell-Boltzmann (MB) distributions as well as MB distributions of different Gaussian orders. We report that in the presence of linear equalization, PS-PAM-8 outperforms uniform PAM-8 in terms of bit error ratio, achievable information rate and operational net bit rate indicating that cap-shaped PS-PAM-8 shows high tolerance against nonlinearities. In this paper, we have focused our investigations on O-band electro-absorption modulated laser unamplified IM/DD systems, which are operated close to the zero dispersion wavelength. ",2303.17248v1
"['Md Sabbir-Bin Hossain', 'Georg Boecherer', 'Talha Rahman', 'Nebojsa Stojanovic', 'Patrick Schulte', 'Stefano Calabrò', 'Jinlong Wei', 'Christian Bluemm', 'Tom Wettlin', 'Changsong Xie', 'Maxim Kuschnerov', 'Stephan Pachnicke']",2022-05-18T09:10:42Z,Experimental Comparison of Cap and Cup Probabilistically Shaped PAM for   O-Band IM/DD Transmission System,"  For 200Gbit/s net rates, uniform PAM-4, 6 and 8 are experimentally compared against probabilistic shaped PAM-8 cap and cup variants. In back-to-back and 20km measurements, cap shaped 80GBd PAM-8 outperforms 72GBd PAM-8 and 83GBd PAM-6 by up to 3.50dB and 0.8dB in receiver sensitivity, respectively ",2205.08805v1
"['Khouloud Abdelli', 'Joo Yeon Cho', 'Florian Azendorf', 'Helmut Griesser', 'Carsten Tropschug', 'Stephan Pachnicke']",2022-03-19T08:56:54Z,Machine Learning-based Anomaly Detection in Optical Fiber Monitoring,"  Secure and reliable data communication in optical networks is critical for high-speed Internet. However, optical fibers, serving as the data transmission medium providing connectivity to billons of users worldwide, are prone to a variety of anomalies resulting from hard failures (e.g., fiber cuts) and malicious physical attacks (e.g., optical eavesdropping (fiber tapping)) etc. Such anomalies may cause network disruption and thereby inducing huge financial and data losses, or compromise the confidentiality of optical networks by gaining unauthorized access to the carried data, or gradually degrade the network operations. Therefore, it is highly required to implement efficient anomaly detection, diagnosis, and localization schemes for enhancing the availability and reliability of optical networks. In this paper, we propose a data driven approach to accurately and quickly detect, diagnose, and localize fiber anomalies including fiber cuts, and optical eavesdropping attacks. The proposed method combines an autoencoder-based anomaly detection and an attention-based bidirectional gated recurrent unit algorithm, whereby the former is used for fault detection and the latter is adopted for fault diagnosis and localization once an anomaly is detected by the autoencoder. We verify the efficiency of our proposed approach by experiments under various anomaly scenarios using real operational data. The experimental results demonstrate that: (i) the autoencoder detects any fiber fault or anomaly with an F1 score of 96.86%; and (ii) the attention-based bidirectional gated recurrent unit algorithm identifies the the detected anomalies with an average accuracy of 98.2%, and localizes the faults with an average root mean square error of 0.19 m. ",2204.07059v1
"['Tom Birkoben', 'Hermann Kohlstedt']",2022-04-27T08:52:54Z,Matter & Mind Matter,"  As a result of a hundred million years of evolution, living animals have adapted extremely well to their ecological niche. Such adaptation implies species-specific interactions with their immediate environment by processing sensory cues and responding with appropriate behavior. Understanding how living creatures perform pattern recognition and cognitive tasks is of particular importance for computing architectures: by studying these information pathways refined over eons of evolution, researchers may be able to streamline the process of developing more highly advanced, energy efficient autonomous systems. With the advent of novel electronic and ionic components along with a deeper understanding of information pathways in living species, a plethora of opportunities to develop completely novel information processing avenues are within reach. Here, we describe the basal information pathways in nervous systems, from the local neuron level to the entire nervous system network. The dual importance of local learning rules is addressed, from spike timing dependent plasticity at the neuron level to the interwoven morphological and dynamical mechanisms of the global network. Basal biological principles are highlighted, including phylogenies, ontogenesis, and homeostasis, with particular emphasis on network topology and dynamics. While in machine learning system training is performed on virgin networks without any a priori knowledge, the approach proposed here distinguishes itself unambiguously by employing growth mechanisms as a guideline to design novel computing architectures. Including fundamental biological information pathways that explore the spatiotemporal fundamentals of nervous systems has untapped potential for the development of entirely novel information processing systems. Finally, a benchmark for neuromorphic systems is suggested. ",2204.12774v1
"['Rene Meyer', 'Hermann Kohlstedt']",2003-12-23T15:57:15Z,1-D Simulation of the Electron Density Distribution in a Novel   Nonvolatile Resistive Random Access Memory Device,"  The operation of a novel nonvolatile memory device based on a conductive ferroelectric/non-ferroelectric thin film multilayer stack is simulated numerically. The simulation involves the self-consistent steady state solution of Poisson's equation and the transport equation for electrons assuming a Drift-Diffusion transport mechanism. Special emphasis is put on the screening of the spontaneous polarization by conduction electrons as a function of the applied voltage. Depending on the orientation of the polarization in the ferroelectric layer, a high and a low resistive state are found giving rise to a hysteretic I-V characteristic. The R_high to R_low ratio ranging from > 50% to several orders of magnitude is calculated as a function of the dopant content. ",cond-mat/0312609v1
"['Doo Seok Jeong', 'Byung-ki Cheong', 'Hermann Kohlstedt']",2011-02-23T12:10:10Z,Pt/Ti/Al2O3/Al tunnel junctions showing electroforming-free bipolar   resistive switching behavior,"  We investigated electroforming-free bipolar resistive switching behavior in Pt/Ti/Al2O3/Al tunnel junctions where the Al2O3 tunnel barrier was naturally formed on Al in air. Various compliance current values for the junction's set switching successfully lead to various resistance values in its low resistance state, suggesting the possibility for multi-level-operation. A mechanism for the bipolar switching is qualitatively discussed in terms of the modulation of the tunnel barrier by the reactive Ti layer on top of the barrier. ",1102.4720v1
"['Sven Dirkmann', 'Mirko Hansen', 'Martin Ziegler', 'Hermann Kohlstedt', 'Thomas Mussenbrock']",2016-05-27T09:59:37Z,The role of ion transport phenomena in memristive double barrier devices,"  In this work we report on the role of ion transport for the dynamic behavior of a double barrier quantum mechanical Al/Al$_2$O$_3$/Nb$_{\text{x}}$O$_{\text{y}}$/Au memristive device based on numerical simulations in conjunction with experimental measurements. The device consists of an ultra-thin Nb$_{\text{x}}$O$_{\text{y}}$ solid state electrolyte between an Al$_2$O$_3$ tunnel barrier and a semiconductor metal interface at an Au electrode. It is shown that the device provides a number of interesting features for potential applications such as an intrinsic current compliance, a relatively long retention time, and no need for an initialization step. Therefore, it is particularly attractive for applications in highly dense random access memories or neuromorphic mixed signal circuits. However, the underlying physical mechanisms of the resistive switching are still not completely understood yet. To investigate the interplay between the current transport mechanisms and the inner atomistic device structure a lumped element circuit model is consistently coupled with 3D kinetic Monte Carlo model for the ion transport. The simulation results indicate that the drift of charged point defects within the Nb$_{\text{x}}$O$_{\text{y}}$ is the key factor for the resistive switching behavior. It is shown in detail that the diffusion of oxygen modifies the local electronic interface states resulting in a change of the interface properties of the double barrier device. ",1605.08564v2
"['Sven Dirkmann', 'Martin Ziegler', 'Mirko Hansen', 'Hermann Kohlstedt', 'Jan Trieschmann', 'Thomas Mussenbrock']",2015-09-01T10:03:56Z,Kinetic Simulation of Filament Growth Dynamics in Memristive   Electrochemical Metallization Devices,"  In this work we report on kinetic Monte-Carlo calculations of resistive switching and the underlying growth dynamics of filaments in an electrochemical metallization device consisting of an Ag/TiO2/Pt sandwich-like thin film system. The developed model is not limited to i) fast time scale dynamics and ii) only one growth and dissolution cycle of metallic filaments. In particular, we present results from the simulation of consecutive cycles. We find that the numerical results are in excellent agreement with experimentally obtained data. Additionally, we observe an unexpected filament growth mode which is in contradiction to the widely acknowledged picture of filament growth, but consistent with recent experimental findings. ",1509.00208v2
"['Tom Birkoben', 'Moritz Drangmeister', 'Finn Zahari', 'Serhiy Yanchuk', 'Philipp Hövel', 'Hermann Kohlstedt']",2019-06-06T07:07:30Z,Bifurcation without parameters in a chaotic system with a memristive   element,"  We investigate the effect of memory on a chaotic system experimentally and theoretically. For this purpose, we use Chua's oscillator as an electrical model system showing chaotic dynamics extended by a memory element in form of a double-barrier memristive device. The device consists of Au/NbO$_\text{x}$/Al$_\text{2}$O$_\text{3}$/Al/Nb layers and exhibits strong analog-type resistive changes depending on the history of the charge flow. In the extended system strong changes in the dynamics of chaotic oscillations are observable. The otherwise fluctuating amplitudes of the Chua system are disrupted by transient silent states. After developing a model for Chua's oscillator with a memristive device, the numerical treatment reveals the underling dynamics as driven by the slow-fast dynamics of the memory element. Furthermore, the stabilizing and destabilizing dynamic bifurcations are identified that are passed by the system during its chaotic behavior. ",1906.02445v1
"['Sahitya Yarragolla', 'Torben Hemke', 'Jan Trieschmann', 'Finn Zahari', 'Hermann Kohlstedt', 'Thomas Mussenbrock']",2021-10-31T20:53:11Z,Stochastic behaviour of an interface-based memristive device,"  A large number of simulation models have been proposed over the years to mimic the electrical behaviour of memristive devices. The models are based either on sophisticated mathematical formulations that do not account for physical and chemical processes responsible for the actual switching dynamics or on multi-physical spatially resolved approaches that include the inherent stochastic behaviour of real-world memristive devices but are computationally very expensive. In contrast to the available models, we present a computationally inexpensive and robust spatially 1D model for simulating interface-type memristive devices. The model efficiently incorporates the stochastic behaviour observed in experiments and can be easily transferred to circuit simulation frameworks. The ion transport, responsible for the resistive switching behaviour, is modelled using the kinetic Cloud-In-a-Cell scheme. The calculated current-voltage characteristics obtained using the proposed model show excellent agreement with the experimental findings. ",2111.00591v1
"['Rouven Lamprecht', 'Luca Vialetto', 'Tobias Gergs', 'Finn Zahari', 'Richard Marquardt', 'Jan Trieschmann', 'Hermann Kohlstedt']",2024-06-27T08:37:47Z,Wedge-type engineered analog   SiO$_\mathrm{x}$/Cu/SiO$_\mathrm{x}$-Memristive Devices for Neuromorphic   Applications,"  This study presents a comprehensive examination of the development of TiN/SiO$_\mathrm{x}$/Cu/SiO$_\mathrm{x}$/TiN memristive devices, engineered for neuromorphic applications using a wedge-type deposition technique and Monte Carlo simulations. Identifying critical parameters for the desired device characteristics can be challenging with conventional trial-and-error approaches, which often obscure the effects of varying layer compositions. By employing an \textit{off-center} thermal evaporation method, we created a thickness gradient of SiO$_\mathrm{x}$ and Cu on a 4-inch wafer, facilitating detailed resistance map analysis through semiautomatic measurements. This allows to investigate in detail the influence of layer composition and thickness on single wafers, thus keeping every other process condition constant. Combining experimental data with simulations provides a precise understanding of the layer thickness distribution and its impact on device performance. Optimizing the SiO$_\mathrm{x}$ layers to be below 12.5 nm, coupled with a discontinuous Cu layer with a nominal thickness lower than 0.6 nm, exhibits analog switching properties with an R$_\mathrm{on}$/R$_\mathrm{off}$ ratio of $>$100, suitable for neuromorphic applications, whereas R $\times$ A analysis shows no clear signs of filamentary switching. Our findings highlight the significant role of carefully choosing the SiO$_\mathrm{x}$ and Cu thickness in determining the switching behavior and provide insights that could lead to the more systematic development of high-performance analog switching components for bio-inspired computing systems. ",2406.18998v1
"['Karlheinz Ochs', 'Martin Ziegler', 'Eloy Hernandez-Guevara', 'Enver Solan', 'Marina Ignatov', 'Mirko Hansen', 'Mahal Singh Gill', 'Hermann Kohlstedt']",2017-04-24T09:15:20Z,Anticipation of digital patterns,"  A memristive device is a novel passive device, which is essentially a resistor with memory. This device can be utilized for novel technical applications like neuromorphic computation. In this paper, we focus on anticipation - a capability of a system to decide how to react in an environment by predicting future states. Especially, we have designed an elementary memristive circuit for the anticipation of digital patterns, where this circuit is based on the capability of an amoeba to anticipate periodically occurring unipolar pulses. The resulting circuit has been verified by digital simulations and has been realized in hardware as well. For the practical realization, we have used an Ag-doped TiO2-x-based memristive device, which has been fabricated in planar capacitor structures on a silicon wafer. The functionality of the circuit is shown by simulations and measurements. Finally, the anticipation of information is demonstrated by using images, where the robustness of this anticipatory circuit against noise and faulty intermediate information is visualized. ",1704.07102v2
"['Simon Fichtner', 'Georg Schönweger', 'Tom-Niklas Kreutzer', 'Fabian Lofink', 'Adrian Petraru', 'Hermann Kohlstedt', 'Bernhard Wagner']",2020-10-12T13:35:05Z,"Ferroelectricity in AlScN: Switching, Imprint and sub-150 nm Films","  The discovery of ferroelectricity in AlScN allowed the first clear observation of the effect in the wurtzite crystal structure, resulting in a material with a previously unprecedented combination of very large coercive fields (2-5 MV/cm) and remnant polarizations (70-110 ${\mu}$C/cm$^2$). We obtained initial insight into the switching dynamics of AlScN, which suggests a domain wall motion limited process progressing from the electrode interfaces. Further, imprint was generally observed in AlScN films and can tentatively be traced to the alignment of charged defects with the internal and external polarization and field, respectively. Potentially crucial from the application point of view, ferroelectricity could be observed in films with thicknesses below 30 nm - as the coercive fields of AlScN were found to be largely independent of thickness between 600 nm and 27 nm. ",2010.05705v1
"['Enver Solan', 'Sven Dirkmann', 'Mirko Hansen', 'Dietmar Schroeder', 'Hermann Kohlstedt', 'Martin Ziegler', 'Thomas Mussenbrock', 'Karlheinz Ochs']",2017-01-19T09:29:11Z,An Enhanced Lumped Element Electrical Model of a Double Barrier   Memristive Device,"  The massive parallel approach of neuromorphic circuits leads to effective methods for solving complex problems. It has turned out that resistive switching devices with a continuous resistance range are potential candidates for such applications. These devices are memristive systems - nonlinear resistors with memory. They are fabricated in nanotechnology and hence parameter spread during fabrication may aggravate reproducible analyses. This issue makes simulation models of memristive devices worthwhile.   Kinetic Monte-Carlo simulations based on a distributed model of the device can be used to understand the underlying physical and chemical phenomena. However, such simulations are very time-consuming and neither convenient for investigations of whole circuits nor for real-time applications, e.g. emulation purposes. Instead, a concentrated model of the device can be used for both fast simulations and real-time applications, respectively. We introduce an enhanced electrical model of a valence change mechanism (VCM) based double barrier memristive device (DBMD) with a continuous resistance range. This device consists of an ultra-thin memristive layer sandwiched between a tunnel barrier and a Schottky-contact. The introduced model leads to very fast simulations by using usual circuit simulation tools while maintaining physically meaningful parameters.   Kinetic Monte-Carlo simulations based on a distributed model and experimental data have been utilized as references to verify the concentrated model. ",1701.08068v1
"['Georg Schönweger', 'Md Redwanul Islam', 'Niklas Wolff', 'Adrian Petraru', 'Lorenz Kienle', 'Hermann Kohlstedt', 'Simon Fichtner']",2022-07-05T07:50:15Z,Ultrathin AlScN for low-voltage driven ferroelectric-based devices,"  Thickness scaling of ferroelectricity in AlScN is a determining factor for its potential application in neuromorphic computing and memory devices. In this letter, we report on ultrathin (10 nm) Al0.72Sc0.28N films that are ferroelectrically switchable at room temperature. All-epitaxial Al0.72Sc0.28N/Pt heterostructures are grown by magnetron sputtering onto GaN/sapphire substrates followed by an in situ Pt capping approach to avoid oxidation of the Al0.72Sc0.28N film surface. Structural characterization by X-ray diffraction and transmission electron microscopy reveals the established epitaxy. The thus obtained high-quality interfaces in combination with the in situ capping is expected to facilitate ferroelectric switching of AlScN in the ultrathin regime. The analysis of the relative permittivity and coercive field dependence on the Al0.72Sc0.28N film thicknesses in the range of 100 nm down to 10 nm indicates only moderate scaling effects, suggesting that the critical thickness for ferroelectricity is not yet approached. Furthermore, the deposited layer stack demonstrates the possibility of including ultrathin ferroelectric AlScN into all-epitaxial GaN-based devices using sputter deposition techniques. Thus, our work highlights the integration and scaling potential of all-epitaxial ultrathin AlScN offering high storage density paired with low voltage operation desired for state of the art ferroelectric memory devices. ",2207.01858v1
"['Georg Schönweger', 'Niklas Wolff', 'Md Redwanul Islam', 'Maike Gremmel', 'Adrian Petraru', 'Lorenz Kienle', 'Hermann Kohlstedt', 'Simon Fichtner']",2023-04-06T07:45:08Z,In-Grain Ferroelectric Switching in Sub-5 nm Thin AlScN Films at 1 V,"  Analog switching in ferroelectric devices promises neuromorphic computing with highest energy efficiency, if limited device scalability can be overcome. To contribute to a solution, we report on the ferroelectric switching characteristics of sub-5 nm thin Al$_{0.74}$Sc$_{0.26}$N films grown on Pt/Ti/SiO2/Si and epitaxial Pt/GaN/sapphire templates by sputter-deposition. In this context, we focus on the following major achievements compared to previously available wurtzite-type ferroelectrics: 1) Record low switching voltages down to 1 V are achieved, which is in a range that can be supplied by standard on-chip voltage sources. 2) Compared to the previously investigated deposition of thinnest Al$_{1-x}$Sc$_x$N films on epitaxial templates, a significantly larger coercive field to breakdown field ratio is observed for Al$_{0.74}$Sc$_{0.26}$N films grown on silicon substrates, the technologically most relevant substrate-type. 3) The formation of true ferroelectric domains in wurtzite-type materials is for the first time demonstrated on the atomic scale by scanning transmission electron microscopy investigations of a sub-5 nm thin partially switched film. The direct observation of inversion domain boundaries within single nm-sized grains supports the theory of a gradual domain-wall motion limited switching process in wurtzite-type ferroelectrics. Ultimately, this should enable the analog switching necessary for mimicking neuromorphic concepts also in highly scaled devices. ",2304.02909v1
"['Julian Strobel', 'Mirko Hansen', 'Sven Dirkmann', 'Krishna Kanth Neelisetty', 'Martin Ziegler', 'Georg Haberfehlner', 'Radian Popescu', 'Gerald Kothleitner', 'Venkata Sai Kiran Chakravadhanula', 'Christian Kübel', 'Hermann Kohlstedt', 'Thomas Mussenbrock', 'Lorenz Kienle']",2017-03-20T13:46:44Z,In depth nano spectroscopic analysis on homogeneously switching double   barrier memristive devices,"  Memristors based on a double barrier design have been analysed by various nano spectroscopic methods to unveil details about its microstructure and conduction mechanism. The device consists of an AlOx tunnel barrier and a NbOy/Au Schottky barrier sandwiched between Nb bottom electrode and Au top electrode. As it was anticipated that the local chemical composition of the tunnel barrier, i.e. oxidation state of the metals as well as concentration and distribution of oxygen ions, have a major influence on electronic conduction, these factors were carefully analysed. A combined approach was chosen in order to reliably investigate electronic states of Nb and O by electron energy-loss spectroscopy as well as map elements whose transition edges exhibit a different energy range by energy-dispersive X-ray spectroscopy like Au and Al. The results conclusively demonstrate significant oxidation of the bottom electrode as well as a small oxygen vacancy concentration in the Al oxide tunnel barrier. Possible scenarios to explain this unexpected additional oxide layer are discussed and kinetic Monte Carlo simulations were applied in order to identify its influence on conduction mechanisms in the device. In light of the strong deviations between observed and originally sought layout, this study highlights the robustness in terms of structural deviations of the double barrier memristor device. ",1703.06741v1
"['Niklas Wolff', 'Georg Schoenweger', 'Isabel Streicher', 'Md Redwanul Islam', 'Nils Braun', 'Patrik Stranak', 'Lutz Kirste', 'Mario Prescher', 'Andriy Lotnyk', 'Hermann Kohlstedt', 'Stefano Leone', 'Lorenz Kienle', 'Simon Fichtner']",2023-12-21T11:37:03Z,Demonstration and STEM Analysis of Ferroelectric Switching in   MOCVD-Grown Single Crystalline Al$_{0.85}$Sc$_{0.15}$N,"  Wurtzite-type Al$_{1-x}$Sc$_x$N solid solutions grown by metal organic chemical vapour deposition are for the first time confirmed to be ferroelectric. The film with 230 nm thickness and x = 0.15 exhibits a coercive field of 5.5 MV/cm at a measurement frequency of 1.5 kHz. Single crystal quality and homogeneous chemical composition of the film was confirmed by X-ray diffraction spectroscopic methods such as time of flight secondary ion mass spectrometry. Annular bright field scanning transmission electron microscopy served to proof the ferroelectric polarization inversion on unit cell level. The single crystal quality further allowed to image the large-scale domain pattern of a wurtzite-type ferroelectric for the first time, revealing a predominantly cone-like domain shape along the c-axis of the material. As in previous work, this again implies the presence of strong polarization discontinuities along this crystallographic axis, which could be suitable for current transport. The domains are separated by narrow domain walls, for which an upper thickness limit of 3 nm was deduced, but which could potentially be atomically sharp. We are confident that these results will advance the commencing integration of wurtzite-type ferroelectrics to GaN as well as generally III-N based heterostructures and devices. ",2312.13759v1
"['MD Redwanul Islam', 'Niklas Wolff', 'Mohammed Yassine', 'Georg Schönweger', 'Björn Christian', 'Hermann Kohlstedt', 'Oliver Ambacher', 'Fabian Lofink', 'Lorenz Kienle', 'Simon Fichtner']",2021-05-18T07:53:35Z,On the exceptional temperature stability of ferroelectric AlScN thin   films,"  Through its dependence on low symmetry crystal phases, ferroelectricity is inherently a property tied to the lower temperature ranges of the phase diagram for a given material. This paper presents conclusive evidence that in the case of ferroelectric AlScN, low temperature has to be seen as a purely relative term, since its ferroelectric-to-paraelectric transition temperature is confirmed to surpass 1100{\deg}C and thus the transition temperature of virtually any other thin film ferroelectric. We arrived at this conclusion through investigating the structural stability of 0.4 - 2 ${\mu}$m thick Al$_{0.73}$Sc$_{0.27}$N films grown on Mo bottom electrodes via in situ high-temperature X-ray diffraction and permittivity measurements. Our studies reveal the wurtzite-type structure of Al$_{0.73}$Sc$_{0.27}$N is conserved during the entire 1100{\deg}C annealing cycle, apparent through a constant c over a lattice parameter ratio. In situ permittivity measurements performed up to 1000{\deg}C strongly support this conclusion and include what could be the onset of a diverging permittivity only at the very upper end of the measurement interval. Our in situ measurements are well-supported by ex situ (scanning) transmission electron microscopy and polarization and capacity hysteresis measurements. These results confirm the structural stability on the sub-${\mu}$m scale next to the stability of the inscribed polarization during the complete 1100{\deg}C annealing treatment. Thus, AlScN is the first readily available thin film ferroelectric with a temperature stability that surpasses virtually all thermal budgets occurring in microtechnology, be it during fabrication or the lifetime of a device - even in harshest environments. ",2105.08331v1
"['Jan Trieschmann', 'Thomas Mussenbrock']",2015-05-12T06:18:37Z,Transport of Sputtered Particles in Capacitive Sputter Sources,"  The transport of sputtered aluminum inside a multi frequency capacitively coupled plasma chamber is simulated by means of a kinetic test multi-particle approach. A novel consistent set of scattering parameters obtained for a modified variable hard sphere collision model is presented for both argon and aluminum. An angular dependent Thompson energy distribution is fitted to results from Monte Carlo simulations and used for the kinetic simulation of the transport of sputtered aluminum. For the proposed configuration the transport of sputtered particles is characterized under typical process conditions at a gas pressure of p=0.5 Pa. It is found that -- due to the peculiar geometric conditions -- the transport can be understood in a one dimensional picture, governed by the interaction of the imposed and backscattered particle fluxes. It is shown that the precise geometric features play an important role only in proximity to the electrode edges, where the effect of backscattering from the outside chamber volume becomes the governing mechanism. ",1505.02883v2
"['Jan Trieschmann', 'Thomas Mussenbrock']",2017-09-17T15:26:25Z,Kinetic Bandgap Analysis of Plasma Photonic Crystals,"  The dispersion relation of plasma and plasma-dielectric photonic multilayer structures is approached in terms of a one-dimensional Particle-in-Cell simulation. For several plasma-dielectric configurations, the system response is obtained using a pulsed excitation and a subsequent two-dimensional frequency analysis. It is first shown that the dispersion relation of a single, homogeneous plasma slab is well described by the cold-plasma model even at low pressures of 1 Pa. The study is extended to the simulation of plasma photonic crystals with a variety of configurations, based on the work of Hojo and Mase [J. Plasma Fusion Res. 80, 89 (2004)]. Considering a one-dimensional plasma photonic crystal made from alternating layers of dielectric and homogeneous plasma slabs, it is shown that the assumption of a cold-plasma description is well justified also in this case. Moreover, in this work the results are reformatted and analyzed in a band diagram representation, in particular based on the lattice constant $a$. Based on these considerations a scaling invariant representation is presented, utilizing a generalized set of parameters. The study is completed with an exemplary comparison of three plasma-dielectric photonic crystal configurations and their corresponding band diagrams. ",1709.05679v2
"['Tobias Gergs', 'Thomas Mussenbrock', 'Jan Trieschmann']",2022-08-24T15:19:28Z,Charge-optimized many-body interaction potential for AlN revisited to   explore plasma-surface interactions,"  Plasma-surface interactions during AlN thin film sputter deposition could be studied by means of reactive molecular dynamics (RMD) methods. This requires an interaction potential that describes all species as well as wall interactions (e.g., particle emission, damage formation) appropriately. However, previous works focused on the establishment of AlN bulk potentials. Although for the third-generation charge-optimized many-body (COMB3) potential at least a single reference surface was taken into account, surface interactions are subject to limited reliability only. The demand for a revised COMB3 AlN potential is met in two steps: First, the Ziegler-Biersack-Littmark potential is tapered and the variable charge model QTE$^+$ is implemented to account for high-energy collisions and distant charge transport, respectively. Second, the underlying parameterization is reworked by applying a self-adaptive evolution strategy implemented in the GARFfield software. Four wurtzite, three zinc blende and three rock salt surfaces are considered. An example study on the ion bombardment induced particle emission and point defect formation reveals that the revised COMB3 AlN potential is appropriate for the accurate investigation of plasma-surface interactions by means of RMD simulations. ",2208.11605v1
"['Jan Trieschmann', 'Luca Vialetto', 'Tobias Gergs']",2023-06-30T20:48:35Z,Machine learning for advancing low-temperature plasma modeling and   simulation,"  Machine learning has had an enormous impact in many scientific disciplines. Also in the field of low-temperature plasma modeling and simulation it has attracted significant interest within the past years. Whereas its application should be carefully assessed in general, many aspects of plasma modeling and simulation have benefited substantially from recent developments within the field of machine learning and data-driven modeling. In this survey, we approach two main objectives: (a) We review the state-of-the-art focusing on approaches to low-temperature plasma modeling and simulation. By dividing our survey into plasma physics, plasma chemistry, plasma-surface interactions, and plasma process control, we aim to extensively discuss relevant examples from literature. (b) We provide a perspective of potential advances to plasma science and technology. We specifically elaborate on advances possibly enabled by adaptation from other scientific disciplines. We argue that not only the known unknowns, but also unknown unknowns may be discovered due to the inherent propensity of data-driven methods to spotlight hidden patterns in data. ",2307.00131v2
"['Torben Hemke', 'Robin Struck', 'Sahitya Yarragolla', 'Tobias Gergs', 'Jan Trieschmann', 'Thomas Mussenbrock']",2024-06-26T18:11:10Z,Physics-based Modeling and Simulation of Nanoparticle Networks,"  This study presents the computational modeling and simulation of silver nanoparticle networks (NPNs), which, in the realm of neuromorphic computation, suggest to be a promising candidate for nontraditional computation methods. The modeling of the networks construction, its electrical properties and model parameters are derived from well-established physical principles. ",2406.18666v1
"['Jan Trieschmann', 'Thomas Mussenbrock']",2016-08-07T16:04:05Z,Kinetic analysis of negative power deposition in low pressure plasmas,"  The negative power absorption in low pressure plasmas is investigated by means of an analyical model which couples Boltzmann's equation and the quasi-stationary Maxwell's equation. Exploiting standard Hilbert space methods an explicit solution for both, the electric field and the distribution function of the electrons for a bounded discharge configuration subject to an unsymmetrical excitation has been found for the first time. The model is applied to a low pressure inductively coupled plasma discharge. In this context particularly the anomalous skin effect and the effect of phase mixing is discussed. The analytical solution is compared with results from electromagnetic full wave particle in cell simulations. Excellent agreement between the analytical and the numerical results is found. ",1608.02233v1
"['Florian Krüger', 'Tobias Gergs', 'Jan Trieschmann']",2018-10-10T13:23:36Z,Machine learning plasma-surface interface for coupling sputtering and   gas-phase transport simulations,"  Thin film processing by means of sputter deposition inherently depends on the interaction of energetic particles with a target surface and the subsequent particle transport. The length and time scales of the underlying physical phenomena span orders of magnitudes. A theoretical description which bridges all time and length scales is not practically possible. Advantage can be taken particularly from the well-separated time scales of the fundamental surface and plasma processes. Initially, surface properties may be calculated from a surface model and stored for a number of representative cases. Subsequently, the surface data may be provided to gas-phase transport simulations via appropriate model interfaces (e.g., analytic expressions or look-up tables) and utilized to define insertion boundary conditions. During run-time evaluation, however, the maintained surface data may prove to be not sufficient. In this case, missing data may be obtained by interpolation (common), extrapolation (inaccurate), or be supplied on-demand by the surface model (computationally inefficient). In this work, a potential alternative is established based on machine learning techniques using artificial neural networks. As a proof of concept, a multilayer perceptron network is trained and verified with sputtered particle distributions obtained from transport of ions in matter based simulations for Ar projectiles bombarding a Ti-Al composite. It is demonstrated that the trained network is able to predict the sputtered particle distributions for unknown, arbitrarily shaped incident ion energy distributions. It is consequently argued that the trained network may be readily used as a machine learning based model interface (e.g., by quasi-continuously sampling the desired sputtered particle distributions from the network), which is sufficiently accurate also in scenarios which have not been previously trained. ",1810.04510v1
"['Tobias Gergs', 'Borislav Borislavov', 'Jan Trieschmann']",2021-09-03T09:51:32Z,An efficient plasma-surface interaction surrogate model for sputtering   processes based on autoencoder neural networks,"  Simulations of thin film sputter deposition require the separation of the plasma and material transport in the gas-phase from the growth/sputtering processes at the bounding surfaces. Interface models based on analytic expressions or look-up tables inherently restrict this complex interaction to a bare minimum. A machine learning model has recently been shown to overcome this remedy for Ar ions bombarding a Ti-Al composite target. However, the chosen network structure (i.e., a multilayer perceptron) provides approximately 4 million degrees of freedom, which bears the risk of overfitting the relevant dynamics and complicating the model to an unreliable extend. This work proposes a conceptually more sophisticated but parameterwise simplified regression artificial neural network for an extended scenario, considering a variable instead of a single fixed Ti-Al stoichiometry. A convolutional $\beta$-variational autoencoder is trained to reduce the high-dimensional energy-angular distribution of sputtered particles to a latent space representation of only two components. In addition to a primary decoder which is trained to reconstruct the input energy-angular distribution, a secondary decoder is employed to reconstruct the mean energy of incident Ar ions as well as the present Ti-Al composition. The mutual latent space is hence conditioned on these quantities. The trained primary decoder of the variational autoencoder network is subsequently transferred to a regression network, for which only the mapping to the particular latent space has to be learned. While obtaining a competitive performance, the number of degrees of freedom is drastically reduced to 15,111 and 486 parameters for the primary decoder and the remaining regression network, respectively. The underlying methodology is general and can easily be extended to more complex physical descriptions with a minimal amount of data required. ",2109.01406v2
"['Torben Hemke', 'Jan Trieschmann', 'Alexander Wollny', 'Ralf Peter Brinkmann', 'Thomas Mussenbrock']",2011-05-23T14:14:45Z,Numerical study of secondary electron emission in a coaxial   radio-frequency driven plasma jet at atmospheric pressure,  In this work we investigate a numerical model of a coaxial RF-driven plasma jet operated at atmospheric pressure. Due to the cylindrical symmetry an adequate 2-D representation of the otherwise 3-dimensional structure is used. A helium-oxygen chemistry reaction scheme is applied. We study the effect of secondary electrons emitted at the inner electrode as well as the inserted dielectric tube and discuss their impact on the discharge behavior. We conclude that a proper choice of materials can improve the desired mode of operation of such plasma jets in terms of materials and surface processing. ,1105.4509v1
"['Jan Trieschmann', 'Frederik Schmidt', 'Thomas Mussenbrock']",2016-07-14T10:24:08Z,Particle-in-Cell/Test-Particle Simulations of Technological Plasmas:   Sputtering Transport in Capacitive Radio Frequency Discharges,"  The paper provides a tutorial to the conceptual layout of a self-consistently coupled Particle-In-Cell/Test-Particle model for the kinetic simulation of sputtering transport in capacitively coupled plasmas at low gas pressures. It explains when a kinetic approach is actually needed and which numerical concepts allow for the inherent nonequilibrium behavior of the charged and neutral particles. At the example of a generic sputtering discharge both the fundamentals of the applied Monte Carlo methods as well as the conceptual details in the context of the sputtering scenario are elaborated on. Finally, two in the context of sputtering transport simulations often exploited assumptions, namely on the energy distribution of impinging ions as well as on the test particle approach, are validated for the proposed example discharge. ",1607.04069v1
"['Frederik Schmidt', 'Jan Trieschmann', 'Tobias Gergs', 'Thomas Mussenbrock']",2019-02-06T09:10:37Z,A generic method for equipping arbitrary rf discharge simulation   frameworks with external lumped element circuits,"  External electric circuits attached to radio-frequency plasma discharges are essential for the power transfer into the discharge and are, therefore, a key element for plasma operation. Many plasma simulations, however, simplify or even neglect the external network. This is because a solution of the circuit's auxiliary differential equations following Kirchhoff's laws is required, which can become a tedious task especially for large circuits. This work proposes a method, which allows to include electric circuits in any desired radio-frequency plasma simulation. Conceptually, arbitrarily complex external networks may be incorporated in the form of a simple netlist. The suggested approach is based on the harmonic balance concept, which splits the whole system into the nonlinear plasma and the linear circuit contribution. A mathematical formulation of the influence of the applied voltage on the current for each specific harmonic is required and proposed. It is demonstrated that this method is applicable for both simple global plasma models as well as more complex spatially resolved Particle-in-Cell simulations. ",1902.02074v1
"['Tobias Gergs', 'Thomas Mussenbrock', 'Jan Trieschmann']",2022-11-09T10:44:03Z,Physics-separating artificial neural networks for predicting initial   stages of Al sputtering and thin film deposition in Ar plasma discharges,"  Simulations of Al thin film sputter depositions rely on accurate plasma and surface interaction models. Establishing the latter commonly requires a higher level of abstraction and means to dismiss the fundamental atomic fidelity. Previous works on sputtering processes addressed this issue by establishing machine learning surrogate models, which include a basic surface state (i.e., stoichiometry) as static input. In this work, an evolving surface state and defect structure are introduced to jointly describe sputtering and growth with physics-separating artificial neural networks. The data describing the plasma-surface interactions stem from hybrid reactive molecular dynamics/time-stamped force bias Monte Carlo simulations of Al neutrals and Ar$^+$ ions impinging onto Al(001) surfaces. It is demonstrated that the fundamental processes are comprehensively described by taking the surface state as well as defect structure into account. Hence, a machine learning plasma-surface interaction surrogate model is established that resolves the inherent kinetics with high physical fidelity. The resulting model is not restricted to input from modeling and simulation, but may similarly be applied to experimental input data. ",2211.04796v1
"['Tobias Gergs', 'Thomas Mussenbrock', 'Jan Trieschmann']",2023-01-09T17:20:32Z,Physics-separating artificial neural networks for predicting sputtering   and thin film deposition of AlN in Ar/N$_2$ discharges on experimental   timescales,"  Understanding and modeling plasma-surface interactions frame a multi-scale as well as multi-physics problem. Scale-bridging machine learning surface surrogate models have been demonstrated to perceive the fundamental atomic fidelity for the physical vapor deposition of pure metals. However, the immense computational cost of the data-generating simulations render a practical application with predictions on relevant timescales impracticable. This issue is resolved in this work for the sputter deposition of AlN in Ar/N$_2$ discharges by developing a scheme that populates the parameter spaces effectively. Hybrid reactive molecular dynamics / time-stamped force-bias Monte Carlo simulations of randomized plasma-surface interactions / diffusion processes are used to setup a physics-separating artificial neural network. The application of this generic machine learning model to a specific experimental reference case study enables the systematic analysis of the particle flux emission as well as underlying system state (e.g., composition, mass density, stress, point defect structure) evolution within process times of up to 45 minutes. ",2301.03524v1
"['Tobias Gergs', 'Frederik Schmidt', 'Thomas Mussenbrock', 'Jan Trieschmann']",2020-06-07T14:15:51Z,Generalized Method for Charge Transfer Equilibration in Reactive   Molecular Dynamics,"  Variable charge models (e.g., EEM, QEq, ES+) in reactive molecular dynamics simulations often inherently impose a global charge transfer between atoms (approximating each system as ideal metal). Consequently, most surface processes (e.g., adsorption, desorption, deposition, sputtering) are affected, potentially causing dubious dynamics. This issue is meant to be addressed by the ACKS2 and QTPIE model, which are based on the Kohn-Sham density functional theory as well as a charge transfer restricting extension to the QEq model (approximating each system as ideal insulator), respectively. In a brief review of the QEq and the QTPIE model, their applicability for studying surface interactions is assessed in this work. Following this reasoning, the demand for a revised generalization of the QEq and QTPIE model is proposed, called charge transfer equilibration model or in short QTE model. This method is derived from the equilibration of constrained charge transfer variables, instead of considering atomic charge variables. The latter, however, are obtained by a respective transformation, employing an extended Lagrangian method. We moreover propose a mirror boundary condition and its implementation to accelerate surface investigations. The models proposed in this work facilitate reactive molecular dynamics simulations which describe various materials and surface phenomena appropriately. ",2006.04157v2
"['Sahitya Yarragolla', 'Torben Hemke', 'Jan Trieschmann', 'Thomas Mussenbrock']",2024-01-25T20:57:21Z,Non-zero crossing current-voltage characteristics of interface-type   resistive switching devices,"  A number of memristive devices, mainly ReRAMs, have been reported to exhibit a unique non-zero crossing hysteresis attributed to the interplay of resistive and not yet fully understood `capacitive', and `inductive' effects. This work exploits a kinetic simulation model based on the stochastic cloud-in-a-cell method to capture these effects. The model, applied to Au/BiFeO$_{3}$/Pt/Ti interface-type devices, incorporates vacancy transport and capacitive contributions. The resulting nonlinear response, characterized by hysteresis, is analyzed in detail, providing an in-depth physical understanding of the virtual effects. Capacitive effects are modeled across different layers, revealing their significant role in shaping the non-zero crossing hysteresis behavior. Results from kinetic simulations demonstrate the impact of frequency-dependent impedance on the non-zero crossing phenomenon. This model provides insights into the effects of various device material properties, such as Schottky barrier height, device area and oxide layer on the non-zero crossing point. ",2401.14507v1
"['Sahitya Yarragolla', 'Torben Hemke', 'Jan Trieschmann', 'Thomas Mussenbrock']",2024-01-29T11:09:06Z,Coexistence of resistive capacitive and virtual inductive effects in   memristive devices,"  This paper examines the coexistence of resistive, capacitive, and inertia (virtual inductive) effects in memristive devices, focusing on ReRAM devices, specifically the interface-type or non-filamentary analog switching devices. A physics-inspired compact model is used to effectively capture the underlying mechanisms governing resistive switching in NbO$_{\rm x}$ and BiFeO$_{3}$ based on memristive devices. The model includes different capacitive components in metal-insulator-metal structures to simulate capacitive effects. Drift and diffusion of particles are modeled and correlated with particles' inertia within the system. Using the model, we obtain the I-V characteristics of both devices that show good agreement with experimental findings and the corresponding C-V characteristics. This model also replicates observed non-zero crossing hysteresis in perovskite-based devices. Additionally, the study examines how the reactance of the device changes in response to variations in the device area and length. ",2401.16057v1
"['Frederik Schmidt', 'Thomas Mussenbrock', 'Jan Trieschmann']",2018-04-16T12:35:29Z,Consistent simulation of capacitive radio-frequency discharges and   external matching networks,"  External matching networks are crucial and necessary for operating capacitively coupled plasmas in order to maximize the absorbed power. Experiments show that external circuits in general heavily interact with the plasma in a nonlinear way. This interaction has to be taken into account in order to be able to design suitable networks, e.g., for plasma processing systems. For a complete understanding of the underlying physics of this coupling, a nonlinear simulation approach which considers both the plasma and the circuit dynamics can provide useful insights. In this work, the coupling of an equivalent circuit plasma model and an electric external circuit composed of lumped elements is discussed. The plasma model itself is self-consistent in the sense that the plasma density and the electron temperature is calculated from the absorbed power based on a global plasma chemistry model. The approach encompasses all elements present in real plasma systems, i.e., the discharge itself, the matching network, the power generator as well as stray loss elements. While the main results of this work is the conceptual approach itself, at the example of a single-frequency capacitively coupled discharge its applicability is demonstrated. It is shown that it provides an effective and efficient way to analyze and understand the nonlinear dynamics of real plasma systems and, furthermore, may be applied to synthesize optimal matching networks. ",1804.05638v1
"['Jan Trieschmann', 'Mohammed Shihab', 'Daniel Szeremley', 'Abd Elfattah Elgendy', 'Sara Gallian', 'Denis Eremin', 'Ralf Peter Brinkmann', 'Thomas Mussenbrock']",2012-08-10T18:47:30Z,Ion energy distribution functions behind the sheaths of magnetized and   non magnetized radio frequency discharges,"  The effect of a magnetic field on the characteristics of capacitively coupled radio frequency discharges is investigated and found to be substantial. A one-dimensional particle-in-cell simulation shows that geometrically symmetric discharges can be asymmetrized by applying a spatially inhomogeneous magnetic field. This effect is similar to the recently discovered electrical asymmetry effect. Both effects act independently, they can work in the same direction or compensate each other. Also the ion energy distribution functions at the electrodes are strongly affected by the magnetic field, although only indirectly. The field influences not the dynamics of the sheath itself but rather its operating conditions, i.e., the ion flux through it and voltage drop across it. To support this interpretation, the particle-in-cell results are compared with the outcome of the recently proposed ensemble-in-spacetime algorithm. Although that scheme resolves only the sheath and neglects magnetization, it is able to reproduce the ion energy distribution functions with very good accuracy, regardless of whether the discharge is magnetized or not. ",1208.2248v2
"['Jan Trieschmann', 'Axel Wright Larsen', 'Thomas Mussenbrock', 'Søren Bang Korsholm']",2021-04-29T12:41:45Z,Kinetic simulation of electron cyclotron resonance assisted gas   breakdown in split-biased waveguides for ITER collective Thomson scattering   diagnostic,"  For the measurement of the dynamics of fusion-born alpha particles $E_\alpha \leq 3.5$ MeV in ITER using collective Thomson scattering (CTS), safe transmission of a gyrotron beam at mm-wavelength (1 MW, 60 GHz) passing the electron cyclotron resonance (ECR) in the in-vessel tokamak `port plug' vacuum is a prerequisite. Depending on neutral gas pressure and composition, ECR-assisted gas breakdown may occur at the location of the resonance, which must be mitigated for diagnostic performance and safety reasons. The concept of a split electrically biased waveguide (SBWG) has been previously demonstrated in [C.P. Moeller, U.S. Patent 4,687,616 (1987)]. The waveguide is longitudinally split and a kV bias voltage applied between the two halves. Electrons are rapidly removed from the central region of high radio frequency electric field strength, mitigating breakdown. As a full scale experimental investigation of gas and electromagnetic field conditions inside the ITER equatorial port plugs is currently unattainable, a corresponding Monte Carlo simulation study is presented. Validity of the Monte Carlo electron model is demonstrated with a prediction of ECR breakdown and the mitigation pressure limits for the above quoted reference case with $^1$H$_2$ (and pollutant high $Z$ elements). For the proposed ITER CTS design with a 88.9 mm inner diameter SBWG, ECR breakdown is predicted to occur down to a pure $^1$H$_2$ pressure of 0.3 Pa, while mitigation is shown to be effective at least up to 10 Pa using a bias voltage of 1 kV. The analysis is complemented by results for relevant electric/magnetic field arrangements and limitations of the SBWG mitigation concept are addressed. ",2104.14303v2
"['Tobias Gergs', 'Thomas Mussenbrock', 'Jan Trieschmann']",2021-10-01T12:41:30Z,Molecular Dynamics Study on the Role of Ar Ions in the Sputter   Deposition of Al Thin Films,"  Molecular dynamics simulations are often used to study sputtering and thin film growth. Compressive stresses in these thin films are generally assumed to be caused by a combination of forward sputtered (peened) built-in particles and entrapped working gas atoms. While the former are assumed to hold a predominant role, the effect of the latter on the interaction dynamics as well as thin film properties are scarcely clarified (concurrent or causative). The inherent overlay of the ion bombardment induced processes render an isolation of their contribution impracticable. In this work, this issue is addressed by comparing the results of two case studies on the sputter deposition of Al thin films in Ar working gas. In the first run Ar atoms are fully retained. In the second run they are artificially neglected, as implanted Ar atoms are assumed to outgas anyhow and not alter the ongoing dynamics significantly. Both case studies have in common that the consecutive impingement of 100 particles (i.e., Ar$^+$ ions, Al atoms) onto Al(001) surfaces for ion energies in the range of 3 eV to 300 eV as well as Al/Ar$^+$ flux ratios from 0 to 1 are considered. The surface interactions are simulated by means of hybrid reactive molecular dynamics/force-biased Monte Carlo simulations and characterized in terms of mass density, Ar concentration, biaxial stress, shear stress, ring statistical connectivity profile, Ar gas porosity, Al vacancy density, and root-mean-squared roughness. Ultimately, implanted Ar atoms are found to form subnanometer sized eventually outgassing clusters for ion energies exceeding 100 eV. They fundamentally govern a variety of surface processes (e.g., forward sputtering/peening) and surface properties (e.g., compressive stresses) in the considered operating regime. ",2110.00356v2
"['Kirsten Bobzin', 'Ralf Peter Brinkmann', 'Thomas Mussenbrock', 'Nazlim Bagcivan', 'Ricardo Henrique Brugnara', 'Marcel Schäfer', 'Jan Trieschmann']",2013-05-24T16:55:42Z,Continuum and Kinetic Simulations of the Neutral Gas Flow in an   Industrial Physical Vapor Deposition Reactor,"  Magnetron sputtering used for physical vapor deposition processes often requires gas pressures well below 1 Pa. Under these conditions the gas flow in the reactor is usually determined by a Knudsen number of about one, i.e., a transition regime between the hydrodynamic and the rarefied gas regime. In the first, the gas flow is well described by the Navier-Stokes equations, while in the second a kinetic approach via the Boltzmann equation is necessary. In this paper the neutral gas flow of argon and molecular nitrogen gas inside an industrial scale plasma reactor was simulated using both a fluid model and a fully kinetic Direct Simulation Monte Carlo model. By comparing both model results the validity of the fluid model was checked. Although in both models a Maxwell-Boltzmann energy distribution of the neutral particles is the natural outcome, the results of the gas flow differ significantly. The fluid model description breaks down, due to the inappropriate assumption of a fluid continuum. This is due to exclusion of non-local effects in the multi dimensional velocity space, as well as invalid gas/wall interactions. Only the kinetic model is able to provide an accurate physical description of the gas flow in the transition regime. Our analysis is completed with a brief investigation of different definitions of the local Knudsen number. We conclude that the most decisive parameter - the spatial length scale L - has to be very careful chosen in order to obtain a reasonable estimate of the gas flow regime. ",1305.5793v2
"['Sara Gallian', 'Jan Trieschmann', 'Thomas Mussenbrock', 'Ralf Peter Brinkmann', 'William N. G. Hitchon']",2014-11-25T10:27:26Z,Analytic model of the energy distribution function for highly energetic   electrons in magnetron plasmas,"  This paper analyzes a situation which is common for magnetized technical plasmas such as dc magnetron discharges and HiPIMS systems, where secondary electrons enter the plasma after being accelerated in the cathode fall and encounter a nearly uniform bulk. An analytic calculation of the distribution function of hot electrons is presented; these are described as an initially monoenergetic beam that slows down by Coulomb collisions with a Maxwellian distribution of bulk (cold) electrons, and by inelastic collisions with neutrals. Although this analytical solution is based on a steady-state assumption, a comparison of the characteristic time-scales suggests that it may be applicable to a variety of practical time-dependent discharges, and it may be used to introduce kinetic effects into models based on the hypothesis of Maxwellian electrons. The results are verified for parameters appropriate to HiPIMS discharges, by means of time-dependent and fully-kinetic numerical calculations. ",1411.6793v3
"['Frederik Schmidt', 'Julian Schulze', 'Erik Johnson', 'Jean-Paul Booth', 'Douglas Keil', 'David M. French', 'Jan Trieschmann', 'Thomas Mussenbrock']",2018-04-27T06:30:17Z,Multi frequency matching for voltage waveform tailoring,"  Customized voltage waveforms composed of a number of frequencies and used as the excitation of radio-frequency plasmas can control various plasma parameters such as energy distribution functions, homogeneity of the ionflux or ionization dynamics. So far this technology, while being extensively studied in academia, has yet to be established in applications. One reason for this is the lack of a suitable multi-frequency matching network that allows for maximum power absorption for each excitation frequency that is generated and transmitted via a single broadband amplifier. In this work, a method is introduced for designing such a network based on network theory and synthesis. Using this method, a circuit simulation is established that connects an exemplary matching network to an equivalent circuit plasma model of a capacitive radio-frequency discharge. It is found that for a range of gas pressures and number of excitation frequencies the matching conditions can be satisfied, which proves the functionality and feasibility of the proposed concept. Based on the proposed multi-frequency impedance matching, tailored voltage waveforms can be used at an industrial level. ",1804.10357v1
"['Ihda Chaerony Siffa', 'Markus M. Becker', 'Klaus-Dieter Weltmann', 'Jan Trieschmann']",2023-06-13T08:00:59Z,Towards a Machine-Learned Poisson Solver for Low-Temperature Plasma   Simulations in Complex Geometries,"  Poisson's equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson's equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly-randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accurate solutions. Furthermore, it generalizes well on new reactor geometries such as reference geometries found in the literature. To increase the numerical accuracy of the solutions required in LTP simulations, we employ a conventional iterative solver to refine the raw predictions, especially to recover the high-frequency features not resolved by the initial prediction. With this, the proposed learned Poisson solver provides the required accuracy and is potentially faster than a pure GPU-based conventional iterative solver. This opens up new possibilities for developing a generic and high-performing learned Poisson solver for LTP systems in complex geometries. ",2306.07604v1
"['Sahitya Yarragolla', 'Torben Hemke', 'Fares Jalled', 'Tobias Gergs', 'Jan Trieschmann', 'Tolga Arul', 'Thomas Mussenbrock']",2024-02-07T13:45:43Z,Nonlinear behavior of memristive devices for hardware security   primitives and neuromorphic computing systems,"  Nonlinearity is a crucial characteristic for implementing hardware security primitives or neuromorphic computing systems. The main feature of all memristive devices is this nonlinear behavior observed in their current-voltage characteristics. To comprehend the nonlinear behavior, we have to understand the coexistence of resistive, capacitive, and inertia (virtual inductive) effects in these devices. These effects originate from corresponding physical and chemical processes in memristive devices. A physics-inspired compact model is employed to model and simulate interface-type RRAMs such as Au/BiFeO$_{3}$/Pt/Ti, Au/Nb$_{\rm x}$O$_{\rm y}$/Al$_{2}$O$_{3}$/Nb, while accounting for the modeling of capacitive and inertia effects. The simulated current-voltage characteristics align well with experimental data and accurately capture the non-zero crossing hysteresis generated by capacitive and inductive effects. This study examines the response of two devices to increasing frequencies, revealing a shift in their nonlinear behavior characterized by a reduced hysteresis range and increased chaotic behavior, as observed through internal state attractors. Fourier series analysis utilizing a sinusoidal input voltage of varying amplitudes and frequencies indicates harmonics or frequency components that considerably influence the functioning of RRAMs. Moreover, we propose and demonstrate the use of the frequency spectra as one of the fingerprints for memristive devices. ",2402.04848v2
"['Teresa de los Arcos', 'Peter Awakowicz', 'Marc Böke', 'Nils Boysen', 'Ralf Peter Brinkmann', 'Rainer Dahlmann', 'Anjana Devi', 'Denis Eremin', 'Jonas Franke', 'Tobias Gergs', 'Jonathan Jenderny', 'Efe Kemaneci', 'Thomas D. Kühne', 'Simon Kusmierz', 'Thomas Mussenbrock', 'Jens Rubner', 'Jan Trieschmann', 'Matthias Wessling', 'Xiaofan Xie', 'David Zanders', 'Frederik Zysk', 'Guido Grundmeier']",2023-06-26T15:55:41Z,PECVD and PEALD on polymer substrates (Part II): Understanding and   tuning of barrier and membrane properties of thin films,"  This feature article presents insights concerning the correlation of PECVD and PEALD thin film structures with their barrier or membrane properties. While in principle similar precursor gases and processes can be applied, the adjustment of deposition parameters for different polymer substrates can lead to either an effective diffusion barrier or selective permeabilities. In both cases the understanding of the film growth and the analysis of the pore size distribution and the pore surface chemistry is of utmost importance for the understanding of the related transport properties of small molecules. In this regard the article presents both concepts of thin film engineering and analytical as well as theoretical approaches leading to a comprehensive description of the state of the art in this field. Moreover, based on the presented correlation of film structure and molecular transport properties perspectives of future relevant research in this area is presented. ",2306.14797v1
"['Rushil Anirudh', 'Rick Archibald', 'M. Salman Asif', 'Markus M. Becker', 'Sadruddin Benkadda', 'Peer-Timo Bremer', 'Rick H. S. Budé', 'C. S. Chang', 'Lei Chen', 'R. M. Churchill', 'Jonathan Citrin', 'Jim A Gaffney', 'Ana Gainaru', 'Walter Gekelman', 'Tom Gibbs', 'Satoshi Hamaguchi', 'Christian Hill', 'Kelli Humbird', 'Sören Jalas', 'Satoru Kawaguchi', 'Gon-Ho Kim', 'Manuel Kirchen', 'Scott Klasky', 'John L. Kline', 'Karl Krushelnick', 'Bogdan Kustowski', 'Giovanni Lapenta', 'Wenting Li', 'Tammy Ma', 'Nigel J. Mason', 'Ali Mesbah', 'Craig Michoski', 'Todd Munson', 'Izumi Murakami', 'Habib N. Najm', 'K. Erik J. Olofsson', 'Seolhye Park', 'J. Luc Peterson', 'Michael Probst', 'Dave Pugmire', 'Brian Sammuli', 'Kapil Sawlani', 'Alexander Scheinker', 'David P. Schissel', 'Rob J. Shalloo', 'Jun Shinagawa', 'Jaegu Seong', 'Brian K. Spears', 'Jonathan Tennyson', 'Jayaraman Thiagarajan', 'Catalin M. Ticoş', 'Jan Trieschmann', 'Jan van Dijk', 'Brian Van Essen', 'Peter Ventzek', 'Haimin Wang', 'Jason T. L. Wang', 'Zhehui Wang', 'Kristian Wende', 'Xueqiao Xu', 'Hiroshi Yamada', 'Tatsuya Yokoyama', 'Xinhua Zhang']",2022-05-31T14:37:06Z,2022 Review of Data-Driven Plasma Science,"  Data science and technology offer transformative tools and methods to science. This review article highlights latest development and progress in the interdisciplinary field of data-driven plasma science (DDPS). A large amount of data and machine learning algorithms go hand in hand. Most plasma data, whether experimental, observational or computational, are generated or collected by machines today. It is now becoming impractical for humans to analyze all the data manually. Therefore, it is imperative to train machines to analyze and interpret (eventually) such data as intelligently as humans but far more efficiently in quantity. Despite the recent impressive progress in applications of data science to plasma science and technology, the emerging field of DDPS is still in its infancy. Fueled by some of the most challenging problems such as fusion energy, plasma processing of materials, and fundamental understanding of the universe through observable plasma phenomena, it is expected that DDPS continues to benefit significantly from the interdisciplinary marriage between plasma science and data science into the foreseeable future. ",2205.15832v1
"['Fatih Ilgaz', 'Elizaveta Spetzler', 'Patrick Wiegand', 'Franz Faupel', 'Robert Rieger', 'Jeffrey McCord', 'Benjamin Spetzler']",2023-12-05T17:19:43Z,Miniaturized Double-Wing Delta-E Effect Sensors,"  Magnetoelastic composites are integral elements of sensors and actuators utilizing magnetostriction for their functionality. Their sensitivity typically scales with the saturation magnetostriction and inversely with magnetic anisotropy. However, this makes the devices prone to minuscule residual anisotropic stress from the fabrication process, impairing their performance and reproducibility, hence limiting their suitability for arrays. This study presents a shadow mask deposition technology combined with a free-free magnetoelectric microresonator design intended to minimize residual stress and inhomogeneity in the magnetoelastic layer. Resonators are experimentally and theoretically analyzed regarding local stress anisotropy, magnetic anisotropy, and the {\Delta}E effect in several resonance modes. Further, the sensitivity is analyzed in the example of {\Delta}E-effect sensors. The results demonstrate a device-to-device variation of the resonance frequency < 0.2 % with sensitivities comparable with macroscopic {\Delta}E-effect sensors. The reproducibility is drastically improved over previous magnetoelastic device arrays. This development marks a step forward in the reproducibility and homogeneity of magnetoelastic resonators and contributes to the feasibility of large-scale, integrated sensor arrays. ",2312.02903v1
"['Chenbo Zhang', 'Yintao Song', 'Maike Wegner', 'Eckhard Quandt', 'Xian Chen']",2019-02-11T17:50:28Z,Battery Detached Energy Conversion by Pyroelectric Effect,"  We propose a pyroelectric energy conversion device that converts heat directly to electricity. In contrast to conventional pyroelectric energy conversion designs, this energy harvesting system is detached from any external power sources, operating only under periodically varying temperature. Such detachment unambiguously attributes the converted electricity to heat that drives the change of polarization in the pyroelectric material, not to the electric field alternation caused by the external battery. Using pure and Zr doped BaTiO$_3$, we demonstrate the electricity generation in consecutive temperature cycles. We further develop a thermodynamic model for the energy conversion system. Our model suggests that the work output is rate dependent: the work output per cycle is linearly dependent on the heat/cooling frequency below the predicted threshold. The linearity is confirmed by experiments, and the threshold frequency is derived by theory. Finally we propose a figure of merit that separates the materials intrinsic properties from the system design parameters. The figure of merit guides the future material development and device improvement. Our work clears out confusions and reforms the foundation for pyroelectric materials' resurgence as a competitor for green electricity. ",1902.04018v1
"['Georgios Grekas', 'Patricia-Lia Pop-Ghe', 'Eckhard Quandt', 'Richard D. James']",2023-02-03T15:07:22Z,Theory of intermediate twinning and spontaneous polarization in the   phase transformations of ferroelectric potassium sodium niobate,"  Potassium sodium niobate is considered a prominent material system as a substitute for toxic lead-containing ferroelectric materials. It exhibits first-order phase transformations and ferroelectricity with potential applications ranging from energy conversion to innovative cooling technologies, hereby addressing urgent societal challenges. However, a major obstacle in the application of potassium sodium niobate is its multi-scale heterogeneity and the lack of understanding of its phase transition pathway and microstructure. This can be seen from the findings of Pop-Ghe 2021 et al. [1] which also reveal the occurrence of intermediate twinning during the phase transition. Here we show that intermediate twinning is a consequence of energy minimization. We employ a geometrically nonlinear electroelastic energy function for potassium sodium niobate, including the cubic-tetragonal-orthorhombic transformations and ferroelectricity. The construction of the minimizers is based on compatibility conditions which ensure continuous deformations and pole-free interfaces. These minimizers agree with the experimental observations, including laminates between the tetragonal variants under the cubic to tetragonal transformation, crossing twins under the tetragonal to orthorhombic transformation, intermediate twinning and spontaneous polarization. This shows how the full nonlinear electroelastic model provides a powerful tool in understanding, exploring and tailoring the electromechanical properties of complex ferroelectric ceramics. ",2302.01797v1
"['Phillip Durdaut', 'Anne Kittmann', 'Enrico Rubiola', 'Jean-Michel Friedt', 'Eckhard Quandt', 'Reinhard Knöchel', 'Michael Höft']",2019-01-05T15:10:09Z,Noise Analysis of Open-Loop and Closed-Loop SAW Magnetic Field Sensor   Systems,"  Transmission surface acoustic wave (SAW) sensors are widely used in various fields of application. In order to maximize the limit of detection (LOD) of such sensor systems, it is of high importance to understand and to be able to quantify the relevant noise sources. In this paper, low noise readout systems for the application with a SAW delay line magnetic field sensor in an open-loop and closed-loop configuration are presented and analyzed with regard to their phase noise contribution. By applying oscillator phase noise theory to closed-loop sensor systems, it is shown that the phase noise of the SAW delay line oscillator can be predicted accurately. This allows the derivation of expressions for the limits of detection for both readout structures. Based on these equations, the equivalence between the LOD of open-loop and closed-loop SAW delay line readout can be shown analytically, assuming that the sensor contributes the dominant phase noise. This equality is verified by measurements. These results are applicable to all kinds of phase sensitive delay line sensors. ",1901.01428v2
"['Phillip Durdaut', 'Enrico Rubiola', 'Jean-Michel Friedt', 'Cai Müller', 'Benjamin Spetzler', 'Christine Kirchhof', 'Dirk Meyners', 'Eckhard Quandt', 'Franz Faupel', 'Jeffrey McCord', 'Reinhard Knöchel', 'Michael Höft']",2020-03-02T18:18:55Z,Phase Sensitivity and Phase Noise of Cantilever-Type Magnetoelastic   Sensors Based on the $Δ$E Effect,"  Magnetoelastic sensors for the detection of low-frequency and low-amplitude magnetic fields are in the focus of research since more than 30 years. In order to minimize the limit of detection (LOD) of such sensor systems, it is of high importance to understand and to be able to quantify the relevant noise sources. In this contribution, cantilever-type electromechanic and magnetoelastic resonators, respectively, are comprehensively investigated and mathematically described not only with regard to their phase sensitivity but especially to the extent of the sensor-intrinsic phase noise. Both measurements and calculations reveal that the fundamental LOD is limited by additive phase noise due to thermal-mechanical noise of the resonator, i.e. by thermally induced random vibrations of the cantilever, and by thermal-electrical noise of the piezoelectric material. However, due to losses in the magnetic material parametric flicker phase noise arises, limiting the overall performance. In particular it is shown that the LOD is virtually independent of the magnetic sensitivity but is solely determined by the magnetic losses. Instead of the sensitivity, the magnetic losses, represented by the material's effective complex permeability, should be considered as the most important parameter for the further improvement of such sensors in the future. This implication is not only valid for magnetoelastic cantilevers but also applies to any type of magnetoelastic resonator. ",2003.01085v1
"['Armin Reimers', 'Jannik Rank', 'Erik Greve', 'Morten Möller', 'Sören Kaps', 'Jörg Bahr', 'Rainer Adelung', 'Fabian Schütt']",2023-11-17T12:37:31Z,Graphene-based thermopneumatic generator for on-board pressure supply of   soft robots,"  Various fields, including medical and human interaction robots, gain advantages from the development of bioinspired soft actuators. Many recently developed grippers are pneumatics that require external pressure supply systems, thereby limiting the autonomy of these robots. This necessitates the development of scalable and efficient on-board pressure generation systems. While conventional air compression systems are hard to miniaturize, thermopneumatic systems that joule-heat a transducer material to generate pressure present a promising alternative. However, the transducer materials of previously reported thermopneumatic systems demonstrate high heat capacities and limited surface area resulting in long response times and low operation frequencies. This study presents a thermopneumatic pressure generator using aerographene, a highly porous (>99.99 %) network of interconnected graphene microtubes, as lightweight and low heat capacity transducer material. An aerographene pressurizer module (AGPM) can pressurize a reservoir of 4.2 cm3 to about ~140 mbar in 50 ms. Periodic operation of the AGPM for 10 s at 0.66 Hz can further increase the pressure in the reservoir to ~360 mbar. It is demonstrated that multiple AGPMs can be operated parallelly or in series for improved performance. For example, three parallelly operated AGPMs can generate pressure pulses of ~215 mbar. Connecting AGPMs in series increases the maximum pressure achievable by the system. It is shown that three AGPMs working in series can pressurize the reservoir to ~2000 mbar in about 2.5 min. The AGPM's minimalistic design can be easily adapted to circuit boards, making the concept a promising fit for the on-board pressure supply of soft robots. ",2311.10488v1
"['Soeren Kaps', 'Rainer Adelung', 'Michael Scharnberg', 'Franz Faupel', 'Srdjan Milenkovic', 'Achim Walter Hassel']",2014-08-22T12:09:01Z,Determining superhydrophobic surfaces from an expanded Cassie Baxter   equation describing simple wettability experiments,"  The characterization of the wetting on superhydrophobic surfaces is rather complex. Usual contact angle experiments are difficult to perform and the lateral movement of droplets as well as the pinning at point defects on the surface can disturb the measurements. Even if precise contact angle measurements can be performed the information gain is limited if the surface is heterogeneously wetted. This results in the possibility of two surfaces with different roughness, different surface energy and thus different underlying wetting mechanisms exhibiting the same contact angle. We introduce the utilization of dynamic wetting experiments as an additional surface probe which allows a better characterization of superhydrophobic surfaces. A theoretical model is presented which describes the spreading of water jets on a superhydrophobic surface and allows the determination of the wetted fraction of a heterogeneously wetted superhydrophobic surface. The determined values for the wetted fraction identify a common problem when building artificial super hydrophobic surfaces and can fundamentally improve their understanding. ",1408.5273v1
"['Armin Reimers', 'Ala Bouhanguel', 'Erik Greve', 'Morten Möller', 'Lena Marie Saure', 'Sören Kaps', 'Lasse Wegner', 'Ali Shaygan Nia', 'Xinliang Feng', 'Fabian Schütt', 'Yves Andres', 'Rainer Adelung']",2023-05-25T14:00:48Z,"Multifunctional, Self-Cleaning Air Filters Based on Graphene-Enhanced   Ceramic Networks","  Particulate air pollution is taking a huge toll on modern society, being associated with more than three million deaths per year. In addition, airborne infectious microorganism can spread dangerous diseases, further elevating the problem. A common way to mitigate the risks of airborne particles is by air filtration. However, conventional air filters usually do not provide any functionality beyond particle removal. They are unable to inactivate accumulated contaminants and therefore need periodic maintenance and replacement to remain operational and safe. This work presents a multifunctional, self-cleaning air filtration system which utilizes a novel graphene-enhanced air filter medium (GeFM). The hybrid network of the GeFM combines the passive structure-based air filtration properties of an underlying ceramic network with additional active features based on the functional properties of a graphene thin film. The GeFM is able to capture >95 % of microorganisms and particles larger than 1 $\mu$m and can be repetitively Joule-heated to >300 {\deg}C for several hours without signs of degradation. Hereby, built-up organic particulate matter and microbial contaminants are effectively decomposed, regenerating the GeFM. Additionally, the GeFM provides unique options to monitor the filter's air troughput and loading status during operation. The active features of the GeFM can drastically improve filter life-time and safety, offering great potential for the development of safer and more sustainable air filtration solutions to face the future challenges of air pollution and pandemics. ",2305.16374v1
"['Igor Barg', 'Niklas Kohlmann', 'Florian Rasch', 'Thomas Strunskus', 'Rainer Adelung', 'Lorenz Kienle', 'Franz Faupel', 'Stefan Schröder', 'Fabian Schütt']",2022-10-18T07:36:26Z,"Strain-invariant, highly water stable all-organic soft conductors based   on ultralight multi-layered foam-like framework structures","  Soft and flexible conductors are essential in the development of soft robots, wearable electronics, as well as electronic tissue and implants. However, conventional soft conductors are inherently characterized by a large change in conductance upon mechanical deformation or under alternating environmental conditions, e.g., humidity, drastically limiting their application potential and performance. Here, we demonstrate a novel concept for the development of strain-invariant, fatigue resistant and highly water stable soft conductor. By combining different thin film technologies in a three-dimensional fashion, we develop nano- and micro-engineered, multi-layered (< 50 nm), ultra-lightweight (< 15 mg/cm$^3$) foam-like composite framework structures based on PEDOT:PSS and PTFE. The all-organic composite framework structures are characterized by conductivities of up to 184 S/m, remaining strain-invariant between 80 % compressive and 25 % tensile strain. We further show, that the multi-layered composites are characterized by properties that surpass that of framework structures based on the individual materials. Both, the initial electrical and mechanical properties of the composite framework structures are retained during long-term cycling, even after 2000 cycles at 50 % compression. Furthermore, the PTFE functionalization renders the framework structure highly hydrophobic, resulting in stable electrical properties, even when immersed in water for up to 30 days. The here presented concept overcomes the previous limitations of strain-invariant soft conductors and demonstrates for the first time a versatile approach for the development of innovative multi-scaled and multi-layered functional materials, for applications in soft electronics, energy storage and conversion, sensing, catalysis, water and air purification, as well as biomedicine. ",2210.09650v1
"['Francesco De Nicola', 'Ilenia Viola', 'Lorenzo Donato Tenuzzo', 'Florian Rasch', 'Martin R. Lohe', 'Ali Shaygan Nia', 'Fabian Schütt', 'Xinliang Feng', 'Rainer Adelung', 'Stefano Lupi']",2020-02-06T10:07:19Z,Wetting Properties of Graphene Aerogels,"  Graphene hydrophobic coatings paved the way towards a new generation of optoelectronic and fluidic devices. Nevertheless, such hydrophobic thin films rely only on graphene non-polar surface, rather than taking advantage of its surface roughness. Furthermore, graphene is typically not self-standing. Differently, carbon aerogels have high porosity, large effective surface area due to their surface roughness, and very low mass density, which make them a promising candidate as a super-hydrophobic material for novel technological applications. However, despite a few works reporting the general super-hydrophobic and lipophilic behavior of the carbon aerogels, a detailed characterization of their wetting properties is still missing, to date. Here, the wetting properties of graphene aerogels are demonstrated in detail. Without any chemical functionalization or patterning of their surface, the samples exhibit a super-lipophilic state and a stationary super-hydrophobic state with a contact angle up to $150\pm15^{\deg}$ and low contact angle hysteresis $\approx15^{\deg}$, owing to the fakir effect. In addition, the adhesion force of the graphene aerogels in contact with the water droplets and their surface tension are evaluated. For instance, the unique wettability and enhanced liquid absorption of the graphene aerogels can be exploited for reducing contamination from oil spills and chemical leakage accidents. ",2002.02183v1
"['Lena M. Saure', 'Niklas Kohlmann', 'Haoyi Qiu', 'Shwetha Shetty', 'Ali Shaygan Nia', 'Narayanan Ravishankar', 'Xinliang Feng', 'Alexander Szameit', 'Lorenz Kienle', 'Rainer Adelung', 'Fabian Schütt']",2023-03-24T14:13:51Z,Hybrid aeromaterials for enhanced and rapid volumetric photothermal   response,"  Conversion of light into heat is essential for a broad range of technologies such as solar thermal heating, catalysis and desalination. Three-dimensional (3D) carbon nanomaterial-based aerogels have shown to hold great promise as photothermal transducer materials. However, till now, their light-to-heat conversion is limited by surface-near absorption, resulting in a strong heat localization only at the illuminated surface region, while most of the aerogel volume remains unused. We present an innovative fabrication concept for highly porous (>99.9%) photothermal hybrid aeromaterials, that enable an ultra-rapid and volumetric photothermal response with an enhancement by a factor of around 2.5 compared to the pristine variant. The hybrid aeromaterial is based on strongly light-scattering framework structures composed of interconnected hollow silicon dioxide (SiO${_2}$) microtubes, which are functionalized with extremely low amounts (in order of a few ${\mu}$g cm${^-}$${^3}$) of reduced graphene oxide (rGO) nanosheets, acting as photothermal agents. Tailoring the density of rGO within the framework structure enables us to control both, light scattering and light absorption, and thus the volumetric photothermal response. We further show that by rapid and repeatable gas activation these transducer materials expand the field of photothermal applications, like untethered light-powered and -controlled microfluidic pumps and soft pneumatic actuators. ",2303.14014v2
"['Margarethe Hauck', 'Lena M. Saure', 'Berit Zeller-Plumhoff', 'Sören Kaps', 'Jörg Hammel', 'Caprice Mohr', 'Lena Rieck', 'Ali Shaygan Nia', 'Xinliang Feng', 'Nicola M. Pugno', 'Rainer Adelung', 'Fabian Schütt']",2023-03-24T09:43:36Z,Overcoming water diffusion limitations in hydrogels via microtubular   graphene networks for soft actuators,"  Hydrogel-based soft actuators can operate in sensitive environments, bridging the gap of rigid machines interacting with soft matter. However, while stimuli-responsive hydrogels can undergo extreme reversible volume changes of up to ~90%, water transport in hydrogel actuators is in general limited by their poroelastic behavior. For poly(N-isopropylacrylamide) (PNIPAM) the actuation performance is even further compromised by the formation of a dense skin layer. Here we show, that incorporating a bioinspired microtube graphene network into a PNIPAM matrix with a total porosity of only 5.4 % dramatically enhances actuation dynamics by up to ~400 % and actuation stress by ~4000 % without sacrificing the mechanical stability, overcoming the water transport limitations. The graphene network provides both untethered light-controlled and electrically-powered actuation. We anticipate that the concept provides a versatile platform for enhancing the functionality of soft matter by combining responsive and two-dimensional materials, paving the way towards designing soft intelligent matter. ",2303.13878v1
"['Lena M. Saure', 'Jonas Lumma', 'Niklas Kohlmann', 'Torge Hartig', 'Ercules E. S. Teotonio', 'Shwetha Shetty', 'Narayanan Ravishankar', 'Lorenz Kienle', 'Franz Faupel', 'Stefan Schröder', 'Rainer Adelung', 'Huayna Terraschke', 'Fabian Schütt']",2023-07-12T14:36:29Z,Functional light diffusers based on hybrid CsPbBr$_3$/SiO$_2$   aero-framework structures for laser light illumination and conversion,"  The new generation of laser-based solid-state lighting (SSL) white light sources requires new material systems capable of withstanding, diffusing and converting high intensity laser light. State-of-the-art systems use a blue light emitting diode (LED) or laser diode (LD) in combination with color conversion materials, such as yellow emitting Ce-doped phosphors or red and green emitting quantum dots (QD), to produce white light. However, for laser-based high-brightness illumination in particular, thermal management is a major challenge, and in addition, a light diffuser is required to diffuse the highly focused laser beam. Here, we present a hybrid material system that simultaneously enables efficient, uniform light distribution and color conversion of a blue LD, while ensuring good thermal management even at high laser powers of up to 5W. A highly open porous (> 99%) framework structure of hollow SiO$_2$ microtubes is utilized as an efficient light diffuser that can drastically reduce speckle contrast. By further functionalizing the microtubes with halide perovskite QDs (SiO$_2$@CsPbBr$_3$ as model system) color conversion from UV to visible light is achieved. Under laser illumination, the open porous structure prevents heat accumulation and thermal quenching of the QDs. By depositing an ultrathin (~ 5.5 nm) film of poly(ethylene glycol dimethyl acrylate) (pEGDMA) via initiated chemical vapor deposition (iCVD), the luminescent stability of the QDs against moisture is enhanced. The demonstrated hybrid material system paves the way for the design of advanced and functional laser light diffusers and converters that can meet the challenges associated with laser-based SSL applications. ",2307.06197v1
"['James C. Blakesley', 'Ruy S. Bonilla', 'Marina Freitag', 'Alex M. Ganose', 'Nicola Gasparini', 'Pascal Kaienburg', 'George Koutsourakis', 'Jonathan D. Major', 'Jenny Nelson', 'Nakita K. Noel', 'Bart Roose', 'Jae Sung Yun', 'Simon Aliwell', 'Pietro P. Altermatt', 'Tayebeh Ameri', 'Virgil Andrei', 'Ardalan Armin', 'Diego Bagnis', 'Jenny Baker', 'Hamish Beath', 'Mathieu Bellanger', 'Philippe Berrouard', 'Jochen Blumberger', 'Stuart A. Boden', 'Hugo Bronstein', 'Matthew J. Carnie', 'Chris Case', 'Fernando A. Castro', 'Yi-Ming Chang', 'Elmer Chao', 'Tracey M. Clarke', 'Graeme Cooke', 'Pablo Docampo', 'Ken Durose', 'James R. Durrant', 'Marina R. Filip', 'Richard H. Friend', 'Jarvist M. Frost', 'Elizabeth A. Gibson', 'Alexander J. Gillett', 'Pooja Goddard', 'Severin N. Habisreutinger', 'Martin Heeney', 'Arthur D. Hendsbee', 'Louise C. Hirst', 'M. Saiful Islam', 'K. D. G. Imalka Jayawardena', 'Michael B. Johnston', 'Matthias Kauer', 'Jeff Kettle', 'Ji-Seon Kim', 'Dan Lamb', 'David Lidzey', 'Jihoo Lim', 'Roderick MacKenzie', 'Nigel Mason', 'Iain McCulloch', 'Keith P. McKenna', 'Sebastian B. Meier', 'Paul Meredith', 'Graham Morse', 'John D. Murphy', 'Chris Nicklin', 'Paloma Ortega-Arriaga', 'Thomas Osterberg', 'Jay B. Patel', 'Anthony Peaker', 'Moritz Riede', 'Martyn Rush', 'James W. Ryan', 'David O. Scanlon', 'Peter J. Skabara', 'Franky So', 'Henry J. Snaith', 'Ludmilla Steier', 'Jarla Thiesbrummel', 'Alessandro Troisi', 'Craig Underwood', 'Karsten Walzer', 'Trystan Watson', 'J. Michael Walls', 'Aron Walsh', 'Lucy D. Whalley', 'Benedict Winchester', 'Samuel D. Stranks', 'Robert L. Z. Hoye']",2023-10-30T10:43:25Z,Roadmap on Photovoltaic Absorber Materials for Sustainable Energy   Conversion,"  Photovoltaics (PVs) are a critical technology for curbing growing levels of anthropogenic greenhouse gas emissions, and meeting increases in future demand for low-carbon electricity. In order to fulfil ambitions for net-zero carbon dioxide equivalent (CO<sub>2</sub>eq) emissions worldwide, the global cumulative capacity of solar PVs must increase by an order of magnitude from 0.9 TWp in 2021 to 8.5 TWp by 2050 according to the International Renewable Energy Agency, which is considered to be a highly conservative estimate. In 2020, the Henry Royce Institute brought together the UK PV community to discuss the critical technological and infrastructure challenges that need to be overcome to address the vast challenges in accelerating PV deployment. Herein, we examine the key developments in the global community, especially the progress made in the field since this earlier roadmap, bringing together experts primarily from the UK across the breadth of the photovoltaics community. The focus is both on the challenges in improving the efficiency, stability and levelized cost of electricity of current technologies for utility-scale PVs, as well as the fundamental questions in novel technologies that can have a significant impact on emerging markets, such as indoor PVs, space PVs, and agrivoltaics. We discuss challenges in advanced metrology and computational tools, as well as the growing synergies between PVs and solar fuels, and offer a perspective on the environmental sustainability of the PV industry. Through this roadmap, we emphasize promising pathways forward in both the short- and long-term, and for communities working on technologies across a range of maturity levels to learn from each other. ",2310.19430v1
"['Simon Fichtner', 'Niklas Wolff', 'Fabian Lofink', 'Lorenz Kienle', 'Bernhard Wagner']",2018-10-18T09:36:55Z,AlScN: A III-V semiconductor based ferroelectric,"  Ferroelectric switching is unambigiously demonstrated for the first time in a III-V semiconductor based material: AlScN -- A discovery which could help to satisfy the urgent demand for thin film ferroelectrics with high performance and good technological compatibility with generic semiconductor technology which arises from a multitude of memory, micro/nano-actuator and emerging applications based on controlling electrical polarization. The appearance of ferroelectricity in AlScN can be related to the continuous distortion of the original wurtzite-type crystal structure towards a layered-hexagonal structure with increasing Sc content and tensile strain, which is expected to be extendable to other III-nitride based solid solutions. Coercive fields which are systematically adjustable by more than 3 MV/cm, high remnant polarizations in excess of 100 \mu C/cm$^2$ which constitute the first experimental estimate of the previously inaccessible spontaneous polarization in a III-nitride based material, an almost ideally square-like hysteresis resulting in excellent piezoelectric linearity over a wide strain interval from -0.3% to +0.4% as well as a paraelectric transition temperature in excess of 600{\deg}C are confirmed. This intriguing combination of properties is to our knowledge as of now unprecedented in the field of polycrystalline ferroelectric thin films and promises to significantly advance the commencing integration of ferroelectric functionality to micro- and nanotechnology, while at the same time providing substantial insight to one of the central open questions of the III-nitride semiconductors - that of their actual spontaneous polarization. ",1810.07968v2
"['Khurram Saleem', 'Ulrich Schürmann', 'Lena Grandin', 'Christian Horn', 'Fritz Jürgens', 'Johannes Müller', 'Claus von Carnap-Bornheim', 'Lorenz Kienle']",2023-08-21T21:52:22Z,Decorative coating or corrosion product? TEM and SEM Study on a Late   Neolithic Axe to find origins of silver metal on the surface,"  Here we report on the analysis of a metallic Late Neolithic copper axe by means of Scanning Electron Microscopy (SEM) and Transmission Electron Microscopy (TEM). The axe was found at Eskilstorp, south-west Scania during archaeological excavations in autumn of 2015. It showed a hint of silver en-richment on the surface which is unusual for Late Neolithic axes from Pile type hoard. To identify the origin of the silver content, we extracted a thin lamella of the axe interior using Focused Ion Beam (FIB) technology to reach the internal structure to keep the process extremely minimally invasive. The results revealed the presence of porous external layer of copper oxide enriched with silver particles. It is shown with the corrosion test performed on copper-silver experimental replica that the silver en-richment is attributed to the selective dissolution of copper metal as a resut of corrosion process on the axe. The corrosion experiment is performed in the presence of an organic electrolyte such as Hemic Acid (HA) which replicates soil and natural water environment. In the scope of this research work, we found a strong evidence that the silver on the surface of the axe was not decrative layer and instead remained on the surface due to its more noble character while copper leached out into the soil due to corrosion. ",2309.15120v1
"['Jacob Johny', 'Oleg Prymak', 'Marius Kamp', 'Florent Calvo', 'Se-Ho Kim', 'Anna Tymoczko', 'Ayman El-Zoka', 'Christoph Rehbock', 'Ulrich Schürmann', 'Baptiste Gault', 'Lorenz Kienle', 'Stephan Barcikowski']",2021-04-27T21:51:19Z,Multidimensional thermally-induced transformation of nest-structured   complex Au-Fe nanoalloys towards equilibrium,"  Bimetallic nanoparticles are often superior candidates for a wide range of technological and biomedical applications, thanks to their enhanced catalytic, optical, and magnetic properties, which are often better than their monometallic counterparts. Most of their properties strongly depend on their chemical composition, crystallographic structure, and phase distribution. However, little is known of how their crystal structure, on the nanoscale, transforms over time at elevated temperatures, even though this knowledge is highly relevant in case nanoparticles are used in, e.g., high-temperature catalysis. Au-Fe is a promising bimetallic system where the low-cost and magnetic Fe is combined with catalytically active and plasmonic Au. Here, we report on the in situ temporal evolution of the crystalline ordering in Au-Fe nanoparticles, obtained from a modern laser ablation in liquids synthesis. Our in-depth analysis, complemented by dedicated atomistic simulations, includes a detailed structural characterization by X-ray diffraction and transmission electron microscopy as well as atom probe tomography to reveal elemental distributions down to a single atom resolution. We show that the Au-Fe nanoparticles initially exhibit highly complex internal nested nanostructures with a wide range of compositions, phase distributions, and size-depended microstrains. The elevated temperature induces a diffusion-controlled recrystallization and phase merging, resulting in the formation of a single face-centered-cubic ultrastructure in contact with a body-centered cubic phase, which demonstrates the metastability of these structures. Uncovering these unique nanostructures with nested features could be highly attractive from a fundamental viewpoint as they could give further insights into the nanoparticle formation mechanism under non-equilibrium conditions. ",2104.13491v1
"['Giovanni Masciocchi', 'Johannes Wilhelmus van der Jagt', 'Maria-Andromachi Syskaki', 'Alessio Lamperti', 'Niklas Wolff', 'Andriy Lotnyk', 'Jürgen Langer', 'Lorenz Kienle', 'Gerhard Jakob', 'Benjamin Borie', 'Andreas Kehlberger', 'Dafine Ravelosona', 'Mathias Kläui']",2022-07-06T07:56:56Z,Control of magnetoelastic coupling in Ni/Fe multilayers using He$^+$ ion   irradiation,"  This study reports the effects of post-growth He$^+$ irradiation on the magneto-elastic properties of a $Ni$ /$Fe$ multi-layered stack. The progressive intermixing caused by He$^+$ irradiation at the interfaces of the multilayer allows us to tune the saturation magnetostriction value with increasing He$^+$ fluences, and even to induce a reversal of the sign of the magnetostrictive effect. Additionally, the critical fluence at which the absolute value of the magnetostriction is dramatically reduced is identified. Therefore insensitivity to strain of the magnetic stack is nearly reached, as required for many applications. All the above mentioned effects are attributed to the combination of the negative saturation magnetostriction of sputtered Ni, Fe layers and the positive magnetostriction of the Ni$_{x}$Fe$_{1-x}$ alloy at the intermixed interfaces, whose contribution is gradually increased with irradiation. Importantly the irradiation does not alter the layers polycrystalline structure, confirming that post-growth He$^+$ ion irradiation is an excellent tool to tune the magneto-elastic properties of magnetic samples. A new class of spintronic devices can be envisioned with a material treatment able to arbitrarily change the magnetostriction with ion-induced ""magnetic patterning"". ",2207.02493v1
['Christian Huck'],2007-05-21T16:01:52Z,Discrete Tomography of Icosahedral Model Sets,"  The discrete tomography of B-type and F-type icosahedral model sets is investigated, with an emphasis on reconstruction and uniqueness problems. These are motivated by the request of materials science for the unique reconstruction of quasicrystalline structures from a small number of images produced by quantitative high resolution transmission electron microscopy. ",0705.3005v2
['Christian Huck'],2008-01-21T16:19:04Z,A Note on Affinely Regular Polygons,"  The affinely regular polygons in certain planar sets are characterized. It is also shown that the obtained results apply to cyclotomic model sets and, additionally, have consequences in the discrete tomography of these sets. ",0801.3218v1
['Christian Huck'],2008-10-02T12:52:01Z,Discrete Tomography of F-Type Icosahedral Model Sets,  We address the problem of uniquely reconstructing F-type icosahedral quasicrystals from few images produced by quantitative high resolution transmission electron microscopy and explain recent results in the discrete tomography of these sets. ,0810.0407v1
['Christian Huck'],2008-11-21T14:40:26Z,A Note on Coincidence Isometries of Modules in Euclidean Space,  It is shown that the coincidence isometries of certain modules in Euclidean $n$-space can be decomposed into a product of at most $n$ coincidence reflections defined by their non-zero elements. This generalizes previous results obtained for lattices to situations that are relevant in quasicrystallography. ,0811.3551v1
['Christian Huck'],2008-11-21T14:25:30Z,On the Existence of $U$-Polygons of Class $c\geq 4$ in Planar Point Sets,"  For a finite set $U$ of directions in the Euclidean plane, a convex non-degenerate polygon $P$ is called a $U$-polygon if every line parallel to a direction of $U$ that meets a vertex of $P$ also meets another vertex of $P$. We characterize the numbers of edges of $U$-polygons of class $c\geq4$ with all their vertices in certain subsets of the plane and derive explicit results in the case of cyclotomic model sets. ",0811.3546v1
['Christian Huck'],2007-11-28T16:06:32Z,Uniqueness in Discrete Tomography of Delone Sets with Long-Range Order,"  We address the problem of determining finite subsets of Delone sets $\varLambda\subset\R^d$ with long-range order by $X$-rays in prescribed $\varLambda$-directions, i.e., directions parallel to non-zero interpoint vectors of $\varLambda$. Here, an $X$-ray in direction $u$ of a finite set gives the number of points in the set on each line parallel to $u$. For our main result, we introduce the notion of algebraic Delone sets $\varLambda\subset\R^2$ and derive a sufficient condition for the determination of the convex subsets of these sets by $X$-rays in four prescribed $\varLambda$-directions. ",0711.4525v2
['Christian Huck'],2012-11-27T14:42:50Z,Magic numbers in the discrete tomography of cyclotomic model sets,"  We report recent progress in the problem of distinguishing convex subsets of cyclotomic model sets $\varLambda$ by (discrete parallel) X-rays in prescribed $\varLambda$-directions. It turns out that for any of these model sets $\varLambda$ there exists a `magic number' $m_{\varLambda}$ such that any two convex subsets of $\varLambda$ can be distinguished by their X-rays in any set of $m_{\varLambda}$ prescribed $\varLambda$-directions. In particular, for pentagonal, octagonal, decagonal and dodecagonal model sets, the least possible numbers are in that very order 11, 9, 11 and 13. ",1211.6318v1
['Christian Huck'],2017-12-08T10:40:52Z,On the logarithmic probability that a random integral ideal is $\mathscr   A$-free,"  This extends a theorem of Davenport and Erd\""os on sequences of rational integers to sequences of integral ideals in arbitrary number fields $K$. More precisely, we introduce a logarithmic density for sets of integral ideals in $K$ and provide a formula for the logarithmic density of the set of so-called $\mathscr A$-free ideals, i.e. integral ideals that are not multiples of any ideal from a fixed set $\mathscr A$. ",1712.03015v1
['Christian Huck'],2007-01-04T15:27:02Z,Uniqueness in Discrete Tomography of Planar Model Sets,"  The problem of determining finite subsets of characteristic planar model sets (mathematical quasicrystals) $\varLambda$, called cyclotomic model sets, by parallel $X$-rays is considered. Here, an $X$-ray in direction $u$ of a finite subset of the plane gives the number of points in the set on each line parallel to $u$. For practical reasons, only $X$-rays in $\varLambda$-directions, i.e., directions parallel to non-zero elements of the difference set $\varLambda - \varLambda$, are permitted. In particular, by combining methods from algebraic number theory and convexity, it is shown that the convex subsets of a cyclotomic model set $\varLambda$, i.e., finite sets $C\subset \varLambda$ whose convex hulls contain no new points of $\varLambda$, are determined, among all convex subsets of $\varLambda$, by their $X$-rays in four prescribed $\varLambda$-directions, whereas any set of three $\varLambda$-directions does not suffice for this purpose. We also study the interactive technique of successive determination in the case of cyclotomic model sets, in which the information from previous $X$-rays is used in deciding the direction for the next $X$-ray. In particular, it is shown that the finite subsets of any cyclotomic model set $\varLambda$ can be successively determined by two $\varLambda$-directions. All results are illustrated by means of well-known examples, i.e., the cyclotomic model sets associated with the square tiling, the triangle tiling, the tiling of Ammann-Beenker, the T\""ubingen triangle tiling and the shield tiling. ",math/0701141v2
"['Michael Baake', 'Christian Huck']",2006-10-23T15:34:27Z,Discrete Tomography of Penrose Model Sets,  Various theoretical and algorithmic aspects of inverse problems in discrete tomography of planar Penrose model sets are discussed. These are motivated by the demand of materials science for the reconstruction of quasicrystalline structures from a small number of images produced by quantitative high resolution transmission electron microscopy. ,math-ph/0610056v1
"['Christian Huck', 'Michael Baake']",2014-02-10T16:31:44Z,Dynamical properties of $k$-free lattice points,"  We revisit the visible points of a lattice in Euclidean $n$-space together with their generalisations, the $k$th-power-free points of a lattice, and study the corresponding dynamical system that arises via the closure of the lattice translation orbit. Our analysis extends previous results obtained by Sarnak and by Cellarosi and Sinai for the special case of square-free integers and sheds new light on previous joint work with Peter Pleasants. ",1402.2202v1
"['Peter A. B. Pleasants', 'Christian Huck']",2011-12-07T17:07:41Z,Entropy and diffraction of the $k$-free points in $n$-dimensional   lattices,  We consider the $k$th-power-free points in $n$-dimensional lattices and explicitly calculate their entropies and diffraction spectra. This is of particular interest since these sets have holes of unbounded inradius. ,1112.1629v3
"['Michael Baake', 'Christian Huck']",2015-01-06T15:24:52Z,Ergodic properties of visible lattice points,"  Recently, the dynamical and spectral properties of square-free integers, visible lattice points and various generalisations have received increased attention. One reason is the connection of one-dimensional examples such as $\mathscr B$-free numbers with Sarnak's conjecture on the `randomness' of the M\""obius function, another the explicit computability of correlation functions as well as eigenfunctions for these systems together with intrinsic ergodicity properties. Here, we summarise some of the results, with focus on spectral and dynamical aspects, and expand a little on the implications for mathematical diffraction theory. ",1501.01198v2
"['Christian Huck', 'Michael Spiess']",2011-01-21T14:44:51Z,Solution of a uniqueness problem in the discrete tomography of algebraic   Delone sets,"  We consider algebraic Delone sets $\varLambda$ in the Euclidean plane and address the problem of distinguishing convex subsets of $\varLambda$ by X-rays in prescribed $\varLambda$-directions, i.e., directions parallel to nonzero interpoint vectors of $\varLambda$. Here, an X-ray in direction $u$ of a finite set gives the number of points in the set on each line parallel to $u$. It is shown that for any algebraic Delone set $\varLambda$ there are four prescribed $\varLambda$-directions such that any two convex subsets of $\varLambda$ can be distinguished by the corresponding X-rays. We further prove the existence of a natural number $c_{\varLambda}$ such that any two convex subsets of $\varLambda$ can be distinguished by their X-rays in any set of $c_{\varLambda}$ prescribed $\varLambda$-directions. In particular, this extends a well-known result of Gardner and Gritzmann on the corresponding problem for planar lattices to nonperiodic cases that are relevant in quasicrystallography. ",1101.4149v1
"['Christian Huck', 'Christoph Richard']",2014-12-19T11:58:44Z,On pattern entropy of weak model sets,"  We study point sets arising from cut-and-project constructions. An important class is weak model sets, which include squarefree numbers and visible lattice points. For such model sets, we give a non-trivial upper bound on their pattern entropy in terms of the volume of the window boundary in internal space. This proves a conjecture by R.V. Moody. ",1412.6307v4
"['Christian Huck', 'Markus Moll', 'Johan Nilsson']",2014-02-10T15:23:06Z,Discrete tomography: Magic numbers for $N$-fold symmetry,"  We consider the problem of distinguishing convex subsets of $n$-cyclotomic model sets $\varLambda$ by (discrete parallel) X-rays in prescribed $\varLambda$-directions. In this context, a `magic number' $m_{\varLambda}$ has the property that any two convex subsets of $\varLambda$ can be distinguished by their X-rays in any set of $m_{\varLambda}$ prescribed $\varLambda$-directions. Recent calculations suggest that (with one exception in the case $n=4$) the least possible magic number for $n$-cyclotomic model sets might just be $N+1$, where $N=\operatorname{lcm}(n,2)$. ",1402.2183v2
"['Michael Baake', 'Friedrich Götze', 'Christian Huck', 'Tobias Jakobi']",2014-02-12T13:49:28Z,Radial spacing distributions from planar points sets,"  In this paper, we explore the radial projection method for locally finite point sets and provide numerical examples for different types of order. The main question is whether the method is suitable to analyse order in a quantitive way. Our findings indicate that the answer is affermative. In this context, we also study local visibility conditions for certain types of aperiodic point sets. ",1402.2818v2
"['Michael Baake', 'Christian Huck', 'Nicolae Strungaru']",2015-12-22T15:44:02Z,On weak model sets of extremal density,"  The theory of regular model sets is highly developed, but does not cover examples such as the visible lattice points, the k-th power-free integers, or related systems. They belong to the class of weak model sets, where the window may have a boundary of positive measure, or even consists of boundary only. The latter phenomena are related to the topological entropy of the corresponding dynamical system and to various other unusual properties. Under a rather natural extremality assumption on the density of the weak model set we establish its pure point diffraction nature. We derive an explicit formula that can be seen as the generalisation of the case of regular model sets. Furthermore, the corresponding natural patch frequency measure is shown to be ergodic. Since weak model sets of extremal density are generic for this measure, one obtains that the dynamical spectrum of the hull is pure point as well. ",1512.07129v2
"['Michael Baake', 'Alvaro Bustos', 'Christian Huck', 'Mariusz Lemanczyk', 'Andreas Nickel']",2019-10-30T14:24:27Z,Number-theoretic positive entropy shifts with small centraliser and   large normaliser,"  Higher-dimensional binary shifts of number-theoretic origin with positive topological entropy are considered. We are particularly interested in analysing their symmetries and extended symmetries. They form groups, known as the topological centraliser and normaliser of the shift dynamical system, which are natural topological invariants. Here, our focus is on shift spaces with trivial centralisers, but large normalisers. In particular, we discuss several systems where the normaliser is an infinite extension of the centraliser, including the visible lattice points and the $k$-free integers in some real quadratic number fields. ",1910.13876v2
"['P. Alonso-Gonzalez', 'P. Albella', 'F. Neubrech', 'Christian Huck', 'J. Chen', 'F. Golmar', 'F. Casanova', 'L. E. Hueso', 'A. Pucci', 'J. Aizpurua', 'R. Hillenbrand']",2013-01-23T09:48:12Z,Experimental Verification of the Spectral Shift between Near- and   Far-Field Peak Intensities of Plasmonic Nanoantennas,  Theory predicts a distinct spectral shift between the near- and far-field optical responses of plasmonic antennas. Here we combine near-field optical microscopy and far-field spectroscopy of individual infrared-resonant nanoantennas to verify experimentally this spectral shift. Numerical calculations corroborate our experimental results. We furthermore discuss the implications of this effect in surface-enhanced infrared spectroscopy (SEIRS). ,1301.5448v1
"['Maria Caterina Giordano', 'Michael Tzschoppe', 'Matteo Barelli', 'Jochen Vogt', 'Christian Huck', 'Filippo Canepa', 'Annemarie Pucci', 'Francesco Buatier de Mongeot']",2020-04-05T11:53:59Z,Self-Organized Nanorod Arrays for Large-Area Surface-Enhanced Infrared   Absorption,"  Capabilities of highly sensitive surface-enhanced infrared absorption (SEIRA) spectroscopy are demonstrated by exploiting large-area templates ($cm^2$) based on self-organized (SO) nanorod antennas. We engineered highly dense arrays of gold nanorod antennas featuring polarization-sensitive localized plasmon resonances, tunable over a broadband near- and mid-infrared (IR) spectrum, in overlap with the so-called 'functional group' window. We demonstrate polarization-sensitive SEIRA activity, homogeneous over macroscopic areas and stable in time, by exploiting prototype self-assembled monolayers of IR-active octadecanthiol (ODT) molecules. The strong coupling between the plasmonic excitation and molecular stretching modes gives rise to characteristic Fano resonances in SEIRA. The SO engineering of the active hotspots in the arrays allows us to achieve signal amplitude improved up to 5.7%. This figure is competitive to the response of lithographic nanoantennas and is stable when the optical excitation spot varies from the micro- to macroscale, thus enabling highly sensitive SEIRA spectroscopy with cost-effective nanosensor devices. ",2004.02169v1
"['Danish Patel', 'Prateek Gupta', 'Carlo Scalo', 'Thomas Rothermel', 'Markus Kuhn']",2017-01-29T15:44:16Z,Towards Impedance Characterization of Carbon-Carbon Ultrasonically   Absorptive Cavities via the Inverse Helmholtz Problem,"  We present a numerical method to determine the complex acoustic impedance at the open surface of an arbitrarily shaped cavity, associated to an impinging planar acoustic wave with a given wavenumber vector and frequency. We have achieved this by developing the first inverse Helmholtz Solver (iHS), which implicitly reconstructs the complex acoustic waveform--at a given frequency--up to the unknown impedance boundary, hereby providing the spatial distribution of impedance as a result of the calculation for that given frequency. We show that the algebraic closure conditions required by the inverse Helmholtz problem are physically related to the assignment of the spatial distribution of the pressure phase over the unknown impedance boundary. The iHS is embarrassingly parallelizable over the frequency domain allowing for the straightforward determination of the full broadband impedance at every point of the target boundary. In this paper, we restrict our analysis to two-dimensions only. We first validate our results against Rott's quasi one-dimensional thermoacoustic theory for viscid and inviscid constant-area rectangular ducts, test our iHS in a fully unstructured fashion with a geometrically complex cavity, and finally, present a simplified, two-dimensional analysis of a sample of carbon-carbon ultrasonically absorptive coatings (C/C UACs) manufactured in DLR-Stuttgart, and used in the hypersonic transition delay experiments by Wagner et al. in AIAA 2012-5865. The final goal is to model C/C UACs with multi-oscillator Time Domain Impedance Boundary Conditions (TDIBC) developed by Lin et al. in JFM (2016) to be applied in direct numerical simulations (DNS) focused on the overlying boundary layer, eliminating the need to simultaneously resolve the microscopic porous structure of the C/C UACs. ",1701.08391v1
"['André Beauducel', 'Anja Leue', 'Norbert Hilger']",2016-09-18T06:44:38Z,Treating reflective indicators as causal-formative indicators in order   to compute factor score estimates or unit-weighted scales,"  Individual scores on common factors are required in some applied settings (e.g., business and marketing settings). Common factors are based on reflective indicators, but their scores cannot unambiguously be determined. Therefore, factor score estimates and unit-weighted scales are used in order to provide individual scores. It is shown that these scores are based on treating the reflective indicators as if they were causal-formative indicators. This modification of the causal status of the indicators should be justified. Therefore, the fit of the models implied by factor score estimates and unit-weighted scales should be investigated in order to ascertain the validity of the scores. ",1609.05430v2
"['Bernd Simon', 'Andrea Ortiz', 'Walid Saad', 'Anja Klein']",2023-09-19T13:07:15Z,Decentralized Online Learning in Task Assignment Games for Mobile   Crowdsensing,"  The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative ""free-sensing"" mechanism significantly improves the MU's learning process while reducing collisions during task allocation. The stable regret of CA-MAB-SFS, i.e., the loss of learning, is analytically shown to be bounded by a sublinear function, ensuring the convergence to a stable optimal solution. Simulation results show that CA-MAB-SFS increases the MUs' and the MCSP's satisfaction compared to state-of-the-art methods while reducing the average task completion time by at least 16%. ",2309.10594v1
"['Mahdi Chehimi', 'Bernd Simon', 'Walid Saad', 'Anja Klein', 'Don Towsley', 'Mérouane Debbah']",2023-05-22T03:39:18Z,Matching Game for Optimized Association in Quantum Communication   Networks,"  Enabling quantum switches (QSs) to serve requests submitted by quantum end nodes in quantum communication networks (QCNs) is a challenging problem due to the heterogeneous fidelity requirements of the submitted requests and the limited resources of the QCN. Effectively determining which requests are served by a given QS is fundamental to foster developments in practical QCN applications, like quantum data centers. However, the state-of-the-art on QS operation has overlooked this association problem, and it mainly focused on QCNs with a single QS. In this paper, the request-QS association problem in QCNs is formulated as a matching game that captures the limited QCN resources, heterogeneous application-specific fidelity requirements, and scheduling of the different QS operations. To solve this game, a swap-stable request-QS association (RQSA) algorithm is proposed while considering partial QCN information availability. Extensive simulations are conducted to validate the effectiveness of the proposed RQSA algorithm. Simulation results show that the proposed RQSA algorithm achieves a near-optimal (within 5%) performance in terms of the percentage of served requests and overall achieved fidelity, while outperforming benchmark greedy solutions by over 13%. Moreover, the proposed RQSA algorithm is shown to be scalable and maintain its near-optimal performance even when the size of the QCN increases. ",2305.12682v1
"['Baishun Li', 'Guoliang Li', 'Jun Cheng', 'John Peterson', 'Wei Cui']",2016-04-25T04:36:00Z,The Point Spread Function Reconstruction by Using Moffatlets - I,"  The shear measurement is a crucial task in the current and the future weak lensing survey projects. And the reconstruction of the point spread function(PSF) is one of the essential steps. In this work, we present three different methods, including Gaussianlets, Moffatlets and EMPCA to quantify their efficiency on PSF reconstruction using four sets of simulated LSST star images. Gaussianlets and Moffatlets are two different sets of basis functions whose profiles are based on Gaussian and Moffat functions respectively. Expectation Maximization(EM) PCA is a statistical method performing iterative procedure to find principal components of an ensemble of star images. Our tests show that: 1) Moffatlets always perform better than Gaussianlets. 2) EMPCA is more compact and flexible, but the noise existing in the Principal Components (PCs) will contaminate the size and ellipticity of PSF while Moffatlets keeps them very well. ",1604.07126v1
