ORCID,Author,Institution,Title,PubDate,Abstract,DOI,IDlist,flag,found_words
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",Consistent coupling of positions and rotations for embedding 1D Cosserat   beams into 3D solid volumes,1970,"  This article proposes a mortar type finite element formulation for consistently embedding curved, slender beams, i.e. 1D Cosserat continua, into 3D solid volumes. A consistent 1D-3D coupling scheme for this problem type is proposed, which enforces both positional and rotational constraints. Since Boltzmann continua exhibit no inherent rotational degrees of freedom, suitable definitions of orthonormal triads are investigated that are representative for the orientation of material directions in the 3D solid. The rotation tensor defined by the polar decomposition of the deformation gradient is demonstrated to represent these material directions in a L2-optimal manner. Subsequently, objective rotational coupling constraints between beam and solid are formulated and enforced in a variationally consistent framework. Eventually, finite element discretization of all primary fields results in an embedded mortar formulation for rotational and translational constraint enforcement. Based on carefully chosen numerical test cases, the proposed scheme is demonstrated to exhibit a consistent spatial convergence behavior and to offer the up-scaling potential for studying real-life engineering applications such as fiber-reinforced composite materials. ",https://doi.org/10.1007/s00466-021-02111-4,2107.11151v1,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",A novel mesh regularization approach based on finite element distortion   potentials: Application to material expansion processes with extreme volume   change,1970,"  The accuracy of finite element solutions is closely tied to the mesh quality. In particular, geometrically nonlinear problems involving large and strongly localized deformations often result in prohibitively large element distortions. In this work, we propose a novel mesh regularization approach allowing to restore a non-distorted high-quality mesh in an adaptive manner without the need for expensive re-meshing procedures. The core idea of this approach lies in the definition of a finite element distortion potential considering contributions from different distortion modes such as skewness and aspect ratio of the elements. The regularized mesh is found by minimization of this potential. Moreover, based on the concept of spatial localization functions, the method allows to specify tailored requirements on mesh resolution and quality for regions with strongly localized mechanical deformation and mesh distortion. In addition, while existing mesh regularization schemes often keep the boundary nodes of the discretization fixed, we propose a mesh-sliding algorithm based on variationally consistent mortar methods allowing for an unrestricted tangential motion of nodes along the problem boundary. Especially for problems involving significant surface deformation (e.g., frictional contact), this approach allows for an improved mesh relaxation as compared to schemes with fixed boundary nodes. To transfer data such as tensor-valued history variables of the material model from the old (distorted) to the new (regularized) mesh, a structure-preserving invariant interpolation scheme for second-order tensors is employed, which has been proposed in our previous work and is designed to preserve important mechanical properties of tensor-valued data such as objectivity and positive definiteness... {continued see pdf} ",Kein DOI-Link verfügbar,2307.07582v2,Yes,potent(2)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)","Generalized Section-Section Interaction Potentials in the Geometrically   Exact Beam Theory: Modeling of Intermolecular Forces, Asymptotic Limit as   Strain-Energy Function, and Formulation of Rotational Constraints",1970,"  The present contribution proposes a universal framework to formulate generalized section-section interaction potentials (SSIP) within the geometrically exact beam theory. By exploiting the fundamental kinematic assumption of undeformable cross-sections, an objective (i.e., frame-invariant) description of SSIPs via a minimal set of six (translational and rotational) relative coordinates, either in spatial or in material form, is proposed. Based on work-pairing, work-conjugated section-section interaction forces and moments, either in spatial or in material form, are identified that can be consistently derived from a variational principle. Interestingly, it is shown that hyperelastic stored-energy functions relating the deformation measures and stress-resultants of the well-known geometrically exact Simo-Reissner beam theory can also be identified as SSIPs when considering the asymptotic limit of small relative distances and rotations between the interacting cross-sections. Moreover, the proposed variational problem formulation is demonstrated to be of a very general nature, thus allowing for the formulation of translational and rotational constraints between arbitrarily oriented cross-sections based on either a penalty or a Lagrange multiplier potential. Possible applications include fiber-based structures and materials in technical and biological systems, where the proposed approach allows to model short- or long-ranged inter-molecular (e.g., electrostatic, van der Waals or repulsive steric) interactions between fibers in geometrically complex arrangements and to formulate translational and rotational coupling constraints between different fibers (e.g., cross-linked polymer chains) or between fibers and a matrix phase (e.g., fiber-reinforced composites). ",Kein DOI-Link verfügbar,2105.10032v2,Yes,potent(2)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",From RAGs to riches: Using large language models to write documents for   clinical trials,1970,"  Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing. ",Kein DOI-Link verfügbar,2402.16406v1,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)","Analytical disk-cylinder interaction potential laws for the   computational modeling of adhesive, deformable (nano)fibers",1970,"  The analysis of complex fibrous systems or materials on the micro- and nanoscale, which have a high practical relevance for many technical or biological systems, requires accurate analytical descriptions of the adhesive and repulsive forces acting on the fiber surfaces. While such analytical expressions are generally needed both for theoretical studies and for computer-based simulations, the latter motivates us here to derive disk-cylinder interaction potential laws that are valid for arbitrary mutual orientations in the decisive regime of small surface separations. The chosen type of fundamental point-pair interaction follows the simple Lennard-Jones model with inverse power laws for both the adhesive van der Waals part and the steric, repulsive part. We present three different solutions, ranging from highest accuracy to the best trade-off between simplicity of the expression and sufficient accuracy for our intended use. The validity of simplifying approximations and the accuracy of the derived potential laws is thoroughly analyzed, using both numerical and analytical reference solutions for specific interaction cases. Most importantly, the correct asymptotic scaling behavior in the decisive regime of small separations is achieved, and also the theoretically predicted $(1\!/\!\sin\!\alpha)$-angle dependence (for non-parallel cylinders) is obtained by the proposed analytical solutions. As we show in the outlook to our current research, the derived analytical disk-cylinder interaction potential laws may be used to formulate highly efficient computational models for the interaction of arbitrarily curved fibers, such that the disk represents the cross-section of the first and the cylinder a local approximation to the shape of the second fiber. ",Kein DOI-Link verfügbar,2208.03074v3,Yes,potent(3)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)","A Computational Model for Molecular Interactions Between Curved Slender   Fibers Undergoing Large 3D Deformations With a Focus on Electrostatic, van   der Waals and Repulsive Steric Forces",1970,"  This contribution proposes the first 3D beam-to-beam interaction model for molecular interactions between curved slender fibers undergoing large deformations. While the general model is not restricted to a specific beam formulation, in the present work it is combined with the geometrically exact beam theory and discretized via the finite element method. A direct evaluation of the total interaction potential for general 3D bodies requires the integration of contributions from molecule or charge distributions over the volumes of the interaction partners, leading to a 6D integral (two nested 3D integrals) that has to be solved numerically. Here, we propose a novel strategy to formulate reduced section-to-section interaction laws for the resultant interaction potential between a pair of cross-sections of two slender fibers such that only two 1D integrals along the fibers' length directions have to be solved numerically. This section-to-section interaction potential (SSIP) approach yields a significant gain in efficiency, which is essential to enable the simulation of relevant time and length scales for many practical applications. In a first step, the generic structure of SSIP laws, which is suitable for the most general interaction scenario (e.g. fibers with arbitrary cross-section shape and inhomogeneous atomic/charge density within the cross-section) is presented. Assuming circular, homogeneous cross-sections, in a next step, specific analytical expressions for SSIP laws describing short-range volume interactions (e.g. van der Waals or steric interactions) and long-range surface interactions (e.g. Coulomb interactions) are proposed. The validity of the SSIP laws as well as the accuracy and robustness of the general SSIP approach to beam-to-beam interactions is thoroughly verified by means of a set of numerical examples considering steric repulsion, electrostatic or van der Waals adhesion. ",Kein DOI-Link verfügbar,1907.12997v2,Yes,potent(3)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",Asymptotically consistent and computationally efficient modeling of   short-ranged molecular interactions between curved slender fibers undergoing   large 3D deformations,1970,"  This article proposes a novel computational modeling approach for short-ranged molecular interactions between curved slender fibers undergoing large 3D deformations, and gives a detailed overview how it fits into the framework of existing fiber or beam interaction models, either considering microscale molecular or macroscale contact effects. The direct evaluation of a molecular interaction potential between two general bodies in 3D space would require to integrate molecule densities over two 3D volumes, leading to a sixfold integral to be solved numerically. By exploiting the short-range nature of the considered class of interaction potentials as well as the fundamental kinematic assumption of undeformable fiber cross-sections, as typically applied in mechanical beam theories, a recently derived, closed-form analytical solution is applied for the interaction potential between a given section of the first fiber (slave beam) and the entire second fiber (master beam). This novel approach based on a pre-defined section-beam interaction potential (SBIP) requires only one single integration step along the slave beam length to be performed numerically. In terms of accuracy, the total beam-beam interaction potential resulting from this approach is shown to exhibit an asymptotically consistent angular and distance scaling behavior. In addition to elementary two-fiber systems, carefully chosen to verify accuracy and asymptotic consistence of the proposed SBIP approach, a potential practical application in form of adhesive nanofiber-grafted surfaces is studied. Involving a large number of helicoidal fibers undergoing large 3D deformations, arbitrary mutual fiber orientations as well as frequent local fiber pull-off and snap-into-contact events, this example demonstrates the robustness and computational efficiency of the new approach. ",Kein DOI-Link verfügbar,2208.03149v4,Yes,potent(6)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)","Thermophysical Phenomena in Metal Additive Manufacturing by Selective   Laser Melting: Fundamentals, Modeling, Simulation and Experimentation",1970,"  Among the many additive manufacturing (AM) processes for metallic materials, selective laser melting (SLM) is arguably the most versatile in terms of its potential to realize complex geometries along with tailored microstructure. However, the complexity of the SLM process, and the need for predictive relation of powder and process parameters to the part properties, demands further development of computational and experimental methods. This review addresses the fundamental physical phenomena of SLM, with a special emphasis on the associated thermal behavior. Simulation and experimental methods are discussed according to three primary categories. First, macroscopic approaches aim to answer questions at the component level and consider for example the determination of residual stresses or dimensional distortion effects prevalent in SLM. Second, mesoscopic approaches focus on the detection of defects such as excessive surface roughness, residual porosity or inclusions that occur at the mesoscopic length scale of individual powder particles. Third, microscopic approaches investigate the metallurgical microstructure evolution resulting from the high temperature gradients and extreme heating and cooling rates induced by the SLM process. Consideration of physical phenomena on all of these three length scales is mandatory to establish the understanding needed to realize high part quality in many applications, and to fully exploit the potential of SLM and related metal AM processes. ",https://doi.org/10.1615/AnnualRevHeatTransfer.2018019042,1709.09510v1,Yes,"versatile(1), potent(2)"
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",A novel smoothed particle hydrodynamics and finite element coupling   scheme for fluid-structure interaction: the sliding boundary particle   approach,1970,"  A novel numerical formulation for solving fluid-structure interaction (FSI) problems is proposed where the fluid field is spatially discretized using smoothed particle hydrodynamics (SPH) and the structural field using the finite element method (FEM). As compared to fully mesh- or grid-based FSI frameworks, due to the Lagrangian nature of SPH this framework can be easily extended to account for more complex fluids consisting of multiple phases and dynamic phase transitions. Moreover, this approach facilitates the handling of large deformations of the fluid domain respectively the fluid-structure interface without additional methodological and computational efforts. In particular, to achieve an accurate representation of interaction forces between fluid particles and structural elements also for strongly curved interface geometries, the novel sliding boundary particle approach is proposed to ensure full support of SPH particles close to the interface. The coupling of the fluid and the structural field is based on a Dirichlet-Neumann partitioned approach, where the fluid field is the Dirichlet partition with prescribed interface displacements and the structural field is the Neumann partition subject to interface forces. To overcome instabilities inherent to weakly coupled schemes an iterative fixed-point coupling scheme is employed. Several numerical examples in form of well-known benchmark tests are considered to validate the accuracy, stability, and robustness of the proposed formulation. Finally, the filling process of a highly flexible thin-walled balloon-like container is studied, representing a model problem close to potential application scenarios of the proposed scheme in the field of biomechanics. ",https://doi.org/10.1016/j.cma.2021.113922,2010.09526v2,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",An SPH framework for fluid-solid and contact interaction problems   including thermo-mechanical coupling and reversible phase transitions,1970,"  The present work proposes an approach for fluid-solid and contact interaction problems including thermo-mechanical coupling and reversible phase transitions. The solid field is assumed to consist of several arbitrarily-shaped, undeformable but mobile rigid bodies, that are evolved in time individually and allowed to get into mechanical contact with each other. The fluid field generally consists of multiple liquid or gas phases. All fields are spatially discretized using the method of smoothed particle hydrodynamics (SPH). This approach is especially suitable in the context of continually changing interface topologies and dynamic phase transitions without the need for additional methodological and computational effort for interface tracking as compared to mesh- or grid-based methods. Proposing a concept for the parallelization of the computational framework, in particular concerning a computationally efficient evaluation of rigid body motion, is an essential part of this work. Finally, the accuracy and robustness of the proposed framework is demonstrated by several numerical examples in two and three dimensions, involving multiple rigid bodies, two-phase flow, and reversible phase transitions, with a focus on two potential application scenarios in the fields of engineering and biomechanics: powder bed fusion additive manufacturing (PBFAM) and disintegration of food boluses in the human stomach. The efficiency of the parallel computational framework is demonstrated by a strong scaling analysis. ",https://doi.org/10.1186/s40323-021-00200-w,2103.07925v1,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",A highly efficient computational approach for part-scale microstructure   predictions in Ti-6Al-4V additive manufacturing,1970,"  Fast and efficient simulations of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique. The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties. When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes. This article proposes a scan-resolved approach to the coupled thermo-microstructural problem. Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\alpha_s$-phase, martensite $\alpha_m$-phase and $\beta$-phase. The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions. A performance model and numerical examples verify the high degree of optimization. We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and geometry. Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days. The numerical examples include a prediction of the microstructure on the full NIST AM Benchmark cantilever specimen. ",Kein DOI-Link verfügbar,2402.17580v1,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",A highly efficient computational framework for fast scan-resolved   simulations of metal additive manufacturing processes on the scale of real   parts,1970,"  This article proposes a novel high-performance computing approach for the prediction of the temperature field in powder bed fusion (PBF) additive manufacturing processes. In contrast to many existing approaches to part-scale simulations, the underlying computational model consistently resolves physical scan tracks without additional heat source scaling, agglomeration strategies or any other heuristic modeling assumptions. A growing, adaptively refined mesh accurately captures all details of the laser beam motion. Critically, the fine spatial resolution required for resolved scan tracks in combination with the high scan velocities underlying these processes mandates the use of comparatively small time steps to resolve the underlying physics. Explicit time integration schemes are well-suited for this setting, while unconditionally stable implicit time integration schemes are employed for the interlayer cool down phase governed by significantly larger time scales. These two schemes are combined and implemented in an efficient fast operator evaluation framework providing significant performance gains and optimization opportunities. The capabilities of the novel framework are demonstrated through realistic AM examples on the centimeter scale including the first scan-resolved simulation of the entire NIST AM Benchmark cantilever specimen, with a computation time of less than one day. Apart from physical insights gained through these simulation examples, also numerical aspects are thoroughly studied on basis of weak and strong parallel scaling tests. As potential applications, the proposed thermal PBF simulation framework can serve as a basis for microstructure and thermo-mechanical predictions on the part-scale, but also to assess the influence of scan pattern and part geometry on melt pool shape and temperature, which are important indicators for well-known process instabilities. ",Kein DOI-Link verfügbar,2302.05164v3,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)","A Versatile SPH Modeling Framework for Coupled Microfluid-Powder   Dynamics in Additive Manufacturing: Binder Jetting, Material Jetting,   Directed Energy Deposition and Powder Bed Fusion",1970,"  Many additive manufacturing (AM) technologies rely on powder feedstock, which is fused to form the final part either by melting or by chemical binding with subsequent sintering. In both cases, process stability and resulting part quality depend on dynamic interactions between powder particles and a fluid phase, i.e., molten metal or liquid binder. The present work proposes a versatile computational modeling framework for simulating such coupled microfluid-powder dynamics problems involving thermo-capillary flow and reversible phase transitions. In particular, a liquid and a gas phase are interacting with a solid phase that consists of a substrate and mobile powder particles while simultaneously considering temperature-dependent surface tension and wetting effects. In case of laser-metal interactions, the effect of rapid evaporation is incorporated through additional mechanical and thermal interface fluxes. All phase domains are spatially discretized using smoothed particle hydrodynamics. The method's Lagrangian nature is beneficial in the context of dynamically changing interface topologies. Special care is taken in the formulation of phase transitions, which is crucial for the robustness of the computational scheme. While the underlying model equations are of a very general nature, the proposed framework is especially suitable for the mesoscale modeling of various AM processes. To this end, the generality and robustness of the computational modeling framework is demonstrated by several application-motivated examples representing the specific AM processes binder jetting, material jetting, directed energy deposition, and powder bed fusion. Among others, it is shown how the dynamic impact of droplets in binder jetting or the evaporation-induced recoil pressure in powder bed fusion leads to powder motion, distortion of the powder packing structure, and powder particle ejection. ",Kein DOI-Link verfügbar,2201.01677v2,Yes,versatile(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",Patient-specific coronary angioplasty simulations -- a mixed-dimensional   finite element modeling approach,1970,"  Coronary angioplasty with stent implantation is the most frequently used interventional treatment for coronary artery disease. However, reocclusion within the stent, referred to as in-stent restenosis, occurs in up to 10% of lesions. It is widely accepted that mechanical loads on the vessel wall strongly affect adaptive and maladaptive mechanisms. Yet, the role of procedural and lesion-specific influence on restenosis risk remains understudied. Computational modeling of the stenting procedure can provide new mechanistic insights, such as local stresses, that play a significant role in tissue growth and remodeling. Previous simulation studies often featured simplified artery and stent geometries and cannot be applied to real-world examples. Realistic simulations were computationally expensive since they featured fully resolved stenting device models. The aim of this work is to develop and present a mixed-dimensional formulation to simulate the patient-specific stenting procedure with a reduced-dimensional beam model for the stent and 3D models for the artery. In addition to presenting the numerical approach, we apply it to realistic cases to study the intervention's mechanical effect on the artery and correlate the findings with potential high-risk locations for in-stent restenosis. We found that high artery wall stresses develop during the coronary intervention in severely stenosed areas and at the stent boundaries. Herewith, we lay the groundwork for further studies towards preventing in-stent restenosis after coronary angioplasty. ",Kein DOI-Link verfügbar,2407.13276v1,Yes,potent(1)
0000-0002-3501-1696,Christoph Meier,"Massachusetts Institute of Technology (MIT), Technical University of Munich (TUM)",Physics-Based Modeling and Predictive Simulation of Powder Bed Fusion   Additive Manufacturing Across Length Scales,1970,"  Powder bed fusion additive manufacturing (PBFAM) of metals has the potential to enable new paradigms of product design, manufacturing and supply chains while accelerating the realization of new technologies in the medical, aerospace, and other industries. Currently, wider adoption of PBFAM is held back by difficulty in part qualification, high production costs and low production rates, as extensive process tuning, post-processing, and inspection are required before a final part can be produced and deployed. Physics-based modeling and predictive simulation of PBFAM offers the potential to advance fundamental understanding of physical mechanisms that initiate process instabilities and cause defects. In turn, these insights can help link process and feedstock parameters with resulting part and material properties, thereby predicting optimal processing conditions and inspiring the development of improved processing hardware, strategies and materials. This work presents recent developments of our research team in the modeling of metal PBFAM processes spanning length scales, namely mesoscale powder modeling, mesoscale melt pool modeling, macroscale thermo-solid-mechanical modeling and microstructure modeling. Ongoing work in experimental validation of these models is also summarized. In conclusion, we discuss the interplay of these individual submodels within an integrated overall modeling approach, along with future research directions. ",https://doi.org/10.1002/gamm.202100014,2103.16982v2,Yes,potent(2)
0000-0001-6252-7658,Matthew Baugh,"Imperial College London, Imperial College London Department of Computing",Trade-offs in Fine-tuned Diffusion Models Between Accuracy and   Interpretability,1970,"  Recent advancements in diffusion models have significantly impacted the trajectory of generative machine learning research, with many adopting the strategy of fine-tuning pre-trained models using domain-specific text-to-image datasets. Notably, this method has been readily employed for medical applications, such as X-ray image synthesis, leveraging the plethora of associated radiology reports. Yet, a prevailing concern is the lack of assurance on whether these models genuinely comprehend their generated content. With the evolution of text-conditional image generation, these models have grown potent enough to facilitate object localization scrutiny. Our research underscores this advancement in the critical realm of medical imaging, emphasizing the crucial role of interpretability. We further unravel a consequential trade-off between image fidelity as gauged by conventional metrics and model interpretability in generative diffusion models. Specifically, the adoption of learnable text encoders when fine-tuning results in diminished interpretability. Our in-depth exploration uncovers the underlying factors responsible for this divergence. Consequently, we present a set of design principles for the development of truly interpretable generative models. Code is available at https://github.com/MischaD/chest-distillation. ",Kein DOI-Link verfügbar,2303.17908v2,Yes,potent(1)
0000-0001-6252-7658,Matthew Baugh,"Imperial College London, Imperial College London Department of Computing",DISYRE: Diffusion-Inspired SYnthetic REstoration for Unsupervised   Anomaly Detection,1970,"  Unsupervised Anomaly Detection (UAD) techniques aim to identify and localize anomalies without relying on annotations, only leveraging a model trained on a dataset known to be free of anomalies. Diffusion models learn to modify inputs $x$ to increase the probability of it belonging to a desired distribution, i.e., they model the score function $\nabla_x \log p(x)$. Such a score function is potentially relevant for UAD, since $\nabla_x \log p(x)$ is itself a pixel-wise anomaly score. However, diffusion models are trained to invert a corruption process based on Gaussian noise and the learned score function is unlikely to generalize to medical anomalies. This work addresses the problem of how to learn a score function relevant for UAD and proposes DISYRE: Diffusion-Inspired SYnthetic REstoration. We retain the diffusion-like pipeline but replace the Gaussian noise corruption with a gradual, synthetic anomaly corruption so the learned score function generalizes to medical, naturally occurring anomalies. We evaluate DISYRE on three common Brain MRI UAD benchmarks and substantially outperform other methods in two out of the three tasks. ",Kein DOI-Link verfügbar,2311.15453v2,Yes,potent(1)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Different Regular Black Holes: Geodesic Structures of Test Particles,1970,"  This paper investigates the metric of previously proposed regular black holes, calculates their effective potentials, and plots the curves of the effective potentials. By determining the conserved quantities, the dynamical equations for particles and photons near the black hole are derived. The analysis encompasses timelike and null geodesics in different spacetimes, including bound geodesics, unstable circular geodesics, stable circular geodesics, and escape geodesics. The findings are presented through figures and tables. Furthermore, the bound geodesics of the four regular black hole spacetimes are analyzed, examining the average distance of particle orbits from the center of the event horizon, the precession behavior of the perihelion, and the probability of particles appearing inside the outer event horizon during motion. Based on these analyses, a general formula is proposed, which yields the existing metrics when specific parameter values are chosen. The impact of parameter variations on the effective potential and geodesics is then computed using this new formula. ",Kein DOI-Link verfügbar,2309.12932v1,Yes,potent(3)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Scalar field quasinormal modes of noncommutative high dimensional   Schwarzschild-Tangherlini black hole spacetime with smeared matter sources,1970,"  We investigate the massless scalar quasinormal modes (QNMs) of the noncommutative $D$-dimensional Schwarzschild-Tangherlini black hole spacetime in this paper. By using the Wentzel-Kramers-Brillouin (WKB) approximation method, the asymptotic iterative method (AIM) and the inverted potential method (IPM) method, we made a detail analysis of the massless scalar QNM frequencies by varying the general smeared matter distribution and the allowable characteristic parameters ($k$ and $\theta$) corresponding to different dimensions. It is found that the nonconvergence of the high order WKB approximation exists in the QNMs frequencies of scalar perturbation around the noncommutative $D$-dimensional Schwarzschild black holes. We conclude that the 3rd WKB result should be more reliable than those of the high order WKB method since our numerical results are also verified by the AIM method and the IPM method. In the dimensional range of $4\leq D \leq7$, the scalar QNMs as a function of the different papameters (the noncommutative parameter $\theta$, the smeared matter distribution parameter $k$, the multipole number $l$ and the main node number $n$) are obtained. Moreover, we study the dynamical evolution of a scalar field in the background of the noncommutative high dimensional Schwarzschild-Tangherlini black hole. ",https://doi.org/10.1016/j.nuclphysb.2020.115217,2012.00320v1,Yes,potent(1)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Quasinormal modes of scalar field coupled to Einstein's tensor in the   non-commutative geometry inspired black hole,1970,"  We investigate the quasinormal modes (QNMs) of the scalar field coupled to the Einstein's tensor in the non-commutative geometry inspired black hole spacetime. It is found that the lapse function of the non-commutative black hole metric can be represented by a Kummer's confluent hypergeometric function, which can effectively solve the problem that the numerical results of the QNMs are sensitive to the model parameters and make the QNMs values more reliable. We make a careful analysis of the scalar QNM frequencies by using several numerical methods, and find that the numerical results obtained by the new WKB method (the Pad\'e approximants) and the Mashhoon method (P$\ddot{\text{o}}$schl-Teller potential method) are quite different from those obtained by the asymptotic iterative method (AIM) and time-domain integration method when the non-commutative parameter $\theta$ and coupling parameter $\eta$ are large. The most obvious difference is that the numerical results obtained by the AIM and the time-domain integration method appear a critical value $\eta_c$ with an increase of $\eta$, which leads to the dynamical instability. After carefully analyzing the numeral results, we conclude that the numerical results obtained by the AIM and the time-domain integration method are closer to the theoretical values than those obtained by the WKB method and the Mashhoon method, when the $\theta$ and $\eta$ are large. Moreover, through a numerical fitting, we obtain that the functional relationship between the threshold $\eta_c$ and the non-commutative parameter $\theta$ satisfies $\eta_{c}=a\theta^{b}+c$ for a fixed $l$ approximately. We find that the stability of dynamics can be ensured in the $\eta<\eta_c(\theta, l)$ region. ",https://doi.org/10.1016/j.nuclphysb.2021.115595,2012.03004v2,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation,1970,"  We study the potential of noisy labels y to pretrain semantic segmentation models in a multi-modal learning framework for geospatial applications. Specifically, we propose a novel Cross-modal Sample Selection method (CromSS) that utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c modelled by multiple sensors/modalities d of a given geospatial scene. Consistency of predictions across sensors $d$ is jointly informed by the entropy of P^{(d)}(x,c). Noisy label sampling we determine by the confidence of each sensor d in the noisy class label, P^{(d)}(x,c=y(x)). To verify the performance of our approach, we conduct experiments with Sentinel-1 (radar) and Sentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12 dataset. We pair those scenes with 9-class noisy labels sourced from the Google Dynamic World project for pretraining. Transfer learning evaluations (downstream task) on the DFC2020 dataset confirm the effectiveness of the proposed method for remote sensing image segmentation. ",Kein DOI-Link verfügbar,2405.01217v1,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",AutoLCZ: Towards Automatized Local Climate Zone Mapping from Rule-Based   Remote Sensing,1970,"  Local climate zones (LCZs) established a standard classification system to categorize the landscape universe for improved urban climate studies. Existing LCZ mapping is guided by human interaction with geographic information systems (GIS) or modelled from remote sensing (RS) data. GIS-based methods do not scale to large areas. However, RS-based methods leverage machine learning techniques to automatize LCZ classification from RS. Yet, RS-based methods require huge amounts of manual labels for training.   We propose a novel LCZ mapping framework, termed AutoLCZ, to extract the LCZ classification features from high-resolution RS modalities. We study the definition of numerical rules designed to mimic the LCZ definitions. Those rules model geometric and surface cover properties from LiDAR data. Correspondingly, we enable LCZ classification from RS data in a GIS-based scheme. The proposed AutoLCZ method has potential to reduce the human labor to acquire accurate metadata. At the same time, AutoLCZ sheds light on the physical interpretability of RS-based methods. In a proof-of-concept for New York City (NYC) we leverage airborne LiDAR surveys to model 4 LCZ features to distinguish 10 LCZ types. The results indicate the potential of AutoLCZ as promising avenue for large-scale LCZ mapping from RS data. ",Kein DOI-Link verfügbar,2405.13993v1,Yes,potent(2)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",Naive Gabor Networks for Hyperspectral Image Classification,1970,"  Recently, many convolutional neural network (CNN) methods have been designed for hyperspectral image (HSI) classification since CNNs are able to produce good representations of data, which greatly benefits from a huge number of parameters. However, solving such a high-dimensional optimization problem often requires a large amount of training samples in order to avoid overfitting. Additionally, it is a typical non-convex problem affected by many local minima and flat regions. To address these problems, in this paper, we introduce naive Gabor Networks or Gabor-Nets which, for the first time in the literature, design and learn CNN kernels strictly in the form of Gabor filters, aiming to reduce the number of involved parameters and constrain the solution space, and hence improve the performances of CNNs. Specifically, we develop an innovative phase-induced Gabor kernel, which is trickily designed to perform the Gabor feature learning via a linear combination of local low-frequency and high-frequency components of data controlled by the kernel phase. With the phase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to automatically adapt to the local harmonic characteristics of the HSI data and thus yields more representative harmonic features. Also, this kernel can fulfill the traditional complex-valued Gabor filtering in a real-valued manner, hence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our newly developed Gabor-Nets on three well-known HSIs, suggesting that our proposed Gabor-Nets can significantly improve the performance of CNNs, particularly with a small training set. ",Kein DOI-Link verfügbar,1912.03991v2,Yes,innovative(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford","SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for   Self-Supervised Learning in Earth Observation",1970,"  Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12. ",Kein DOI-Link verfügbar,2211.07044v2,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",Deep Semantic Model Fusion for Ancient Agricultural Terrace Detection,1970,"  Discovering ancient agricultural terraces in desert regions is important for the monitoring of long-term climate changes on the Earth's surface. However, traditional ground surveys are both costly and limited in scale. With the increasing accessibility of aerial and satellite data, machine learning techniques bear large potential for the automatic detection and recognition of archaeological landscapes. In this paper, we propose a deep semantic model fusion method for ancient agricultural terrace detection. The input data includes aerial images and LiDAR generated terrain features in the Negev desert. Two deep semantic segmentation models, namely DeepLabv3+ and UNet, with EfficientNet backbone, are trained and fused to provide segmentation maps of ancient terraces and walls. The proposed method won the first prize in the International AI Archaeology Challenge. Codes are available at https://github.com/wangyi111/international-archaeology-ai-challenge. ",Kein DOI-Link verfügbar,2308.02225v1,Yes,potent(1)
0000-0002-3083-6613,Adam Baker,"University of Oxford, University of Oxford Worcester College",A comparative study of artificial intelligence and human doctors for the   purpose of triage and diagnosis,1970,"  Online symptom checkers have significant potential to improve patient care, however their reliability and accuracy remain variable. We hypothesised that an artificial intelligence (AI) powered triage and diagnostic system would compare favourably with human doctors with respect to triage and diagnostic accuracy. We performed a prospective validation study of the accuracy and safety of an AI powered triage and diagnostic system. Identical cases were evaluated by both an AI system and human doctors. Differential diagnoses and triage outcomes were evaluated by an independent judge, who was blinded from knowing the source (AI system or human doctor) of the outcomes. Independently of these cases, vignettes from publicly available resources were also assessed to provide a benchmark to previous studies and the diagnostic component of the MRCGP exam. Overall we found that the Babylon AI powered Triage and Diagnostic System was able to identify the condition modelled by a clinical vignette with accuracy comparable to human doctors (in terms of precision and recall). In addition, we found that the triage advice recommended by the AI System was, on average, safer than that of human doctors, when compared to the ranges of acceptable triage provided by independent expert judges, with only a minimal reduction in appropriateness. ",Kein DOI-Link verfügbar,1806.10698v1,Yes,potent(1)
0000-0002-4923-724X,Jinsheng Lu,"Harvard University, Zhejiang University",Adhesion-assisted nanoscale rotary locomotor in non-liquid environments,1970,"  Rotation in micro/nanoscale provides extensive applications in mechanical actuation$^{1, 2}$, cargo delivery$^{3, 4}$, and biomolecule manipulation$^{5, 6}$. Light can be used to induce a mechanical rotation remotely, instantly and precisely$^{7-13}$, where liquid throughout serves as a must-have enabler to suspend objects and remove impact of adhesion. Achieving light-driven motion in non-liquid environments faces formidable challenges, since micro-sized objects experience strong adhesion and intend to be stuck to contact surfaces. Adhesion force for a usual micron-sized object could reach a high value$^{14, 15}$ (nN - {\mu}N) which is several orders of magnitude higher than both its gravity (~ pN) and typical value of optical force (~ pN) in experiments$^{16}$. Here, in air and vacuum, we show counter-intuitive adhesion-assisted rotary locomotion of a micron-sized metal nanoplate with ~30 nm-thickness, revolving around a microfiber. This locomotor is powered by pulsed light guided into the fiber, as a coordinated consequence of photothermally induced surface acoustic wave on the nanoplate and favorable configuration of plate-fiber geometry. The locomotor crawls stepwise with sub-nanometer locomotion resolution actuated by designed light pulses. Furthermore, we can control the rotation velocity and step resolution by varying the repetition rate and pulse power, respectively. A light-actuated micromirror scanning with 0.001{\deg} resolution is then demonstrated based on this rotary locomotor. It unfolds unprecedented application potential for integrated micro-opto-electromechanical systems, outer-space all-optical precision mechanics and controls, laser scanning for miniature lidar systems, etc. ",Kein DOI-Link verfügbar,1804.08063v1,Yes,potent(1)
0000-0001-8270-3233,Harry Levine,Harvard University,Controlling many-body dynamics with driven quantum scars in Rydberg atom   arrays,1970,"  Controlling non-equilibrium quantum dynamics in many-body systems is an outstanding challenge as interactions typically lead to thermalization and a chaotic spreading throughout Hilbert space. We experimentally investigate non-equilibrium dynamics following rapid quenches in a many-body system composed of 3 to 200 strongly interacting qubits in one and two spatial dimensions. Using a programmable quantum simulator based on Rydberg atom arrays, we probe coherent revivals corresponding to quantum many-body scars. Remarkably, we discover that scar revivals can be stabilized by periodic driving, which generates a robust subharmonic response akin to discrete time-crystalline order. We map Hilbert space dynamics, geometry dependence, phase diagrams, and system-size dependence of this emergent phenomenon, demonstrating novel ways to steer entanglement dynamics in many-body systems and enabling potential applications in quantum information science. ",https://doi.org/10.1126/science.abg2530,2012.12276v1,Yes,potent(1)
0000-0001-8270-3233,Harry Levine,Harvard University,Probing Topological Spin Liquids on a Programmable Quantum Simulator,1970,"  Quantum spin liquids, exotic phases of matter with topological order, have been a major focus of explorations in physical science for the past several decades. Such phases feature long-range quantum entanglement that can potentially be exploited to realize robust quantum computation. We use a 219-atom programmable quantum simulator to probe quantum spin liquid states. In our approach, arrays of atoms are placed on the links of a kagome lattice and evolution under Rydberg blockade creates frustrated quantum states with no local order. The onset of a quantum spin liquid phase of the paradigmatic toric code type is detected by evaluating topological string operators that provide direct signatures of topological order and quantum correlations. Its properties are further revealed by using an atom array with nontrivial topology, representing a first step towards topological encoding. Our observations enable the controlled experimental exploration of topological quantum matter and protected quantum information processing. ",https://doi.org/10.1126/science.abi8794,2104.04119v1,Yes,potent(1)
0000-0003-2167-1623,Zhuocong Xiao,University of Cambridge,Ferrotronics for the creation of band gaps in Graphene,1970,"  We experimentally demonstrate a simple graphene/ ferrolectric device, termed Ferrotronic (electronic effect from ferroelectric) device in which the band-structure of single-layer graphene is modified. The device architecture consists of graphene deposited on a ferroelectric substrate which encodes a periodic surface potential achieved through domain engineering. This structure takes advantage of the nature of conduction through graphene to modulate the Fermi velocity of the charge carriers by the variations in surface potential, leading to the emergence of energy mini-bands and a band gap at the superlattice Brillouin zone boundary. Our work represents a simple route to building circuits whose functionality is controlled by the underlying substrate. ",Kein DOI-Link verfügbar,2112.07444v1,Yes,potent(2)
0000-0002-5552-4536,Eiko Yoneki,University of Cambridge,IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning   for Diverse Workloads,1970,"  This study introduces the Instance-Aware Index Advisor (IA2), a novel deep reinforcement learning (DRL)-based approach for optimizing index selection in databases facing large action spaces of potential candidates. IA2 introduces the Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference State-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index selection by understanding workload-index dependencies and employing adaptive action masking. This method includes a comprehensive workload model, enhancing its ability to adapt to unseen workloads and ensuring robust performance across diverse database environments. Evaluation on benchmarks such as TPC-H reveals IA2's suggested indexes' performance in enhancing runtime, securing a 40% reduction in runtime for complex TPC-H workloads compared to scenarios without indexes, and delivering a 20% improvement over existing state-of-the-art DRL-based index advisors. ",https://doi.org/10.1145/3642970.3655839,2404.05777v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its   applications on real-world medical records,1970,"  Federated machine learning is a versatile and flexible tool to utilize distributed data from different sources, especially when communication technology develops rapidly and an unprecedented amount of data could be collected on mobile devices nowadays. Federated learning method exploits not only the data but the computational power of all devices in the network to achieve more efficient model training. Nevertheless, while most traditional federated learning methods work well for homogeneous data and tasks, adapting the method to a different heterogeneous data and task distribution is challenging. This limitation has constrained the applications of federated learning in real-world contexts, especially in healthcare settings. Inspired by the fundamental idea of meta-learning, in this study we propose a new algorithm, which is an integration of federated learning and meta-learning, to tackle this issue. In addition, owing to the advantage of transfer learning for model generalization, we further improve our algorithm by introducing partial parameter sharing. We name this method partial meta-federated learning (PMFL). Finally, we apply the algorithms to two medical datasets. We show that our algorithm could obtain the fastest training speed and achieve the best performance when dealing with heterogeneous medical datasets. ",Kein DOI-Link verfügbar,2112.05321v2,Yes,versatile(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Pancreatic Cancer ROSE Image Classification Based on Multiple Instance   Learning with Shuffle Instances,1970,"  The rapid on-site evaluation (ROSE) technique can significantly ac-celerate the diagnostic workflow of pancreatic cancer by immediately analyzing the fast-stained cytopathological images with on-site pathologists. Computer-aided diagnosis (CAD) using the deep learning method has the potential to solve the problem of insufficient pathology staffing. However, the cancerous patterns of ROSE images vary greatly between different samples, making the CAD task extremely challenging. Besides, due to different staining qualities and various types of acquisition devices, the ROSE images also have compli-cated perturbations in terms of color distribution, brightness, and contrast. To address these challenges, we proposed a novel multiple instance learning (MIL) approach using shuffle patches containing the instances, which adopts the patch-based learning strategy of Vision Transformers. With the re-grouped bags of shuffle instances and their bag-level soft labels, the approach utilizes a MIL head to make the model focus on the features from the pancreatic cancer cells, rather than that from various perturbations in ROSE images. Simultaneously, combined with a classification head, the model can effectively identify the gen-eral distributive patterns across different instances. The results demonstrate the significant improvements in the classification accuracy with more accurate at-tention regions, indicating that the diverse patterns of ROSE images are effec-tively extracted, and the complicated perturbations of ROSE images are signifi-cantly eliminated. It also suggests that the MIL with shuffle instances has great potential in the analysis of cytopathological images. ",Kein DOI-Link verfügbar,2206.03080v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Uncertainty-Based Extensible Codebook for Discrete Federated Learning in   Heterogeneous Data Silos,1970,"  Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos. Through experiments conducted on five datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby outperforming contemporary state-of-the-art methods. The source code is available at https://github.com/destiny301/uefl. ",Kein DOI-Link verfügbar,2402.18888v2,Yes,innovative(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Leveraging LLMs to Predict Affective States via Smartphone Sensor   Features,1970,"  As mental health issues for young adults present a pressing public health concern, daily digital mood monitoring for early detection has become an important prospect. An active research area, digital phenotyping, involves collecting and analysing data from personal digital devices such as smartphones (usage and sensors) and wearables to infer behaviours and mental health. Whilst this data is standardly analysed using statistical and machine learning approaches, the emergence of large language models (LLMs) offers a new approach to make sense of smartphone sensing data. Despite their effectiveness across various domains, LLMs remain relatively unexplored in digital mental health, particularly in integrating mobile sensor data. Our study aims to bridge this gap by employing LLMs to predict affect outcomes based on smartphone sensing data from university students. We demonstrate the efficacy of zero-shot and few-shot embedding LLMs in inferring general wellbeing. Our findings reveal that LLMs can make promising predictions of affect measures using solely smartphone sensing data. This research sheds light on the potential of LLMs for affective state prediction, emphasizing the intricate link between smartphone behavioral patterns and affective states. To our knowledge, this is the first work to leverage LLMs for affective state prediction and digital phenotyping tasks. ",Kein DOI-Link verfügbar,2407.08240v1,Yes,"intricate(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Channel Estimation and Projection for RIS-assisted MIMO Using Zadoff-Chu   Sequences,1970,"  The reconfigurable intelligent surface (RIS) technology is a promising enabler for millimeter wave (mmWave) wireless communications, as it can potentially provide spectral efficiency comparable to the conventional massive multiple-input multiple-output (MIMO) but with significantly lower hardware complexity. In this paper, we focus on the estimation and projection of the uplink RIS-aided massive MIMO channel, which can be time-varying. We propose to let the user equipments (UE) transmit Zadoff-Chu (ZC) sequences and let the base station (BS) conduct maximum likelihood (ML) estimation of the uplink channel. The proposed scheme is computationally efficient: it uses ZC sequences to decouple the estimation of the frequency and time offsets; it uses the space-alternating generalized expectation-maximization (SAGE) method to reduce the high-dimensional problem due to the multipaths to multiple lower-dimensional ones per path. Owing to the estimation of the Doppler frequency offsets, the time-varying channel state can be projected, which can significantly lower the overhead of the pilots for channel estimation. The numerical simulations verify the effectiveness of the proposed scheme. ",Kein DOI-Link verfügbar,2202.10038v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Predicting Affective States from Screen Text Sentiment,1970,"  The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing. ",https://doi.org/10.1145/3675094.3678489,2408.12844v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",In-Plane Magnon Valve Effect in Magnetic Insulator/Heavy Metal/ Magnetic   Insulator Device,1970,"  We propose an in-plane magnon valve (MV), a sandwich structure composed of ferromagnetic insulator/heavy metal/ferromagnetic insulator (MI/HM/MI). When the magnetizations of the two MI layers are parallel, the longitudinal conductance in the HM layer is greater than that in the antiparallel state according to the magnetic proximity effect, termed as the in-plane magnon valve effect. We investigate the dependence of MV ratio (MVR), which is the relative change in longitudinal conductance between the parallel and antiparallel MV states, on the difference in electronic structure between magnetized and non-magnetized metal atoms, revealing that MVR can reach 100%. Additionally, the dependence of MVR on the thickness of metal layer is analyzed, revealing an exponential decrease with increasing thickness. Then we investigate the dependence of HM layer conductance on the relative angle between the magnetizations of two MI layers, illustrating the potential of MV as a magneto-sensitive magnonic sensor. We also investigate the effect of Joule heating on the measurement signal based on the spin Seebeck effect. Two designed configurations are proposed according to whether the electron current is parallel or perpendicular to the magnetization of the MI layer. In the parallel configuration, the transverse voltage differs between the parallel and antiparallel MV states. While in the perpendicular configuration, the longitudinal resistance differs. Quantitative numerical results indicate the feasibility of detecting a voltage signal using the first configuration in experiments. Our work contributes valuable insights for the design, development and integration of magnon devices ",Kein DOI-Link verfügbar,2312.17413v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",IDentity with Locality: An ideal hash for gene sequence search,1970,"  Gene sequence search is a fundamental operation in computational genomics. Due to the petabyte scale of genome archives, most gene search systems now use hashing-based data structures such as Bloom Filters (BF). The state-of-the-art systems such as Compact bit-slicing signature index (COBS) and Repeated And Merged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene representation and identification. The standard recipe is to cast the gene search problem as a sequence of membership problems testing if each subsequent gene substring (called kmer) of Q is present in the set of kmers of the entire gene database D. We observe that RH functions, which are crucial to the memory and the computational advantage of BF, are also detrimental to the system performance of gene-search systems. While subsequent kmers being queried are likely very similar, RH, oblivious to any similarity, uniformly distributes the kmers to different parts of potentially large BF, thus triggering excessive cache misses and causing system slowdown. We propose a novel hash function called the Identity with Locality (IDL) hash family, which co-locates the keys close in input space without causing collisions. This approach ensures both cache locality and key preservation. IDL functions can be a drop-in replacement for RH functions and help improve the performance of information retrieval systems. We give a simple but practical construction of IDL function families and show that replacing the RH with IDL functions reduces cache misses by a factor of 5x, thus improving query and indexing times of SOTA methods such as COBS and RAMBO by factors up to 2x without compromising their quality. We also provide a theoretical analysis of the false positive rate of BF with IDL functions. Our hash function is the first study that bridges Locality Sensitive Hash (LSH) and RH to obtain cache efficiency. ",Kein DOI-Link verfügbar,2406.14901v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Fixed-price Diffusion Mechanism Design,1970,"  We consider a fixed-price mechanism design setting where a seller sells one item via a social network, but the seller can only directly communicate with her neighbours initially. Each other node in the network is a potential buyer with a valuation derived from a common distribution. With a standard fixed-price mechanism, the seller can only sell the item among her neighbours. To improve her revenue, she needs more buyers to join in the sale. To achieve this, we propose the very first fixed-price mechanism to incentivize the seller's neighbours to inform their neighbours about the sale and to eventually inform all buyers in the network to improve seller's revenue. Compared with the existing mechanisms for the same purpose, our mechanism does not require the buyers to reveal their valuations and it is computationally easy. More importantly, it guarantees that the improved revenue is at least 1/2 of the optimal. ",Kein DOI-Link verfügbar,1905.05450v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing for Connectivity with Budget,1970,"  We consider a cost sharing problem to connect all nodes in a weighted undirected graph, where the weight of each edge represents the cost to use the edge for the connectivity and the cost has to be shared among all connected nodes. There is one node called the source to which all the other nodes want to connect and it does not share the costs of the connectivity. As a node may need to go through other nodes to reach the source, the intermediate nodes may behave strategically to block the connection by cutting the edges adjacent to them. To prevent such strategical behavior, we design cost sharing mechanisms to incentivize all nodes not to cut any edge so that we can minimize the total cost for connecting all the nodes. ",Kein DOI-Link verfügbar,2201.05976v2,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing under Private Valuation and Connection Control,1970,"  We consider a cost sharing problem on a weighted undirected graph, where all the nodes want to connect to a special node called source, and they need to share the total cost (weights) of the used edges. Each node except for the source has a private valuation of the connection, and it may block others' connections by strategically cutting its adjacent edges to reduce its cost share, which may increase the total cost. We aim to design mechanisms to prevent the nodes from misreporting their valuations and cutting their adjacent edges. We first show that it is impossible for such a mechanism to further satisfy budget balance (cover the total cost) and efficiency (maximize social welfare). Then, we design two feasible cost sharing mechanisms that incentivize each node to offer all its adjacent edges and truthfully report its valuation, and also satisfy either budget balance or efficiency. ",Kein DOI-Link verfügbar,2303.03083v1,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing under Private Costs and Connection Control on Directed   Acyclic Graphs,1970,"  We consider a cost sharing problem on a weighted directed acyclic graph (DAG) with a source node to which all the other nodes want to connect. The cost (weight) of each edge is private information reported by multiple contractors, and among them, only one contractor is selected as the builder. All the nodes except for the source need to share the total cost of the used edges. However, they may block others' connections to the source by strategically cutting their outgoing edges to reduce their cost share, which may increase the total cost of connectivity. To minimize the total cost of connectivity, we design a cost sharing mechanism to incentivize each node to offer all its outgoing edges and each contractor to report all the edges' weights truthfully, and show the properties of the proposed mechanism. In addition, our mechanism outperforms the two benchmark mechanisms. ",Kein DOI-Link verfügbar,2311.08903v1,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for   Robotic Exploration in the Dark,1970,"  Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments. ",Kein DOI-Link verfügbar,2403.10814v1,Yes,innovative(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Testing of Autonomous Driving Systems: Where Are We and Where Should We   Go?,1970,"  Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners' needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing. ",Kein DOI-Link verfügbar,2106.12233v5,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College","Streaming quanta sensors for online, high-performance imaging and vision",1970,"  Recently quanta image sensors (QIS) -- ultra-fast, zero-read-noise binary image sensors -- have demonstrated remarkable imaging capabilities in many challenging scenarios. Despite their potential, the adoption of these sensors is severely hampered by (a) high data rates and (b) the need for new computational pipelines to handle the unconventional raw data. We introduce a simple, low-bandwidth computational pipeline to address these challenges. Our approach is based on a novel streaming representation with a small memory footprint, efficiently capturing intensity information at multiple temporal scales. Updating the representation requires only 16 floating-point operations/pixel, which can be efficiently computed online at the native frame rate of the binary frames. We use a neural network operating on this representation to reconstruct videos in real-time (10-30 fps). We illustrate why such representation is well-suited for these emerging sensors, and how it offers low latency and high frame rate while retaining flexibility for downstream computer vision. Our approach results in significant data bandwidth reductions ~100X and real-time image reconstruction and computer vision -- $10^4$-$10^5$ reduction in computation than existing state-of-the-art approach while maintaining comparable quality. To the best of our knowledge, our approach is the first to achieve online, real-time image reconstruction on QIS. ",Kein DOI-Link verfügbar,2406.00859v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Learning Scalable Structural Representations for Link Prediction with   Bloom Signatures,1970,"  Graph neural networks (GNNs) have shown great potential in learning on graphs, but they are known to perform sub-optimally on link prediction tasks. Existing GNNs are primarily designed to learn node-wise representations and usually fail to capture pairwise relations between target nodes, which proves to be crucial for link prediction. Recent works resort to learning more expressive edge-wise representations by enhancing vanilla GNNs with structural features such as labeling tricks and link prediction heuristics, but they suffer from high computational overhead and limited scalability. To tackle this issue, we propose to learn structural link representations by augmenting the message-passing framework of GNNs with Bloom signatures. Bloom signatures are hashing-based compact encodings of node neighborhoods, which can be efficiently merged to recover various types of edge-wise structural features. We further show that any type of neighborhood overlap-based heuristic can be estimated by a neural network that takes Bloom signatures as input. GNNs with Bloom signatures are provably more expressive than vanilla GNNs and also more scalable than existing edge-wise models. Experimental results on five standard link prediction benchmarks show that our proposed model achieves comparable or better performance than existing edge-wise GNN models while being 3-200 $\times$ faster and more memory-efficient for online inference. ",Kein DOI-Link verfügbar,2312.16784v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Generating Progressive Images from Pathological Transitions via   Diffusion Model,1970,"  Deep learning is widely applied in computer-aided pathological diagnosis, which alleviates the pathologist workload and provide timely clinical analysis. However, most models generally require large-scale annotated data for training, which faces challenges due to the sampling and annotation scarcity in pathological images. The rapid developing generative models shows potential to generate more training samples from recent studies. However, they also struggle in generalization diversity with limited training data, incapable of generating effective samples. Inspired by the pathological transitions between different stages, we propose an adaptive depth-controlled diffusion (ADD) network to generate pathological progressive images for effective data augmentation. This novel approach roots in domain migration, where a hybrid attention strategy guides the bidirectional diffusion, blending local and global attention priorities. With feature measuring, the adaptive depth-controlled strategy ensures the migration and maintains locational similarity in simulating the pathological feature transition. Based on tiny training set (samples less than 500), the ADD yields cross-domain progressive images with corresponding soft-labels. Experiments on two datasets suggest significant improvements in generation diversity, and the effectiveness with generated progressive samples are highlighted in downstream classifications. The code is available at https://github.com/Rowerliu/ADD. ",Kein DOI-Link verfügbar,2311.12316v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Transformer-based Selective Super-Resolution for Efficient Image   Refinement,1970,"  Conventional super-resolution methods suffer from two drawbacks: substantial computational cost in upscaling an entire large image, and the introduction of extraneous or potentially detrimental information for downstream computer vision tasks during the refinement of the background. To solve these issues, we propose a novel transformer-based algorithm, Selective Super-Resolution (SSR), which partitions images into non-overlapping tiles, selects tiles of interest at various scales with a pyramid architecture, and exclusively reconstructs these selected tiles with deep features. Experimental results on three datasets demonstrate the efficiency and robust performance of our approach for super-resolution. Compared to the state-of-the-art methods, the FID score is reduced from 26.78 to 10.41 with 40% reduction in computation cost for the BDD100K dataset. The source code is available at https://github.com/destiny301/SSR. ",Kein DOI-Link verfügbar,2312.05803v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",RecGS: Removing Water Caustic with Recurrent Gaussian Splatting,1970,"  Water caustics are commonly observed in seafloor imaging data from shallow-water areas. Traditional methods that remove caustic patterns from images often rely on 2D filtering or pre-training on an annotated dataset, hindering the performance when generalizing to real-world seafloor data with 3D structures. In this paper, we present a novel method Recurrent Gaussian Splatting (RecGS), which takes advantage of today's photorealistic 3D reconstruction technology, 3DGS, to separate caustics from seafloor imagery. With a sequence of images taken by an underwater robot, we build 3DGS recurrently and decompose the caustic with low-pass filtering in each iteration. In the experiments, we analyze and compare with different methods, including joint optimization, 2D filtering, and deep learning approaches. The results show that our method can effectively separate the caustic from the seafloor, improving the visual appearance, and can be potentially applied on more problems with inconsistent illumination. ",Kein DOI-Link verfügbar,2407.10318v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College","When Cyber-Physical Systems Meet AI: A Benchmark, an Evaluation, and a   Way Forward",1970,"  Cyber-physical systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we initiate to create a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify the current challenges and explore future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS systems to achieve optimal performance and reliability. ",https://doi.org/10.1145/3510457.3513049,2111.04324v2,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",TARGET: Automated Scenario Generation from Traffic Rules for Testing   Autonomous Vehicles,1970,"  Ensuring the safety and robustness of autonomous driving systems (ADSs) is imperative. One of the crucial methods towards this assurance is the meticulous construction and execution of test scenarios, a task often regarded as tedious and laborious. In response to this challenge, this paper introduces TARGET, an end-to-end framework designed for the automatic generation of test scenarios grounded in established traffic rules. Specifically, we design a domain-specific language (DSL) with concise and expressive syntax for scenario descriptions. To handle the natural language complexity and ambiguity in traffic rule descriptions, we leverage a large language model to automatically extract knowledge from traffic rules and convert the traffic rule descriptions to DSL representations. Based on these representations, TARGET synthesizes executable test scenario scripts to render the testing scenarios in a simulator. Comprehensive evaluations of the framework were conducted on four distinct ADSs, yielding a total of 217 test scenarios spread across eight diverse maps. These scenarios identify approximately 700 rule violations, collisions, and other significant issues, including navigation failures. Moreover, for each detected anomaly, TARGET provides detailed scenario recordings and log reports, significantly easing the process of troubleshooting and root cause analysis. Two of these causes have been confirmed by the ADS developers; one is corroborated by an existing bug report from the ADS, and the other one is attributed to the limited functionality of the ADS. ",Kein DOI-Link verfügbar,2305.06018v2,Yes,meticulous(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",An ensemble learning approach for software semantic clone detection,1970,"  Code clone is a serious problem in software and has the potential to software defects, maintenance overhead, and licensing violations. Therefore, clone detection is important for reducing maintenance effort and improving code quality during software evolution. A variety of clone detection techniques have been proposed to identify similar code in software. However, few of them can efficiently detect semantic clones (functionally similar code without any syntactic resemblance). Recently, several deep learning based clone detectors are proposed to detect semantic clones. However, these approaches have high cost in data labelling and model training. In this paper, we propose a novel approach that leverages word embedding and ensemble learning techniques to detect semantic clones. Our evaluation on a commonly used clone benchmark, BigCloneBench, shows that our approach significantly improves the precision and recall of semantic clone detection, in comparison to a token-based clone detector, SourcererCC, and another deep learning based clone detector, CDLH. ",Kein DOI-Link verfügbar,2010.04336v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Knowledge-Based Version Incompatibility Detection for Deep Learning,1970,"  Version incompatibility issues are rampant when reusing or reproducing deep learning models and applications. Existing techniques are limited to library dependency specifications declared in PyPI. Therefore, these techniques cannot detect version issues due to undocumented version constraints or issues involving hardware drivers or OS. To address this challenge, we propose to leverage the abundant discussions of DL version issues from Stack Overflow to facilitate version incompatibility detection. We reformulate the problem of knowledge extraction as a Question-Answering (QA) problem and use a pre-trained QA model to extract version compatibility knowledge from online discussions. The extracted knowledge is further consolidated into a weighted knowledge graph to detect potential version incompatibilities when reusing a DL project. Our evaluation results show that (1) our approach can accurately extract version knowledge with 84% accuracy, and (2) our approach can accurately identify 65% of known version issues in 10 popular DL projects with a high precision (92%), while two state-of-the-art approaches can only detect 29% and 6% of these issues with 33% and 17% precision respectively. ",Kein DOI-Link verfügbar,2308.13276v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Where Do Large Language Models Fail When Generating Code?,1970,"  Large Language Models (LLMs) have shown great potential in code generation. However, current LLMs still cannot reliably generate correct code. Moreover, it is unclear what kinds of code generation errors LLMs can make. To address this, we conducted an empirical study to analyze incorrect code snippets generated by six popular LLMs on the HumanEval dataset. We analyzed these errors alongside two dimensions of error characteristics -- semantic characteristics and syntactic characteristics -- to derive a comprehensive code generation error taxonomy for LLMs through open coding and thematic analysis. We then labeled all 557 incorrect code snippets based on this taxonomy. Our results showed that the six LLMs exhibited similar distributions of syntactic characteristics while different distributions of semantic characteristics. Furthermore, we analyzed the correlation between different error characteristics and factors such as task complexity, code length, and test-pass rate. Finally, we highlight the challenges that LLMs may encounter when generating code and propose implications for future research on reliable code generation with LLMs. ",Kein DOI-Link verfügbar,2406.08731v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Shuffle Instances-based Vision Transformer for Pancreatic Cancer ROSE   Image Classification,1970,"  The rapid on-site evaluation (ROSE) technique can signifi-cantly accelerate the diagnosis of pancreatic cancer by im-mediately analyzing the fast-stained cytopathological images. Computer-aided diagnosis (CAD) can potentially address the shortage of pathologists in ROSE. However, the cancerous patterns vary significantly between different samples, making the CAD task extremely challenging. Besides, the ROSE images have complicated perturbations regarding color distribution, brightness, and contrast due to different staining qualities and various acquisition device types. To address these challenges, we proposed a shuffle instances-based Vision Transformer (SI-ViT) approach, which can reduce the perturbations and enhance the modeling among the instances. With the regrouped bags of shuffle instances and their bag-level soft labels, the approach utilizes a regression head to make the model focus on the cells rather than various perturbations. Simultaneously, combined with a classification head, the model can effectively identify the general distributive patterns among different instances. The results demonstrate significant improvements in the classification accuracy with more accurate attention regions, indicating that the diverse patterns of ROSE images are effectively extracted, and the complicated perturbations are significantly reduced. It also suggests that the SI-ViT has excellent potential in analyzing cytopathological images. The code and experimental results are available at https://github.com/sagizty/MIL-SI. ",Kein DOI-Link verfügbar,2208.06833v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",CPIA Dataset: A Comprehensive Pathological Image Analysis Dataset for   Self-supervised Learning Pre-training,1970,"  Pathological image analysis is a crucial field in computer-aided diagnosis, where deep learning is widely applied. Transfer learning using pre-trained models initialized on natural images has effectively improved the downstream pathological performance. However, the lack of sophisticated domain-specific pathological initialization hinders their potential. Self-supervised learning (SSL) enables pre-training without sample-level labels, which has great potential to overcome the challenge of expensive annotations. Thus, studies focusing on pathological SSL pre-training call for a comprehensive and standardized dataset, similar to the ImageNet in computer vision. This paper presents the comprehensive pathological image analysis (CPIA) dataset, a large-scale SSL pre-training dataset combining 103 open-source datasets with extensive standardization. The CPIA dataset contains 21,427,877 standardized images, covering over 48 organs/tissues and about 100 kinds of diseases, which includes two main data types: whole slide images (WSIs) and characteristic regions of interest (ROIs). A four-scale WSI standardization process is proposed based on the uniform resolution in microns per pixel (MPP), while the ROIs are divided into three scales artificially. This multi-scale dataset is built with the diagnosis habits under the supervision of experienced senior pathologists. The CPIA dataset facilitates a comprehensive pathological understanding and enables pattern discovery explorations. Additionally, to launch the CPIA dataset, several state-of-the-art (SOTA) baselines of SSL pre-training and downstream evaluation are specially conducted. The CPIA dataset along with baselines is available at https://github.com/zhanglab2021/CPIA_Dataset. ",Kein DOI-Link verfügbar,2310.17902v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Building Open-Ended Embodied Agent via Language-Policy Bidirectional   Adaptation,1970,"  Building embodied agents on integrating Large Language Models (LLMs) and Reinforcement Learning (RL) have revolutionized human-AI interaction: researchers can now leverage language instructions to plan decision-making for open-ended tasks. However, existing research faces challenges in meeting the requirement of open-endedness. They typically either train LLM/RL models to adapt to a fixed counterpart, limiting exploration of novel skills and hindering the efficacy of human-AI interaction. To this end, we present OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a pre-trained LLM to translate human instructions into goals for planning, and goal-conditioned training a policy for decision-making; (2) co-training to align the LLM and policy, achieving instruction open-endedness. We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agent trained with OpenPAL not only comprehends arbitrary instructions but also exhibits efficient execution. These results suggest that OpenPAL holds the potential to construct open-ended embodied agents in practical scenarios. ",Kein DOI-Link verfügbar,2401.00006v3,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",CellMix: A General Instance Relationship based Method for Data   Augmentation Towards Pathology Image Classification,1970,"  In pathology image analysis, obtaining and maintaining high-quality annotated samples is an extremely labor-intensive task. To overcome this challenge, mixing-based methods have emerged as effective alternatives to traditional preprocessing data augmentation techniques. Nonetheless, these methods fail to fully consider the unique features of pathology images, such as local specificity, global distribution, and inner/outer-sample instance relationships. To better comprehend these characteristics and create valuable pseudo samples, we propose the CellMix framework, which employs a novel distribution-oriented in-place shuffle approach. By dividing images into patches based on the granularity of pathology instances and shuffling them within the same batch, the absolute relationships between instances can be effectively preserved when generating new samples. Moreover, we develop a curriculum learning-inspired, loss-driven strategy to handle perturbations and distribution-related noise during training, enabling the model to adaptively fit the augmented data. Our experiments in pathology image classification tasks demonstrate state-of-the-art (SOTA) performance on 7 distinct datasets. This innovative instance relationship-centered method has the potential to inform general data augmentation approaches for pathology image classification. The associated codes are available at https://github.com/sagizty/CellMix. ",Kein DOI-Link verfügbar,2301.11513v2,Yes,"innovative(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Can Steering Wheel Detect Your Driving Fatigue?,1970,"  Automated Driving System (ADS) has attracted increasing attention from both industrial and academic communities due to its potential for increasing the safety, mobility and efficiency of existing transportation systems. The state-of-the-art ADS follows the human-in-the-loop (HITL) design, where the driver's anomalous behaviour is closely monitored by the system. Though many approaches have been proposed for detecting driver fatigue, they largely depend on vehicle driving parameters and facial features, which lacks reliability. Approaches using physiological based sensors (e.g., electroencephalogram or electrocardiogram) are either too clumsy to wear or impractical to install. In this paper, we propose a novel driver fatigue detection method by embedding surface electromyography (sEMG) sensors on a steering wheel. Compared with the existing methods, our approach is able to collect bio-signals in a non-intrusive way and detect driver fatigue at an earlier stage. The experimental results show that our approach outperforms existing methods with the weighted average F1 scores about 90%. We also propose promising future directions to deploy this approach in real-life settings, such as applying multimodal learning using several supplementary sensors. ",Kein DOI-Link verfügbar,2010.10327v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",A Declarative Metamorphic Testing Framework for Autonomous Driving,1970,"  Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions. ",Kein DOI-Link verfügbar,2012.10672v4,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs,1970,"  Anomaly detection aims to distinguish abnormal instances that deviate significantly from the majority of benign ones. As instances that appear in the real world are naturally connected and can be represented with graphs, graph neural networks become increasingly popular in tackling the anomaly detection problem. Despite the promising results, research on anomaly detection has almost exclusively focused on static graphs while the mining of anomalous patterns from dynamic graphs is rarely studied but has significant application value. In addition, anomaly detection is typically tackled from semi-supervised perspectives due to the lack of sufficient labeled data. However, most proposed methods are limited to merely exploiting labeled data, leaving a large number of unlabeled samples unexplored. In this work, we present semi-supervised anomaly detection (SAD), an end-to-end framework for anomaly detection on dynamic graphs. By a combination of a time-equipped memory bank and a pseudo-label contrastive learning module, SAD is able to fully exploit the potential of large unlabeled samples and uncover underlying anomalies on evolving graph streams. Extensive experiments on four real-world datasets demonstrate that SAD efficiently discovers anomalies from dynamic graphs and outperforms existing advanced methods even when provided with only little labeled data. ",Kein DOI-Link verfügbar,2305.13573v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Text as Image: Learning Transferable Adapter for Multi-Label   Classification,1970,"  Pre-trained vision-language models have notably accelerated progress of open-world concept recognition. Their impressive zero-shot ability has recently been transferred to multi-label image classification via prompt tuning, enabling to discover novel labels in an open-vocabulary manner. However, this paradigm suffers from non-trivial training costs, and becomes computationally prohibitive for a large number of candidate labels. To address this issue, we note that vision-language pre-training aligns images and texts in a unified embedding space, making it potential for an adapter network to identify labels in visual modality while be trained in text modality. To enhance such cross-modal transfer ability, a simple yet effective method termed random perturbation is proposed, which enables the adapter to search for potential visual embeddings by perturbing text embeddings with noise during training, resulting in better performance in visual modality. Furthermore, we introduce an effective approach to employ large language models for multi-label instruction-following text generation. In this way, a fully automated pipeline for visual label recognition is developed without relying on any manual data. Extensive experiments on public benchmarks show the superiority of our method in various multi-label classification tasks. ",Kein DOI-Link verfügbar,2312.04160v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",From Perfect to Noisy World Simulation: Customizable Embodied   Multi-modal Perturbations for SLAM Robustness Benchmarking,1970,"  Embodied agents require robust navigation systems to operate in unstructured environments, making the robustness of Simultaneous Localization and Mapping (SLAM) models critical to embodied agent autonomy. While real-world datasets are invaluable, simulation-based benchmarks offer a scalable approach for robustness evaluations. However, the creation of a challenging and controllable noisy world with diverse perturbations remains under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. The pipeline comprises a comprehensive taxonomy of sensor and motion perturbations for embodied multi-modal (specifically RGB-D) sensing, categorized by their sources and propagation order, allowing for procedural composition. We also provide a toolbox for synthesizing these perturbations, enabling the transformation of clean environments into challenging noisy simulations. Utilizing the pipeline, we instantiate the large-scale Noisy-Replica benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced RGB-D SLAM models. Our extensive analysis uncovers the susceptibilities of both neural (NeRF and Gaussian Splatting -based) and non-neural SLAM models to disturbances, despite their demonstrated accuracy in standard benchmarks. Our code is publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation. ",Kein DOI-Link verfügbar,2406.16850v1,Yes,invaluable(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Sulfur Vacancy Related Optical Transitions in Graded Alloys of MoxW1-xS2   Monolayers,1970,"  Engineering the electronic bandgap is of utmost importance in diverse domains ranging from information processing and communication technology to sensing and renewable energy applications. Transition metal dichalcogenides (TMDCs) provide an ideal platform for achieving this goal through techniques including alloying, doping, and creating in-plane or out-of-plane heterostructures. Here, we report on the synthesis and characterization of atomically controlled two-dimensional graded alloy of MoxW1-xS2, wherein the center region is Mo rich and gradually transitions towards a higher concentration of W atoms at the edges. This unique alloy structure leads to a continuously tunable bandgap, ranging from 1.85 eV in the center to 1.95 eV at the edges consistent with the larger band gap of WS2 relative to MoS2. Aberration-corrected high-angle annular dark-field scanning transmission electron microscopy showed the presence of sulfur monovacancy, VS, whose concentration varied across the graded MoxW1-xS2 layer as a function of Mo content with the highest value in the Mo rich center region. Optical spectroscopy measurements supported by ab initio calculations reveal a doublet electronic state of VS, which was split due to the spin-orbit interaction, with energy levels close to the conduction band or deep in the band gap depending on whether the vacancy is surrounded by W atoms or Mo atoms. This unique electronic configuration of VS in the alloy gave rise to four spin-allowed optical transitions between the VS levels and the valence bands. Our work highlights the potential of simultaneous defect and optical engineering of novel devices based on these 2D monolayers. ",Kein DOI-Link verfügbar,2308.14990v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",PharmacyGPT: The AI Pharmacist,1970,"  In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies. ",Kein DOI-Link verfügbar,2307.10432v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Photo-degradation Protection in 2D In-Plane Heterostructures Revealed by   Hyperspectral Nanoimaging: the Role of Nano-Interface 2D Alloys,1970,"  Single-layer heterostructures exhibit striking quasiparticle properties and many-body interaction effects that hold promise for a range of applications. However, their properties can be altered by intrinsic and extrinsic defects, thus diminishing their applicability. Therefore, it is of paramount importance to identify defects and understand 2D materials' degradation over time using advanced multimodal imaging techniques as well as stabilize degradation via built-in interface protection. Here we implemented a liquid-phase precursor approach to synthesize 2D in-plane MoS2-WS2 heterostructures exhibiting nanoscale alloyed interfaces and map exotic interface effects during photo-degradation using a novel combination of hyperspectral tip-enhanced photoluminescence, Raman and near-field nanoscopy. Surprisingly, 2D alloyed regions exhibit remarkable thermal and photo-degradation stability providing protection against oxidation. Coupled with surface and interface strain, 2D alloy regions create localized potential wells that concentrate excitonic species via a charge carrier funneling effect. These results provide a clear understanding of the importance of 2D alloys as systems able to withstand degradation effects over time, and could be now used to stabilize optoelectronic devices based on 2D materials. ",Kein DOI-Link verfügbar,2005.11361v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Wafer-scale epitaxial growth of single orientation WS2 monolayers on   sapphire,1970,"  Realization of wafer-scale single-crystal films of transition metal dichalcogenides (TMDs) such as tungsten sulfide requires epitaxial growth and coalescence of oriented domains to form a continuous monolayer. The domains must be oriented in the same crystallographic direction on the substrate to avoid the formation of metallic inversion domain boundaries (IDBs) which are a common feature of layered chalcogenides. Here we demonstrate fully-coalesced single orientation tungsten sulfide monolayers on 2-inch diameter c-plane sapphire by metalorganic chemical vapor deposition using a multi-step growth process. High growth temperatures and sulfur/metal ratios were required to reduce domain misorientation and achieve epitaxial tungsten sulfide monolayers with low in-plane rotational twist (0.09 deg). Transmission electron microscopy analysis reveals that the tungsten sulfide monolayers lack IDBs but instead have translational boundaries that arise when tungsten sulfide domains with slightly off-set lattices merge together. By adjusting the monolayer growth rate, the density of translational boundaries and bilayer coverage were significantly reduced. The preferred orientation of domains is attributed to the presence of steps on the sapphire surface coupled with growth conditions promote surface diffusion and oriented attachment. The transferred tungsten sulfide monolayers show neutral and charged exciton emission at 80K with negligible defect-related luminescence. Back-gated tungsten sulfide field effect transistors exhibited mobility of 16 cm2/Vs. The results demonstrate the potential of achieving wafer-scale TMD monolayers free of inversion domains with properties approaching that of exfoliated flakes. ",Kein DOI-Link verfügbar,2006.10952v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Directivity modulation of exciton emission using single dielectric   nanospheres,1970,"  Coupling emitters with nanoresonators is an effective strategy to control light emission at the subwavelength scale with high efficiency. Low-loss dielectric nanoantennas hold particular promise for this purpose, owing to their strong Mie resonances. Herein, we explore a highly miniaturized platform for the control of emission based on individual subwavelength Si nanospheres (SiNSs) to modulate the directional excitation and exciton emission of two-dimensional transition metal dichalcogenides (2D TMDs). A modified Mie theory for dipole-sphere hybrid systems is derived to instruct the optimal design for desirable modulation performance. Controllable forward-to-backward intensity ratios are experimentally validated in 532 nm laser excitation and 635 nm exciton emission from a monolayer WS2. Versatile light emission control along all device orientations is achieved for different emitters and excitation wavelengths, benefiting from the facile size control and isotropic shape of SiNSs. Simultaneous modulation of excitation and emission via a single SiNS at visible wavelengths significantly improves the efficiency and directivity of TMD exciton emission and leads to the potential of multifunctional integrated photonics. Overall, our work opens promising opportunities for nanophotonics and polaritonic systems, enabling efficient manipulation, enhancement and reconfigurability of light-matter interactions. ",Kein DOI-Link verfügbar,2010.02342v1,Yes,"versatile(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Remote-contact catalysis for target-diameter semiconducting carbon   nanotube array,1970,"  Electrostatic catalysis has been an exciting development in chemical synthesis (beyond enzymes catalysis) in recent years, boosting reaction rates and selectively producing certain reaction products. Most of the studies to date have been focused on using external electric field (EEF) to rearrange the charge distribution in small molecule reactions such as Diels-Alder addition, carbene reaction, etc. However, in order for these EEFs to be effective, a field on the order of 1 V/nm (10 MV/cm) is required, and the direction of the EEF has to be aligned with the reaction axis. Such a large and oriented EEF will be challenging for large-scale implementation, or materials growth with multiple reaction axis or steps. Here, we demonstrate that the energy band at the tip of an individual single-walled carbon nanotube (SWCNT) can be spontaneously shifted in a high-permittivity growth environment, with its other end in contact with a low-work function electrode (e.g., hafnium carbide or titanium carbide). By adjusting the Fermi level at a point where there is a substantial disparity in the density of states (DOS) between semiconducting (s-) and metallic (m-) SWCNTs, we achieve effective electrostatic catalysis for s-SWCNT growth assisted by a weak EEF perturbation (200V/cm). This approach enables the production of high-purity (99.92%) s-SWCNT horizontal arrays with narrow diameter distribution (0.95+-0.04 nm), targeting the requirement of advanced SWCNT-based electronics for future computing. These findings highlight the potential of electrostatic catalysis in precise materials growth, especially for s-SWCNTs, and pave the way for the development of advanced SWCNT-based electronics. ",Kein DOI-Link verfügbar,2404.02981v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Autonomous Investigations over WS$_2$ and Au{111} with Scanning Probe   Microscopy,1970,"  Individual atomic defects in 2D materials impact their macroscopic functionality. Correlating the interplay is challenging, however, intelligent hyperspectral scanning tunneling spectroscopy (STS) mapping provides a feasible solution to this technically difficult and time consuming problem. Here, dense spectroscopic volume is collected autonomously via Gaussian process regression, where convolutional neural networks are used in tandem for spectral identification. Acquired data enable defect segmentation, and a workflow is provided for machine-driven decision making during experimentation with capability for user customization. We provide a means towards autonomous experimentation for the benefit of both enhanced reproducibility and user-accessibility. Hyperspectral investigations on WS$_2$ sulfur vacancy sites are explored, which is combined with local density of states confirmation on the Au{111} herringbone reconstruction. Chalcogen vacancies, pristine WS$_2$, Au face-centered cubic, and Au hexagonal close packed regions are examined and detected by machine learning methods to demonstrate the potential of artificial intelligence for hyperspectral STS mapping. ",https://doi.org/10.1038/s41524-022-00777-9,2110.03351v5,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Giant room-temperature nonlinearities from a monolayer Janus topological   semiconductor,1970,"  Nonlinear optical materials possess wide applications, ranging from terahertz and mid-infrared detection to energy harvesting. Recently, the correlations between nonlinear optical responses and topological properties, such as Berry curvature and the quantum metric tensor, have stimulated great interest. Here, we report giant room-temperature nonlinearities in an emergent non-centrosymmetric two-dimensional topological material, the Janus transition metal dichalcogenides in the 1T' phase, which are synthesized by an advanced atomic-layer substitution method. High harmonic generation, terahertz emission spectroscopy, and second harmonic generation measurements consistently reveal orders-of-the-magnitude enhancement in terahertz-frequency nonlinearities of 1T' MoSSe (e.g., > 50 times higher than 2H MoS$_2$ for 18th order harmonic generation; > 20 times higher than 2H MoS$_2$ for terahertz emission). It is elucidated that such colossal nonlinear optical responses come from topological band mixing and strong inversion symmetry breaking due to the Janus structure. Our work defines general protocols for designing materials with large nonlinearities and preludes the applications of topological materials in optoelectronics down to the monolayer limit. This two-dimensional form of topological materials also constitute a unique platform for examining origin of the anomalous high-harmonic generation, with potential applications as building blocks for scalable attosecond sources. ",Kein DOI-Link verfügbar,2304.00750v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Holistic Evaluation of Language Models,1970,"  Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models. ",Kein DOI-Link verfügbar,2211.09110v2,Yes,potent(1)
0000-0001-5298-7482,Christoph Eigen,"Jesus College, University of Cambridge, University of Cambridge",Observation of Weak Collapse in a Bose-Einstein Condensate,1970,"  We study the collapse of an attractive atomic Bose-Einstein condensate prepared in the uniform potential of an optical-box trap. We characterise the critical point for collapse and the collapse dynamics, observing universal behaviour in agreement with theoretical expectations. Most importantly, we observe a clear experimental signature of the counterintuitive weak collapse, namely that making the system more unstable can result in a smaller particle loss. We experimentally determine the scaling laws that govern the weak-collapse atom loss, providing a benchmark for the general theories of nonlinear wave phenomena. ",https://doi.org/10.1103/PhysRevX.6.041058,1609.00352v2,Yes,potent(1)
0000-0002-0694-5297,Gwenhivir Wyatt-Moon,"University of Cambridge, University of Cambridge Trinity Hall",Inkjet printed circuits with two-dimensional semiconductor inks for   high-performance electronics,1970,"  Air-stable semiconducting inks suitable for complementary logic are key to create low-power printed integrated circuits (ICs). High-performance printable electronic inks with two-dimensional materials have the potential to enable the next generation of high performance, low-cost printed digital electronics. Here we demonstrate air-stable, low voltage (< 5 V) operation of inkjet-printed n-type molybdenum disulfide (MoS2) and p-type indacenodithiophene-co-benzothiadiazole (IDT-BT) field-effect transistors (FETs), estimating a switching time of {\tau} ~ 3.3 {\mu}s for the MoS2 FETs. We achieve this by engineering high-quality MoS2 and air-stable IDT-BT inks suitable for inkjet-printing complementary pairs of n-type MoS2 and p-type IDT-BT FETs. We then integrate MoS2 and IDT-BT FETs to realise inkjet-printed complementary logic inverters with a voltage gain |Av| ~ 4 when in resistive load configuration and |Av| ~ 1.36 in complementary configuration. These results represent a key enabling step towards ubiquitous long-term stable, low-cost printed digital ICs. ",Kein DOI-Link verfügbar,2011.12359v1,Yes,potent(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",Selective C-C Coupling by Spatially Confined Dimeric Metal Centers,1970,"  Direct conversion of carbon dioxide (CO2) to high-energy fuels and high-value chemicals is a fascinating sustainable strategy. For most of the current electrocatalysts for CO2 reduction, however, multi-carbon products are inhibited by large overpotentials and low selectivity. For practical applications, there remains a big gap of knowledge in proper manipulation of the C-C coupling process. Herein, we exploit dispersed 3d transition metal dimers as spatially confined dual reaction centers for selective reduction of CO2 to liquid fuels. Various nitrogenated holey carbon monolayers are shown to be promising templates to stabilize these metal dimers and dictate their electronic structures, allowing precise control of the catalytic activity and product selectivity. By comprehensive first-principles calculations, we screen the suitable transition metal dimers that universally have high activity for ethanol (C2H5OH). Furthermore, remarkable selectivity for C2H5OH against other C1 and C2 products is found for Fe2 dimer anchored on C2N monolayer. The correlation between the activity and d band center of the supported metal dimer as well as the role of electronic coupling between the metal dimer and the carbon substrates are thoroughly elucidated. ",Kein DOI-Link verfügbar,2001.06160v1,Yes,potent(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",Zero-shot Aspect-level Sentiment Classification via Explicit Utilization   of Aspect-to-Document Sentiment Composition,1970,"  As aspect-level sentiment labels are expensive and labor-intensive to acquire, zero-shot aspect-level sentiment classification is proposed to learn classifiers applicable to new domains without using any annotated aspect-level data. In contrast, document-level sentiment data with ratings are more easily accessible. In this work, we achieve zero-shot aspect-level sentiment classification by only using document-level reviews. Our key intuition is that the sentiment representation of a document is composed of the sentiment representations of all the aspects of that document. Based on this, we propose the AF-DSC method to explicitly model such sentiment composition in reviews. AF-DSC first learns sentiment representations for all potential aspects and then aggregates aspect-level sentiments into a document-level one to perform document-level sentiment classification. In this way, we obtain the aspect-level sentiment classifier as the by-product of the document-level sentiment classifier. Experimental results on aspect-level sentiment classification benchmarks demonstrate the effectiveness of explicit utilization of sentiment composition in document-level sentiment classification. Our model with only 30k training data outperforms previous work utilizing millions of data. ",Kein DOI-Link verfügbar,2209.02276v1,Yes,potent(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",Exploring Periodicity and Interactivity in Multi-Interest Framework for   Sequential Recommendation,1970,"  Sequential recommendation systems alleviate the problem of information overload, and have attracted increasing attention in the literature. Most prior works usually obtain an overall representation based on the user's behavior sequence, which can not sufficiently reflect the multiple interests of the user. To this end, we propose a novel method called PIMI to mitigate this issue. PIMI can model the user's multi-interest representation effectively by considering both the periodicity and interactivity in the item sequence. Specifically, we design a periodicity-aware module to utilize the time interval information between user's behaviors. Meanwhile, an ingenious graph is proposed to enhance the interactivity between items in user's behavior sequence, which can capture both global and local item features. Finally, a multi-interest extraction module is applied to describe user's multiple interests based on the obtained item representation. Extensive experiments on two real-world datasets Amazon and Taobao show that PIMI outperforms state-of-the-art methods consistently. ",https://doi.org/10.24963/ijcai.2021/197,2106.04415v1,Yes,ingenious(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",Is ChatGPT Equipped with Emotional Dialogue Capabilities?,1970,"  This report presents a study on the emotional dialogue capability of ChatGPT, an advanced language model developed by OpenAI. The study evaluates the performance of ChatGPT on emotional dialogue understanding and generation through a series of experiments on several downstream tasks. Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses. Furthermore, the study suggests potential avenues for future research directions. ",Kein DOI-Link verfügbar,2304.09582v1,Yes,potent(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",SAPT: A Shared Attention Framework for Parameter-Efficient Continual   Learning of Large Language Models,1970,"  The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks. ",Kein DOI-Link verfügbar,2401.08295v3,Yes,potent(1)
0000-0003-2302-1763,Yanyan Zhao,"Stanford University, Stanford University School of Medicine",Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A   Survey,1970,"  Compared to traditional sentiment analysis, which only considers text, multimodal sentiment analysis needs to consider emotional signals from multimodal sources simultaneously and is therefore more consistent with the way how humans process sentiment in real-world scenarios. It involves processing emotional information from various sources such as natural language, images, videos, audio, physiological signals, etc. However, although other modalities also contain diverse emotional cues, natural language usually contains richer contextual information and therefore always occupies a crucial position in multimodal sentiment analysis. The emergence of ChatGPT has opened up immense potential for applying large language models (LLMs) to text-centric multimodal tasks. However, it is still unclear how existing LLMs can adapt better to text-centric multimodal sentiment analysis tasks. This survey aims to (1) present a comprehensive review of recent research in text-centric multimodal sentiment analysis tasks, (2) examine the potential of LLMs for text-centric multimodal sentiment analysis, outlining their approaches, advantages, and limitations, (3) summarize the application scenarios of LLM-based multimodal sentiment analysis technology, and (4) explore the challenges and potential research directions for multimodal sentiment analysis in the future. ",Kein DOI-Link verfügbar,2406.08068v2,Yes,potent(3)
0000-0003-0784-7987,Michael Snyder,"Stanford University, Stanford University School of Medicine",Semi-supervised Cooperative Learning for Multiomics Data Fusion,1970,"  Multiomics data fusion integrates diverse data modalities, ranging from transcriptomics to proteomics, to gain a comprehensive understanding of biological systems and enhance predictions on outcomes of interest related to disease phenotypes and treatment responses. Cooperative learning, a recently proposed method, unifies the commonly-used fusion approaches, including early and late fusion, and offers a systematic framework for leveraging the shared underlying relationships across omics to strengthen signals. However, the challenge of acquiring large-scale labeled data remains, and there are cases where multiomics data are available but in the absence of annotated labels. To harness the potential of unlabeled multiomcis data, we introduce semi-supervised cooperative learning. By utilizing an ""agreement penalty"", our method incorporates the additional unlabeled data in the learning process and achieves consistently superior predictive performance on simulated data and a real multiomics study of aging. It offers an effective solution to multiomics data fusion in settings with both labeled and unlabeled data and maximizes the utility of available data resources, with the potential of significantly improving predictive models for diagnostics and therapeutics in an increasingly multiomics world. ",Kein DOI-Link verfügbar,2308.01458v1,Yes,potent(2)
0000-0003-0784-7987,Michael Snyder,"Stanford University, Stanford University School of Medicine",Active Learning Pipeline for Brain Mapping in a High Performance   Computing Environment,1970,"  This paper describes a scalable active learning pipeline prototype for large-scale brain mapping that leverages high performance computing power. It enables high-throughput evaluation of algorithm results, which, after human review, are used for iterative machine learning model training. Image processing and machine learning are performed in a batch layer. Benchmark testing of image processing using pMATLAB shows that a 100$\times$ increase in throughput (10,000%) can be achieved while total processing time only increases by 9% on Xeon-G6 CPUs and by 22% on Xeon-E5 CPUs, indicating robust scalability. The images and algorithm results are provided through a serving layer to a browser-based user interface for interactive review. This pipeline has the potential to greatly reduce the manual annotation burden and improve the overall performance of machine learning-based brain mapping. ",Kein DOI-Link verfügbar,2006.14684v1,Yes,potent(1)
0000-0002-8586-8444,Charlotte Williams,UCL,Predictors of Social Distancing and Mask-Wearing Behavior: Panel Survey   in Seven U.S. States,1970,"  This paper presents preliminary summary results from a longitudinal study of participants in seven U.S. states during the COVID-19 pandemic. In addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. We pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. We note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. These findings can help inform the design of policy interventions regarding targeting specific demographic groups, which can help reduce the transmission speed of the COVID-19 virus. ",Kein DOI-Link verfügbar,2009.13103v1,Yes,potent(1)
0000-0002-6275-2289,David Tuckett,UCL,Measuring Financial Sentiment to Predict Financial Instability: A New   Approach based on Text Analysis,1970,"  Following the financial crisis of the late 2000s, policy makers have shown considerable interest in monitoring financial stability. Several central banks now publish indices of financial stress, which are essentially based upon market related data. In this paper, we examine the potential for improving the indices by deriving information about emotion shifts in the economy. We report on a new approach, based on the content analysis of very large text databases, and termed directed algorithmic text analysis. The algorithm identifies, very rapidly, shifts through time in the relations between two core emotional groups. The method is robust. The same word-list is used to identify the two emotion groups across different studies. Membership of the words in the lists has been validated in psychological experiments. The words consist of everyday English words with no specific economic meaning. Initial results show promise. An emotion index capturing shifts between the two emotion groups in texts potentially referring to the whole US economy improves the one-quarter ahead consensus forecasts for real GDP growth. More specifically, the same indices are shown to Granger cause both the Cleveland and St Louis Federal Reserve Indices of Financial Stress. ",Kein DOI-Link verfügbar,1508.05357v1,Yes,potent(2)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",A Multilayer Correlated Topic Model,1970,"  We proposed a novel multilayer correlated topic model (MCTM) to analyze how the main ideas inherit and vary between a document and its different segments, which helps understand an article's structure. The variational expectation-maximization (EM) algorithm was derived to estimate the posterior and parameters in MCTM. We introduced two potential applications of MCTM, including the paragraph-level document analysis and market basket data analysis. The effectiveness of MCTM in understanding the document structure has been verified by the great predictive performance on held-out documents and intuitive visualization. We also showed that MCTM could successfully capture customers' popular shopping patterns in the market basket analysis. ",Kein DOI-Link verfügbar,2101.02028v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Flow to Rare Events: An Application of Normalizing Flow in Temporal   Importance Sampling for Automated Vehicle Validation,1970,"  Automated Vehicle (AV) validation based on simulated testing requires unbiased evaluation and high efficiency. One effective solution is to increase the exposure to risky rare events while reweighting the probability measure. However, characterizing the distribution of risky events is particularly challenging due to the paucity of samples and the temporality of continuous scenario variables. To solve it, we devise a method to represent, generate, and reweight the distribution of risky rare events. We decompose the temporal evolution of continuous variables into distribution components based on conditional probability. By introducing the Risk Indicator Function, the distribution of risky rare events is theoretically precipitated out of naturalistic driving distribution. This targeted distribution is practically generated via Normalizing Flow, which achieves exact and tractable probability evaluation of intricate distribution. The rare event distribution is then demonstrated as the advantageous Importance Sampling distribution. We also promote the technique of temporal Importance Sampling. The combined method, named as TrimFlow, is executed to estimate the collision rate of Car-following scenarios as a tentative practice. The results showed that sampling background vehicle maneuvers from rare event distribution could evolve testing scenarios to hazardous states. TrimFlow reduced 86.1% of tests compared to generating testing scenarios according to their exposure in the naturalistic driving environment. In addition, the TrimFlow method is not limited to one specific type of functional scenario. ",Kein DOI-Link verfügbar,2407.07320v1,Yes,intricate(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",The Gaussian Interference Relay Channel: Improved Achievable Rates and   Sum Rate Upperbounds Using a Potent Relay,1970,"  We consider the Gaussian interference channel with an intermediate relay as a main building block for cooperative interference networks. On the achievability side, we consider compress-and-forward based strategies. Specifically, a generalized compress-and-forward strategy, where the destinations jointly decode the compression indices and the source messages, is shown to improve upon the compress-and-forward strategy which sequentially decodes the compression indices and source messages, and the recently proposed generalized hash-and-forward strategy. We also construct a nested lattice code based compute-and-forward relaying scheme, which outperforms other relaying schemes when the direct link is weak. In this case, it is shown that, with a relay, the interference link can be useful for decoding the source messages. Noting the need for upperbounding the capacity for this channel, we propose a new technique with which the sum rate can be bounded. In particular, the sum capacity is upperbounded by considering the channel when the relay node has abundant power and is named potent for that reason. For the Gaussian interference relay channel with potent relay, we study the strong and the weak interference regimes and establish the sum capacity, which, in turn, serve as upperbounds for the sum capacity of the GIFRC with finite relay power. Numerical results demonstrate that upperbounds are tighter than the cut-set bound, and coincide with known achievable sum rates for many scenarios of interest. Additionally, the degrees of freedom of the GIFRC are shown to be 2 when the relay has large power, achievable using compress-and-forward. ",Kein DOI-Link verfügbar,1102.0043v1,Yes,potent(2)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",On The Birch and Swinnerton-Dyer Conjecture for CM Elliptic Curves over   $\BQ$,1970,"  For CM elliptic curve over rational field with analytic rank one, for any potential good ordinary prime p, not dividing the number of roots of unity in the complex multiplication field, we show the p-part of its Shafarevich-Tate group has order predicted by the Birch and Swinnerton-Dyer conjecture. ",Kein DOI-Link verfügbar,1605.01481v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Coordinated Frequency-Constrained Stochastic Economic Dispatch for   Integrated Transmission and Distribution System via Distributed Optimization,1970,"  When large-scale uncertain centralized and distributed renewable energy sources are connected to a power system, separate dispatching of the transmission power system (TPS) and the active distribution network (ADN) will lower the network security and frequency security of the system. To address these problems, this paper proposes a coordinated frequency-constrained stochastic economic dispatch (CFC-SED) model for an integrated transmission and distribution (ITD) system. In this model, the dynamic frequency security constraints and network security constraints of the ITD system are constructed, and the joint chance constraints are adopted to handle the uncertainty. Then, the control parameters of inverter-based resources, the base point power, and the regulation reserve of all dispatchable resources in the ITD system are jointly optimized for the minimum operating cost. TPS and ADNs can deliver base point power bidirectionally and provide frequency regulation support bidirectionally, which extend the existing reserve assumption in ITD dispatch and enhance the operational security of the ITD system. Moreover, based on the alternating direction of multiplier algorithm, a two-layer distributed optimization framework is proposed to solve the CFC-SED model. Case studies show that the CFC-SED model can fully utilize the potential of multiple regulation resources to improve the security performance of the ITD system, and TPS and ADNs can be coordinated efficiently through the proposed distributed optimization framework. ",Kein DOI-Link verfügbar,2305.11440v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Towards the Theory of Unsupervised Federated Learning: Non-asymptotic   Analysis of Federated EM Algorithms,1970,"  While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. Several federated EM algorithms have gained popularity in practice, however, their theoretical foundations are often lacking. In this paper, we first introduce a federated gradient EM algorithm (FedGrEM) designed for the unsupervised learning of mixture models, which supplements the existing federated EM algorithms by considering task heterogeneity and potential adversarial attacks. We present a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on specific statistical models to characterize the explicit estimation error of model parameters and mixture proportions. Our theory elucidates when and how FedGrEM outperforms local single-task learning with insights extending to existing federated EM algorithms. This bridges the gap between their practical success and theoretical understanding. Our numerical results validate our theory, and demonstrate FedGrEM's superiority over existing unsupervised federated learning benchmarks. ",Kein DOI-Link verfügbar,2310.15330v3,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture   Models,1970,"  Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees. ",Kein DOI-Link verfügbar,2209.15224v4,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",The Simple Single Field Inflation Models and the Running of Spectral   Index,1970,  The BICEP2 experiment confirms the existence of primordial gravitational wave with the tensor-to-scalar ratio $r=0$ ruled out at $7\sigma$ level. The consistency of this large value of $r$ with the {\em Planck} data requires a large negative running $n'_s$ of the scalar spectral index. Herein we propose two types of the single field inflation models with simple potentials to study the possibility of the consistency of the models with the BICEP2 and {\em Planck} observations. One type of model suggested herein is realized in the supergravity model building. These models fail to provide the needed $n'_s$ even though both can fit the tensor-to-scalar ratio and spectral index. ,https://doi.org/10.1007/s11433-014-5519-9,1404.7214v2,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",DiffusionPipe: Training Large Diffusion Models with Efficient Pipelines,1970,"  Diffusion models have emerged as dominant performers for image generation. To support training large diffusion models, this paper studies pipeline parallel training of diffusion models and proposes DiffusionPipe, a synchronous pipeline training system that advocates innovative pipeline bubble filling technique, catering to structural characteristics of diffusion models. State-of-the-art diffusion models typically include trainable (the backbone) and non-trainable (e.g., frozen input encoders) parts. We first unify optimal stage partitioning and pipeline scheduling of single and multiple backbones in representative diffusion models with a dynamic programming approach. We then propose to fill the computation of non-trainable model parts into idle periods of the pipeline training of the backbones by an efficient greedy algorithm, thus achieving high training throughput. Extensive experiments show that DiffusionPipe can achieve up to 1.41x speedup over pipeline parallel methods and 1.28x speedup over data parallel training on popular diffusion models. ",Kein DOI-Link verfügbar,2405.01248v1,Yes,innovative(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",High-Dimensional Fault Tolerance Testing of Highly Automated Vehicles   Based on Low-Rank Models,1970,"  Ensuring fault tolerance of Highly Automated Vehicles (HAVs) is crucial for their safety due to the presence of potentially severe faults. Hence, Fault Injection (FI) testing is conducted by practitioners to evaluate the safety level of HAVs. To fully cover test cases, various driving scenarios and fault settings should be considered. However, due to numerous combinations of test scenarios and fault settings, the testing space can be complex and high-dimensional. In addition, evaluating performance in all newly added scenarios is resource-consuming. The rarity of critical faults that can cause security problems further strengthens the challenge. To address these challenges, we propose to accelerate FI testing under the low-rank Smoothness Regularized Matrix Factorization (SRMF) framework. We first organize the sparse evaluated data into a structured matrix based on its safety values. Then the untested values are estimated by the correlation captured by the matrix structure. To address high dimensionality, a low-rank constraint is imposed on the testing space. To exploit the relationships between existing scenarios and new scenarios and capture the local regularity of critical faults, three types of smoothness regularization are further designed as a complement. We conduct experiments on car following and cut in scenarios. The results indicate that SRMF has the lowest prediction error in various scenarios and is capable of predicting rare critical faults compared to other machine learning models. In addition, SRMF can achieve 1171 acceleration rate, 99.3% precision and 91.1% F1 score in identifying critical faults. To the best of our knowledge, this is the first work to introduce low-rank models to FI testing of HAVs. ",Kein DOI-Link verfügbar,2407.21069v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",DUNE: Improving Accuracy for Sketch-INT Network Measurement Systems,1970,"  In-band Network Telemetry (INT) and sketching algorithms are two promising directions for measuring network traffics in real time. To combine sketch with INT and preserve their advantages, a representative approach is to use INT to send a switch sketch in small pieces (called sketchlets) to end-host for reconstructing an identical sketch. However, in this paper, we reveal that when naively selecting buckets to sketchlets, the end-host reconstructed sketch is inaccurate. To overcome this problem, we present DUNE, an innovative sketch-INT network measurement system. DUNE incorporates two key innovations: First, we design a novel scatter sketchlet that is more efficient in transferring measurement data by allowing a switch to select individual buckets to add to sketchlets; Second, we propose lightweight data structures for tracing ""freshness"" of the sketch buckets, and present algorithms for smartly selecting buckets that contain valuable measurement data to send to end-host. We theoretically prove the effectiveness of our proposed methods, and implement a prototype on commodity programmable switch. The results of extensive experiments driven by real-world traffics on DUNE suggest that our proposed system can substantially improve the measurement accuracy at a trivial cost. ",Kein DOI-Link verfügbar,2212.04816v1,Yes,"innovative(1), fresh(1)"
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models   Adapting,1970,"  Recently, prompt-based methods have emerged as a new alternative `parameter-efficient fine-tuning' paradigm, which only fine-tunes a small number of additional parameters while keeping the original model frozen. However, despite achieving notable results, existing prompt methods mainly focus on `what to add', while overlooking the equally important aspect of `where to add', typically relying on the manually crafted placement. To this end, we propose a region-based Adaptive Visual Prompt, named AdaViPro, which integrates the `where to add' optimization of the prompt into the learning process. Specifically, we reconceptualize the `where to add' optimization as a problem of regional decision-making. During inference, AdaViPro generates a regionalized mask map for the whole image, which is composed of 0 and 1, to designate whether to apply or discard the prompt in each specific area. Therefore, we employ Gumbel-Softmax sampling to enable AdaViPro's end-to-end learning through standard back-propagation. Extensive experiments demonstrate that our AdaViPro yields new efficiency and accuracy trade-offs for adapting pre-trained models. ",Kein DOI-Link verfügbar,2403.13282v2,Yes,notable(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Lancet: Accelerating Mixture-of-Experts Training via Whole Graph   Computation-Communication Overlapping,1970,"  The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters. However, it faces the challenge of extended all-to-all communication latency during the training process. Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation. Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements. In our study, we extend the scope of this challenge by considering overlap at the broader training graph level. During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining. In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations. We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training. Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%. Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions. ",Kein DOI-Link verfügbar,2404.19429v1,Yes,"notable(1), potent(1)"
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large   Language Models,1970,"  There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in-domain data augmentation, equipping LLMs with considerable capabilities for code-aided mathematical reasoning. However, continually training these models on augmented data derived from a few datasets such as GSM8K may impair their generalization abilities and restrict their effectiveness to a narrow range of question types. Conversely, the potential of improving such LLMs by leveraging large-scale, expert-written, diverse math question-answer pairs remains unexplored. To utilize these resources and tackle unique challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvement. Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness of the proposed paradigm. ",Kein DOI-Link verfügbar,2408.15565v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Accelerating Evolutionary Neural Architecture Search via Multi-Fidelity   Evaluation,1970,"  Evolutionary neural architecture search (ENAS) has recently received increasing attention by effectively finding high-quality neural architectures, which however consumes high computational cost by training the architecture encoded by each individual for complete epochs in individual evaluation. Numerous ENAS approaches have been developed to reduce the evaluation cost, but it is often difficult for most of these approaches to achieve high evaluation accuracy. To address this issue, in this paper we propose an accelerated ENAS via multifidelity evaluation termed MFENAS, where the individual evaluation cost is significantly reduced by training the architecture encoded by each individual for only a small number of epochs. The balance between evaluation cost and evaluation accuracy is well maintained by suggesting a multi-fidelity evaluation, which identifies the potentially good individuals that cannot survive from previous generations by integrating multiple evaluations under different numbers of training epochs. For high diversity of neural architectures, a population initialization strategy is devised to produce different neural architectures varying from ResNet-like architectures to Inception-like ones. Experimental results on CIFAR-10 show that the architecture obtained by the proposed MFENAS achieves a 2.39% test error rate at the cost of only 0.6 GPU days on one NVIDIA 2080TI GPU, demonstrating the superiority of the proposed MFENAS over state-of-the-art NAS approaches in terms of both computational cost and architecture quality. The architecture obtained by the proposed MFENAS is then transferred to CIFAR-100 and ImageNet, which also exhibits competitive performance to the architectures obtained by existing NAS approaches. The source code of the proposed MFENAS is available at https://github.com/DevilYangS/MFENAS/. ",Kein DOI-Link verfügbar,2108.04541v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Stabilizing RLHF through Advantage Model and Selective Rehearsal,1970,"  Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates. ",Kein DOI-Link verfügbar,2309.10202v1,Yes,strategically(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Fine-Grained Self-Endorsement Improves Factuality and Reasoning,1970,"  This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations. Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses. Compared with prior ensemble methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level selection, our approach can better alleviate hallucinations, especially for longform generation tasks. Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons. Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application. ",Kein DOI-Link verfügbar,2402.15631v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research","Toward Self-Improvement of LLMs via Imagination, Searching, and   Criticizing",1970,"  Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs. ",Kein DOI-Link verfügbar,2404.12253v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning,1970,"  Schizophrenia is a chronic neuropsychiatric disorder that causes distinct structural alterations within the brain. We hypothesize that deep learning applied to a structural neuroimaging dataset could detect disease-related alteration and improve classification and diagnostic accuracy. We tested this hypothesis using a single, widely available, and conventional T1-weighted MRI scan, from which we extracted the 3D whole-brain structure using standard post-processing methods. A deep learning model was then developed, optimized, and evaluated on three open datasets with T1-weighted MRI scans of patients with schizophrenia. Our proposed model outperformed the benchmark model, which was also trained with structural MR images using a 3D CNN architecture. Our model is capable of almost perfectly (area under the ROC curve = 0.987) distinguishing schizophrenia patients from healthy controls on unseen structural MRI scans. Regional analysis localized subcortical regions and ventricles as the most predictive brain regions. Subcortical structures serve a pivotal role in cognitive, affective, and social functions in humans, and structural abnormalities of these regions have been associated with schizophrenia. Our finding corroborates that schizophrenia is associated with widespread alterations in subcortical brain structure and the subcortical structural information provides prominent features in diagnostic classification. Together, these results further demonstrate the potential of deep learning to improve schizophrenia diagnosis and identify its structural neuroimaging signatures from a single, standard T1-weighted brain MRI. ",Kein DOI-Link verfügbar,2206.12980v2,Yes,"pivotal(1), potent(1)"
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Evaluating the Effect of Longitudinal Dose and INR Data on Maintenance   Warfarin Dose Predictions,1970,"  Warfarin, a commonly prescribed drug to prevent blood clots, has a highly variable individual response. Determining a maintenance warfarin dose that achieves a therapeutic blood clotting time, as measured by the international normalized ratio (INR), is crucial in preventing complications. Machine learning algorithms are increasingly being used for warfarin dosing; usually, an initial dose is predicted with clinical and genotype factors, and this dose is revised after a few days based on previous doses and current INR. Since a sequence of prior doses and INR better capture the variability in individual warfarin response, we hypothesized that longitudinal dose response data will improve maintenance dose predictions. To test this hypothesis, we analyzed a dataset from the COAG warfarin dosing study, which includes clinical data, warfarin doses and INR measurements over the study period, and maintenance dose when therapeutic INR was achieved. Various machine learning regression models to predict maintenance warfarin dose were trained with clinical factors and dosing history and INR data as features. Overall, dose revision algorithms with a single dose and INR achieved comparable performance as the baseline dose revision algorithm. In contrast, dose revision algorithms with longitudinal dose and INR data provided maintenance dose predictions that were statistically significantly much closer to the true maintenance dose. Focusing on the best performing model, gradient boosting (GB), the proportion of ideal estimated dose, i.e., defined as within $\pm$20% of the true dose, increased from the baseline (54.92%) to the GB model with the single (63.11%) and longitudinal (75.41%) INR. More accurate maintenance dose predictions with longitudinal dose response data can potentially achieve therapeutic INR faster, reduce drug-related complications and improve patient outcomes with warfarin therapy. ",Kein DOI-Link verfügbar,2105.02625v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Quantum Phase Diffusion in a Small Underdamped Josephson Junction,1970,  Quantum phase diffusion in a small underdamped Nb/AlO$_x$/Nb junction ($\sim$ 0.4 $\mu$m$^2$) is demonstrated in a wide temperature range of 25-140 mK where macroscopic quantum tunneling (MQT) is the dominant escape mechanism. We propose a two-step transition model to describe the switching process in which the escape rate out of the potential well and the transition rate from phase diffusion to the running state are considered. The transition rate extracted from the experimental switching current distribution follows the predicted Arrhenius law in the thermal regime but is greatly enhanced when MQT becomes dominant. ,https://doi.org/10.1103/PhysRevLett.107.067004,1101.2250v2,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Coherent population transfer between weakly-coupled states in a   ladder-type superconducting qutrit,1970,"  Stimulated Raman adiabatic passage (STIRAP) offers significant advantages for coherent population transfer between un- or weakly-coupled states and has the potential of realizing efficient quantum gate, qubit entanglement, and quantum information transfer. Here we report on the realization of STIRAP in a superconducting phase qutrit - a ladder-type system in which the ground state population is coherently transferred to the second-excited state via the dark state subspace. The result agrees well with the numerical simulation of the master equation, which further demonstrates that with the state-of-the-art superconducting qutrits the transfer efficiency readily exceeds $99\%$ while keeping the population in the first-excited state below $1\%$. We show that population transfer via STIRAP is significantly more robust against variations of the experimental parameters compared to that via the conventional resonant $\pi$ pulse method. Our work opens up a new venue for exploring STIRAP for quantum information processing using the superconducting artificial atoms. ",https://doi.org/10.1038/ncomms11018,1508.01849v1,Yes,potent(1)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",The Solenoidal Large Intensity Device (SoLID) for JLab 12 GeV,1970,"  The Solenoidal Large Intensity Device (SoLID) is a new experimental apparatus planned for Hall A at the Thomas Jefferson National Accelerator Facility (JLab). SoLID will combine large angular and momentum acceptance with the capability to handle very high data rates at high luminosity. With a slate of approved high-impact physics experiments, SoLID will push JLab to a new limit at the QCD intensity frontier that will exploit the full potential of its 12 GeV electron beam. In this paper, we present an overview of the rich physics program that can be realized with SoLID, which encompasses the tomography of the nucleon in 3-D momentum space from Semi-Inclusive Deep Inelastic Scattering (SIDIS), expanding the phase space in the search for new physics and novel hadronic effects in parity-violating DIS (PVDIS), a precision measurement of $J/\psi$ production at threshold that probes the gluon field and its contribution to the proton mass, tomography of the nucleon in combined coordinate and momentum space with deep exclusive reactions, and more. To meet the challenging requirements, the design of SoLID described here takes full advantage of recent progress in detector, data acquisition and computing technologies. In addition, we outline potential experiments beyond the currently approved program and discuss the physics that could be explored should upgrades of CEBAF become a reality in the future. ",Kein DOI-Link verfügbar,2209.13357v3,Yes,potent(2)
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",RestGPT: Connecting Large Language Models with Real-World RESTful APIs,1970,"  Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI. RestGPT and RestBench is publicly available at https://restgpt.github.io/. ",Kein DOI-Link verfügbar,2306.06624v2,Yes,"meticulous(1), meticulously(1)"
0000-0003-3107-5550,Ye Tian,"The University of Melbourne, The University of Melbourne Research",Quantum and classical resonant escapes of a strongly-driven Josephson   junction,1970,"  The properties of phase escape in a dc SQUID at 25 mK, which is well below quantum-to-classical crossover temperature $T_{cr}$, in the presence of strong resonant ac driving have been investigated. The SQUID contains two Nb/Al-AlO$_{x} $/Nb tunnel junctions with Josephson inductance much larger than the loop inductance so it can be viewed as a single junction having adjustable critical current. We find that with increasing microwave power $W$ and at certain frequencies $\nu $ and $\nu $/2, the single primary peak in the switching current distribution, \textrm{which is the result of macroscopic quantum tunneling of the phase across the junction}, first shifts toward lower bias current $I$ and then a resonant peak develops. These results are explained by quantum resonant phase escape involving single and two photons with microwave-suppressed potential barrier. As $W$ further increases, the primary peak gradually disappears and the resonant peak grows into a single one while shifting further to lower $I$. At certain $W$, a second resonant peak appears, which can locate at very low $I$ depending on the value of $\nu $. Analysis based on the classical equation of motion shows that such resonant peak can arise from the resonant escape of the phase particle with extremely large oscillation amplitude resulting from bifurcation of the nonlinear system. Our experimental result and theoretical analysis demonstrate that at $T\ll T_{cr}$, escape of the phase particle could be dominated by classical process, such as dynamical bifurcation of nonlinear systems under strong ac driving. ",https://doi.org/10.1103/PhysRevB.81.144518,1004.1863v2,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Disorder effects in topological insulator thin films,1970,"  Thin films of topological insulators (TI) attract large attention because of expected topological effects from the inter-surface hybridization of Dirac points. However, these effects may be depleted by unexpectedly large energy smearing $\Gamma$ of surface Dirac points by the random potential of abundant Coulomb impurities. We show that in a typical TI film with large dielectric constant $\sim 50$ sandwiched between two low dielectric constant layers, the Rytova-Chaplik-Entin-Keldysh modification of the Coulomb potential of a charge impurity allows a larger number of the film impurities to contribute to $\Gamma$. As a result, $\Gamma$ is large and independent of the TI film thickness $d$ for $d > 5$ nm. In thinner films $\Gamma$ grows with decreasing $d$ due to reduction of screening by the hybridization gap. We study the surface conductivity away from the neutrality point and at the neutrality point. In the latter case, we find the maximum TI film thickness at which the hybridization gap is still able to make a TI film insulating and allow observation of the quantum spin Hall effect, $d_{\max} \sim 7$ nm. ",https://doi.org/10.1103/PhysRevB.103.165409,2102.00352v5,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Disorder effects in topological insulator nanowires,1970,"  Three-dimensional topological insulator (TI) nanowires with quantized surface subband spectra are studied as a main component of Majorana bound states (MBS) devices. However, such wires are known to have large concentration $N \sim 10^{19}$ cm$^{-3}$ of Coulomb impurities. It is believed that a MBS device can function only if the amplitude of long-range fluctuations of the random Coulomb potential $\Gamma$ is smaller than the subband gap $\Delta$. Here we calculate $\Gamma$ for recently experimentally studied large-dielectric-constant (Bi$_{1-x}$Sb$_x$)$_2$Te$_{3}$ wires in a small-dielectric-constant environment (no superconductor). We show that provided by such a dielectric-constant contrast, the confinement of electric field of impurities within the wire allows more distant impurities to contribute into $\Gamma$, leading to $\Gamma \sim 3\Delta$. We also calculate a TI wire resistance as a function of the Fermi level and carrier concentration due to scattering on Coulomb and neutral impurities, and do not find observable discrete subband-spectrum related oscillations at $N \gtrsim 10^{18}$ cm$^{-3}$. ",https://doi.org/10.1103/PhysRevB.104.054205,2105.05390v5,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",McShane identities for Higher Teichmüller theory and the   Goncharov-Shen potential,1970,"  We derive generalizations of McShane's identity for higher ranked surface group representations by studying a family of mapping class group invariant functions introduced by Goncharov and Shen which generalize the notion of horocycle lengths. In particular, we obtain McShane-type identities for finite-area cusped convex real projective surfaces by generalizing the Birman--Series geodesic scarcity theorem. More generally, we establish McShane-type identities for positive surface group representations with loxodromic boundary monodromy, as well as McShane-type inequalities for general rank positive representations with unipotent boundary monodromy. Our identities are systematically expressed in terms of projective invariants, and we study these invariants: we establish boundedness and Fuchsian rigidity results for triple and cross ratios. We apply our identities to derive the simple spectral discreteness of unipotent-bordered positive representations, collar lemmas, and generalizations of the Thurston metric. ",Kein DOI-Link verfügbar,1901.02032v4,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Metal-Insulator Transition in $n$-type bulk crystals and films of   strongly compensated SrTiO$_3$,1970,"  We start by analyzing experimental data of Spinelli [A. Spinelli, M. A. Torija, C. Liu, C. Jan, and C. Leighton, Phys. Rev. B 81, 155110 (2010)] for conductivity of $n$-type bulk crystals of SrTiO$_3$ (STO) with broad electron concentration $n$ range of $4\times 10^{15}$ - $4 \times10^{20} $ cm$^{-3}$, at low temperatures. We obtain good fit of the conductivity data, $\sigma(n)$, by the Drude formula for $n \geq n_c \simeq 3 \times 10^{16} $ cm$^{-3}$ assuming that used for doping insulating STO bulk crystals are strongly compensated and the total concentration of background charged impurities is $N = 10^{19}$ cm$^{-3}$. At $n< n_c$, the conductivity collapses with decreasing $n$ and the Drude theory fit fails. We argue that this is the metal-insulator transition (MIT) in spite of the very large Bohr radius of hydrogen-like donor state $a_B \simeq 700$ nm with which the Mott criterion of MIT for a weakly compensated semiconductor, $na_B^3 \simeq 0.02$, predicts $10^{5}$ times smaller $n_c$. We try to explain this discrepancy in the framework of the theory of the percolation MIT in a strongly compensated semiconductor with the same $N=10^{19}$ cm$^{-3}$. In the second part of this paper, we develop the percolation MIT theory for films of strongly compensated semiconductors. We apply this theory to doped STO films with thickness $d \leq 130$ nm and calculate the critical MIT concentration $n_c(d)$. We find that, for doped STO films on insulating STO bulk crystals, $n_c(d)$ grows with decreasing $d$. Remarkably, STO films in a low dielectric constant environment have the same $n_c(d)$. This happens due to the Rytova-Keldysh modification of a charge impurity potential which allows a larger number of the film charged impurities to contribute to the random potential. ",https://doi.org/10.1103/PhysRevMaterials.5.044606,2102.11783v4,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Conductivity of two-dimensional small gap semiconductors and topological   insulators in strong Coulomb disorder,1970,"  We are honored to dedicate this article to Emmanuel Rashba on the occasion of his 95 birthday. In the ideal disorder-free situation, a two-dimensional band gap insulator has an activation energy for conductivity equal to half the band gap, $\Delta$. But transport experiments usually exhibit a much smaller activation energy at low temperature, and the relation between this activation energy and $\Delta$ is unclear. Here we consider the temperature-dependent conductivity of a two-dimensional narrow gap semiconductor on a substrate containing Coulomb impurities, mostly focusing on the case when amplitude of the random potential $\Gamma \gg \Delta$. We show that the conductivity generically exhibits three regimes and only the highest temperature regime exhibits an activation energy that reflects the band gap. At lower temperatures, the conduction proceeds through nearest-neighbor or variable-range hopping between electron and hole puddles created by the disorder. We show that the activation energy and characteristic temperature associated with these processes steeply collapse near a critical impurity concentration. Larger concentrations lead to an exponentially small activation energy and exponentially long localization length, which in mesoscopic samples can appear as a disorder-induced insulator-to-metal transition. We arrive at a similar disorder driven steep insulator-metal transition in thin films of three-dimensional topological insulators with very large dielectric constant, where due to confinement of electric field internal Coulomb impurities create larger disorder potential. Away from neutrality point this unconventional insulator-to-metal transition is augmented by conventional metal-insulator transition at small impurity concentrations, so that we arrive at disorder-driven re-entrant metal-insulator-metal transition. ",https://doi.org/10.1134/S1063776122100065,2201.11652v2,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",An Energy Efficient Semi-static Power Control and Link Adaptation Scheme   in UMTS HSDPA,1970,"  High speed downlink packet access (HSDPA) has been successfully applied in commercial systems and improves user experience significantly. However, it incurs substantial energy consumption. In this paper, we address this issue by proposing a novel energy efficient semi-static power control and link adaptation scheme in HSDPA. Through estimating the EE under different modulation and coding schemes (MCSs) and corresponding transmit power, the proposed scheme can determine the most energy efficient MCS level and transmit power at the Node B. And then the Node B configure the optimal MCS level and transmit power. In order to decrease the signaling overhead caused by the configuration, a dual trigger mechanism is employed. After that, we extend the proposed scheme to the multiple input multiple output (MIMO) scenarios. Simulation results confirm the significant EE improvement of our proposed scheme. Finally, we give a discussion on the potential EE gain and challenge of the energy efficient mode switching between single input multiple output (SIMO) and MIMO configuration in HSDPA. ",Kein DOI-Link verfügbar,1202.1340v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Network Construction with Ordered Constraints,1970,"  In this paper, we study the problem of constructing a network by observing ordered connectivity constraints, which we define herein. These ordered constraints are made to capture realistic properties of real-world problems that are not reflected in previous, more general models. We give hardness of approximation results and nearly-matching upper bounds for the offline problem, and we study the online problem in both general graphs and restricted sub-classes. In the online problem, for general graphs, we give exponentially better upper bounds than exist for algorithms for general connectivity problems. For the restricted classes of stars and paths we are able to find algorithms with optimal competitive ratios, the latter of which involve analysis using a potential function defined over pq-trees. ",Kein DOI-Link verfügbar,1702.07292v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Time-Guided High-Order Attention Model of Longitudinal Heterogeneous   Healthcare Data,1970,"  Due to potential applications in chronic disease management and personalized healthcare, the EHRs data analysis has attracted much attention of both researchers and practitioners. There are three main challenges in modeling longitudinal and heterogeneous EHRs data: heterogeneity, irregular temporality and interpretability. A series of deep learning methods have made remarkable progress in resolving these challenges. Nevertheless, most of existing attention models rely on capturing the 1-order temporal dependencies or 2-order multimodal relationships among feature elements. In this paper, we propose a time-guided high-order attention (TGHOA) model. The proposed method has three major advantages. (1) It can model longitudinal heterogeneous EHRs data via capturing the 3-order correlations of different modalities and the irregular temporal impact of historical events. (2) It can be used to identify the potential concerns of medical features to explain the reasoning process of the healthcare model. (3) It can be easily expanded into cases with more modalities and flexibly applied in different prediction tasks. We evaluate the proposed method in two tasks of mortality prediction and disease ranking on two real world EHRs datasets. Extensive experimental results show the effectiveness of the proposed model. ",Kein DOI-Link verfügbar,1912.00773v1,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Energy efficiency analysis of ammonia-fueled power systems for vehicles   considering residual heat recovery,1970,"  Ammonia, known as a good hydrogen carrier, shows great potential for use as a zero-carbon fuel for vehicles. However, both the internal combustion engine (ICE) and the proton exchange membrane fuel cell (PEMFC), the currently available engines used by the vehicle, require hydrogen decomposed from ammonia. On-board hydrogen production is an energy-intensive process that significantly reduces system efficiency. Therefore, energy recovery from the system's residual heat is essential to promote system efficiency. ICEs and FCs require different amounts of hydrogen, and they produce residual heat of different quality and quantity, so the system efficiency is not only determined by the engine operating point, but also by the measures and ratios of residual heat recovery. To thoroughly understand the relationships between system energy efficiency and system configuration as well as system parameters, this paper takes three typical power systems with different configurations as our objects. Models of three systems are set up for system energy efficiency analysis, and carry out simulations under different conditions to conduct system output power and energy efficiency. By analyzing the simulation results, the factors that most significantly impact the system efficiency are identified, the guidelines for system design and parameter optimization are proposed. ",Kein DOI-Link verfügbar,2406.11296v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Stratospheric Clouds Do Not Impede JWST Transit Spectroscopy for   Exoplanets with Earth-Like Atmospheres,1970,"  The James Webb Space Telescope (JWST) will provide an opportunity to investigate the atmospheres of potentially habitable planets. Aerosols, significantly mute molecular features in transit spectra because they prevent light from probing the deeper layers of the atmosphere. Earth occasionally has stratospheric/high tropospheric clouds at 15-20 km that could substantially limit the observable depth of the underlying atmosphere. We use solar occultations of Earth's atmosphere to create synthetic JWST transit spectra of Earth analogs orbiting dwarf stars. Unlike previous investigations, we consider both clear and cloudy sightlines from the SCISAT satellite. We find that the maximum difference in effective thickness of the atmosphere between a clear and globally cloudy atmosphere is 8.5 km at 2.28 microns with a resolution of 0.02 microns. After incorporating the effects of refraction and Pandexo's noise modeling, we find that JWST would not be able to detect Earth like stratospheric clouds if an exo-Earth was present in the TRAPPIST-1 system, as the cloud spectrum differs from the clear spectrum by a maximum of 10 ppm. These stratospheric clouds are also not robustly detected by TauREx when performing spectral retrieval for a cloudy TRAPPIST-1 planet. However, if an Earth size planet were to orbit in a white dwarf's habitable zone, then, we predict that JWST's NIRSpec would be able to detect its stratospheric clouds after only 4 transits. We conclude that stratospheric clouds would not impede JWST transit spectroscopy or the detection of biosignatures for Earth-like atmospheres. ",https://doi.org/10.1093/mnras/stac1869,2207.00015v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Designing a Deep Learning-Driven Resource-Efficient Diagnostic System   for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and   Improving Patient Survival in Developing Countries,1970,"  Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model training efficiency. The visual comparisons between the model prediction and ground truth have demonstrated that the MobileNetV2 diagnostic models can identify very small cancerous nodes embedded in a large area of normal cells which is challenging for manual image analysis. Equally Important, the light weighted MobleNetV2 models were computationally efficient and ready for mobile devices or devices of low computational power. These advances empower the development of a resource-efficient and high performing AI-based metastatic breast cancer diagnostic system that can adapt to under-resourced healthcare facilities in developing countries. This research provides an innovative technological solution to address the long delays in metastatic breast cancer diagnosis and the consequent disparity in patient survival outcome in developing countries. ",Kein DOI-Link verfügbar,2308.02597v1,Yes,innovative(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Conductivity of two-dimensional narrow gap semiconductors subjected to   strong Coulomb disorder,1970,"  In the ideal disorder-free situation, a two-dimensional band gap insulator has an activation energy for conductivity equal to half the band gap $\Delta$. But transport experiments usually exhibit a much smaller activation energy at low temperature, and the relation between this activation energy and $\Delta$ is unclear. Here we consider the temperature-dependent conductivity of a two-dimensional insulator on a substrate containing Coulomb impurities, with random potential amplitude $\Gamma \gg \Delta$. We show that the conductivity generically exhibits three regimes of conductivity, and only the highest temperature regime exhibits an activation energy that reflects the band gap. At lower temperatures, the conduction proceeds through activated hopping or Efros-Shklovskii variable-range hopping between electron and hole puddles created by the disorder. We show that the activation energy and characteristic temperature associated with these processes steeply collapse near a critical impurity concentration. Larger concentrations lead to an exponentially small activation energy and exponentially long localization length, which in mesoscopic samples can appear as a disorder-induced insulator-to-metal transition. We also arrive at a similar steep disorder driven insulator-metal transition in thin films of three-dimensional topological insulators with large dielectric constant, for which Coulomb impurities inside the film create a large disorder potential due to confinement of their electric field inside the film. ",https://doi.org/10.1103/PhysRevB.105.054206,2111.09466v4,Yes,potent(2)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Automatic diagnosis of cardiac magnetic resonance images based on   semi-supervised learning,1970,"  Cardiac magnetic resonance imaging (MRI) is a pivotal tool for assessing cardiac function. Precise segmentation of cardiac structures is imperative for accurate cardiac functional evaluation. This paper introduces a semi-supervised model for automatic segmentation of cardiac images and auxiliary diagnosis. By harnessing cardiac MRI images and necessitating only a small portion of annotated image data, the model achieves fully automated, high-precision segmentation of cardiac images, extraction of features, calculation of clinical indices, and prediction of diseases. The provided segmentation results, clinical indices, and prediction outcomes can aid physicians in diagnosis, thereby serving as auxiliary diagnostic tools. Experimental results showcase that this semi-supervised model for automatic segmentation of cardiac images and auxiliary diagnosis attains high accuracy in segmentation and correctness in prediction, demonstrating substantial practical guidance and application value. ",Kein DOI-Link verfügbar,2405.14300v1,Yes,pivotal(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",USFM: A Universal Ultrasound Foundation Model Generalized to Tasks and   Organs towards Label Efficient Image Analysis,1970,"  Inadequate generality across different organs and tasks constrains the application of ultrasound (US) image analysis methods in smart healthcare. Building a universal US foundation model holds the potential to address these issues. Nevertheless, the development of such foundational models encounters intrinsic challenges in US analysis, i.e., insufficient databases, low quality, and ineffective features. In this paper, we present a universal US foundation model, named USFM, generalized to diverse tasks and organs towards label efficient US image analysis. First, a large-scale Multi-organ, Multi-center, and Multi-device US database was built, comprehensively containing over two million US images. Organ-balanced sampling was employed for unbiased learning. Then, USFM is self-supervised pre-trained on the sufficient US database. To extract the effective features from low-quality US images, we proposed a spatial-frequency dual masked image modeling method. A productive spatial noise addition-recovery approach was designed to learn meaningful US information robustly, while a novel frequency band-stop masking learning approach was also employed to extract complex, implicit grayscale distribution and textural variations. Extensive experiments were conducted on the various tasks of segmentation, classification, and image enhancement from diverse organs and diseases. Comparisons with representative US image analysis models illustrate the universality and effectiveness of USFM. The label efficiency experiments suggest the USFM obtains robust performance with only 20% annotation, laying the groundwork for the rapid development of US models in clinical practices. ",Kein DOI-Link verfügbar,2401.00153v2,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",High-fidelity achromatic metalens imaging via deep neural network,1970,"  Meta-optics are attracting intensive interest as alternatives to traditional optical systems comprising multiple lenses and diffractive elements. Among applications, single metalens imaging is highly attractive due to the potential for achieving significant size reduction and simplified design. However, single metalenses exhibit severe chromatic aberration arising from material dispersion and the nature of singlet optics, making them unsuitable for full-color imaging requiring achromatic performance. In this work, we propose and validate a deep learning-based single metalens imaging system to overcome chromatic aberration in varied scenarios. The developed deep learning networks computationally reconstruct raw imaging captures through reliably refocusing red, green and blue channels to eliminate chromatic aberration and enhance resolution without altering the metalens hardware. The networks demonstrate consistent enhancement across different aperture sizes and focusing distances. Images outside the training set and real-world photos were also successfully reconstructed. Our approach provides a new means to achieve achromatic metalenses without complex engineering, enabling practical and simplified implementation to overcome inherent limitations of meta-optics. ",Kein DOI-Link verfügbar,2308.00211v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for   Mobile Phones,1970,"  Atrial fibrillation (AF) is characterized by irregular electrical impulses originating in the atria, which can lead to severe complications and even death. Due to the intermittent nature of the AF, early and timely monitoring of AF is critical for patients to prevent further exacerbation of the condition. Although ambulatory ECG Holter monitors provide accurate monitoring, the high cost of these devices hinders their wider adoption. Current mobile-based AF detection systems offer a portable solution. However, these systems have various applicability issues, such as being easily affected by environmental factors and requiring significant user effort. To overcome the above limitations, we present AcousAF, a novel AF detection system based on acoustic sensors of smartphones. Particularly, we explore the potential of pulse wave acquisition from the wrist using smartphone speakers and microphones. In addition, we propose a well-designed framework comprised of pulse wave probing, pulse wave extraction, and AF detection to ensure accurate and reliable AF detection. We collect data from 20 participants utilizing our custom data collection application on the smartphone. Extensive experimental results demonstrate the high performance of our system, with 92.8% accuracy, 86.9% precision, 87.4% recall, and 87.1% F1 Score. ",https://doi.org/10.1145/3675094.3678488,2408.04912v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Macroscopic and direct light propulsion of bulk graphene material,1970,"  It has been a great challenge to achieve the direct light manipulation of matter on a bulk scale. In this work, the direct light propulsion of matter was observed on a macroscopic scale for the first time using a bulk graphene based material. The unique structure and properties of graphene and the morphology of the bulk graphene material make it capable of not only absorbing light at various wavelengths but also emitting energetic electrons efficiently enough to drive the bulk material following Newtonian mechanics. Thus, the unique photonic and electronic properties of individual graphene sheets are manifested in the response of the bulk state. These results offer an exciting opportunity to bring about bulk scale light manipulation with the potential to realize long-sought proposals in areas such as the solar sail and space transportation driven directly by sunlight. ",Kein DOI-Link verfügbar,1505.04254v1,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",Entropic and Near-Field Improvements of Thermoradiative Cells,1970,"  A p-n junction maintained at above ambient temperature can work as a heat engine, converting some of the supplied heat into electricity and rejecting entropy by interband emission. Such thermoradiative cells have potential to harvest low-grade heat into electricity. By analyzing the entropy content of different spectral components of thermal radiation, we identify an approach to increase the efficiency of thermoradiative cells via spectrally selecting long-wavelength photons for radiative exchange. Furthermore, we predict that the near-field photon extraction by coupling photons generated from interband electronic transition to phonon polariton modes on the surface of a heat sink can increase the conversion efficiency as well as the power generation density, providing more opportunities to efficiently utilize terrestrial emission for clean energy. An ideal InSb thermoradiative cell can achieve a maximum efficiency and power density up to 20.4 % and 327 Wm-2, respectively, between a hot source at 500K and a cold sink at 300K. However, sub-bandgap and non-radiative losses will significantly degrade the cell performance. ",https://doi.org/10.1038/srep34837,1606.09297v2,Yes,potent(1)
0000-0001-8144-1227,Yi Huang,"Monash University, The University of Melbourne, The University of Melbourne: Melbourne",A holistically 3D-printed flexible millimeter-wave Doppler radar:   Towards fully printed high-frequency multilayer flexible hybrid electronics   systems,1970,"  Flexible hybrid electronics (FHE) is an emerging technology enabled through the integration of advanced semiconductor devices and 3D printing technology. It unlocks tremendous market potential by realizing low-cost flexible circuits and systems that can be conformally integrated into various applications. However, the operating frequencies of most reported FHE systems are relatively low. It is also worth to note that reported FHE systems have been limited to relatively simple design concept (since complex systems will impose challenges in aspects such as multilayer interconnections, printing materials, and bonding layers). Here, we report a fully 3D-printed flexible four-layer millimeter-wave Doppler radar (i.e., a millimeter-wave FHE system). The sensing performance and flexibility of the 3D-printed radar are characterized and validated by general field tests and bending tests, respectively. Our results demonstrate the feasibility of developing fully 3D-printed high-frequency multilayer FHE, which can be conformally integrated into irregular surfaces (e.g., vehicle bumpers) for applications such as vehicle radars and wearable electronics. ",Kein DOI-Link verfügbar,2302.12428v1,Yes,potent(1)
0000-0003-4694-8158,Yuwei Chen,The University of Melbourne,"Energy Circuit-based Integrated Energy Management System: Theory,   Implementation, and Application",1970,"  Integrated energy systems (IESs), in which various energy flows are interconnected and coordinated to release potential flexibility for more efficient and secure operation, have drawn increasing attention in recent years. In this article, an integrated energy management system (IEMS) that performs online analysis and optimization on coupling energy flows in an IES is comprehensively introduced. From the theory perspective, an energy circuit method (ECM) that models natural gas networks and heating networks in the frequency domain is discussed. This method extends the electric circuit modeling of power systems to IESs and enables the IEMS to manage large-scale IESs. From the implementation perspective, the architecture design and function development of the IEMS are presented. Tutorial examples with illustrative case studies are provided to demonstrate its functions of dynamic state estimation, energy flow analysis, security assessment and control, and optimal energy flow. From the application perspective, real-world engineering demonstrations that apply IEMSs in managing building-scale, park-scale, and city-scale IESs are reported. The economic and environmental benefits obtained in these demonstration projects indicate that the IEMS has broad application prospects for a low/zero-carbon future energy system. ",Kein DOI-Link verfügbar,2206.12800v3,Yes,potent(1)
0009-0005-3606-2822,Kevin Yang,The University of Melbourne,THOUGHTSCULPT: Reasoning with Intermediate Revision and Search,1970,"  We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage). ",Kein DOI-Link verfügbar,2404.05966v1,Yes,potent(1)
0009-0005-3606-2822,Kevin Yang,The University of Melbourne,The modified Poynting theorem and the concept of mutual energy,1970,"  The goal of this article is to derive the reciprocity theorem, mutual energy theorem from Poynting theorem instead of from Maxwell equation. The Poynting theorem is generalized to the modified Poynting theorem. In the modified Poynting theorem the electromagnetic field is superimposition of different electromagnetic fields including the retarded potential and advanced potential, time-offset field. The media epsilon (permittivity) and mu (permeability) can also be different in the different fields. The concept of mutual energy is introduced which is the difference between the total energy and self-energy. Mixed mutual energy theorem is derived. We derive the mutual energy from Fourier domain. We obtain the time-reversed mutual energy theorem and the mutual energy theorem. Then we derive the mutual energy theorem in time-domain. The instantaneous modified mutual energy theorem is derived. Applying time-offset transform and time integral to the instantaneous modified mutual energy theorem, the time-correlation modified mutual energy theorem is obtained. Assume there are two electromagnetic fields one is retarded potential and one is advanced potential, the convolution reciprocity theorem can be derived. Corresponding to the modified time-correlation mutual energy theorem and the time-convolution reciprocity theorem in Fourier domain, there is the modified mutual energy theorem and the Lorentz reciprocity theorem. Hence all mutual energy theorem and the reciprocity theorems are put in one frame of the concept of the mutual energy. 3 new Complementary theorems are derived. The inner product is introduced for two different electromagnetic fields in both time domain and Fourier domain for the application of the wave expansion. ",Kein DOI-Link verfügbar,1503.02006v3,Yes,potent(4)
0009-0005-3606-2822,Kevin Yang,The University of Melbourne,The principle of the mutual energy,1970,"  Advanced potential solution of Maxwell equations isn't often accepted. We have proven if without advanced potential, it is not possible to satisfy the Maxwell equations. We also shown that it is not the Poynting vector related energy current transferring energy in the space and it is the mutual energy really did that. A important result of the mutual energy theorem is that the advanced potential can suck energy from the transmitter. This energy is equal to the energy received at the receiver. Hence a transmitter can not send any energy out without the receiver. For two remote objects, the energy is transferred only can by the mutual energy of a retarded potential from the source together with an advanced potential from the sink. If the sucked energy is discrete, the summation of mutual energy current of the infinite background atoms or currents, which can be seen as receivers, is a random process. This means that the photon energy sent by the transmitter is actually grabbed by the receiver. Hence the photon from very beginning knows their destination. This receiver send advanced potential to the transmitter. This explanation also avoided the wave function collapse. The retarded potential first reached the receiver, cause the current in the receiver, the current of receiver send a advanced potential to the transmitter with a reversed time, in the same time, a photon minus-time-instantly runs from receiver to transmitter. In our normal feeling, the photon is still runs from the transmitter to the receiver with a positive time. How to transfer superluminal signal using advanced potential is also discussed.Maxwell equations as principle of the theory of the electromagnetic fields is replaced by the mutual energy principle. ",Kein DOI-Link verfügbar,1606.08710v10,Yes,potent(9)
0009-0005-3606-2822,Kevin Yang,The University of Melbourne,The mutual energy current interpretation for quantum mechanics,1970,"  Quantum physics has the probability interpretation. From the knowledge of light, we know that wave is always spread out, and hence the electron wave should also spread out. That means the electron wave beam should like the light wave beam become diverged from the source. When the electron is received by an atom we thought the wave collapse. The place to collapse is depends on the probability calculated from the square of absolute value of the wave function. The recent new discovery tell us that the light is not just wave, it is a combination of waves, retarded potential and advanced potential. These two potentials together produce the mutual energy current or referred as M-current. Another light energy current is P-current related to Poynting vector. We found P-current doesn't carry any energy for light. The contribution of P-current to energy transfer can be omitted. The light energy is transferred only by M-current. The beam of M-current doesn't like the beam of P-current which is diverged from the source, instead, the M-current beam first diverges from a source and then converged to a sink. Since the M-current at the place to be received is localized at one electron, the concept of wave function collapse is needless. The probability results of light is because that we have use P-current to roughly calculate the M-current. We thought if Schr\""odinger knew today's light theory, he would for sure also build his wave theory for quantum mechanics similar to the new light theory with M-current. Hence we claim that the M-current theory is not only suitable to the light but also can be applied to the quantum physics. This means all particles are M-current. The M-current is composed of not only the retarded wave, but also the advanced wave. M-current is an inner product of a retarded and an advanced waves. ",Kein DOI-Link verfügbar,1608.08055v2,Yes,potent(3)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled   Prescriptive Maintenance Framework,1970,"  Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces an integrated framework that leverages the capabilities of the Transformer model-based neural networks and deep reinforcement learning (DRL) algorithms to optimize system maintenance actions. Our approach employs the Transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the remaining useful life (RUL) of an equipment. Additionally, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions, compared to the other prevalent machine learning-based methods. Our proposed approach provides an innovative data-driven framework for industry machine systems, accurately forecasting equipment lifespans and optimizing maintenance schedules, thereby reducing downtime and cutting costs. ",Kein DOI-Link verfügbar,2309.16935v3,Yes,innovative(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",The Algebraic Expressions of Huygens Principle and Holographic Principle   of Light,1970,"  Huygens principle (HP) is the cornerstone of wave optics, its mathematical model is a boundary value problem of wave equation. The solutions of this mathematical model should be partial derivative u sub n independent and satisfy the form of retarded potential. In the engaged formulas, only the Rayleigh-Sommerfeld diffraction formula (RSDF) satisfies these two restrictions. Unfortunately, the HP requires spherical boundary, while the boundary of RSDF is an infinite plane. Besides that, we find the the geometric constructions of HP and holographic principle of light (HPL) are complementary. Here we derive out the complete expressions of HP and HPL with spherical boundary, based on the method of images. Furthermore, the HP, HPL and RSDF are combined into one new principle that if the boundary of a vacuum region is a spherical surface or an infinite plane, all the light in this vacuum region is determined by the light on the boundary. ",Kein DOI-Link verfügbar,2001.06654v1,Yes,potent(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",3D-Carbon: An Analytical Carbon Modeling Tool for 3D and 2.5D Integrated   Circuits,1970,"  Environmental sustainability is crucial for Integrated Circuits (ICs) across their lifecycle, particularly in manufacturing and use. Meanwhile, ICs using 3D/2.5D integration technologies have emerged as promising solutions to meet the growing demands for computational power. However, there is a distinct lack of carbon modeling tools for 3D/2.5D ICs. Addressing this, we propose 3D-Carbon, an analytical carbon modeling tool designed to quantify the carbon emissions of 3D/2.5D ICs throughout their life cycle. 3D-Carbon factors in both potential savings and overheads from advanced integration technologies, considering practical deployment constraints like bandwidth. We validate 3D-Carbon's accuracy against established baselines and illustrate its utility through case studies in autonomous vehicles. We believe that 3D-Carbon lays the initial foundation for future innovations in developing environmentally sustainable 3D/2.5D ICs. Our open-source code is available at https://github.com/UMN-ZhaoLab/3D-Carbon. ",Kein DOI-Link verfügbar,2307.08060v4,Yes,potent(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",Metamaterial-Controlled Parity-Time Symmetry in Non-Hermitian Wireless   Power Transfer Systems,1970,"  Inductive wireless power transfer (WPT) systems can be effectively described as non-Hermitian systems using the coupled-mode theory. In these systems, parity-time (PT) symmetric states facilitate efficient power transfer. Traditionally, passive resonators have been used as relay devices in such systems to extend transmission distance; however, this approach may induce additional eigenstates with broken PT symmetry, particularly when specific spatial arrangements of the relay resonators, dependent on the positions of the transmitting (Tx) and receiving (Rx) resonators, are not maintained. This limitation hampers applications like free positioning WPT. To address this challenge, we introduce a multibody WPT system employing metamaterial controlled PT symmetry, which circumvents the constraints of physical arrangement. We utilize inverse design to configure the metamaterial, targeting a specific resonance mode that controls the effective coupling coefficients. Our approach ensures that a PT-symmetric state emerges when these coefficients, relating to the metamaterial and both the Tx and Rx resonators, are balanced. We confirm the stability of this state in a strong coupling regime, both theoretically and experimentally. Our experiments demonstrate the formation of PT-symmetric states governed by the metamaterial's resonant mode, achievable even with varying sizes and positions of the Tx and Rx in relation to the metamaterial. Moreover, we show that the PT symmetric state is attainable with different spatial configurations of the Rx resonator. This finding underscores our system's potential for free-positioning WPT, significantly broadening its applicability. ",Kein DOI-Link verfügbar,2312.04829v2,Yes,potent(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university","A Survey of Federated Unlearning: A Taxonomy, Challenges and Future   Directions",1970,"  The evolution of privacy-preserving Federated Learning (FL) has led to an increasing demand for implementing the right to be forgotten. The implementation of selective forgetting is particularly challenging in FL due to its decentralized nature. This complexity has given rise to a new field, Federated Unlearning (FU). FU emerges as a strategic solution to address the increasing need for data privacy, including the implementation of the `right to be forgotten'. The primary challenge in developing FU approaches lies in balancing the trade-offs in privacy, security, utility, and efficiency, as these elements often have competing requirements. Achieving an optimal equilibrium among these facets is crucial for maintaining the effectiveness and usability of FL systems while adhering to privacy and security standards. This survey provides a comprehensive analysis of existing FU methods, incorporating a detailed review of the various evaluation metrics. Furthermore, we unify these diverse methods and metrics into an experimental framework. Additionally, the survey discusses potential future research directions in FU. Finally, a continually updated repository of related open-source materials is available at: https://github.com/abbottyanginchina/Awesome-Federated-Unlearning. ",Kein DOI-Link verfügbar,2310.19218v3,Yes,potent(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",Phrase Table as Recommendation Memory for Neural Machine Translation,1970,"  Neural Machine Translation (NMT) has drawn much attention due to its promising translation performance recently. However, several studies indicate that NMT often generates fluent but unfaithful translations. In this paper, we propose a method to alleviate this problem by using a phrase table as recommendation memory. The main idea is to add bonus to words worthy of recommendation, so that NMT can make correct predictions. Specifically, we first derive a prefix tree to accommodate all the candidate target phrases by searching the phrase translation table according to the source sentence. Then, we construct a recommendation word set by matching between candidate target phrases and previously translated target words by NMT. After that, we determine the specific bonus value for each recommendable word by using the attention vector and phrase translation probability. Finally, we integrate this bonus value into NMT to improve the translation results. The extensive experiments demonstrate that the proposed methods obtain remarkable improvements over the strong attentionbased NMT. ",Kein DOI-Link verfügbar,1805.09960v1,Yes,commendable(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",Chat-3D: Data-efficiently Tuning Large Language Model for Universal   Dialogue of 3D Scenes,1970,"  3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. Our experiments show that Chat-3D achieves an impressive ability to comprehend diverse instructions for 3D scenes, engage in intricate spatial reasoning, and incorporate external knowledge into its responses. Chat-3D achieves a 75.6% relative score compared with GPT-4 on the constructed instruction dataset. ",Kein DOI-Link verfügbar,2308.08769v1,Yes,intricate(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",Intell-dragonfly: A Cybersecurity Attack Surface Generation Engine Based   On Artificial Intelligence-generated Content Technology,1970,"  With the rapid development of the Internet, cyber security issues have become increasingly prominent. Traditional cyber security defense methods are limited in the face of ever-changing threats, so it is critical to seek innovative attack surface generation methods. This study proposes Intell-dragonfly, a cyber security attack surface generation engine based on artificial intelligence generation technology, to meet the challenges of cyber security. Based on ChatGPT technology, this paper designs an automated attack surface generation process, which can generate diversified and personalized attack scenarios, targets, elements and schemes. Through experiments in a real network environment, the effect of the engine is verified and compared with traditional methods, which improves the authenticity and applicability of the attack surface. The experimental results show that the ChatGPT-based method has significant advantages in the accuracy, diversity and operability of attack surface generation. Furthermore, we explore the strengths and limitations of the engine and discuss its potential applications in the field of cyber security. This research provides a novel approach to the field of cyber security that is expected to have a positive impact on defense and prevention of cyberthreats. ",Kein DOI-Link verfügbar,2311.00240v1,Yes,"innovative(1), potent(1)"
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",De novo Drug Design using Reinforcement Learning with Multiple GPT   Agents,1970,"  De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT. ",Kein DOI-Link verfügbar,2401.06155v1,Yes,"pivotal(1), potent(1)"
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",A Survey of 6G Wireless Communications: Emerging Technologies,1970,"  While fifth-generation (5G) communications are being rolled out around the world, sixth-generation (6G) communications have attracted much attention from both the industry and academia. Compared with 5G, 6G will have a wider frequency band, higher transmission rate, spectrum efficiency, greater connection capacity, shorter delay, wider coverage and stronger anti-interference capability, so as to meet the various network requirements for industries. In this paper, we present a survey of potential essential technologies in 6G. In particular, we will introduce artificial intelligence, intelligent surfaces, terahertz communications technologies in detail, while giving a brief introduction to other potential technologies, including visible light communications, Blockchain-enabled wireless network, advanced duplex and holographic radio. ",Kein DOI-Link verfügbar,2004.08549v3,Yes,potent(2)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university","Learning Traffic Crashes as Language: Datasets, Benchmarks, and What-if   Causal Analyses",1970,"  The increasing rate of road accidents worldwide results not only in significant loss of life but also imposes billions financial burdens on societies. Current research in traffic crash frequency modeling and analysis has predominantly approached the problem as classification tasks, focusing mainly on learning-based classification or ensemble learning methods. These approaches often overlook the intricate relationships among the complex infrastructure, environmental, human and contextual factors related to traffic crashes and risky situations. In contrast, we initially propose a large-scale traffic crash language dataset, named CrashEvent, summarizing 19,340 real-world crash reports and incorporating infrastructure data, environmental and traffic textual and visual information in Washington State. Leveraging this rich dataset, we further formulate the crash event feature learning as a novel text reasoning problem and further fine-tune various large language models (LLMs) to predict detailed accident outcomes, such as crash types, severity and number of injuries, based on contextual and environmental factors. The proposed model, CrashLLM, distinguishes itself from existing solutions by leveraging the inherent text reasoning capabilities of LLMs to parse and learn from complex, unstructured data, thereby enabling a more nuanced analysis of contributing factors. Our experiments results shows that our LLM-based approach not only predicts the severity of accidents but also classifies different types of accidents and predicts injury outcomes, all with averaged F1 score boosted from 34.9% to 53.8%. Furthermore, CrashLLM can provide valuable insights for numerous open-world what-if situational-awareness traffic safety analyses with learned reasoning features, which existing models cannot offer. We make our benchmark, datasets, and model public available for further exploration. ",Kein DOI-Link verfügbar,2406.10789v1,Yes,intricate(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",A New MRAM-based Process In-Memory Accelerator for Efficient Neural   Network Training with Floating Point Precision,1970,"  The excellent performance of modern deep neural networks (DNNs) comes at an often prohibitive training cost, limiting the rapid development of DNN innovations and raising various environmental concerns. To reduce the dominant data movement cost of training, process in-memory (PIM) has emerged as a promising solution as it alleviates the need to access DNN weights. However, state-of-the-art PIM DNN training accelerators employ either analog/mixed signal computing which has limited precision or digital computing based on a memory technology that supports limited logic functions and thus requires complicated procedure to realize floating point computation. In this paper, we propose a spin orbit torque magnetic random access memory (SOT-MRAM) based digital PIM accelerator that supports floating point precision. Specifically, this new accelerator features an innovative (1) SOT-MRAM cell, (2) full addition design, and (3) floating point computation. Experiment results show that the proposed SOT-MRAM PIM based DNN training accelerator can achieve 3.3$\times$, 1.8$\times$, and 2.5$\times$ improvement in terms of energy, latency, and area, respectively, compared with a state-of-the-art PIM based DNN training accelerator. ",Kein DOI-Link verfügbar,2003.01551v2,Yes,innovative(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",CoRTEx: Contrastive Learning for Representing Terms via Explanations   with Applications on Constructing Biomedical Knowledge Graphs,1970,"  Objective: Biomedical Knowledge Graphs play a pivotal role in various biomedical research domains. Concurrently, term clustering emerges as a crucial step in constructing these knowledge graphs, aiming to identify synonymous terms. Due to a lack of knowledge, previous contrastive learning models trained with Unified Medical Language System (UMLS) synonyms struggle at clustering difficult terms and do not generalize well beyond UMLS terms. In this work, we leverage the world knowledge from Large Language Models (LLMs) and propose Contrastive Learning for Representing Terms via Explanations (CoRTEx) to enhance term representation and significantly improves term clustering. Materials and Methods: The model training involves generating explanations for a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning, considering term and explanation embeddings simultaneously, and progressively introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH algorithm is designed for efficient clustering of a new ontology. Results: We established a clustering test set and a hard negative test set, where our model consistently achieves the highest F1 score. With CoRTEx embeddings and the modified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical Informatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries to ChatGPT. Case studies highlight the model's efficacy in handling challenging samples, aided by information from explanations. Conclusion: By aligning terms to their explanations, CoRTEx demonstrates superior accuracy over benchmark models and robustness beyond its training set, and it is suitable for clustering terms for large-scale biomedical ontologies. ",Kein DOI-Link verfügbar,2312.08036v1,Yes,pivotal(1)
0000-0002-0856-2724,Yang Zhao,"Peking University, peking university",Advancing Real-time Pandemic Forecasting Using Large Language Models: A   COVID-19 Case Study,1970,"  Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior. Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers. Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models. This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs. The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions. The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models. This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future. ",Kein DOI-Link verfügbar,2404.06962v1,Yes,potent(1)
0000-0003-1817-6357,Jie Zheng,Peking University,One Step at a Time: Does Gradualism Build Coordination?,1970,"  This study investigates a potential mechanism to promote coordination. With theoretical guidance using a belief-based learning model, we conduct a multi-period, binary-choice, and weakest-link laboratory coordination experiment to study the effect of gradualism - increasing the required levels (stakes) of contributions slowly over time rather than requiring a high level of contribution immediately - on group coordination performance. We randomly assign subjects to three treatments: starting and continuing at a high stake, starting at a low stake but jumping to a high stake after a few periods, and starting at a low stake while gradually increasing the stakes over time (the Gradualism treatment). We find that relative to the other two treatments, groups coordinate most successfully at high stakes in the Gradualism treatment. We also find evidence that supports the belief-based learning model. These findings point to a simple mechanism for promoting successful voluntary coordination. ",https://doi.org/10.1287/mnsc.2018.3210,2006.01386v1,Yes,potent(1)
0000-0003-1817-6357,Jie Zheng,Peking University,Performance Evaluation of the Symmetrical Quasi-Classical Dynamics   Method based on Meyer-Miller Mapping Hamiltonian in the Treatment of   Site-Exciton Models,1970,"  The symmetrical quasi-classical dynamics method based on the Meyer-Miller mapping Hamiltonian (MM-SQC) shows the great potential in the treatment of the nonadiabatic dynamics of complex systems. We performed the comprehensive benchmark calculations to evaluate the performance of the MM-SQC method in various site-exciton models with respect to the accurate results of quantum dynamics method multilayer multiconfigurational time-dependent Hartree (ML-MCTDH). The parameters of the site-exciton models are chosen to represent a few of prototypes used in the description of photoinduced excitonic dynamics processes in photoharvesting systems and organic solar cells, which include the rather board situations with the fast or slow bath and different system-bath couplings. When the characteristic frequency of the bath is low, the MM-SQC method performs extremely well, and it gives almost the identical results to those of ML-MCTDH. When the fast bath is considered, the deviations exist between the MM-SQC and ML-MCTDH results if the high-frequency bath modes are improperly treated by the classical manner. When the so-called adiabatic renormalization was employed to construct the reduced Hamiltonian by freezing high-frequency modes, the MM-SQC dynamics can give the results comparable to the ML-MCTDH ones. Thus, the MM-SQC method itself provide reasonable results in all test site-exciton models, while the proper treatments of the bath modes must be employed. The possible dependence of the MM-SQC dynamics on the different initial sampling methods for the nuclear degrees of freedom is also discussed. ",https://doi.org/10.1063/1.5047002,1807.01818v4,Yes,potent(1)
0000-0003-1817-6357,Jie Zheng,Peking University,SLInterpreter: An Exploratory and Iterative Human-AI Collaborative   System for GNN-based Synthetic Lethal Prediction,1970,"  Synthetic Lethal (SL) relationships, though rare among the vast array of gene combinations, hold substantial promise for targeted cancer therapy. Despite advancements in AI model accuracy, there is still a significant need among domain experts for interpretive paths and mechanism explorations that align better with domain-specific knowledge, particularly due to the high costs of experimentation. To address this gap, we propose an iterative Human-AI collaborative framework with two key components: 1) Human-Engaged Knowledge Graph Refinement based on Metapath Strategies, which leverages insights from interpretive paths and domain expertise to refine the knowledge graph through metapath strategies with appropriate granularity. 2) Cross-Granularity SL Interpretation Enhancement and Mechanism Analysis, which aids experts in organizing and comparing predictions and interpretive paths across different granularities, uncovering new SL relationships, enhancing result interpretation, and elucidating potential mechanisms inferred by Graph Neural Network (GNN) models. These components cyclically optimize model predictions and mechanism explorations, enhancing expert involvement and intervention to build trust. Facilitated by SLInterpreter, this framework ensures that newly generated interpretive paths increasingly align with domain knowledge and adhere more closely to real-world biological principles through iterative Human-AI collaboration. We evaluate the framework's efficacy through a case study and expert interviews. ",Kein DOI-Link verfügbar,2407.14770v1,Yes,potent(1)
0000-0003-1817-6357,Jie Zheng,Peking University,Probing the shape of the primordial curvature power spectrum and the   energy scale of reheating with pulsar timing arrays,1970,"  The stochastic gravitational wave background (SGWB) provides a unique opportunity to probe the early Universe, potentially encoding information about the primordial curvature power spectrum and the energy scale of reheating. Recent observations by collaborations such as NANOGrav, PPTA, EPTA+InPTA, and CPTA have detected a stochastic common-spectrum signal, which may originate from scalar-induced gravitational waves (SIGWs) generated by primordial curvature perturbations during inflation. In this study, we explore the hypothesis that the NANOGrav signal is sourced by SIGWs and aim to constrain the shape of the primordial curvature power spectrum and the reheating energy scale using the NANOGrav 15-year data set. We model the primordial curvature power spectrum with a lognormal form and focus on the case where the equation of state during reheating is $w=1/6$, corresponding to an inflaton potential $V(\phi) \sim \phi^{14/5}$. Employing Bayesian inference, we obtain posterior distributions for the lognormal power spectrum parameters and the reheating temperature. Our results indicate a narrow peak in the primordial power spectrum ($\Delta < 0.001$ at 90\% confidence) and a lower bound on the reheating temperature ($T_{\rm rh} \geq 0.1 {\rm GeV}$), consistent with Big Bang Nucleosynthesis constraints. The best-fit SIGW energy density spectrum exhibits a distinct turning point around $f \sim 10^{-8.1}\,{\rm Hz}$, corresponding to the transition from reheating to the radiation-dominated era. This feature, combined with the sharp high-frequency decrease due to the narrow primordial power spectrum peak, offers a unique signature for probing early Universe properties. ",Kein DOI-Link verfügbar,2407.15501v1,Yes,potent(2)
0000-0003-1817-6357,Jie Zheng,Peking University,Initial Sampling in Symmetrical Quasiclassical Dynamics Based on   Li-Miller Mapping Hamiltonian,1970,"  A symmetrical quasiclassical (SQC) dynamics approach based on the Li-Miller (LM) mapping Hamiltonian (SQC-LM) was employed to describe nonadiabatic dynamics. In principle, the different initial sampling procedures may be applied in the SQC-LM dynamics, and the results may be dependent on the initial sampling. We provided various initial sampling approaches and checked their influence. We selected two groups of models including site-exciton models for exciton dynamics and linear vibronic coupling models for conical intersections to test the performance of SQC-LM dynamics with the different initial sampling methods. The results were examined with respect to those of the accurate multilayer multiconfigurational time-dependent Hartree (ML-MCTDH) quantum dynamics. For both two models, the SQC-LM method more-or-less gives a reasonable description of the population dynamics, while the influence of the initial sampling approaches on the final results is noticeable. It seems that the proper initial sampling methods should be determined by the system under study. This indicates that the combination of the SQC-LM method with a suitable sampling approach may be a potential method in the description of nonadiabatic dynamics. ",https://doi.org/10.1039/C9CP03975A,1811.00257v2,Yes,potent(1)
0000-0003-1817-6357,Jie Zheng,Peking University,Evaluation of GPT and BERT-based models on identifying protein-protein   interactions in biomedical text,1970,"  Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformers (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the performance of PPI identification of multiple GPT and BERT models using three manually curated gold-standard corpora: Learning Language in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference Database with 163 PPIs in 145 sentences, and Interaction Extraction Performance Assessment with 335 PPIs in 486 sentences. BERT-based models achieved the best overall performance, with BioBERT achieving the highest recall (91.95%) and F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%). Interestingly, despite not being explicitly trained for biomedical texts, GPT-4 achieved commendable performance, comparable to the top-performing BERT models. It achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of 86.49% on the LLL dataset. These results suggest that GPT models can effectively detect PPIs from text data, offering promising avenues for application in biomedical literature mining. Further research could explore how these models might be fine-tuned for even more specialized tasks within the biomedical domain. ",Kein DOI-Link verfügbar,2303.17728v2,Yes,commendable(1)
0000-0003-1817-6357,Jie Zheng,Peking University,PASTO: Strategic Parameter Optimization in Recommendation Systems --   Probabilistic is Better than Deterministic,1970,"  Real-world recommendation systems often consist of two phases. In the first phase, multiple predictive models produce the probability of different immediate user actions. In the second phase, these predictions are aggregated according to a set of 'strategic parameters' to meet a diverse set of business goals, such as longer user engagement, higher revenue potential, or more community/network interactions. In addition to building accurate predictive models, it is also crucial to optimize this set of 'strategic parameters' so that primary goals are optimized while secondary guardrails are not hurt. In this setting with multiple and constrained goals, this paper discovers that a probabilistic strategic parameter regime can achieve better value compared to the standard regime of finding a single deterministic parameter. The new probabilistic regime is to learn the best distribution over strategic parameter choices and sample one strategic parameter from the distribution when each user visits the platform. To pursue the optimal probabilistic solution, we formulate the problem into a stochastic compositional optimization problem, in which the unbiased stochastic gradient is unavailable. Our approach is applied in a popular social network platform with hundreds of millions of daily users and achieves +0.22% lift of user engagement in a recommendation task and +1.7% lift in revenue in an advertising optimization scenario comparing to using the best deterministic parameter strategy. ",Kein DOI-Link verfügbar,2108.09076v1,Yes,potent(1)
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",Constructing black holes in Einstein-Maxwell-scalar theory,1970,"  Exact black hole solutions in the Einstein-Maxwell-scalar theory are constructed. They are the extensions of dilaton black holes in de Sitter or anti de Sitter universe. As a result, except for a scalar potential, a coupling function between the scalar field and the Maxwell invariant is present. Then the corresponding Smarr formula and the first law of thermodynamics are investigated. ",https://doi.org/10.1088/1361-6382/abf2f5,2005.14476v2,Yes,potent(1)
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",When the regularized Lovelock tensors are kinetically coupled to scalar   field,1970,"  The recently proposed regularized Lovelock tensors are kinetically coupled to the scalar field. The resulting equation of motion is second order. In particular, it is found that when the $p=3$ regularized Lovelock tensor is kinetically coupled to the scalar field, the scalar field is the potential candidate of cosmic dark energy. ",Kein DOI-Link verfügbar,2006.15586v2,Yes,potent(1)
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",Black hole and cosmos with multiple horizons and multiple singularities   in vector-tensor theories,1970,"  A stationary and spherically symmetric black hole (For example, Reissner-Nordstrom black hole or Kerr-Newman black hole) has at most one singularity and two horizons. One horizon is the outer event horizon and the other is the inner Cauchy horizon. Can we construct static and spherically symmetric black hole solutions with N horizons and M singularities? De Sitter cosmos has only one apparent horizon. Can we construct cosmos solutions with N horizons? In this article, we present the static and spherically symmetric black hole and cosmos solutions with N horizons and M singularities in the vector-tensor theories. Following these motivations, we also construct the black hole solutions with a firewall. The deviation of these black hole solutions from the usual ones can be potentially tested by future measurements of gravitational waves. ",https://doi.org/10.1103/PhysRevD.97.104013,1711.00996v9,Yes,potent(1)
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",TR-GAN: Topology Ranking GAN with Triplet Loss for Retinal Artery/Vein   Classification,1970,"  Retinal artery/vein (A/V) classification lays the foundation for the quantitative analysis of retinal vessels, which is associated with potential risks of various cardiovascular and cerebral diseases. The topological connection relationship, which has been proved effective in improving the A/V classification performance for the conventional graph based method, has not been exploited by the deep learning based method. In this paper, we propose a Topology Ranking Generative Adversarial Network (TR-GAN) to improve the topology connectivity of the segmented arteries and veins, and further to boost the A/V classification performance. A topology ranking discriminator based on ordinal regression is proposed to rank the topological connectivity level of the ground-truth, the generated A/V mask and the intentionally shuffled mask. The ranking loss is further back-propagated to the generator to generate better connected A/V masks. In addition, a topology preserving module with triplet loss is also proposed to extract the high-level topological features and further to narrow the feature distance between the predicted A/V mask and the ground-truth. The proposed framework effectively increases the topological connectivity of the predicted A/V masks and achieves state-of-the-art A/V classification performance on the publicly available AV-DRIVE dataset. ",Kein DOI-Link verfügbar,2007.14852v1,Yes,potent(1)
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",Multi-Anchor Active Domain Adaptation for Semantic Segmentation,1970,"  Unsupervised domain adaption has proven to be an effective approach for alleviating the intensive workload of manual annotation by aligning the synthetic source-domain data and the real-world target-domain samples. Unfortunately, mapping the target-domain distribution to the source-domain unconditionally may distort the essential structural information of the target-domain data. To this end, we firstly propose to introduce a novel multi-anchor based active learning strategy to assist domain adaptation regarding the semantic segmentation task. By innovatively adopting multiple anchors instead of a single centroid, the source domain can be better characterized as a multimodal distribution, thus more representative and complimentary samples are selected from the target domain. With little workload to manually annotate these active samples, the distortion of the target-domain distribution can be effectively alleviated, resulting in a large performance gain. The multi-anchor strategy is additionally employed to model the target-distribution. By regularizing the latent representation of the target samples compact around multiple anchors through a novel soft alignment loss, more precise segmentation can be achieved. Extensive experiments are conducted on public datasets to demonstrate that the proposed approach outperforms state-of-the-art methods significantly, along with thorough ablation study to verify the effectiveness of each component. ",Kein DOI-Link verfügbar,2108.08012v1,Yes,"innovative(1), innovatively(1)"
0000-0003-2445-5387,Shuang Yu,"Peking University, peking university",Toward the Automated Construction of Probabilistic Knowledge Graphs for   the Maritime Domain,1970,"  International maritime crime is becoming increasingly sophisticated, often associated with wider criminal networks. Detecting maritime threats by means of fusing data purely related to physical movement (i.e., those generated by physical sensors, or hard data) is not sufficient. This has led to research and development efforts aimed at combining hard data with other types of data (especially human-generated or soft data). Existing work often assumes that input soft data is available in a structured format, or is focused on extracting certain relevant entities or concepts to accompany or annotate hard data. Much less attention has been given to extracting the rich knowledge about the situations of interest implicitly embedded in the large amount of soft data existing in unstructured formats (such as intelligence reports and news articles). In order to exploit the potentially useful and rich information from such sources, it is necessary to extract not only the relevant entities and concepts but also their semantic relations, together with the uncertainty associated with the extracted knowledge (i.e., in the form of probabilistic knowledge graphs). This will increase the accuracy of and confidence in, the extracted knowledge and facilitate subsequent reasoning and learning. To this end, we propose Maritime DeepDive, an initial prototype for the automated construction of probabilistic knowledge graphs from natural language data for the maritime domain. In this paper, we report on the current implementation of Maritime DeepDive, together with preliminary results on extracting probabilistic events from maritime piracy incidents. This pipeline was evaluated on a manually crafted gold standard, yielding promising results. ",https://doi.org/10.23919/FUSION49465.2021.9626935,2305.02471v1,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Relation Modeling and Distillation for Learning with Noisy Labels,1970,"  Learning with noisy labels has become an effective strategy for enhancing the robustness of models, which enables models to better tolerate inaccurate data. Existing methods either focus on optimizing the loss function to mitigate the interference from noise, or design procedures to detect potential noise and correct errors. However, their effectiveness is often compromised in representation learning due to the dilemma where models overfit to noisy labels. To address this issue, this paper proposes a relation modeling and distillation framework that models inter-sample relationships via self-supervised learning and employs knowledge distillation to enhance understanding of latent associations, which mitigate the impact of noisy labels. Specifically, the proposed method, termed RMDNet, includes two main modules, where the relation modeling (RM) module implements the contrastive learning technique to learn representations of all data, an unsupervised approach that effectively eliminates the interference of noisy tags on feature extraction. The relation-guided representation learning (RGRL) module utilizes inter-sample relation learned from the RM module to calibrate the representation distribution for noisy samples, which is capable of improving the generalization of the model in the inference phase. Notably, the proposed RMDNet is a plug-and-play framework that can integrate multiple methods to its advantage. Extensive experiments were conducted on two datasets, including performance comparison, ablation study, in-depth analysis and case study. The results show that RMDNet can learn discriminative representations for noisy data, which results in superior performance than the existing methods. ",Kein DOI-Link verfügbar,2405.19606v2,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Cross-Training with Multi-View Knowledge Fusion for Heterogenous   Federated Learning,1970,"  Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve the generalization capability. However, the data heterogeneity between sources may lead models to gradually forget previously acquired knowledge when undergoing cross-training to adapt to new tasks or data sources. We argue that integrating personalized and global knowledge to gather information from multiple perspectives could potentially improve performance. To achieve this goal, this paper presents a novel approach that enhances federated learning through a cross-training scheme incorporating multi-view information. Specifically, the proposed method, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process. The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge. The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples. Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study. The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods. ",Kein DOI-Link verfügbar,2405.20046v1,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Bond-orientational Order in Melting of Colloidal Crystals,1970,"  Using Brownian dynamics simulation, we study the orientational order in melting transition of colloidal systems with $'$soft$'$ Yukawa potential. The bond-orientational order parameter $\Phi_{6}$ and the bond-orientational order function $g_B(r)$ are calculated in two-dimensional systems. It is found that a two-stage transition and the hexatic phase are indeed existent in two-dimensional melitng, which is consistent with the prediction of the Kosterlitz-Thouless-Halperin-Nelson-Young theory. For comparing with the melting process in three-dimensional systems, the probability distribution of single-particle local order parameter is introduced. Based on the extensive simulations, it is qualitatively suggested that the breakdown of local order only occurs on the fractional part of the colloidal systems for the two-dimensional melting, but in three-dimensional melting, this breakdown takes place on the whole systems at the same time. ",Kein DOI-Link verfügbar,cond-mat/0603229v2,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Impact of ALD-Deposited Ultrathin Nitride Layers on Carrier Lifetimes   and Photoluminescence Efficiency in CdTe/MgCdTe Double Heterostructures,1970,"  This work evaluates the passivation effectiveness of ultrathin nitride layers (SiNx, AlN, TiN) deposited via atomic layer deposition on CdTe/MgCdTe double heterostructures for solar cell applications. Time-resolved photoluminescence and photoluminescence measurements revealed enhanced carrier lifetimes and reduced surface recombination, indicating improved passivation effectiveness. These results underscore the potential of SiNx as a promising passivation material to improve the efficiency of CdTe solar cells. ",Kein DOI-Link verfügbar,2408.10696v1,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Mode Collapse and Regularity of Optimal Transportation Maps,1970,"  This work builds the connection between the regularity theory of optimal transportation map, Monge-Amp\`{e}re equation and GANs, which gives a theoretic understanding of the major drawbacks of GANs: convergence difficulty and mode collapse.   According to the regularity theory of Monge-Amp\`{e}re equation, if the support of the target measure is disconnected or just non-convex, the optimal transportation mapping is discontinuous. General DNNs can only approximate continuous mappings. This intrinsic conflict leads to the convergence difficulty and mode collapse in GANs.   We test our hypothesis that the supports of real data distribution are in general non-convex, therefore the discontinuity is unavoidable using an Autoencoder combined with discrete optimal transportation map (AE-OT framework) on the CelebA data set. The testing result is positive. Furthermore, we propose to approximate the continuous Brenier potential directly based on discrete Brenier theory to tackle mode collapse. Comparing with existing method, this method is more accurate and effective. ",Kein DOI-Link verfügbar,1902.02934v1,Yes,potent(1)
0000-0003-1124-2374,Xin Qi,"Peking University, Peking University First Hospital",Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting   Linear Accelerator,1970,"  The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over $85\%$ within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields. ",Kein DOI-Link verfügbar,2305.13869v2,Yes,potent(1)
0000-0001-7224-9954,Yangyang Zhang,"Peking University, Peking University First Hospital",STAR-RIS Assisted Full-Duplex Communication Networks,1970,"  Different from conventional reconfigurable intelligent surfaces (RIS), a recent innovation called simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged, aimed at achieving complete 360-degree coverage in communication networks. Additionally, fullduplex (FD) technology is recognized as a potent approach for enhancing spectral efficiency by enabling simultaneous transmission and reception within the same time and frequency resources. In this study, we investigate the performance of a STAR-RIS-assisted FD communication system. The STAR-RIS is strategically placed at the cell-edge to facilitate communication for users located in this challenging region, while cell-center users can communicate directly with the FD base station (BS). We employ a non-orthogonal multiple access (NOMA) pairing scheme and account for system impairments, such as self-interference at the BS and imperfect successive interference cancellation (SIC). We derive closed-form expressions for the ergodic rates in both the up-link and down-link communications and extend our analysis to bidirectional communication between cell-center and cell-edge users. Furthermore, we formulate an optimization problem aimed at maximizing the ergodic sum-rate. This optimization involves adjusting the amplitudes and phase-shifts of the STAR-RIS elements and allocating total transmit power efficiently. To gain deeper insights into the achievable rates of STAR-RIS-aided FD systems, we explore the impact of various system parameters through numerical results. ",Kein DOI-Link verfügbar,2309.15037v1,Yes,"potent(1), strategically(1)"
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Structuring and sampling complex conformation space: Weighted ensemble   dynamics simulations,1970,"  Based on multiple simulation trajectories, which started from dispersively selected initial conformations, the weighted ensemble dynamics method is designed to robustly and systematically explore the hierarchical structure of complex conformational space through the spectral analysis of the variance-covariance matrix of trajectory-mapped vectors. Non-degenerate ground state of the matrix directly predicts the ergodicity of simulation data. The ground state could be adopted as statistical weights of trajectories to correctly reconstruct the equilibrium properties, even though each trajectory only explores part of the conformational space. Otherwise, the degree of degeneracy simply gives the number of meta-stable states of the system under the time scale of individual trajectory. Manipulation on the eigenvectors leads to the classification of trajectories into non-transition ones within the states and transition ones between them. The transition states may also be predicted without a priori knowledge of the system. We demonstrate the application of the general method both to the system with a one-dimensional glassy potential and with the one of alanine dipeptide in explicit solvent. ",https://doi.org/10.1103/PhysRevE.80.026707,0908.3773v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",A Continuous Opinion Dynamic Model in Co-evolving Networks--A Novel   Group Decision Approach,1970,"  Opinion polarization is a ubiquitous phenomenon in opinion dynamics. In contrast to the traditional consensus oriented group decision making (GDM) framework, this paper proposes a framework with the co-evolution of both opinions and relationship networks to improve the potential consensus level of a group and help the group reach a stable state. Taking the bound of confidence and the degree of individual's persistence into consideration, the evolution of the opinion is driven by the relationship among the group. Meanwhile, the antagonism or cooperation of individuals presented by the network topology also evolve according to the dynamic opinion distances. Opinions are convergent and the stable state will be reached in this co-evolution mechanism. We further explored this framework through simulation experiments. The simulation results verify the influence of the level of persistence on the time cost and indicate the influence of group size, the initial topology of networks and the bound of confidence on the number of opinion clusters. ",Kein DOI-Link verfügbar,1705.05981v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Large-Scale Modeling of Mobile User Click Behaviors Using Deep Learning,1970,"  Modeling tap or click sequences of users on a mobile device can improve our understandings of interaction behavior and offers opportunities for UI optimization by recommending next element the user might want to click on. We analyzed a large-scale dataset of over 20 million clicks from more than 4,000 mobile users who opted in. We then designed a deep learning model that predicts the next element that the user clicks given the user's click history, the structural information of the UI screen, and the current context such as the time of the day. We thoroughly investigated the deep model by comparing it with a set of baseline methods based on the dataset. The experiments show that our model achieves 48% and 71% accuracy (top-1 and top-3) for predicting next clicks based on a held-out dataset of test users, which significantly outperformed all the baseline methods with a large margin. We discussed a few scenarios for integrating the model in mobile interaction and how users can potentially benefit from the model. ",Kein DOI-Link verfügbar,2108.05342v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",The surface-tension-driven Benard conventions and unique sub-grain   cellular microstructures in 316L steel selective laser melting,1970,"  The unique sub-grain patterns have been found in some particular alloys (316L, Al-Si, Co-Cr-Mo) selective laser melting (SLM), the submicron-scale cellular, elongated cellular or even band structures are always coexisting inside one single macro-solidified grain. Furthermore, the cellular structures are symmetrical with hexagonal, pentagonal and square cellular patterns where the cellular size is only around 1{\mu}m. Single-layer and bulk 316L SLM experiments are presented that reveals the forming mechanism of these sub-grain cellular microstructures. Complex cellular sub-micron patterns were formed by the local convection and B\'enard Instabilities in front of the solid/liquid (S/L) interface (so-called mushy zones) affected by intricate temperature and surface tension gradients. In other words, this nonlinear self-organization phenomenon (B\'enard Instability) occurring at the S/L interface is superimposed on the macro-grain solidification process to form the sub-grain patterns/structures and elemental microsegregations. This simple and unified explanation can be expanded to other eutectic alloys formed by SLM, like the Al-Si system. ",Kein DOI-Link verfügbar,1801.01408v1,Yes,intricate(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Estimation and Inference for High-dimensional Multi-response Growth   Curve Model,1970,"  A growth curve model (GCM) aims to characterize how an outcome variable evolves, develops and grows as a function of time, along with other predictors. It provides a particularly useful framework to model growth trend in longitudinal data. However, the estimation and inference of GCM with a large number of response variables faces numerous challenges, and remains underdeveloped. In this article, we study the high-dimensional multivariate-response linear GCM, and develop the corresponding estimation and inference procedures. Our proposal is far from a straightforward extension, and involves several innovative components. Specifically, we introduce a Kronecker product structure, which allows us to effectively decompose a very large covariance matrix, and to pool the correlated samples to improve the estimation accuracy. We devise a highly non-trivial multi-step estimation approach to estimate the individual covariance components separately and effectively. We also develop rigorous statistical inference procedures to test both the global effects and the individual effects, and establish the size and power properties, as well as the proper false discovery control. We demonstrate the effectiveness of the new method through both intensive simulations, and the analysis of a longitudinal neuroimaging data for Alzheimer's disease. ",Kein DOI-Link verfügbar,2312.16769v1,Yes,innovative(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Large Language Model for Vulnerability Detection and Repair: Literature   Review and the Road Ahead,1970,"  The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous recent studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors. ",Kein DOI-Link verfügbar,2404.02525v2,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",EdgeVision: Towards Collaborative Video Analytics on Distributed Edges   for Performance Maximization,1970,"  Deep Neural Network (DNN)-based video analytics significantly improves recognition accuracy in computer vision applications. Deploying DNN models at edge nodes, closer to end users, reduces inference delay and minimizes bandwidth costs. However, these resource-constrained edge nodes may experience substantial delays under heavy workloads, leading to imbalanced workload distribution. While previous efforts focused on optimizing hierarchical device-edge-cloud architectures or centralized clusters for video analytics, we propose addressing these challenges through collaborative distributed and autonomous edge nodes. Despite the intricate control involved, we introduce EdgeVision, a Multiagent Reinforcement Learning (MARL)- based framework for collaborative video analytics on distributed edges. EdgeVision enables edge nodes to autonomously learn policies for video preprocessing, model selection, and request dispatching. Our approach utilizes an actor-critic-based MARL algorithm enhanced with an attention mechanism to learn optimal policies. To validate EdgeVision, we construct a multi-edge testbed and conduct experiments with real-world datasets. Results demonstrate a performance enhancement of 33.6% to 86.4% compared to baseline methods. ",Kein DOI-Link verfügbar,2211.03102v3,Yes,intricate(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Extreme Generative Image Compression by Learning Text Embedding from   Diffusion Models,1970,"  Transferring large amount of high resolution images over limited bandwidth is an important but very challenging task. Compressing images using extremely low bitrates (<0.1 bpp) has been studied but it often results in low quality images of heavy artifacts due to the strong constraint in the number of bits available for the compressed data. It is often said that a picture is worth a thousand words but on the other hand, language is very powerful in capturing the essence of an image using short descriptions. With the recent success of diffusion models for text-to-image generation, we propose a generative image compression method that demonstrates the potential of saving an image as a short text embedding which in turn can be used to generate high-fidelity images which is equivalent to the original one perceptually. For a given image, its corresponding text embedding is learned using the same optimization process as the text-to-image diffusion model itself, using a learnable text embedding as input after bypassing the original transformer. The optimization is applied together with a learning compression model to achieve extreme compression of low bitrates <0.1 bpp. Based on our experiments measured by a comprehensive set of image quality metrics, our method outperforms the other state-of-the-art deep learning methods in terms of both perceptual quality and diversity. ",Kein DOI-Link verfügbar,2211.07793v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Capturing Popularity Trends: A Simplistic Non-Personalized Approach for   Enhanced Item Recommendation,1970,"  Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated state-of-the-art recommendation methods. Since PARE prioritizes item popularity over personalized user preferences, it can enhance existing recommendation methods as a complementary component. Our experiments demonstrate that integrating PARE with existing recommendation methods significantly surpasses the performance of standalone models, highlighting PARE's potential as a complement to existing recommendation methods. Furthermore, the simplicity of PARE makes it immensely practical for industrial applications and a valuable baseline for future research. ",https://doi.org/10.1145/3583780.3614801,2308.08799v1,Yes,potent(2)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Rewriting the Code: A Simple Method for Large Language Model Augmented   Code Search,1970,"  In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at \url{https://github.com/Alex-HaochenLi/ReCo}. ",Kein DOI-Link verfügbar,2401.04514v2,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for   Effective Sequential Recommendation,1970,"  Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whitening transformation, which transforms the anisotropic text feature distribution into an isotropic Gaussian distribution. Our experiments show that whitening pre-trained text embeddings in the sequential model can significantly improve recommendation performance. However, the full whitening operation might break the potential manifold of items with similar text semantics. To preserve the original semantics while benefiting from the isotropy of the whitened text features, we introduce WhitenRec+, an ensemble approach that leverages both fully whitened and relaxed whitened item representations for effective recommendations. We further discuss and analyze the benefits of our design through experiments and proofs. Experimental results on three public benchmark datasets demonstrate that WhitenRec+ outperforms state-of-the-art methods for sequential recommendation. ",Kein DOI-Link verfügbar,2402.10602v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Low-Dimensional Federated Knowledge Graph Embedding via Knowledge   Distillation,1970,"  Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a light-weight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD. ",Kein DOI-Link verfügbar,2408.05748v1,Yes,potent(2)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Bribery in Rating Systems: A Game-Theoretic Perspective,1970,"  Rating systems play a vital role in the exponential growth of service-oriented markets. As highly rated online services usually receive substantial revenue in the markets, malicious sellers seek to boost their service evaluation by manipulating the rating system with fake ratings. One effective way to improve the service evaluation is to hire fake rating providers by bribery. The fake ratings given by the bribed buyers influence the evaluation of the service, which further impacts the decision-making of potential buyers. In this paper, we study the bribery of a rating system with multiple sellers and buyers via a game-theoretic perspective. In detail, we examine whether there exists an equilibrium state in the market in which the rating system is expected to be bribery-proof: no bribery strategy yields a strictly positive gain. We first collect real-world data for modeling the bribery problem in rating systems. On top of that, we analyze the problem of bribery in a rating system as a static game. From our analysis, we conclude that at least a Nash equilibrium can be reached in the bribery game of rating systems. ",Kein DOI-Link verfügbar,1911.10014v4,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Multi-LLM Collaboration + Data-Centric Innovation = 2x Better   Vulnerability Repair,1970,"  The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system.   To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs through data-centric innovation. Specifically, VulMaster introduces the utilization and combination of various types of input data, including complete vulnerable code of any size, vulnerable code structures, and expert knowledge from the CWE system. Additionally, VulMaster leverages the collaboration between two Large Language Models (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backbone LLM, fine-tuned with the training data, while ChatGPT supplements by providing missing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2\% to 20.0\%, 21.3\% to 29.3\%, and 32.5\% to 40.9\%, respectively. ",Kein DOI-Link verfügbar,2401.15459v3,Yes,notable(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Hyperdynamics for entropic systems: time-space compression and pair   correlation function approximation,1970,"  We develop a generalized hyperdynamics method, which is able to simulate slow dynamics in atomistic general (both energy and entropy-dominated) systems. We show that a few functionals of the pair correlation function, involving two-body entropy, form a low-dimensional collective space, which is a good approximation that is able to distinguish stable and transitional conformations. A bias potential, which raises the energy in stable regions, is constructed on the fly. We examine the slowly nucleation processes of a Lennard-Jones gas and show that our new method can generate correct long time dynamics without a prior knowledge. ",https://doi.org/10.1103/PhysRevE.74.035701,cond-mat/0608377v1,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface   Modeling,1970,"  User interface modeling is inherently multimodal, which involves several distinct types of data: images, structures and language. The tasks are also diverse, including object detection, language generation and grounding. In this paper, we present VUT, a Versatile UI Transformer that takes multimodal input and simultaneously accomplishes 5 distinct tasks with the same model. Our model consists of a multimodal Transformer encoder that jointly encodes UI images and structures, and performs UI object detection when the UI structures are absent in the input. Our model also consists of an auto-regressive Transformer model that encodes the language input and decodes output, for both question-answering and command grounding with respect to the UI. Our experiments show that for most of the tasks, when trained jointly for multi-tasks, VUT substantially reduces the number of models and footprints needed for performing multiple tasks, while achieving accuracy exceeding or on par with baseline models trained for each individual task. ",Kein DOI-Link verfügbar,2112.05692v1,Yes,versatile(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",AI-lead Court Debate Case Investigation,1970,"  The multi-role judicial debate composed of the plaintiff, defendant, and judge is an important part of the judicial trial. Different from other types of dialogue, questions are raised by the judge, The plaintiff, plaintiff's agent defendant, and defendant's agent would be to debating so that the trial can proceed in an orderly manner. Question generation is an important task in Natural Language Generation. In the judicial trial, it can help the judge raise efficient questions so that the judge has a clearer understanding of the case. In this work, we propose an innovative end-to-end question generation model-Trial Brain Model (TBM) to build a Trial Brain, it can generate the questions the judge wants to ask through the historical dialogue between the plaintiff and the defendant. Unlike prior efforts in natural language generation, our model can learn the judge's questioning intention through predefined knowledge. We do experiments on real-world datasets, the experimental results show that our model can provide a more accurate question in the multi-role court debate scene. ",Kein DOI-Link verfügbar,2010.11604v2,Yes,innovative(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Decentralizing MVCC by Leveraging Visibility,1970,"  Multiversion Concurrency Control (MVCC) is a widely adopted concurrency control mechanism in database systems, which usually utilizes timestamps to resolve conflicts between transactions. However, centralized allocation of timestamps is a potential bottleneck for parallel transaction management. This bottleneck is becoming increasingly visible with the rapidly growing degree of parallelism of today's computing platforms. This paper introduces Visibility based Concurrency Control (ViCC), a series of CC mechanisms that allow transactions to determine their timestamps autonomously, without relying on centralized coordination. As such, ViCC can scale well, rendering it suitable for various multicore and MPP platforms. Extensive experiments are conducted to demonstrate its advantage over existing approaches. ",Kein DOI-Link verfügbar,1704.01355v2,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Towards a Systematic Survey for Carbon Neutral Data Centers,1970,"  Data centers are carbon-intensive enterprises due to their massive energy consumption, and it is estimated that data center industry will account for 8\% of global carbon emissions by 2030. However, both technological and policy instruments for reducing or even neutralizing data center carbon emissions have not been thoroughly investigated. To bridge this gap, this survey paper proposes a roadmap towards carbon-neutral data centers that takes into account both policy instruments and technological methodologies. We begin by presenting the carbon footprint of data centers, as well as some insights into the major sources of carbon emissions. Following that, carbon neutrality plans for major global cloud providers are discussed to summarize current industrial efforts in this direction. In what follows, we introduce the carbon market as a policy instrument to explain how to offset data center carbon emissions in a cost-efficient manner. On the technological front, we propose achieving carbon-neutral data centers by increasing renewable energy penetration, improving energy efficiency, and boosting energy circulation simultaneously. A comprehensive review of existing technologies on these three topics is elaborated subsequently. Based on this, a multi-pronged approach towards carbon neutrality is envisioned and a digital twin-powered industrial artificial intelligence (AI) framework is proposed to make this solution a reality. Furthermore, three key scientific challenges for putting such a framework in place are discussed. Finally, several applications for this framework are presented to demonstrate its enormous potential. ",Kein DOI-Link verfügbar,2110.09284v3,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital","A Survey of Predictive Maintenance: Systems, Purposes and Approaches",1970,"  This paper highlights the importance of maintenance techniques in the coming industrial revolution, reviews the evolution of maintenance techniques, and presents a comprehensive literature review on the latest advancement of maintenance techniques, i.e., Predictive Maintenance (PdM), with emphasis on system architectures, optimization objectives, and optimization methods. In industry, any outages and unplanned downtime of machines or systems would degrade or interrupt a company's core business, potentially resulting in significant penalties and immeasurable reputation and economic loss. Existing traditional maintenance approaches, such as Reactive Maintenance (RM) and Preventive Maintenance (PM), suffer from high prevent and repair costs, inadequate or inaccurate mathematical degradation processes, and manual feature extraction. The incoming fourth industrial revolution is also demanding for a new maintenance paradigm to reduce the maintenance cost and downtime, and increase system availability and reliability. Predictive Maintenance (PdM) is envisioned the solution. In this survey, we first provide a high-level view of the PdM system architectures including PdM 4.0, Open System Architecture for Condition Based Monitoring (OSA-CBM), and cloud-enhanced PdM system. Then, we review the specific optimization objectives, which mainly comprise cost minimization, availability/reliability maximization, and multi-objective optimization. Furthermore, we present the optimization methods to achieve the aforementioned objectives, which include traditional Machine Learning (ML) based and Deep Learning (DL) based approaches. Finally, we highlight the future research directions that are critical to promote the application of DL techniques in the context of PdM. ",Kein DOI-Link verfügbar,1912.07383v2,Yes,potent(1)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Sequential Best-Arm Identification with Application to Brain-Computer   Interface,1970,"  A brain-computer interface (BCI) is a technology that enables direct communication between the brain and an external device or computer system. It allows individuals to interact with the device using only their thoughts, and holds immense potential for a wide range of applications in medicine, rehabilitation, and human augmentation. An electroencephalogram (EEG) and event-related potential (ERP)-based speller system is a type of BCI that allows users to spell words without using a physical keyboard, but instead by recording and interpreting brain signals under different stimulus presentation paradigms. Conventional non-adaptive paradigms treat each word selection independently, leading to a lengthy learning process. To improve the sampling efficiency, we cast the problem as a sequence of best-arm identification tasks in multi-armed bandits. Leveraging pre-trained large language models (LLMs), we utilize the prior knowledge learned from previous tasks to inform and facilitate subsequent tasks. To do so in a coherent way, we propose a sequential top-two Thompson sampling (STTS) algorithm under the fixed-confidence setting and the fixed-budget setting. We study the theoretical property of the proposed algorithm, and demonstrate its substantial empirical improvement through both synthetic data analysis as well as a P300 BCI speller simulator example. ",Kein DOI-Link verfügbar,2305.11908v1,Yes,potent(2)
0000-0002-4048-4017,Xin Zhou,"Peking University, Peking University Third Hospital",Rethinking Negative Pairs in Code Search,1970,"  Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}. ",Kein DOI-Link verfügbar,2310.08069v1,Yes,potent(1)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",CarGameAR: An Integrated AR Car Game Authoring Interface for   Custom-Built Car Programed on Arduino Board,1970,"  In this paper, we present CarGameAR: An Integrated AR Car Game Authoring Interface for Custom-Built Car Programed on Arduino Board. The car consists of an Arduino board, an H-bridge, and motors. The objective of the project is to create a system that can move a car in different directions using a computer application. The system uses Unity software to create a virtual environment where the user can control the car using keyboard commands. The car's motion is achieved by sending signals from the computer to the Arduino board, which then drives the motors through the H-bridge. The project provides a cost-effective and efficient way to build a car, which can be used for educational purposes, such as teaching programming. Moreover, this project is not limited to the control of the car through keyboard commands in a virtual environment. The system can be adapted to support augmented reality (AR) technology, providing an even more immersive and engaging user experience. By integrating the car with AR, the user can control the car's motion using physical gestures and movements, adding an extra layer of interactivity to the system. This makes the car an ideal platform for game development in AR, allowing the user to create driving games that blend the physical and virtual worlds seamlessly. Additionally, the car's affordability and ease of construction make it an accessible and valuable tool for teaching programming and principles in a fun and interactive way. Overall, this project demonstrates the versatility and potential of the car system, highlighting the various applications and possibilities it offers for both education and entertainment. ",Kein DOI-Link verfügbar,2305.00084v1,Yes,potent(1)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",Creative NFT-Copyrighted AR Face Mask Authoring Using Unity3D Editor,1970,"  In this paper, we extend well-designed 3D face masks into AR face masks and demonstrate the possibility of transforming this into an NFT-copyrighted AR face mask that helps authenticate the ownership of the AR mask user so as to improve creative control, brand identification, and ID protection. The output of this project will not only potentially validate the value of the NFT technology but also explore how to combine the NFT technology with AR technology so as to be applied to e-commerce and e-business aspects of the multimedia industry. ",Kein DOI-Link verfügbar,2302.08685v1,Yes,potent(1)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",GraphInstruct: Empowering Large Language Models with Graph Understanding   and Reasoning Capability,1970,"  Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs. We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct. ",Kein DOI-Link verfügbar,2403.04483v2,Yes,potent(1)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and   Evaluation Framework,1970,"  Large language models (LLMs) have excelled in various natural language processing tasks, but challenges in interpretability and trustworthiness persist, limiting their use in high-stakes fields. Causal discovery offers a promising approach to improve transparency and reliability. However, current evaluations are often one-sided and lack assessments focused on interpretability performance. Additionally, these evaluations rely on synthetic data and lack comprehensive assessments of real-world datasets. These lead to promising methods potentially being overlooked. To address these issues, we propose a flexible evaluation framework with metrics for evaluating differences in causal structures and causal effects, which are crucial attributes that help improve the interpretability of LLMs. We introduce the Open Causal Discovery Benchmark (OCDB), based on real data, to promote fair comparisons and drive optimization of algorithms. Additionally, our new metrics account for undirected edges, enabling fair comparisons between Directed Acyclic Graphs (DAGs) and Completed Partially Directed Acyclic Graphs (CPDAGs). Experimental results show significant shortcomings in existing algorithms' generalization capabilities on real data, highlighting the potential for performance improvement and the importance of our framework in advancing causal discovery techniques. ",Kein DOI-Link verfügbar,2406.04598v1,Yes,potent(2)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects,1970,"  Controllable 3D indoor scene synthesis stands at the forefront of technological progress, offering various applications like gaming, film, and augmented/virtual reality. The capability to stylize and de-couple objects within these scenarios is a crucial factor, providing an advanced level of control throughout the editing process. This control extends not just to manipulating geometric attributes like translation and scaling but also includes managing appearances, such as stylization. Current methods for scene stylization are limited to applying styles to the entire scene, without the ability to separate and customize individual objects. Addressing the intricacies of this challenge, we introduce a unique pipeline designed for synthesis 3D indoor scenes. Our approach involves strategically placing objects within the scene, utilizing information from professionally designed bounding boxes. Significantly, our pipeline prioritizes maintaining style consistency across multiple objects within the scene, ensuring a cohesive and visually appealing result aligned with the desired aesthetic. The core strength of our pipeline lies in its ability to generate 3D scenes that are not only visually impressive but also exhibit features like photorealism, multi-view consistency, and diversity. These scenes are crafted in response to various natural language prompts, demonstrating the versatility and adaptability of our model. ",Kein DOI-Link verfügbar,2401.13203v1,Yes,strategically(1)
0000-0002-1468-1586,Hong Huang,"Peking University, Peking University First Hospital",Show Me the Whole World: Towards Entire Item Space Exploration for   Interactive Personalized Recommendations,1970,"  User interest exploration is an important and challenging topic in recommender systems, which alleviates the closed-loop effects between recommendation models and user-item interactions. Contextual bandit (CB) algorithms strive to make a good trade-off between exploration and exploitation so that users' potential interests have chances to expose. However, classical CB algorithms can only be applied to a small, sampled item set (usually hundreds), which forces the typical applications in recommender systems limited to candidate post-ranking, homepage top item ranking, ad creative selection, or online model selection (A/B test).   In this paper, we introduce two simple but effective hierarchical CB algorithms to make a classical CB model (such as LinUCB and Thompson Sampling) capable to explore users' interest in the entire item space without limiting it to a small item set. We first construct a hierarchy item tree via a bottom-up clustering algorithm to organize items in a coarse-to-fine manner. Then we propose a hierarchical CB (HCB) algorithm to explore users' interest in the hierarchy tree. HCB takes the exploration problem as a series of decision-making processes, where the goal is to find a path from the root to a leaf node, and the feedback will be back-propagated to all the nodes in the path. We further propose a progressive hierarchical CB (pHCB) algorithm, which progressively extends visible nodes which reach a confidence level for exploration, to avoid misleading actions on upper-level nodes in the sequential decision-making process. Extensive experiments on two public recommendation datasets demonstrate the effectiveness and flexibility of our methods. ",Kein DOI-Link verfügbar,2110.09905v1,Yes,potent(1)
0000-0002-6430-8712,Lu Yao,"Peking University, Peking University Cancer Hospital",CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale   Attention,1970,"  Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers do not yet possess the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the self-attention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks. The code has been released: https://github.com/cheerss/CrossFormer. ",Kein DOI-Link verfügbar,2108.00154v2,Yes,versatile(1)
0000-0001-9202-5499,Xiaofan Li,"Peking University, Peking University Cancer Hospital",Solving Non-local Fokker-Planck Equations by Deep Learning,1970,"  Physics-informed neural networks (PiNNs) recently emerged as a powerful solver for a large class of partial differential equations under various initial and boundary conditions. In this paper, we propose trapz-PiNNs, physics-informed neural networks incorporated with a modified trapezoidal rule recently developed for accurately evaluating fractional laplacian and solve the space-fractional Fokker-Planck equations in 2D and 3D. We describe the modified trapezoidal rule in detail and verify the second-order accuracy. We demonstrate trapz-PiNNs have high expressive power through predicting solution with low $\mathcal{L}^2$ relative error on a variety of numerical examples. We also use local metrics such as pointwise absolute and relative errors to analyze where could be further improved. We present an effective method for improving performance of trapz-PiNN on local metrics, provided that physical observations of high-fidelity simulation of the true solution are available. Besides the usual advantages of the deep learning solvers such as adaptivity and mesh-independence, the trapz-PiNN is able to solve PDEs with fractional laplacian with arbitrary $\alpha\in (0,2)$ and specializes on rectangular domain. It also has potential to be generalized into higher dimensions. ",https://doi.org/10.1063/5.0128935,2206.03439v1,Yes,potent(1)
0000-0001-9202-5499,Xiaofan Li,"Peking University, Peking University Cancer Hospital",Quantum Algorithm for Maximum Biclique Problem,1970,"  Identifying a biclique with the maximum number of edges bears considerable implications for numerous fields of application, such as detecting anomalies in E-commerce transactions, discerning protein-protein interactions in biology, and refining the efficacy of social network recommendation algorithms. However, the inherent NP-hardness of this problem significantly complicates the matter. The prohibitive time complexity of existing algorithms is the primary bottleneck constraining the application scenarios. Aiming to address this challenge, we present an unprecedented exploration of a quantum computing approach. Efficient quantum algorithms, as a crucial future direction for handling NP-hard problems, are presently under intensive investigation, of which the potential has already been proven in practical arenas such as cybersecurity. However, in the field of quantum algorithms for graph databases, little work has been done due to the challenges presented by the quantum representation of complex graph topologies. In this study, we delve into the intricacies of encoding a bipartite graph on a quantum computer. Given a bipartite graph with n vertices, we propose a ground-breaking algorithm qMBS with time complexity O^*(2^(n/2)), illustrating a quadratic speed-up in terms of complexity compared to the state-of-the-art. Furthermore, we detail two variants tailored for the maximum vertex biclique problem and the maximum balanced biclique problem. To corroborate the practical performance and efficacy of our proposed algorithms, we have conducted proof-of-principle experiments utilizing IBM quantum simulators, of which the results provide a substantial validation of our approach to the extent possible to date. ",Kein DOI-Link verfügbar,2309.04503v1,Yes,potent(1)
0000-0001-9202-5499,Xiaofan Li,"Peking University, Peking University Cancer Hospital",Characterizing Ethereum Upgradable Smart Contracts and Their Security   Implications,1970,"  Upgradeable smart contracts (USCs) have been widely adopted to enable modifying deployed smart contracts. While USCs bring great flexibility to developers, improper usage might introduce new security issues, potentially allowing attackers to hijack USCs and their users. In this paper, we conduct a large-scale measurement study to characterize USCs and their security implications in the wild. We summarize six commonly used USC patterns and develop a tool, USCDetector, to identify USCs without needing source code. Particularly, USCDetector collects various information such as bytecode and transaction information to construct upgrade chains for USCs and disclose potentially vulnerable ones. We evaluate USCDetector using verified smart contracts (i.e., with source code) as ground truth and show that USCDetector can achieve high accuracy with a precision of 96.26%. We then use USCDetector to conduct a large-scale study on Ethereum, covering a total of 60,251,064 smart contracts. USCDetecor constructs 10,218 upgrade chains and discloses multiple real-world USCs with potential security issues. ",Kein DOI-Link verfügbar,2403.01290v1,Yes,potent(3)
0000-0001-9202-5499,Xiaofan Li,"Peking University, Peking University Cancer Hospital",Mean exit time and escape probability for dynamical systems driven by   Levy noise,1970,"  The mean first exit time and escape probability are utilized to quantify dynamical behaviors of stochastic differential equations with non-Gaussian alpha-stable type Levy motions. Both deterministic quantities are characterized by differential-integral equations(i.e.,differential equations with non local terms) but with different exterior conditions. The non-Gaussianity of noises manifests as nonlocality at the level of mean exit time and escape probability. An objective of this paper is to make mean exit time and escape probability as efficient computational tools, to the applied probability community, for quantifying stochastic dynamics. An accurate numerical scheme is developed and validated for computing the mean exit time and escape probability. Asymptotic solution for the mean exit time is given when the pure jump measure in the Levy motion is small.   From both the analytical and numerical results, it is observed that the mean exit time depends strongly on the domain size and the value of alpha in the alpha-stable Levy jump measure. The mean exit time can measure which of the two competing factors in alpha-stable Levy motion, i.e. the jump frequency or the jump size, is dominant in helping a process exit a bounded domain. The escape probability is shown to vary with the underlying vector field(i.e.,drift). The mean exit time and escape probability could become discontinuous at the boundary of the domain, when the process is subject to certain deterministic potential and the value of alpha is in (0,1). ",Kein DOI-Link verfügbar,1201.6015v1,Yes,potent(1)
0000-0001-9202-5499,Xiaofan Li,"Peking University, Peking University Cancer Hospital",USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion   Segmentation,1970,"  Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-saliency regions represent the background. However, intermediate regions can be hard to classify, often due to their proximity to lesion edges or interference from hair or blisters. Rather than risk potential pseudo-labeling errors or learning confusion by forcefully classifying these regions, we consider them as uncertainty regions, exempting them from pseudo-labeling and allowing the network to self-learn. Further, we employ connectivity detection and centrality detection to refine foreground pseudo-labels and reduce noise-induced errors. The application of cycle refining enhances performance further. Our method underwent thorough experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets, demonstrating that its performance is on par with weakly supervised and supervised methods, and exceeds that of other existing unsupervised methods. ",Kein DOI-Link verfügbar,2309.13289v3,Yes,"innovative(1), potent(1)"
0000-0002-5733-7400,Feng Xue,"Peking University, Peking University People's Hospital",Switching intrinsic magnetic skyrmions with controllable magnetic   anisotropy in van der Waals multiferroic heterostructures,1970,"  Magnetic skyrmions, topologically nontrivial whirling spin textures at nanometer scales, have emerged as potential information carriers for spintronic devices. The ability to efficiently create and erase magnetic skyrmions is vital yet challenging for such applications. Based on first-principles studies, we find that switching between intrinsic magnetic skyrmion and high-temperature ferromagnetic states can be achieved in two-dimensional van der Waals (vdW) multiferroic heterostructure CrSeI/In2Te3 by reversing the ferroelectric polarization of In2Te3. The core mechanism of this switching is traced to the controllable magnetic anisotropy of CrSeI influenced by the ferroelectric polarization of In2Te3. We propose a useful descriptor linking the presence of magnetic skyrmions to magnetic parameters, and validate this connection through studies of a variety of similar vdW multiferroic heterostructures. Our work demonstrates that manipulating magnetic skyrmions via tunable magnetic anisotropies in vdW multiferroic heterostructures represents a highly promising and energy-efficient strategy for future development of spintronics. ",https://doi.org/10.1021/acs.nanolett.3c05024,2403.05747v1,Yes,potent(1)
0000-0002-5733-7400,Feng Xue,"Peking University, Peking University People's Hospital",Practical Privacy Attacks on Vertical Federated Learning,1970,"  Federated learning (FL) is a privacy-preserving learning paradigm that allows multiple parities to jointly train a powerful machine learning model without sharing their private data. According to the form of collaboration, FL can be further divided into horizontal federated learning (HFL) and vertical federated learning (VFL). In HFL, participants share the same feature space and collaborate on data samples, while in VFL, participants share the same sample IDs and collaborate on features. VFL has a broader scope of applications and is arguably more suitable for joint model training between large enterprises.   In this paper, we focus on VFL and investigate potential privacy leakage in real-world VFL frameworks. We design and implement two practical privacy attacks: reverse multiplication attack for the logistic regression VFL protocol; and reverse sum attack for the XGBoost VFL protocol. We empirically show that the two attacks are (1) effective - the adversary can successfully steal the private training data, even when the intermediate outputs are encrypted to protect data privacy; (2) evasive - the attacks do not deviate from the protocol specification nor deteriorate the accuracy of the target model; and (3) easy - the adversary needs little prior knowledge about the data distribution of the target participant. We also show the leaked information is as effective as the raw training data in training an alternative classifier. We further discuss potential countermeasures and their challenges, which we hope can lead to several promising research directions. ",Kein DOI-Link verfügbar,2011.09290v3,Yes,potent(2)
0000-0002-5733-7400,Feng Xue,"Peking University, Peking University People's Hospital",Full-duplex in 5G Small Cell Access: SystemDesign and Performance   Aspects,1970,"  Recent achievement in self-interference cancellation algorithms enables potential application of full-duplex (FD) in 5G radio access systems. The exponential growth of data traffic in 5G can be supported by having more spectrum and higher spectral efficiency. FD communication promises to double the spectral efficiency by enabling simultaneous uplink and downlink transmissions in the same frequency band. Yet for cellular access network with FD base stations (BS) serving multiple users (UE), additional BS-to-BS and UE-to-UE interferences due to FD operation could diminish the performance gain if not tackled properly. In this article, we address the practical system design aspects to exploit FD gain at network scale. We propose efficient reference signal design, low-overhead channel state information feedback and signalling mechanisms to enable FD operation, and develop low-complexity power control and scheduling algorithms to effectively mitigate new interference introduced by FD operation. We extensively evaluate FD network-wide performance in various deployment scenarios and traffic environment with detailed LTE PHY/MAC modelling. We demonstrate that FD can achieve not only appreciable throughput gains (1.9x), but also significant transmission latency reduction~(5-8x) compared with the half-duplex system. ",Kein DOI-Link verfügbar,1903.09893v1,Yes,potent(1)
0000-0001-9824-1109,Xiaoqing Hu,"Peking University, Peking University Third Hospital",Measuring charge distribution of molecular cations by atomic Coulomb   probe microscope,1970,"  Imaging the charge distributions and structures of molecules and clusters will promote the understanding of the dynamics of the quantum system. Here, we report a method by using an Ar atom as a tip to probe the charge distributions of benzene (Bz) cations in gas phase. Remarkably, the measured charge distributions of Bz cation (QH =0.204,QC=-0.037)and dication (QH =0.248,QC=0.0853)agree well with the calculated Mulliken distributions,and the structures of Bz dimer is reconstructed by using the measured charge distributions. The structures of two Bz dimer isomers (T-shaped and PD isomers) can be resolved from the measured inter-molecular potential V(R) between two Bz ions, and the structures of Bz dimer agree well with the theoretical predictions. ",https://doi.org/10.1088/0256-307X/39/11/113301,2211.02463v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Experiment on intrinsically nonequilibrium distribution of large ions in   charged small nanopores,1970,"  Recent theoretical research on locally nonchaotic gravitational energy barrier led to an interesting finding: beyond the boundaries of Boltzmann's H-theorem, there may be macroscopic systems with nontrivial energy properties. The fundamental mechanism is rooted in the intrinsically nonequilibrium steady state. In the current investigation, we experimentally verify this concept, with the weak gravitational force being changed to the strong Coulomb force. The tests are performed on capacitive cells with the same nanoporous electrodes and various electrolyte concentrations. The key characteristic is that the nanopore size is only slightly larger than the ion size, less than twice the ion size. The confinement effect of the nanopore walls plays the role of the spontaneously nonequilibrium dimension (SND). At first glance, the capacitive cells exhibit ""normal"" charge curves. However, their steady-state ion distribution significantly differs from thermodynamic equilibrium. The measured potential difference is nearly an order of magnitude larger than the prediction of equilibrium thermodynamics. Such phenomena are in line with the molecular dynamics simulations reported in the open literature. ",Kein DOI-Link verfügbar,2407.04599v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Cross-Entropy Adversarial View Adaptation for Person Re-identification,1970,"  Person re-identification (re-ID) is a task of matching pedestrians under disjoint camera views. To recognise paired snapshots, it has to cope with large cross-view variations caused by the camera view shift. Supervised deep neural networks are effective in producing a set of non-linear projections that can transform cross-view images into a common feature space. However, they typically impose a symmetric architecture, yielding the network ill-conditioned on its optimisation. In this paper, we learn view-invariant subspace for person re-ID, and its corresponding similarity metric using an adversarial view adaptation approach. The main contribution is to learn coupled asymmetric mappings regarding view characteristics which are adversarially trained to address the view discrepancy by optimising the cross-entropy view confusion objective. To determine the similarity value, the network is empowered with a similarity discriminator to promote features that are highly discriminant in distinguishing positive and negative pairs. The other contribution includes an adaptive weighing on the most difficult samples to address the imbalance of within/between-identity pairs. Our approach achieves notable improved performance in comparison to state-of-the-arts on benchmark datasets. ",Kein DOI-Link verfügbar,1904.01755v2,Yes,notable(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Say No to Freeloader: Protecting Intellectual Property of Your Deep   Model,1970,"  Model intellectual property (IP) protection has attracted growing attention as science and technology advancements stem from human intellectual labor and computational expenses. Ensuring IP safety for trainers and owners is of utmost importance, particularly in domains where ownership verification and applicability authorization are required. A notable approach to safeguarding model IP involves proactively preventing the use of well-trained models of authorized domains from unauthorized domains. In this paper, we introduce a novel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which serves as a barrier against illegal transfers from authorized to unauthorized domains. Drawing inspiration from human transitive inference and learning abilities, the CUPI-Domain is designed to obstruct cross-domain transfers by emphasizing the distinctive style features of the authorized domain. This emphasis leads to failure in recognizing irrelevant private style features on unauthorized domains. To this end, we propose novel CUPI-Domain generators, which select features from both authorized and CUPI-Domain as anchors. Then, we fuse the style features and semantic features of these anchors to generate labeled and style-rich CUPI-Domain. Additionally, we design external Domain-Information Memory Banks (DIMB) for storing and updating labeled pyramid features to obtain stable domain class features and domain class-wise style features. Based on the proposed whole method, the novel style and discriminative loss functions are designed to effectively enhance the distinction in style and discriminative features between authorized and unauthorized domains, respectively. Moreover, we provide two solutions for utilizing CUPI-Domain based on whether the unauthorized domain is known: target-specified CUPI-Domain and target-free CUPI-Domain. ",Kein DOI-Link verfügbar,2408.13161v2,Yes,notable(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",TQSim: A Case for Reuse-Focused Tree-Based Quantum Circuit Simulation,1970,"  Quantum computers can speed up computationally hard problems. However, to realize their full potential, we must mitigate qubit errors (from noise) by developing noise-aware algorithms, compilers, and architectures. Thus, simulating quantum programs on classical computers with different noise models is a de-facto tool that is used by researchers and practitioners. Unfortunately, noisy quantum simulators iteratively execute the same circuit across multiple trials (shots), thereby incurring high-performance overheads. To address this, we propose a noisy simulation technique called Tree-Based Quantum Circuit Simulation (TQSim). TQSim exploits the reusability of the intermediate results during the noisy simulation and reduces computation. TQSim dynamically partitions a circuit into several subcircuits. It then reuses the intermediate results from these subcircuits during computation. As compared to a noisy Qulacs-based baseline simulator, TQSim achieves an average speedup of 2.51x across 48 different benchmark circuits. Additionally, across benchmarks, TQSim produces results with a normalized fidelity that is within the 0.016 range of the baseline normalized fidelity. ",Kein DOI-Link verfügbar,2203.13892v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Reachability-Based Confidence-Aware Probabilistic Collision Detection in   Highway Driving,1970,"  Risk assessment is a crucial component of collision warning and avoidance systems in intelligent vehicles. To accurately detect potential vehicle collisions, reachability-based formal approaches have been developed to ensure driving safety, but suffer from over-conservatism, potentially leading to false-positive risk events in complicated real-world applications. In this work, we combine two reachability analysis techniques, i.e., backward reachable set (BRS) and stochastic forward reachable set (FRS), and propose an integrated probabilistic collision detection framework in highway driving. Within the framework, we can firstly use a BRS to formally check whether a two-vehicle interaction is safe; otherwise, a prediction-based stochastic FRS is employed to estimate a collision probability at each future time step. In doing so, the framework can not only identify non-risky events with guaranteed safety, but also provide accurate collision risk estimation in safety-critical events. To construct the stochastic FRS, we develop a neural network-based acceleration model for surrounding vehicles, and further incorporate confidence-aware dynamic belief to improve the prediction accuracy. Extensive experiments are conducted to validate the performance of the acceleration prediction model based on naturalistic highway driving data, and the efficiency and effectiveness of the framework with the infused confidence belief are tested both in naturalistic and simulated highway scenarios. The proposed risk assessment framework is promising in real-world applications. ",Kein DOI-Link verfügbar,2302.07109v2,Yes,potent(2)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Synbit: Synthesizing Bidirectional Programs using Unidirectional   Sketches,1970,"  We propose a technique for synthesizing bidirectional programs from the corresponding unidirectional code plus a few input/output examples. The core ideas are: (1) constructing a sketch using the given unidirectional program as a specification, and (2) filling the sketch in a modular fashion by exploiting the properties of bidirectional programs. These ideas are enabled by our choice of programming language, HOBiT, which is specifically designed to maintain the unidirectional program structure in bidirectional programming, and keep the parts that control bidirectional behavior modular. To evaluate our approach, we implemented it in a tool called Synbit and used it to generate bidirectional programs for intricate microbenchmarks, as well as for a few larger, more realistic problems. We also compared Synbit to a state-of-the-art unidirectional synthesis tool on the task of synthesizing backward computations. ",Kein DOI-Link verfügbar,2108.13783v4,Yes,intricate(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",CircuitFlow: A Domain Specific Language for Dataflow Programming (with   appendices),1970,"  Dataflow applications, such as machine learning algorithms, can run for days, making it desirable to have assurances that they will work correctly. Current tools are not good enough: too often the interactions between tasks are not type-safe, leading to undesirable run-time errors. This paper presents a new declarative Haskell Embedded DSL (eDSL) for dataflow programming: CircuitFlow. Defined as a Symmetric Monoidal Preorder (SMP) on data that models dependencies in the workflow, it has a strong mathematical basis, refocusing on how data flows through an application, resulting in a more expressive solution that not only catches errors statically, but also achieves competitive run-time performance. In our preliminary evaluation, CircuitFlow outperforms the industry-leading Luigi library of Spotify by scaling better with the number of inputs. The innovative creation of CircuitFlow is also of note, exemplifying how to create a modular eDSL whose semantics necessitates effects, and where storing complex type information for program correctness is paramount. ",Kein DOI-Link verfügbar,2111.12420v1,Yes,innovative(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Contrastive Positive Sample Propagation along the Audio-Visual Event   Line,1970,"  Visual and audio signals often coexist in natural environments, forming audio-visual events (AVEs). Given a video, we aim to localize video segments containing an AVE and identify its category. It is pivotal to learn the discriminative features for each video segment. Unlike existing work focusing on audio-visual feature fusion, in this paper, we propose a new contrastive positive sample propagation (CPSP) method for better deep feature representation learning. The contribution of CPSP is to introduce the available full or weak label as a prior that constructs the exact positive-negative samples for contrastive learning. Specifically, the CPSP involves comprehensive contrastive constraints: pair-level positive sample propagation (PSP), segment-level and video-level positive sample activation (PSA$_S$ and PSA$_V$). Three new contrastive objectives are proposed (\emph{i.e.}, $\mathcal{L}_{\text{avpsp}}$, $\mathcal{L}_\text{spsa}$, and $\mathcal{L}_\text{vpsa}$) and introduced into both the fully and weakly supervised AVE localization. To draw a complete picture of the contrastive learning in AVE localization, we also study the self-supervised positive sample propagation (SSPSP). As a result, CPSP is more helpful to obtain the refined audio-visual features that are distinguishable from the negatives, thus benefiting the classifier prediction. Extensive experiments on the AVE and the newly collected VGGSound-AVEL100k datasets verify the effectiveness and generalization ability of our method. ",Kein DOI-Link verfügbar,2211.09980v1,Yes,pivotal(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",A new computational perceived risk model for automated vehicles based on   potential collision avoidance difficulty (PCAD),1970,"  Perceived risk is crucial in designing trustworthy and acceptable vehicle automation systems. However, our understanding of its dynamics is limited, and models for perceived risk dynamics are scarce in the literature. This study formulates a new computational perceived risk model based on potential collision avoidance difficulty (PCAD) for drivers of SAE level 2 driving automation. PCAD uses the 2D safe velocity gap as the potential collision avoidance difficulty, and takes into account collision severity. The safe velocity gap is defined as the 2D gap between the current velocity and the safe velocity region, and represents the amount of braking and steering needed, considering behavioural uncertainty of neighbouring vehicles and imprecise control of the subject vehicle. The PCAD predicts perceived risk both in continuous time and per event. We compare the PCAD model with three state-of-the-art models and analyse the models both theoretically and empirically with two unique datasets: Dataset Merging and Dataset Obstacle Avoidance. The PCAD model generally outperforms the other models in terms of model error, detection rate, and the ability to accurately capture the tendencies of human drivers' perceived risk, albeit at a longer computation time. Additionally, the study shows that the perceived risk is not static and varies with the surrounding traffic conditions. This research advances our understanding of perceived risk in automated driving and paves the way for improved safety and acceptance of driving automation systems. ",Kein DOI-Link verfügbar,2306.08458v1,Yes,potent(2)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Measuring Sociality in Driving Interaction,1970,"  Interacting with other human road users is one of the most challenging tasks for autonomous vehicles. For congruent driving behaviors, it is essential to recognize and comprehend sociality, encompassing both implicit social norms and individualized social preferences of human drivers. To understand and quantify the complex sociality in driving interactions, we propose a Virtual-Game-based Interaction Model (VGIM) that is parameterized by a social preference measurement, Interaction Preference Value (IPV). The IPV is designed to capture the driver's relative inclination towards individual rewards over group rewards. A method for identifying IPV from observed driving trajectory is also developed, with which we assessed human drivers' IPV using driving data recorded in a typical interactive driving scenario, the unprotected left turn. Our findings reveal that (1) human drivers exhibit particular social preference patterns while undertaking specific tasks, such as turning left or proceeding straight; (2) competitive actions could be strategically conducted by human drivers in order to coordinate with others. Finally, we discuss the potential of learning sociality-aware navigation from human demonstrations by incorporating a rule-based humanlike IPV expressing strategy into VGIM and optimization-based motion planners. Simulation experiments demonstrate that (1) IPV identification improves the motion prediction performance in interactive driving scenarios and (2) the dynamic IPV expressing strategy extracted from human driving data makes it possible to reproduce humanlike coordination patterns in the driving interaction. ",Kein DOI-Link verfügbar,2306.13992v2,Yes,"potent(1), strategically(1)"
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Distributed Federated Learning-Based Deep Learning Model for Privacy MRI   Brain Tumor Detection,1970,"  Distributed training can facilitate the processing of large medical image datasets, and improve the accuracy and efficiency of disease diagnosis while protecting patient privacy, which is crucial for achieving efficient medical image analysis and accelerating medical research progress. This paper presents an innovative approach to medical image classification, leveraging Federated Learning (FL) to address the dual challenges of data privacy and efficient disease diagnosis. Traditional Centralized Machine Learning models, despite their widespread use in medical imaging for tasks such as disease diagnosis, raise significant privacy concerns due to the sensitive nature of patient data. As an alternative, FL emerges as a promising solution by allowing the training of a collective global model across local clients without centralizing the data, thus preserving privacy. Focusing on the application of FL in Magnetic Resonance Imaging (MRI) brain tumor detection, this study demonstrates the effectiveness of the Federated Learning framework coupled with EfficientNet-B0 and the FedAvg algorithm in enhancing both privacy and diagnostic accuracy. Through a meticulous selection of preprocessing methods, algorithms, and hyperparameters, and a comparative analysis of various Convolutional Neural Network (CNN) architectures, the research uncovers optimal strategies for image classification. The experimental results reveal that EfficientNet-B0 outperforms other models like ResNet in handling data heterogeneity and achieving higher accuracy and lower loss, highlighting the potential of FL in overcoming the limitations of traditional models. The study underscores the significance of addressing data heterogeneity and proposes further research directions for broadening the applicability of FL in medical image analysis. ",Kein DOI-Link verfügbar,2404.10026v1,Yes,"innovative(1), meticulous(1), potent(1)"
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic   Architecture,1970,"  Video Motion Magnification (VMM) aims to reveal subtle and imperceptible motion information of objects in the macroscopic world. Prior methods directly model the motion field from the Eulerian perspective by Representation Learning that separates shape and texture or Multi-domain Learning from phase fluctuations. Inspired by the frequency spectrum, we observe that the low-frequency components with stable energy always possess spatial structure and less noise, making them suitable for modeling the subtle motion field. To this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion Magnification with a Multi-level Isomorphic Architecture to capture multi-level high-frequency details and a stable low-frequency structure (motion field) in video space. Since high-frequency details and subtle motions are susceptible to information degradation due to their inherent subtlety and unavoidable external interference from noise, we carefully design Sparse High/Low-pass Filters to enhance the integrity of details and motion structures, and a Sparse Frequency Mixer to promote seamless recoupling. Besides, we innovatively design a contrastive regularization for this task to strengthen the model's ability to discriminate irrelevant features, reducing undesired motion magnification. Extensive experiments on both Real-world and Synthetic Datasets show that our FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$ and boosts inference speed by 1.68$\times$ than the latest method. Our code is available at https://github.com/Jiafei127/FD4MM. ",Kein DOI-Link verfügbar,2403.07347v2,Yes,"innovative(1), innovatively(1)"
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable   Frameworks,1970,"  Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of ""retrieve-then-generate"". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies. ",Kein DOI-Link verfügbar,2407.21059v1,Yes,"innovative(1), potent(1)"
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",High Efficiency Rate Control for Versatile Video Coding Based on   Composite Cauchy Distribution,1970,"  In this work, we propose a novel rate control algorithm for Versatile Video Coding (VVC) standard based on its distinct rate-distortion characteristics. By modelling the transform coefficients with the composite Cauchy distribution, higher accuracy compared with traditional distributions has been achieved. Based on the transform coefficient modelling, the theoretically derived R-Q and D-Q models which have been shown to deliver higher accuracy in characterizing RD characteristics for sequences with different content are incorporated into the rate control process. Furthermore, to establish an adaptive bit allocation scheme, the dependency between different levels of frames is modelled by a dependency factor to describe relationship between the reference and to-be-coded frames. Given the derived R-Q and D-Q relationships, as well as the dependency factor, an adaptive bit allocation scheme is developed for optimal bits allocation. We implement the proposed algorithm on VVC Test Model (VTM) 3.0. Experiments show that due to proper bit allocation, for low delay configuration the proposed algorithm can achieve 1.03% BD-Rate saving compared with the default rate control algorithm and 2.96% BD-Rate saving compared with fixed QP scheme. Moreover, 1.29% BD-Rate saving and higher control accuracy have also been observed under the random access configuration. ",Kein DOI-Link verfügbar,2008.11455v2,Yes,versatile(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",A Survey on Large-scale Machine Learning,1970,"  Machine learning can provide deep insights into data, allowing machines to make high-quality predictions and having been widely used in real-world applications, such as text mining, visual classification, and recommender systems. However, most sophisticated machine learning approaches suffer from huge time costs when operating on large-scale data. This issue calls for the need of {Large-scale Machine Learning} (LML), which aims to learn patterns from big data with comparable performance efficiently. In this paper, we offer a systematic survey on existing LML methods to provide a blueprint for the future developments of this area. We first divide these LML methods according to the ways of improving the scalability: 1) model simplification on computational complexities, 2) optimization approximation on computational efficiency, and 3) computation parallelism on computational capabilities. Then we categorize the methods in each perspective according to their targeted scenarios and introduce representative methods in line with intrinsic strategies. Lastly, we analyze their limitations and discuss potential directions as well as open issues that are promising to address in the future. ",Kein DOI-Link verfügbar,2008.03911v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Solutions for Fine-grained and Long-tailed Snake Species Recognition in   SnakeCLEF 2022,1970,"  Automatic snake species recognition is important because it has vast potential to help lower deaths and disabilities caused by snakebites. We introduce our solution in SnakeCLEF 2022 for fine-grained snake species recognition on a heavy long-tailed class distribution. First, a network architecture is designed to extract and fuse features from multiple modalities, i.e. photograph from visual modality and geographic locality information from language modality. Then, logit adjustment based methods are studied to relieve the impact caused by the severe class imbalance. Next, a combination of supervised and self-supervised learning method is proposed to make full use of the dataset, including both labeled training data and unlabeled testing data. Finally, post processing strategies, such as multi-scale and multi-crop test-time-augmentation, location filtering and model ensemble, are employed for better performance. With an ensemble of several different models, a private score 82.65%, ranking the 3rd, is achieved on the final leaderboard. ",Kein DOI-Link verfügbar,2207.01216v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Positive Sample Propagation along the Audio-Visual Event Line,1970,"  Visual and audio signals often coexist in natural environments, forming audio-visual events (AVEs). Given a video, we aim to localize video segments containing an AVE and identify its category. In order to learn discriminative features for a classifier, it is pivotal to identify the helpful (or positive) audio-visual segment pairs while filtering out the irrelevant ones, regardless whether they are synchronized or not. To this end, we propose a new positive sample propagation (PSP) module to discover and exploit the closely related audio-visual pairs by evaluating the relationship within every possible pair. It can be done by constructing an all-pair similarity map between each audio and visual segment, and only aggregating the features from the pairs with high similarity scores. To encourage the network to extract high correlated features for positive samples, a new audio-visual pair similarity loss is proposed. We also propose a new weighting branch to better exploit the temporal correlations in weakly supervised setting. We perform extensive experiments on the public AVE dataset and achieve new state-of-the-art accuracy in both fully and weakly supervised settings, thus verifying the effectiveness of our method. ",Kein DOI-Link verfügbar,2104.00239v2,Yes,pivotal(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",HoSIM: Higher-order Structural Importance based Method for Multiple   Local Community Detection,1970,"  Local community detection has attracted much research attention recently, and many methods have been proposed for the single local community detection that finds a community containing the given set of query nodes. However, nodes may belong to several communities in the network, and detecting all the communities for the query node set, termed as the multiple local community detection (MLCD), is more important as it could uncover more potential information. MLCD is also more challenging because when a query node belongs to multiple communities, it always locates in the complicated overlapping region and the marginal region of communities. Accordingly, detecting multiple communities for such nodes by applying seed expansion methods is insufficient.   In this work, we address the MLCD based on higher-order structural importance (HoSI). First, to effectively estimate the influence of higher-order structures, we propose a new variant of random walk called Active Random Walk to measure the HoSI score between nodes. Then, we propose two new metrics to evaluate the HoSI score of a subgraph to a node and the HoSI score of a node, respectively. Based on the proposed metrics, we present a novel algorithm called HoSIM to detect multiple local communities for a single query node. HoSIM enforces a three-stage processing, namely subgraph sampling, core member identification, and local community detection. The key idea is utilizing HoSI to find and identify the core members of communities relevant to the query node and optimize the generated communities. Extensive experiments illustrate the effectiveness of HoSIM. ",Kein DOI-Link verfügbar,2205.11812v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code   Generation,1970,"  There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 72.73%, 60.40%, and 65.85% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions. ",Kein DOI-Link verfügbar,2308.02828v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Dual-path TokenLearner for Remote Photoplethysmography-based   Physiological Measurement with Facial Videos,1970,"  Remote photoplethysmography (rPPG) based physiological measurement is an emerging yet crucial vision task, whose challenge lies in exploring accurate rPPG prediction from facial videos accompanied by noises of illumination variations, facial occlusions, head movements, \etc, in a non-contact manner. Existing mainstream CNN-based models make efforts to detect physiological signals by capturing subtle color changes in facial regions of interest (ROI) caused by heartbeats. However, such models are constrained by the limited local spatial or temporal receptive fields in the neural units. Unlike them, a native Transformer-based framework called Dual-path TokenLearner (Dual-TL) is proposed in this paper, which utilizes the concept of learnable tokens to integrate both spatial and temporal informative contexts from the global perspective of the video. Specifically, the proposed Dual-TL uses a Spatial TokenLearner (S-TL) to explore associations in different facial ROIs, which promises the rPPG prediction far away from noisy ROI disturbances. Complementarily, a Temporal TokenLearner (T-TL) is designed to infer the quasi-periodic pattern of heartbeats, which eliminates temporal disturbances such as head movements. The two TokenLearners, S-TL and T-TL, are executed in a dual-path mode. This enables the model to reduce noise disturbances for final rPPG signal prediction. Extensive experiments on four physiological measurement benchmark datasets are conducted. The Dual-TL achieves state-of-the-art performances in both intra- and cross-dataset testings, demonstrating its immense potential as a basic backbone for rPPG measurement. The source code is available at \href{https://github.com/VUT-HFUT/Dual-TL}{https://github.com/VUT-HFUT/Dual-TL} ",Kein DOI-Link verfügbar,2308.07771v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA,1970,"  Text-based visual question answering (TextVQA) faces the significant challenge of avoiding redundant relational inference. To be specific, a large number of detected objects and optical character recognition (OCR) tokens result in rich visual relationships. Existing works take all visual relationships into account for answer prediction. However, there are three observations: (1) a single subject in the images can be easily detected as multiple objects with distinct bounding boxes (considered repetitive objects). The associations between these repetitive objects are superfluous for answer reasoning; (2) two spatially distant OCR tokens detected in the image frequently have weak semantic dependencies for answer reasoning; and (3) the co-existence of nearby objects and tokens may be indicative of important visual cues for predicting answers. Rather than utilizing all of them for answer prediction, we make an effort to identify the most important connections or eliminate redundant ones. We propose a sparse spatial graph network (SSGN) that introduces a spatially aware relation pruning technique to this task. As spatial factors for relation measurement, we employ spatial distance, geometric dimension, overlap area, and DIoU for spatially aware pruning. We consider three visual relationships for graph learning: object-object, OCR-OCR tokens, and object-OCR token relationships. SSGN is a progressive graph learning architecture that verifies the pivotal relations in the correlated object-token sparse graph, and then in the respective object-based sparse graph and token-based sparse graph. Experiment results on TextVQA and ST-VQA datasets demonstrate that SSGN achieves promising performances. And some visualization results further demonstrate the interpretability of our method. ",https://doi.org/10.1109/TIP.2023.3310332,2310.09147v1,Yes,pivotal(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Sequencing-enabled Hierarchical Cooperative CAV On-ramp Merging Control   with Enhanced Stability and Feasibility,1970,"  This paper develops a sequencing-enabled hierarchical connected automated vehicle (CAV) cooperative on-ramp merging control framework. The proposed framework consists of a two-layer design: the upper level control sequences the vehicles to harmonize the traffic density across mainline and on-ramp segments while enhancing lower-level control efficiency through a mixed-integer linear programming formulation. Subsequently, the lower-level control employs a longitudinal distributed model predictive control (MPC) supplemented by a virtual car-following (CF) concept to ensure asymptotic local stability, l_2 norm string stability, and safety. Proofs of asymptotic local stability and l_2 norm string stability are mathematically derived. Compared to other prevalent asymptotic local-stable MPC controllers, the proposed distributed MPC controller greatly expands the initial feasible set. Additionally, an auxiliary lateral control is developed to maintain lane-keeping and merging smoothness while accommodating ramp geometric curvature. To validate the proposed framework, multiple numerical experiments are conducted. Results indicate a notable outperformance of our upper-level controller against a distance-based sequencing method. Furthermore, the lower-level control effectively ensures smooth acceleration, safe merging with adequate spacing, adherence to proven longitudinal local and string stability, and rapid regulation of lateral deviations. ",Kein DOI-Link verfügbar,2311.14924v2,Yes,notable(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital","Benchmarking Micro-action Recognition: Dataset, Methods, and   Applications",1970,"  Micro-action is an imperceptible non-verbal behaviour characterised by low-intensity movement. It offers insights into the feelings and intentions of individuals and is important for human-oriented applications such as emotion recognition and psychological assessment. However, the identification, differentiation, and understanding of micro-actions pose challenges due to the imperceptible and inaccessible nature of these subtle human behaviors in everyday life. In this study, we innovatively collect a new micro-action dataset designated as Micro-action-52 (MA-52), and propose a benchmark named micro-action network (MANet) for micro-action recognition (MAR) task. Uniquely, MA-52 provides the whole-body perspective including gestures, upper- and lower-limb movements, attempting to reveal comprehensive micro-action cues. In detail, MA-52 contains 52 micro-action categories along with seven body part labels, and encompasses a full array of realistic and natural micro-actions, accounting for 205 participants and 22,422 video instances collated from the psychological interviews. Based on the proposed dataset, we assess MANet and other nine prevalent action recognition methods. MANet incorporates squeeze-and excitation (SE) and temporal shift module (TSM) into the ResNet architecture for modeling the spatiotemporal characteristics of micro-actions. Then a joint-embedding loss is designed for semantic matching between video and action labels; the loss is used to better distinguish between visually similar yet distinct micro-action categories. The extended application in emotion recognition has demonstrated one of the important values of our proposed dataset and method. In the future, further exploration of human behaviour, emotion, and psychological assessment will be conducted in depth. The dataset and source code are released at https://github.com/VUT-HFUT/Micro-Action. ",https://doi.org/10.1109/TCSVT.2024.3358415,2403.05234v2,Yes,"innovative(1), innovatively(1)"
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Reinforced Negative Sampling over Knowledge Graph for Recommendation,1970,"  Properly handling missing data is a fundamental challenge in recommendation. Most present works perform negative sampling from unobserved data to supply the training of recommender models with negative signals. Nevertheless, existing negative sampling strategies, either static or adaptive ones, are insufficient to yield high-quality negative samples --- both informative to model training and reflective of user real needs. In this work, we hypothesize that item knowledge graph (KG), which provides rich relations among items and KG entities, could be useful to infer informative and factual negative samples. Towards this end, we develop a new negative sampling model, Knowledge Graph Policy Network (KGPolicy), which works as a reinforcement learning agent to explore high-quality negatives. Specifically, by conducting our designed exploration operations, it navigates from the target positive interaction, adaptively receives knowledge-aware negative signals, and ultimately yields a potential negative item to train the recommender. We tested on a matrix factorization (MF) model equipped with KGPolicy, and it achieves significant improvements over both state-of-the-art sampling methods like DNS and IRGAN, and KG-enhanced recommender models like KGAT. Further analyses from different angles provide insights of knowledge-aware sampling. We release the codes and datasets at https://github.com/xiangwang1223/kgpolicy. ",https://doi.org/10.1145/3366423.3380098,2003.05753v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Learning Non-Stationary Time-Series with Dynamic Pattern Extractions,1970,"  The era of information explosion had prompted the accumulation of a tremendous amount of time-series data, including stationary and non-stationary time-series data. State-of-the-art algorithms have achieved a decent performance in dealing with stationary temporal data. However, traditional algorithms that tackle stationary time-series do not apply to non-stationary series like Forex trading. This paper investigates applicable models that can improve the accuracy of forecasting future trends of non-stationary time-series sequences. In particular, we focus on identifying potential models and investigate the effects of recognizing patterns from historical data. We propose a combination of \rebuttal{the} seq2seq model based on RNN, along with an attention mechanism and an enriched set features extracted via dynamic time warping and zigzag peak valley indicators. Customized loss functions and evaluating metrics have been designed to focus more on the predicting sequence's peaks and valley points. Our results show that our model can predict 4-hour future trends with high accuracy in the Forex dataset, which is crucial in realistic scenarios to assist foreign exchange trading decision making. We further provide evaluations of the effects of various loss functions, evaluation metrics, model variants, and components on model performance. ",Kein DOI-Link verfügbar,2111.10559v1,Yes,potent(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Cooperative Data Collection with Multiple UAVs for Information Freshness   in the Internet of Things,1970,"  Maintaining the freshness of information in the Internet of Things (IoT) is a critical yet challenging problem. In this paper, we study cooperative data collection using multiple Unmanned Aerial Vehicles (UAVs) with the objective of minimizing the total average Age of Information (AoI). We consider various constraints of the UAVs, including kinematic, energy, trajectory, and collision avoidance, in order to optimize the data collection process. Specifically, each UAV, which has limited on-board energy, takes off from its initial location and flies over sensor nodes to collect update packets in cooperation with the other UAVs. The UAVs must land at their final destinations with non-negative residual energy after the specified time duration to ensure they have enough energy to complete their missions. It is crucial to design the trajectories of the UAVs and the transmission scheduling of the sensor nodes to enhance information freshness. We model the multi-UAV data collection problem as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP), as each UAV is unaware of the dynamics of the environment and can only observe a part of the sensors. To address the challenges of this problem, we propose a multi-agent Deep Reinforcement Learning (DRL)-based algorithm with centralized learning and decentralized execution. In addition to the reward shaping, we use action masks to filter out invalid actions and ensure that the constraints are met. Simulation results demonstrate that the proposed algorithms can significantly reduce the total average AoI compared to the baseline algorithms, and the use of the action mask method can improve the convergence speed of the proposed algorithm. ",Kein DOI-Link verfügbar,2303.01381v1,Yes,fresh(2)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Scalable Face Image Coding via StyleGAN Prior: Towards Compression for   Human-Machine Collaborative Vision,1970,"  The accelerated proliferation of visual content and the rapid development of machine vision technologies bring significant challenges in delivering visual data on a gigantic scale, which shall be effectively represented to satisfy both human and machine requirements. In this work, we investigate how hierarchical representations derived from the advanced generative prior facilitate constructing an efficient scalable coding paradigm for human-machine collaborative vision. Our key insight is that by exploiting the StyleGAN prior, we can learn three-layered representations encoding hierarchical semantics, which are elaborately designed into the basic, middle, and enhanced layers, supporting machine intelligence and human visual perception in a progressive fashion. With the aim of achieving efficient compression, we propose the layer-wise scalable entropy transformer to reduce the redundancy between layers. Based on the multi-task scalable rate-distortion objective, the proposed scheme is jointly optimized to achieve optimal machine analysis performance, human perception experience, and compression ratio. We validate the proposed paradigm's feasibility in face image compression. Extensive qualitative and quantitative experimental results demonstrate the superiority of the proposed paradigm over the latest compression standard Versatile Video Coding (VVC) in terms of both machine analysis as well as human perception at extremely low bitrates ($<0.01$ bpp), offering new insights for human-machine collaborative compression. ",Kein DOI-Link verfügbar,2312.15622v1,Yes,versatile(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Geometric Prior Based Deep Human Point Cloud Geometry Compression,1970,"  The emergence of digital avatars has raised an exponential increase in the demand for human point clouds with realistic and intricate details. The compression of such data becomes challenging with overwhelming data amounts comprising millions of points. Herein, we leverage the human geometric prior in geometry redundancy removal of point clouds, greatly promoting the compression performance. More specifically, the prior provides topological constraints as geometry initialization, allowing adaptive adjustments with a compact parameter set that could be represented with only a few bits. Therefore, we can envisage high-resolution human point clouds as a combination of geometric priors and structural deviations. The priors could first be derived with an aligned point cloud, and subsequently the difference of features is compressed into a compact latent code. The proposed framework can operate in a play-and-plug fashion with existing learning based point cloud compression methods. Extensive experimental results show that our approach significantly improves the compression performance without deteriorating the quality, demonstrating its promise in a variety of applications. ",https://doi.org/10.1109/TCSVT.2024.3379518,2305.01309v2,Yes,intricate(1)
0000-0002-6448-1267,Meng Wang,"Peking University, Peking University Third Hospital",Normal and superconducting properties of La$_3$Ni$_2$O$_7$,1970,"  This review provides a comprehensive overview of current research on the structural, electronic, and magnetic characteristics of the recently discovered high-temperature superconductor La$_3$Ni$_2$O$_7$ under high pressures. We present the experimental results for synthesizing and characterizing this material, derived from measurements of transport, thermodynamics, and various spectroscopic techniques, and discuss their physical implications. We also explore theoretical models proposed to describe the electronic structures and superconducting pairing symmetry in La$_3$Ni$_2$O$_7$, highlighting the intricate interplay between electronic correlations and magnetic interactions. Despite these advances, challenges remain in growing high-quality samples free of extrinsic phases and oxygen deficiencies and in developing reliable measurement tools for determining diamagnetism and other physical quantities under high pressures. Further investigations in these areas are essential to deepening our understanding of the physical properties of La$_3$Ni$_2$O$_7$ and unlocking its superconducting pairing mechanism. ",Kein DOI-Link verfügbar,2406.04837v1,Yes,intricate(1)
0000-0002-3707-9272,Fang Zhang,"Peking University, Peking University People's Hospital",Combining Deep Reinforcement Learning and Safety Based Control for   Autonomous Driving,1970,"  With the development of state-of-art deep reinforcement learning, we can efficiently tackle continuous control problems. But the deep reinforcement learning method for continuous control is based on historical data, which would make unpredicted decisions in unfamiliar scenarios. Combining deep reinforcement learning and safety based control can get good performance for self-driving and collision avoidance. In this passage, we use the Deep Deterministic Policy Gradient algorithm to implement autonomous driving without vehicles around. The vehicle can learn the driving policy in a stable and familiar environment, which is efficient and reliable. Then we use the artificial potential field to design collision avoidance algorithm with vehicles around. The path tracking method is also taken into consideration. The combination of deep reinforcement learning and safety based control performs well in most scenarios. ",Kein DOI-Link verfügbar,1612.00147v1,Yes,potent(1)
0000-0002-8873-6711,Yibo Ji,"Cornell University, Cornell University Ith, Peking University",Bow varieties as symplectic reductions of $T^*(G/P)$,1970,"  Cherkis bow varieties were introduced as ADHM type description of moduli space of instantons on the Taub-NUT space equivariant under a cyclic group action. They are also models of Coulomb branches of quiver gauge theories of affine type A. In this paper, we realize each bow variety with torus fixed points as a symplectic reduction of a cotangent bundle of a partial flag variety by a unipotent group, and find a slice of this action. By this description, we calculate the equivariant cohomology (and ordinary cohomology) of some of them and answer some questions raisedbefore. This also uses a new result about circle-equivariant cohomology proven in an appendix. We also give an explicit generalized Mirkovic-Vybornov isomorphism for bow varieties in the appendix. ",Kein DOI-Link verfügbar,2312.04696v3,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Algorand,1970,"  A public ledger is a tamperproof sequence of data that can be read and augmented by everyone. Public ledgers have innumerable and compelling uses. They can secure, in plain sight, all kinds of transactions ---such as titles, sales, and payments--- in the exact order in which they occur. Public ledgers not only curb corruption, but also enable very sophisticated applications ---such as cryptocurrencies and smart contracts. They stand to revolutionize the way a democratic society operates. As currently implemented, however, they scale poorly and cannot achieve their potential.   Algorand is a truly democratic and efficient way to implement a public ledger. Unlike prior implementations based on proof of work, it requires a negligible amount of computation, and generates a transaction history that will not ""fork"" with overwhelmingly high probability.   Algorand is based on (a novel and super fast) message-passing Byzantine agreement.   For concreteness, we shall describe Algorand only as a money platform. ",Kein DOI-Link verfügbar,1607.01341v9,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Cons-training tensor networks,1970,"  In this study, we introduce a novel family of tensor networks, termed \textit{constrained matrix product states} (MPS), designed to incorporate exactly arbitrary discrete linear constraints, including inequalities, into sparse block structures. These tensor networks are particularly tailored for modeling distributions with support strictly over the feasible space, offering benefits such as reducing the search space in optimization problems, alleviating overfitting, improving training efficiency, and decreasing model size. Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in U(1) symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario. We further develop a novel canonical form for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules and permit optimal truncation schemes. Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary objective functions subject to discrete linear constraints. Our method's efficacy is demonstrated by solving the quadratic knapsack problem, achieving superior performance compared to a leading nonlinear integer programming solver. Additionally, we analyze the complexity and scalability of our approach, demonstrating its potential in addressing complex constrained combinatorial optimization problems. ",Kein DOI-Link verfügbar,2405.09005v2,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Optimal Provision-After-Wait in Healthcare,1970,"  We investigate computational and mechanism design aspects of scarce resource allocation, where the primary rationing mechanism is through waiting times. Specifically we consider allocating medical treatments to a population of patients. Each patient needs exactly one treatment, and can choose from $k$ hospitals. Hospitals have different costs, which are fully paid by a third party ---the ""payer"". The payer has a fixed budget $B$, and each hospital will have its own waiting time. At equilibrium, each patient will choose his most preferred hospital given his intrinsic preferences and the waiting times. The payer thus computes the waiting times so that at equilibrium the budget constraint is satisfied and the social welfare is maximized.   We first show that the optimization problem is NP-hard, yet if the budget can be relaxed to $(1+\epsilon)B$ for an arbitrarily small $\epsilon$, then the optimum under budget $B$ can be approximated efficiently. Next, we study the endogenous emergence of waiting time from the dynamics between hospitals and patients, and show that there is no need for the payer to explicitly enforce the optimal waiting times. Under certain conditions, all he need is to enforce the amount of money he wants to pay to each hospital. The dynamics will always converge to the desired waiting times in finite time.   We then go beyond equilibrium solutions and investigate the optimization problem over a much larger class of mechanisms containing the equilibrium ones as special cases. With two hospitals, we show that under a natural assumption on the patients' preference profiles, optimal welfare is in fact attained by the randomized assignment mechanism, which allocates patients to hospitals at random subject to the budget constraint, but avoids waiting times.   Finally, we discuss potential policy implications of our results, as well as follow-up directions and open problems. ",Kein DOI-Link verfügbar,1312.1955v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Classical Trajectory Perspective on Double Ionization Dynamics of   Diatomic Molecules Irradiated by Ultrashort Intense Laser Pulses,1970,"  In the present paper, we develop a semiclassical quasi-static model accounting for molecular double ionization in an intense laser pulse. With this model, we achieve insight into the dynamics of two highly-correlated valence electrons under the combined influence of a two-center Coulomb potential and an intense laser field, and reveal the significant influence of molecular alignment on the ratio of double over single ion yield. Analysis on the classical trajectories unveils sub-cycle dynamics of the molecular double ionization. Many interesting features, such as the accumulation of emitted electrons in the first and third quadrants of parallel momentum plane, the regular pattern of correlated momentum with respect to the time delay between closest collision and ionization moment, are revealed and successfully explained by back analyzing the classical trajectories. Quantitative agreement with experimental data over a wide range of laser intensities from tunneling to over-the-barrier regime is presented. ",https://doi.org/10.1103/PhysRevA.77.013403,0708.2132v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Chemical Freeze-out Parameters via a Non-perturbative QCD Approach,1970,"  By analyzing the calculated baryon number susceptibility ratios ${\chi_{1}^{B}}/{\chi_{2}^{B}}$ and ${\chi_{3}^{B}}/{\chi_{1}^{B}}$ in two-flavor system via the Dyson-Schwinger equation approach of QCD, we determine the chemical freeze-out temperature and baryon chemical potential in cases of both thermodynamic limit and finite size. We calculate the center-of-mass energy dependence of the ${\chi_{4}^{B}}/{\chi_{2}^{B}}\, (\kappa \sigma^{2})$ at the freeze-out line and find an excellent agreement with experimental data in $\sqrt{S_{NN}^{}} \geq 19.6\,$GeV region when taking into account the finite size effect. Our calculations indicate that the $\kappa \sigma^{2}$ exhibits a non-monotonic behavior in lower collision energy region. ",Kein DOI-Link verfügbar,1510.07543v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Efficient Uncertainty-aware Decision-making for Automated Driving Using   Guided Branching,1970,"  Decision-making in dense traffic scenarios is challenging for automated vehicles (AVs) due to potentially stochastic behaviors of other traffic participants and perception uncertainties (e.g., tracking noise and prediction errors, etc.). Although the partially observable Markov decision process (POMDP) provides a systematic way to incorporate these uncertainties, it quickly becomes computationally intractable when scaled to the real-world large-size problem. In this paper, we present an efficient uncertainty-aware decision-making (EUDM) framework, which generates long-term lateral and longitudinal behaviors in complex driving environments in real-time. The computation complexity is controlled to an appropriate level by two novel techniques, namely, the domain-specific closed-loop policy tree (DCP-Tree) structure and conditional focused branching (CFB) mechanism. The key idea is utilizing domain-specific expert knowledge to guide the branching in both action and intention space. The proposed framework is validated using both onboard sensing data captured by a real vehicle and an interactive multi-agent simulation platform. We also release the code of our framework to accommodate benchmarking. ",Kein DOI-Link verfügbar,2003.02746v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,EPSILON: An Efficient Planning System for Automated Vehicles in Highly   Interactive Environments,1970,"  In this paper, we present an Efficient Planning System for automated vehicles In highLy interactive envirONments (EPSILON). EPSILON is an efficient interaction-aware planning system for automated driving, and is extensively validated in both simulation and real-world dense city traffic. It follows a hierarchical structure with an interactive behavior planning layer and an optimization-based motion planning layer. The behavior planning is formulated from a partially observable Markov decision process (POMDP), but is much more efficient than naively applying a POMDP to the decision-making problem. The key to efficiency is guided branching in both the action space and observation space, which decomposes the original problem into a limited number of closed-loop policy evaluations. Moreover, we introduce a new driver model with a safety mechanism to overcome the risk induced by the potential imperfectness of prior knowledge. For motion planning, we employ a spatio-temporal semantic corridor (SSC) to model the constraints posed by complex driving environments in a unified way. Based on the SSC, a safe and smooth trajectory is optimized, complying with the decision provided by the behavior planner. We validate our planning system in both simulations and real-world dense traffic, and the experimental results show that our EPSILON achieves human-like driving behaviors in highly interactive traffic flow smoothly and safely without being over-conservative compared to the existing planning methods. ",Kein DOI-Link verfügbar,2108.07993v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Stockformer: A Price-Volume Factor Stock Selection Model Based on   Wavelet Transform and Multi-Task Self-Attention Networks,1970,"  As the Chinese stock market continues to evolve and its market structure grows increasingly complex, traditional quantitative trading methods are facing escalating challenges. Particularly, due to policy uncertainty and the frequent market fluctuations triggered by sudden economic events, existing models often struggle to accurately predict market dynamics. To address these challenges, this paper introduces Stockformer, a price-volume factor stock selection model that integrates wavelet transformation and a multitask self-attention network, aimed at enhancing responsiveness and predictive accuracy regarding market instabilities. Through discrete wavelet transform, Stockformer decomposes stock returns into high and low frequencies, meticulously capturing long-term market trends and short-term fluctuations, including abrupt events. Moreover, the model incorporates a Dual-Frequency Spatiotemporal Encoder and graph embedding techniques to effectively capture complex temporal and spatial relationships among stocks. Employing a multitask learning strategy, it simultaneously predicts stock returns and directional trends. Experimental results show that Stockformer outperforms existing advanced methods on multiple real stock market datasets. In strategy backtesting, Stockformer consistently demonstrates exceptional stability and reliability across market conditions-whether rising, falling, or fluctuating-particularly maintaining high performance during downturns or volatile periods, indicating a high adaptability to market fluctuations. To foster innovation and collaboration in the financial analysis sector, the Stockformer model's code has been open-sourced and is available on the GitHub repository: https://github.com/Eric991005/Multitask-Stockformer. ",Kein DOI-Link verfügbar,2401.06139v2,Yes,"meticulous(1), meticulously(1)"
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Frustrated double ionization of atoms in circularly polarized laser   fields,1970,"  We theoretically study frustrated double ionization (FDI) of atoms subjected to intense circularly polarized laser pulses using a three-dimensional classical model. We find a novel ""knee"" structure of FDI probability as a function of intensity, which is similar to the intensity dependence of nonsequential double ionization probability. The observation of FDI is more favourable when using targets with low ionization potentials and short driving laser wavelengths. This is attributed to the crucial role of recollision therein, which can be experimentally inferred from the photoelectron momentum distribution generated by FDI. This work provides novel physical insights into FDI dynamics with circular polarization. ",https://doi.org/10.1088/1367-2630/abe79d,2009.06258v2,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Thick branes with inner structure in mimetic $f(R)$ gravity,1970,"  In this paper, we study the structure and gravitational resonances of thick branes generated by a mimetic scalar field in $f(R)$ gravity. We obtain several typical thick brane solutions for $f(R)=R+\alpha R^2$. To study their stability, we analyze the tensor perturbation of the metric. It is shown that any thick brane model with $df/dR>0$ is stable and the graviton zero mode can be localized on the brane for each solution, which indicates that the four-dimensional Newtonian gravity can be restored. The effect of the parameter $\alpha$ on the gravitational resonances is studied. As a brane splits into multi sub-branes, the effective potential of the tensor perturbation will have an abundant inner structure with multi-wells, and this will lead to new phenomena of the gravitational resonances. ",https://doi.org/10.1140/epjc/s10052-021-09504-y,2011.03927v4,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Wavelength Dependent Tunneling Delay Time,1970,"  A clear consensus on how long it takes a particle to tunnel through a potential barrier has never been so urgently required, since the electron dynamics in strong-field ionization can be resolved on attosecond time-scale in experiment and the exact nature of the tunneling process is the key to trigger subsequent attosecond techniques. Here a general picture of tunneling time is suggested by introducing a quantum travel time, which is defined as the ratio of the travel distance to the expected value of the velocity operator under the barrier. Specially, if applied to rectangular barrier tunneling, it can retrieve the B\""{u}ttiker-Landauer time $\tau_{BL}$ in the case of an opaque barrier, and has a clear physical meaning in the case of a very thin barrier wherein $\tau_{BL}$ can not be well defined. In the case of strong-field tunneling process, with the help of the newly defined time, the tunneling delay time measured by attoclock experiment can be interpreted as a travel time spent by the electron to tunnel from a point under the barrier to the tunnel exit. In addition, a peculiar oscillation structure in the wavelength dependence of tunneling delay time in deep tunneling regime is observed, which is beyond the scope of adiabatic tunneling picture. This oscillation structure can be attributed to the interference between the ground state tunneling channel and the excited states tunneling channels. ",Kein DOI-Link verfügbar,1903.06897v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Non-Hermitian guided modes and exceptional points using loss-free   negative-index materials,1970,"  We analyze the guided modes in coupled waveguides made of negative-index materials without gain or loss. We show that it supports non-Hermitian phenomenon on the existence of guided mode versus geometric parameters of the structure. The non-Hermitian effect is different from parity-time (PT) symmetry, and can be explained by a simple coupled-mode theory with an anti-PT symmetry. The existence of exceptional points and slow-light effect are discussed. This work highlights the potential of loss-free negative-index materials in the study of non-Hermitian optics. ",https://doi.org/10.1364/OE.487278,2303.15668v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Higher-order exceptional points in loss-free waveguide arrays with   negative-index materials,1970,  Negative-index materials (NIMs) are shown to support optical anti-parity-time (anti-PT) symmetry even when they are lossless. Here we prove the feasibility in achieving higher-order exceptional points (EPs) in loss-free waveguide arrays by utilizing the anti-$\mathcal{PT}$ symmetry induced by NIM. Numerical simulation about a third-order EP fits well with the coupled-mode theory. A scheme of achieving fourth-order EPs is also discussed. This work highlights the potential of loss-free NIMs in the study of non-Hermitian optics. ,Kein DOI-Link verfügbar,2304.07289v2,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Incremental high average-utility itemset mining: survey and challenges,1970,"  The High Average Utility Itemset Mining (HAUIM) technique, a variation of High Utility Itemset Mining (HUIM), uses the average utility of the itemsets. Historically, most HAUIM algorithms were designed for static databases. However, practical applications like market basket analysis and business decision-making necessitate regular updates of the database with new transactions. As a result, researchers have developed incremental HAUIM (iHAUIM) algorithms to identify HAUIs in a dynamically updated database. Contrary to conventional methods that begin from scratch, the iHAUIM algorithm facilitates incremental changes and outputs, thereby reducing the cost of discovery. This paper provides a comprehensive review of the state-of-the-art iHAUIM algorithms, analyzing their unique characteristics and advantages. First, we explain the concept of iHAUIM, providing formulas and real-world examples for a more in-depth understanding. Subsequently, we categorize and discuss the key technologies used by varying types of iHAUIM algorithms, encompassing Apriori-based, Tree-based, and Utility-list-based techniques. Moreover, we conduct a critical analysis of each mining method's advantages and disadvantages. In conclusion, we explore potential future directions, research opportunities, and various extensions of the iHAUIM algorithm. ",https://doi.org/10.1038/s41598-024-60279-0,2407.11425v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,SmartSeed: Smart Seed Generation for Efficient Fuzzing,1970,"  Fuzzing is an automated application vulnerability detection method. For genetic algorithm-based fuzzing, it can mutate the seed files provided by users to obtain a number of inputs, which are then used to test the objective application in order to trigger potential crashes. As shown in existing literature, the seed file selection is crucial for the efficiency of fuzzing. However, current seed selection strategies do not seem to be better than randomly picking seed files. Therefore, in this paper, we propose a novel and generic system, named SmartSeed, to generate seed files towards efficient fuzzing. Specifically, SmartSeed is designed based on a machine learning model to learn and generate high-value binary seeds. We evaluate SmartSeed along with American Fuzzy Lop (AFL) on 12 open-source applications with the input formats of mp3, bmp or flv. We also combine SmartSeed with different fuzzing tools to examine its compatibility. From extensive experiments, we find that SmartSeed has the following advantages: First, it only requires tens of seconds to generate sufficient high-value seeds. Second, it can generate seeds with multiple kinds of input formats and significantly improves the fuzzing performance for most applications with the same input format. Third, SmartSeed is compatible to different fuzzing tools. In total, our system discovers more than twice unique crashes and 5,040 extra unique paths than the existing best seed selection strategy for the evaluated 12 applications. From the crashes found by SmartSeed, we discover 16 new vulnerabilities and have received their CVE IDs. ",Kein DOI-Link verfügbar,1807.02606v3,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack   via Adversarial Perceptual-aware Perturbations,1970,"  DeepFake is becoming a real risk to society and brings potential threats to both individual privacy and political security due to the DeepFaked multimedia are realistic and convincing. However, the popular DeepFake passive detection is an ex-post forensics countermeasure and failed in blocking the disinformation spreading in advance. To address this limitation, researchers study the proactive defense techniques by adding adversarial noises into the source data to disrupt the DeepFake manipulation. However, the existing studies on proactive DeepFake defense via injecting adversarial noises are not robust, which could be easily bypassed by employing simple image reconstruction revealed in a recent study MagDR.   In this paper, we investigate the vulnerability of the existing forgery techniques and propose a novel \emph{anti-forgery} technique that helps users protect the shared facial images from attackers who are capable of applying the popular forgery techniques. Our proposed method generates perceptual-aware perturbations in an incessant manner which is vastly different from the prior studies by adding adversarial noises that is sparse. Experimental results reveal that our perceptual-aware perturbations are robust to diverse image transformations, especially the competitive evasion technique, MagDR via image reconstruction. Our findings potentially open up a new research direction towards thorough understanding and investigation of perceptual-aware adversarial attack for protecting facial images against DeepFakes in a proactive and robust manner. We open-source our tool to foster future research. Code is available at https://github.com/AbstractTeen/AntiForgery/. ",Kein DOI-Link verfügbar,2206.00477v1,Yes,potent(2)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Multi-kink brane in Gauss-Bonnet gravity and its stability,1970,"  Einstein-Gauss-Bonnet gravity in high dimensional spacetime is intriguing. Here, the properties of thick branes generated by a bulk scalar field in the five-dimensional Einstein-Gauss-Bonnet gravity were studied. With the help of the superpotential method, we obtain a series of multi-kink brane solutions. We also analyze the linear stability of the brane system under tensor perturbations and prove that they are stable. The massless graviton is shown to be localized near the brane and hence the four-dimensional Newtonian potential can be recovered. By comparing the properties of these thick branes under different superpotentials we find with some specific choice of superpotential the Gauss-Bonnet term can determine the scalar field are multi-kink or single kink. ",https://doi.org/10.1103/PhysRevD.107.124011,2201.10282v4,Yes,potent(4)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Exploring the Intersection of Complex Aesthetics and Generative AI for   Promoting Cultural Creativity in Rural China after the Post-Pandemic Era,1970,"  This paper explores using generative AI and aesthetics to promote cultural creativity in rural China amidst COVID-19's impact. Through literature reviews, case studies, surveys, and text analysis, it examines art and technology applications in rural contexts and identifies key challenges. The study finds artworks often fail to resonate locally, while reliance on external artists limits sustainability. Hence, nurturing grassroots ""artist villagers"" through AI is proposed. Our approach involves training machine learning on subjective aesthetics to generate culturally relevant content. Interactive AI media can also boost tourism while preserving heritage. This pioneering research puts forth original perspectives on the intersection of AI and aesthetics to invigorate rural culture. It advocates holistic integration of technology and emphasizes AI's potential as a creative enabler versus replacement. Ultimately, it lays the groundwork for further exploration of leveraging AI innovations to empower rural communities. This timely study contributes to growing interest in emerging technologies to address critical issues facing rural China. ",Kein DOI-Link verfügbar,2309.02136v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,"Sok: Comprehensive Security Overview, Challenges, and Future Directions   of Voice-Controlled Systems",1970,"  The integration of Voice Control Systems (VCS) into smart devices and their growing presence in daily life accentuate the importance of their security. Current research has uncovered numerous vulnerabilities in VCS, presenting significant risks to user privacy and security. However, a cohesive and systematic examination of these vulnerabilities and the corresponding solutions is still absent. This lack of comprehensive analysis presents a challenge for VCS designers in fully understanding and mitigating the security issues within these systems.   Addressing this gap, our study introduces a hierarchical model structure for VCS, providing a novel lens for categorizing and analyzing existing literature in a systematic manner. We classify attacks based on their technical principles and thoroughly evaluate various attributes, such as their methods, targets, vectors, and behaviors. Furthermore, we consolidate and assess the defense mechanisms proposed in current research, offering actionable recommendations for enhancing VCS security. Our work makes a significant contribution by simplifying the complexity inherent in VCS security, aiding designers in effectively identifying and countering potential threats, and setting a foundation for future advancements in VCS security research. ",Kein DOI-Link verfügbar,2405.17100v1,Yes,potent(1)
0000-0002-3119-654X,Jing Chen,The University of Hong Kong,Long-range self-interacting dark matter in the Sun,1970,"  We investigate the implications of the long-rang self-interaction on both the self-capture and the annihilation of the self-interacting dark matter (SIDM) trapped in the Sun. Our discussion is based on a specific SIDM model in which DM particles self-interact via a light scalar mediator, or Yukawa potential, in the context of quantum mechanics. Within this framework, we calculate the self-capture rate across a broad region of parameter space. While the self-capture rate can be obtained separately in the Born regime with perturbative method, and the classical limits with the Rutherford formula, our calculation covers the gap between in a non-perturbative fashion. Besides, the phenomelogy of both the Sommerfeld-enhanced s- and p-wave annihilation of the solar SIDM is also involved in our discussion. Moreover, by combining the analysis of the Super-Kamiokande (SK) data and the observed DM relic density, we constrain the nuclear capture rate of the DM particles in the presence of the dark Yukawa potential. The consequence of the long-range dark force on probing the solar SIDM turns out to be significant if the force-carrier is much lighter than the DM particle, and a quantitative analysis is provided. ",https://doi.org/10.1088/1475-7516/2015/12/021,1505.04031v3,Yes,potent(2)
0000-0002-1478-3232,Lingting Zhu,The University of Hong Kong,Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D   Brain MRI Synthesis,1970,"  Cross-modality medical image synthesis is a critical topic and has the potential to facilitate numerous applications in the medical imaging field. Despite recent successes in deep-learning-based generative models, most current medical image synthesis methods rely on generative adversarial networks and suffer from notorious mode collapse and unstable training. Moreover, the 2D backbone-driven approaches would easily result in volumetric inconsistency, while 3D backbones are challenging and impractical due to the tremendous memory cost and training difficulty. In this paper, we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones and present a diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image synthesis. To learn the cross-modality slice-wise mapping, we employ a latent diffusion model and learn a low-dimensional latent space, resulting in high computational efficiency. To enable the 3D image synthesis and mitigate volumetric inconsistency, we further insert a series of volumetric layers in the 2D slice-mapping model and fine-tune them with paired 3D data. This paradigm extends the 2D image diffusion model to a volumetric version with a slightly increasing number of parameters and computation, offering a principled solution for generic cross-modality 3D medical image synthesis. We showcase the effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate that our framework achieves superior synthesis results with volumetric consistency. ",Kein DOI-Link verfügbar,2307.10094v1,Yes,potent(1)
0000-0002-1478-3232,Lingting Zhu,The University of Hong Kong,Generative Enhancement for 3D Medical Images,1970,"  The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging. While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional diffusion models. Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask. By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets. GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in diffusion sampling. Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate counterfactual image synthesis and dataset-level de-enhancement with desired control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference. The code is available at https://github.com/HKU-MedAI/GEM-3D. ",Kein DOI-Link verfügbar,2403.12852v2,Yes,versatile(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,BRC-20: Hope or Hype,1970,"  BRC-20 (short for Bitcoin Request for Comment 20) token mania was a key storyline in the middle of 2023. Setting it apart from conventional ERC-20 token standards on Ethereum, BRC-20 introduces non-fungibility to Bitcoin through an editable field in each satoshi (0.00000001 Bitcoin, the smallest unit), making them unique. In this paper, we pioneer the exploration of this concept, covering its intricate mechanisms, features, and state-of-the-art applications. By analyzing the multi-dimensional data spanning over months with factual investigations, we conservatively comment that while BRC-20 expands Bitcoin's functionality and applicability, it may still not match Ethereum's abundance of decentralized applications and similar ecosystems. ",Kein DOI-Link verfügbar,2310.10652v1,Yes,intricate(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Exploring the Market Dynamics of Liquid Staking Derivatives (LSDs),1970,"  Staking has emerged as a crucial concept following Ethereum's transition to Proof-of-Stake consensus. The introduction of Liquid Staking Derivatives (LSDs) has effectively addressed the illiquidity issue associated with solo staking, gaining significant market attention. This paper analyzes the LSD market dynamics from the perspectives of both liquidity takers (LTs) and liquidity providers (LPs). We first quantify the price discrepancy between the LSD primary and secondary markets. Then we investigate and empirically measure how LTs can leverage such discrepancy to exploit arbitrage opportunities, unveiling the potential barriers to LSD arbitrages. In addition, we evaluate the financial profit and losses experienced by LPs who supply LSDs for liquidity provision. Our results show that 66% of LSD liquidity positions generate returns lower than those from simply holding the corresponding LSDs. ",Kein DOI-Link verfügbar,2402.17748v2,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Domain Adaptive Transfer Learning for Fault Diagnosis,1970,"  Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models fromone machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems. ",Kein DOI-Link verfügbar,1905.06004v1,Yes,potent(2)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Bitcoin Inscriptions: Foundations and Beyond,1970,"  Bitcoin inscription marks a pivotal moment in blockchain technology. This report presents a primary exploration of Bitcoin inscriptions. We dive into the technological underpinnings and offer a detailed comparative analysis between Bitcoin inscriptions and NFTs on other blockchains. Further, we explore a wide range of use cases and significant opportunities for future innovation, including inscription derivative protocols, Bitcoin Layer2 solutions, and interoperability techniques. ",Kein DOI-Link verfügbar,2401.17581v1,Yes,pivotal(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,SoK: Diving into DAG-based Blockchain Systems,1970,"  Blockchain plays an important role in cryptocurrency markets and technology services. However, limitations on high latency and low scalability retard their adoptions and applications in classic designs. Reconstructed blockchain systems have been proposed to avoid the consumption of competitive transactions caused by linear sequenced blocks. These systems, instead, structure transactions/blocks in the form of Directed Acyclic Graph (DAG) and consequently re-build upper layer components including consensus, incentives, \textit{etc.} The promise of DAG-based blockchain systems is to enable fast confirmation (complete transactions within million seconds) and high scalability (attach transactions in parallel) without significantly compromising security. However, this field still lacks systematic work that summarises the DAG technique. To bridge the gap, this Systematization of Knowledge (SoK) provides a comprehensive analysis of DAG-based blockchain systems. Through deconstructing open-sourced systems and reviewing academic researches, we conclude the main components and featured properties of systems, and provide the approach to establish a DAG. With this in hand, we analyze the security and performance of several leading systems, followed by discussions and comparisons with concurrent (scaling blockchain) techniques. We further identify open challenges to highlight the potentiality of DAG-based solutions and indicate their promising directions for future research. ",Kein DOI-Link verfügbar,2012.06128v3,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Decentralized Finance (DeFi): A Survey,1970,"  Decentralized Finance (DeFi) is a new paradigm in the creation, distribution, and utilization of financial services via the integration of blockchain technology. Our research conducts a comprehensive introduction and meticulous classification of various DeFi applications. Beyond that, we thoroughly analyze these risks from both technical and economic perspectives, spanning multiple layers. We point out research gaps and revenues, covering technical advancements, innovative economics, and sociology and ecology optimization. ",Kein DOI-Link verfügbar,2308.05282v2,Yes,"innovative(1), meticulous(1)"
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Security Analysis on Tangle-based Blockchain through Simulation,1970,"  The Tangle-based structure becomes one of the most promising solutions when designing DAG-based blockchain systems. The approach improves the scalability by directly confirming multiple transactions in parallel instead of single blocks in linear. However, the performance gain may bring potential security risks. In this paper, we construct three types of attacks with comprehensive evaluations, namely parasite attack (PS), double spending attack (DS), and hybrid attack (HB). To achieve that, we deconstruct the Tangle-based projects (e.g. IOTA) and abstract the main components to rebuild a simple but flexible network for the simulation. Then, we informally define three smallest actions to build up the attack strategies layer by layer. Based on that, we provide analyses to evaluate different types of attacks. To the best of our knowledge, this is the first study to provide a comprehensive security analysis of Tangle-based blockchains. ",Kein DOI-Link verfügbar,2008.04863v2,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Exploring Unfairness on Proof of Authority: Order Manipulation Attacks   and Remedies,1970,"  Proof of Authority (PoA) is a type of permissioned consensus algorithm with a fixed committee. PoA has been widely adopted by communities and industries due to its better performance and faster finality. In this paper, we explore the \textit{unfairness} issue existing in the current PoA implementations. We have investigated 2,500+ \textit{in the wild} projects and selected 10+ as our main focus (covering Ethereum, Binance smart chain, etc.). We have identified two types of order manipulation attacks to separately break the transaction-level (a.k.a. transaction ordering) and the block-level (sealer position ordering) fairness. Both of them merely rely on honest-but-\textit{profitable} sealer assumption without modifying original settings. We launch these attacks on the forked branches under an isolated environment and carefully evaluate the attacking scope towards different implementations. To date (as of Nov 2021), the potentially affected PoA market cap can reach up to $681,087$ million USD. Besides, we further dive into the source code of selected projects, and accordingly, propose our recommendation for the fix. To the best of knowledge, this work provides the first exploration of the \textit{unfairness} issue in PoA algorithms. ",Kein DOI-Link verfügbar,2203.03008v2,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Formal Security Analysis on dBFT Protocol of NEO,1970,"  NEO is one of the top public chains worldwide. We focus on its backbone consensus protocol, called delegated Byzantine Fault Tolerance (dBFT). The dBFT protocol has been adopted by a variety of blockchain systems such as ONT. dBFT claims to guarantee the security when no more than $f = \lfloor \frac{n}{3} \rfloor$ nodes are Byzantine, where $n$ is the total number of consensus participants. However, we identify attacks to break the claimed security. In this paper, we show our results by providing a security analysis on its dBFT protocol. First, we evaluate NEO's source code and formally present the procedures of dBFT via the state machine replication (SMR) model. Next, we provide a theoretical analysis with two example attacks. These attacks break the security of dBFT with no more than $f$ nodes. Then, we provide recommendations on how to fix the system against the identified attacks. The suggested fixes have been accepted by the NEO official team. Finally, we further discuss the reasons causing such issues, the relationship with current permissioned blockchain systems, and the scope of potential influence. ",Kein DOI-Link verfügbar,2105.07459v3,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Reference-frame-independent measurement-device-independent quantum key   distribution with imperfect sources,1970,"  Reference-frame-independent measurement-device-independent quantum key distribution (RFI-MDI-QKD) can remove all potential detector side-channel attacks and the requirement of real-time alignment of reference frames. However, all previous RFI-MDI-QKD implementations assume the perfect state preparation in sources, which is impractical and may lead to security loopholes. Here, we propose a RFI-MDI-QKD protocol which is robust against state preparation flaws. Comparing to the conventional six-state RFI-MDI-QKD, our scheme can be realized with only four flawed states, which improves the practical security of RFI-MDI-QKD and simplifies the experimental implementation. In addition, simulation results demonstrate that source flaws in Z basis have adverse effect on the performance of RFI-MDI-QKD while the source flaws in X and Y bases have almost no effect. We hope that this work could provide a valuable reference for practical implementations of RFI-MDI-QKD. ",https://doi.org/10.1088/1361-6455/ac0c05,2102.10638v1,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,An Architectural Error Metric for CNN-Oriented Approximate Multipliers,1970,"  As a potential alternative for implementing the large number of multiplications in convolutional neural networks (CNNs), approximate multipliers (AMs) promise both high hardware efficiency and accuracy. However, the characterization of accuracy and design of appropriate AMs are critical to an AM-based CNN (AM-CNN). In this work, the generation and propagation of errors in an AM-CNN are analyzed by considering the CNN architecture. Based on this analysis, a novel AM error metric is proposed to evaluate the accuracy degradation of an AM-CNN, denoted as the architectural mean error (AME). The effectiveness of the AME is assessed in VGG and ResNet on CIFAR-10, CIFAR-100, and ImageNet datasets. Experimental results show that AME exhibits a strong correlation with the accuracy of AM-CNNs, outperforming the other AM error metrics. To predict the accuracy of AM-CNNs, quadratic regression models are constructed based on the AME; the predictions show an average of 3% deviation from the ground-truth values. Compared with a GPU-based simulation, the AME-based prediction is about $10^{6}\times$ faster. ",Kein DOI-Link verfügbar,2408.12836v1,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Exploring Web3 From the View of Blockchain,1970,"  Web3 is the most hyped concept from 2020 to date, greatly motivating the prosperity of the Internet of Value and Metaverse. However, no solid evidence stipulates the exact definition, criterion, or standard in the sense of such a buzzword. To fill the gap, we aim to clarify the term in this work. We narrow down the connotation of Web3 by separating it from high-level controversy argues and, instead, focusing on its protocol, architecture, and evaluation from the perspective of blockchain fields. Specifically, we have identified all potential architectural design types and evaluated each of them by employing the scenario-based architecture evaluation method. The evaluation shows that existing applications are neither secure nor adoptable as claimed. Meanwhile, we also discuss opportunities and challenges surrounding the Web3 space and answer several prevailing questions from communities. A primary result is that Web3 still relies on traditional internet infrastructure, not as independent as advocated. This report, as of June 2022, provides the first strict research on Web3 in the view of blockchain. We hope that this work would provide a guide for the development of future Web3 services. ",Kein DOI-Link verfügbar,2206.08821v1,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,"Potential, Challenges and Future Directions for Deep Learning in   Prognostics and Health Management Applications",1970,"  Deep learning applications have been thriving over the last decade in many different domains, including computer vision and natural language understanding. The drivers for the vibrant development of deep learning have been the availability of abundant data, breakthroughs of algorithms and the advancements in hardware. Despite the fact that complex industrial assets have been extensively monitored and large amounts of condition monitoring signals have been collected, the application of deep learning approaches for detecting, diagnosing and predicting faults of complex industrial assets has been limited. The current paper provides a thorough evaluation of the current developments, drivers, challenges, potential solutions and future research needs in the field of deep learning applied to Prognostics and Health Management (PHM) applications. ",Kein DOI-Link verfügbar,2005.02144v1,Yes,potent(1)
0000-0002-6138-359X,Qin Wang,The University of Hong Kong,Domain Adaptive Semantic Segmentation with Self-Supervised Depth   Estimation,1970,"  Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distribution shift between source and target domain. Leveraging the supervision from auxiliary tasks~(such as depth estimation) has the potential to heal this shift because many visual tasks are closely related to each other. However, such a supervision is not always available. In this work, we leverage the guidance from self-supervised depth estimation, which is available on both domains, to bridge the domain gap. On the one hand, we propose to explicitly learn the task feature correlation to strengthen the target semantic predictions with the help of target depth estimation. On the other hand, we use the depth prediction discrepancy from source and target depth decoders to approximate the pixel-wise adaptation difficulty. The adaptation difficulty, inferred from depth, is then used to refine the target semantic segmentation pseudo-labels. The proposed method can be easily implemented into existing segmentation frameworks. We demonstrate the effectiveness of our approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new state-of-the-art performance of $55.0\%$ and $56.6\%$, respectively. Our code is available at \url{https://qin.ee/corda}. ",Kein DOI-Link verfügbar,2104.13613v2,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,Combinatorial curvature flows with surgery for inversive distance circle   packings on surfaces,1970,"  Inversive distance circle packings introduced by Bowers-Stephenson are natural generalizations of Thurston's circle packings on surfaces. To find piecewise Euclidean metrics on surfaces with prescribed combinatorial curvatures, we introduce the combinatorial Calabi flow, the fractional combinatorial Calabi flow and the combinatorial $p$-th Calabi flow for the Euclidean inversive distance circle packings. Due to the singularities possibly developed by these combinatorial curvature flows, the longtime existence and convergence of these combinatorial curvature flows have been a difficult problem for a long time. To handle the potential singularities along these combinatorial curvature flows, we do surgery along these flows by edge flipping under the weighted Delaunay condition. Using the discrete conformal theory recently established by Bobenko-Lutz for decorated piecewise Euclidean metrics on surfaces, we prove the longtime existence and global convergence for the solutions of these combinatorial curvature flows with surgery. This provides effective algorithms for finding piecewise Euclidean metrics on surfaces with prescribed combinatorial curvatures. ",Kein DOI-Link verfügbar,2308.02271v1,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,A discrete uniformization theorem for decorated piecewise hyperbolic   metrics on surfaces,1970,"  In this paper, we study a natural discretization of the smooth Gaussian curvature on surfaces. A discrete uniformization theorem is established for this discrete Gaussian curvature. We further investigate the prescribing combinatorial curvature problem for a parametrization of this discrete Gaussian curvature, which is called the combinatorial $\alpha$-curvature. To find decorated piecewise hyperbolic metrics with prescribed combinatorial $\alpha$-curvatures, we introduce the combinatorial $\alpha$-Ricci flow for decorated piecewise hyperbolic metrics. To handle the potential singularities along the combinatorial $\alpha$-Ricci flow, we do surgery along the flow by edge flipping under the weighted Delaunay condition. Then we prove the longtime existence and convergence of the combinatorial $\alpha$-Ricci flow with surgery. As an application of the combinatorial $\alpha$-Ricci flow with surgery, we give the existence of decorated piecewise hyperbolic metrics with prescribed combinatorial $\alpha$-curvatures. We further introduce the combinatorial $\alpha$-Calabi flow with surgery and study its longtime behavior. ",Kein DOI-Link verfügbar,2401.05056v1,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,"Parameterized discrete uniformization theorems and curvature flows for   polyhedral surfaces, II",1970,"  This paper investigates the combinatorial $\alpha$-curvature for vertex scaling of piecewise hyperbolic metrics on polyhedral surfaces, which is a parameterized generalization of the classical combinatorial curvature. A discrete uniformization theorem for combinatorial $\alpha$-curvature is established, which generalizes Gu-Guo-Luo-Sun-Wu's discrete uniformization theorem for classical combinatorial curvature. We further introduce combinatorial $\alpha$-Yamabe flow and combinatorial $\alpha$-Calabi flow for vertex scaling to find piecewise hyperbolic metrics with prescribed combinatorial $\alpha$-curvatures. To handle the potential singularities along the combinatorial curvature flows, we do surgery along the flows by edge flipping. Using the discrete conformal theory established by Gu-Guo-Luo-Sun-Wu, we prove the longtime existence and convergence of combinatorial $\alpha$-Yamabe flow and combinatorial $\alpha$-Calabi flow with surgery, which provide effective algorithms for finding piecewise hyperbolic metrics with prescribed combinatorial $\alpha$-curvatures. ",Kein DOI-Link verfügbar,2103.16077v2,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,Parameterized combinatorial curvatures and parameterized combinatorial   curvature flows for discrete conformal structures on polyhedral surfaces,1970,"  Discrete conformal structure on polyhedral surfaces is a discrete analogue of the smooth conformal structure on surfaces that assigns discrete metrics by scalar functions defined on vertices. In this paper, we introduce combinatorial $\alpha$-curvature for discrete conformal structures on polyhedral surfaces, which is a parameterized generalization of the classical combinatorial curvature. Then we prove the local and global rigidity of combinatorial $\alpha$-curvature with respect to discrete conformal structures on polyhedral surfaces, which confirms parameterized Glickenstein rigidity conjecture. To study the Yamabe problem for combinatorial $\alpha$-curvature, we introduce combinatorial $\alpha$-Ricci flow for discrete conformal structures on polyhedral surfaces, which is a generalization of Chow-Luo's combinatorial Ricci flow for Thurston's circle packings and Luo's combinatorial Yamabe flow for vertex scaling on polyhedral surfaces. To handle the potential singularities of the combinatorial $\alpha$-Ricci flow, we extend the flow through the singularities by extending the inner angles in triangles by constants. Under the existence of a discrete conformal structure with prescribed combinatorial curvature, the solution of extended combinatorial $\alpha$-Ricci flow is proved to exist for all time and converge exponentially fast for any initial value. This confirms a parameterized generalization of another conjecture of Glickenstein on the convergence of combinatorial Ricci flow, gives an almost equivalent characterization of the solvability of Yamabe problem for combinatorial $\alpha$-curvature in terms of combinatorial $\alpha$-Ricci flow and provides an effective algorithm for finding discrete conformal structures with prescribed combinatorial $\alpha$-curvatures. ",Kein DOI-Link verfügbar,2105.14714v2,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,M$^2$-3DLaneNet: Exploring Multi-Modal 3D Lane Detection,1970,"  Estimating accurate lane lines in 3D space remains challenging due to their sparse and slim nature. Previous works mainly focused on using images for 3D lane detection, leading to inherent projection error and loss of geometry information. To address these issues, we explore the potential of leveraging LiDAR for 3D lane detection, either as a standalone method or in combination with existing monocular approaches. In this paper, we propose M$^2$-3DLaneNet to integrate complementary information from multiple sensors. Specifically, M$^2$-3DLaneNet lifts 2D features into 3D space by incorporating geometry information from LiDAR data through depth completion. Subsequently, the lifted 2D features are further enhanced with LiDAR features through cross-modality BEV fusion. Extensive experiments on the large-scale OpenLane dataset demonstrate the effectiveness of M$^2$-3DLaneNet, regardless of the range (75m or 100m). ",Kein DOI-Link verfügbar,2209.05996v3,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,Experimental observation of information flow in the   anti-$\mathcal{PT}$-symmetric system,1970,"  The recently theoretical and experimental researches related to $\mathcal{PT}$-symmetric system have attracted unprecedented attention because of various novel features and potentials in extending canonical quantum mechanics. However, as the counterpart of $\mathcal{PT}$-symmetry, there are only a few researches on anti-$\mathcal{PT}$-symmetry. Here, we propose an algorithm for simulating the universal anti-$\mathcal{PT}$-symmetric system with quantum circuit. Utilizing the protocols, an oscillation of information flow is observed for the first time in our Nuclear Magnetic Resonance quantum simulator. We will show that information will recover from the environment completely when the anti-$\mathcal{PT}$-symmetry is broken, whereas no information can be retrieved in the symmetry-unbroken phase. Our work opens the gate for practical quantum simulation and experimental investigation of universal anti-$\mathcal{PT}$-symmetric system in quantum computer. ",https://doi.org/10.1038/s41534-020-0258-4,1906.05073v3,Yes,potent(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,THMA: Tencent HD Map AI System for Creating HD Map Annotations,1970,"  Nowadays, autonomous vehicle technology is becoming more and more mature. Critical to progress and safety, high-definition (HD) maps, a type of centimeter-level map collected using a laser sensor, provide accurate descriptions of the surrounding environment. The key challenge of HD map production is efficient, high-quality collection and annotation of large-volume datasets. Due to the demand for high quality, HD map production requires significant manual human effort to create annotations, a very time-consuming and costly process for the map industry. In order to reduce manual annotation burdens, many artificial intelligence (AI) algorithms have been developed to pre-label the HD maps. However, there still exists a large gap between AI algorithms and the traditional manual HD map production pipelines in accuracy and robustness. Furthermore, it is also very resource-costly to build large-scale annotated datasets and advanced machine learning algorithms for AI-based HD map automatic labeling systems. In this paper, we introduce the Tencent HD Map AI (THMA) system, an innovative end-to-end, AI-based, active learning HD map labeling system capable of producing and labeling HD maps with a scale of hundreds of thousands of kilometers. In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users. THMA has been deployed by the Tencent Map team to provide services to downstream companies and users, serving over 1,000 labeling workers and producing more than 30,000 kilometers of HD map data per day at most. More than 90 percent of the HD map data in Tencent Map is labeled automatically by THMA, accelerating the traditional HD map labeling process by more than ten times. ",Kein DOI-Link verfügbar,2212.11123v1,Yes,innovative(1)
0000-0003-4489-5304,Chao Zheng,The University of Hong Kong,Application of Deep Learning Methods for Distinguishing Gamma-Ray Bursts   from Fermi/GBM TTE Data,1970,"  To investigate GRBs in depth, it is crucial to develop an effective method for identifying GRBs accurately. Current criteria, e.g., onboard blind search, ground blind search, and target search, are limited by manually set thresholds and perhaps miss GRBs, especially for sub-threshold events. We propose a novel approach that utilizes convolutional neural networks (CNNs) to distinguish GRBs and non-GRBs directly. We structured three CNN models, plain-CNN, ResNet, and ResNet-CBAM, and endeavored to exercise fusing strategy models. Count maps of NaI detectors onboard Fermi/GBM were employed as the input samples of datasets and models were implemented to evaluate their performance on different time scale data. The ResNet-CBAM model trained on 64 ms dataset achieves high accuracy overall, which includes residual and attention mechanism modules. The visualization methods of Grad-CAM and t-SNE explicitly displayed that the optimal model focuses on the key features of GRBs precisely. The model was applied to analyze one-year data, accurately identifying approximately 98% of GRBs listed in the Fermi burst catalog, 8 out of 9 sub-threshold GRBs, and 5 GRBs triggered by other satellites, which demonstrated the deep learning methods could effectively distinguish GRBs from observational data. Besides, thousands of unknown candidates were retrieved and compared with the bursts of SGR J1935+2154 for instance, which exemplified the potential scientific value of these candidates indeed. Detailed studies on integrating our model into real-time analysis pipelines thus may improve their accuracy of inspection, and provide valuable guidance for rapid follow-up observations of multi-band telescopes. ",Kein DOI-Link verfügbar,2303.00370v4,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Phrase Grounding by Soft-Label Chain Conditional Random Field,1970,"  The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define so-called Soft-Label Chain CRFs, and present an algorithm that enables convenient end-to-end training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at \url{github.com/liujch1998/SoftLabelCCRF} ",Kein DOI-Link verfügbar,1909.00301v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,DiffPop: Plausibility-Guided Object Placement Diffusion for Image   Composition,1970,"  In this paper, we address the problem of plausible object placement for the challenging task of realistic image composition. We propose DiffPop, the first framework that utilizes plausibility-guided denoising diffusion probabilistic model to learn the scale and spatial relations among multiple objects and the corresponding scene image. First, we train an unguided diffusion model to directly learn the object placement parameters in a self-supervised manner. Then, we develop a human-in-the-loop pipeline which exploits human labeling on the diffusion-generated composite images to provide the weak supervision for training a structural plausibility classifier. The classifier is further used to guide the diffusion sampling process towards generating the plausible object placement. Experimental results verify the superiority of our method for producing plausible and diverse composite images on the new Cityscapes-OP dataset and the public OPA dataset, as well as demonstrate its potential in applications such as data augmentation and multi-object placement tasks. Our dataset and code will be released. ",Kein DOI-Link verfügbar,2406.07852v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Optimizing Investment Strategies with Lazy Factor and Probability   Weighting: A Price Portfolio Forecasting and Mean-Variance Model with   Transaction Costs Approach,1970,"  Market traders often engage in the frequent transaction of volatile assets to optimize their total return. In this study, we introduce a novel investment strategy model, anchored on the 'lazy factor.' Our approach bifurcates into a Price Portfolio Forecasting Model and a Mean-Variance Model with Transaction Costs, utilizing probability weights as the coefficients of laziness factors. The Price Portfolio Forecasting Model, leveraging the EXPMA Mean Method, plots the long-term price trend line and forecasts future price movements, incorporating the tangent slope and rate of change. For short-term investments, we apply the ARIMA Model to predict ensuing prices. The Mean-Variance Model with Transaction Costs employs the Monte Carlo Method to formulate the feasible region. To strike an optimal balance between risk and return, equal probability weights are incorporated as coefficients of the laziness factor. To assess the efficacy of this combined strategy, we executed extensive experiments on a specified dataset. Our findings underscore the model's adaptability and generalizability, indicating its potential to transform investment strategies. ",Kein DOI-Link verfügbar,2306.07928v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Crystal: Introspective Reasoners Reinforced with Self-Feedback,1970,"  Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including ""chain-of-thought"" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback given by the model itself. Experiments show that Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process. Our work ultimately validates the feasibility and potential of reinforcing a neural model with self-feedback. ",Kein DOI-Link verfügbar,2310.04921v2,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,DataCLUE: A Benchmark Suite for Data-centric NLP,1970,"  Data-centric AI has recently proven to be more effective and high-performance, while traditional model-centric AI delivers fewer and fewer benefits. It emphasizes improving the quality of datasets to achieve better model performance. This field has significant potential because of its great practicability and getting more and more attention. However, we have not seen significant research progress in this field, especially in NLP. We propose DataCLUE, which is the first Data-Centric benchmark applied in NLP field. We also provide three simple but effective baselines to foster research in this field (improve Macro-F1 up to 5.7% point). In addition, we conduct comprehensive experiments with human annotators and show the hardness of DataCLUE. We also try an advanced method: the forgetting informed bootstrapping label correction method. All the resources related to DataCLUE, including datasets, toolkit, leaderboard, and baselines, is available online at https://github.com/CLUEbenchmark/DataCLUE ",Kein DOI-Link verfügbar,2111.08647v2,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Transforming Graphs for Enhanced Attribute Clustering: An Innovative   Graph Transformer-Based Method,1970,"  Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifies the graph representation and surmounts the constraints posed by the local attention mechanism. The architecture of GTAGC encompasses graph embedding, integration of the Graph Transformer within the autoencoder structure, and a clustering component. It strategically alternates between graph embedding and clustering, thereby tailoring the Graph Transformer for clustering tasks, whilst preserving the graph's global structural information. Through extensive experimentation on diverse benchmark datasets, GTAGC has exhibited superior performance against existing state-of-the-art graph clustering methodologies. ",Kein DOI-Link verfügbar,2306.11307v3,Yes,"innovative(1), intricate(1), notable(1), strategically(1)"
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,CrossWeigh: Training Named Entity Tagger from Imperfect Annotations,1970,"  Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo: https://github.com/ZihanWangKi/CrossWeigh. ",Kein DOI-Link verfügbar,1909.01441v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Vera: A General-Purpose Plausibility Estimation Model for Commonsense   Statements,1970,"  Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings. ",Kein DOI-Link verfügbar,2305.03695v3,Yes,versatile(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain   Fake News Detection,1970,"  Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-manually verified common knowledge, and fine-grained annotations that surpass conventional binary labels. Furthermore, we formulate three challenging tasks based on FineFake and propose a knowledge-enhanced domain adaptation network. Extensive experiments are conducted on FineFake under various scenarios, providing accurate and reliable benchmarks for future endeavors. The entire FineFake project is publicly accessible as an open-source repository at \url{https://github.com/Accuser907/FineFake}. ",Kein DOI-Link verfügbar,2404.01336v2,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Experimental and Theoretical Exploration of Terahertz Channel   Performance through Glass Doors,1970,"  In the evolving landscape of terahertz communication, the behavior of channels within indoor environments, particularly through glass doors, has garnered significant attention. This paper comprehensively investigates terahertz channel performance under such conditions, employing a measurement setup operational between 113 and 170 GHz. Analyzing scenarios frequently induced by human activity and environmental factors, like door movements, we established a comprehensive theoretical model. This model seamlessly integrates transmission, reflection, absorption, and diffraction mechanisms, leveraging the Fresnel formula, multi-layer transmission paradigm, and knife-edge diffraction theory. Our experimental results and theoretical predictions harmoniously align, revealing intricate dependencies, such as increased power loss at higher frequencies and larger incident angles. Furthermore, door interactions, whether opening or oscillations, significantly impact the terahertz channel. Notably, door edges lead to a power blockage surpassing the transmission loss of the glass itself but remaining inferior to metallic handle interferences. This paper's insights are pivotal for the design and fabrication of terahertz communication systems within indoor settings, pushing the boundaries of efficient and reliable communication. ",Kein DOI-Link verfügbar,2311.07924v3,Yes,"intricate(1), pivotal(1)"
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,High-speed surface-property recognition by 140-GHz frequency,1970,"  In the field of integrated sensing and communication, there's a growing need for advanced environmental perception. The terahertz (THz) frequency band, significant for ultra-high-speed data connections, shows promise in environmental sensing, particularly in detecting surface textures crucial for autonomous system's decision-making. However, traditional numerical methods for parameter estimation in these environments struggle with accuracy, speed, and stability, especially in high-speed scenarios like vehicle-to-everything communications. This study introduces a deep learning approach for identifying surface roughness using a 140-GHz setup tailored for high-speed conditions. A high-speed data acquisition system was developed to mimic real-world scenarios, and a diverse set of rough surface samples was collected for realistic high-speed datasets to train the models. The model was trained and validated in three challenging scenarios: random occlusions, sparse data, and narrow-angle observations. The results demonstrate the method's effectiveness in high-speed conditions, suggesting terahertz frequencies' potential in future sensing and communication applications. ",Kein DOI-Link verfügbar,2311.08599v2,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Ground-to-UAV sub-Terahertz channel measurement and modeling,1970,"  Unmanned Aerial Vehicle (UAV) assisted terahertz (THz) wireless communications have been expected to play a vital role in the next generation of wireless networks. UAVs can serve as either repeaters or data collectors within the communication link, thereby potentially augmenting the efficacy of communication systems. Despite their promise, the channel analysis and modeling specific to THz wireless channels leveraging UAVs remain under explored. This work delves into a ground-to-UAV channel at 140 GHz, with a specific focus on the influence of UAV hovering behavior on channel performance. Employing experimental measurements through an unmodulated channel setup and a geometry-based stochastic model (GBSM) that integrates three-dimensional positional coordinates and beamwidth, this work evaluates the impact of UAV dynamic movements and antenna orientation on channel performance. Our findings highlight the minimal impact of UAV orientation adjustments on channel performance and underscore the diminishing necessity for precise alignment between UAVs and ground stations as beamwidth increases. ",Kein DOI-Link verfügbar,2404.02663v3,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Wideband Coherent Microwave Conversion via Magnon Nonlinearity in Hybrid   Quantum System,1970,"  Frequency conversion is a widely realized physical process in nonlinear systems of optics and electronics. As an emerging nonlinear platform, spintronic devices have the potential to achieve stronger frequency conversion. Here, we demonstrated a microwave frequency conversion method in a hybrid quantum system, integrating nitrogen-vacancy centers in diamond with magnetic thin film CoFeB. We achieve a conversion bandwidth ranging from 0.1 to 12GHz, presenting an up to $\mathrm{25^{th}}$ order frequency conversion and further display the application of this method for frequency detection and qubits coherent control. Distinct from traditional frequency conversion techniques based on nonlinear electric response, our approach employs nonlinear magnetic response in spintronic devices. The nonlinearity, originating from the symmetry breaking such as domain walls in magnetic films, presents that our method can be adapted to hybrid systems of other spintronic devices and spin qubits, expanding the application scope of spintronic devices and providing a promising on-chip platform for coupling quantum systems. ",https://doi.org/10.1038/s44306-024-00035-2,2407.03201v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,MathVista: Evaluating Mathematical Reasoning of Foundation Models in   Visual Contexts,1970,"  Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/. ",Kein DOI-Link verfügbar,2310.02255v3,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Closeby Habitable Exoplanet Survey (CHES). II. An Observation Strategy   for the Target Stars,1970,"  The Closeby Habitable Exoplanet Survey (CHES) constitutes a mission intricately designed to systematically survey approximately 100 solar-type stars located within the immediate proximity of the solar system, specifically within a range of 10 parsecs. The core objective of this mission is the detection and characterization of potentially habitable Earth-like planets or super-Earths within the habitable zone of these stars. The CHES mission obtains high-precision astrometric measurements of planets orbiting the target stars by observing angular distance variations between the target star and reference stars. As a result, we surveyed the relevant parameters of both target and reference stars in detail, conducting a thorough analysis and calculation of the required observation accuracy, the number of observations, and the priority assigned to each target star. Observational emphasis will be concentrated on targets considered of higher priority, ensuring the effectiveness of their observation capabilities. Through this approach, we formulate a five-year observation strategy that will cover all the target stars within a six-month timeframe. The strategy not only fulfills the required observing capability but also exhibit high efficiency simultaneously, providing an executable program for future mission. Over the span of the mission's five-year duration, a cumulative observation time of 29,220 hours will be available. Approximately 86 percent of this, totaling 25,120 hours, is allocated for the observation of target stars. This allocation leaves approximately 4,100 hours for extended scientific observation programs. We have also performed simulated observations based on this strategy and verified its observational capability for exoplanets. ",Kein DOI-Link verfügbar,2408.06338v1,Yes,"intricate(1), potent(1)"
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Strong magnon-magnon coupling in an ultralow damping   all-magnetic-insulator heterostructure,1970,"  Magnetic insulators such as yttrium iron garnets (YIGs) are of paramount importance for spin-wave or magnonic devices as their ultralow damping enables ultralow power dissipation that is free of Joule heating, exotic magnon quantum state, and coherent coupling to other wave excitations. Magnetic insulator heterostructures bestow superior structural and magnetic properties and house immense design space thanks to the strong and engineerable exchange interaction between individual layers. To fully unleash their potential, realizing low damping and strong exchange coupling simultaneously is critical, which often requires high quality interface. Here, we show that such a demand is realized in an all-insulator thulium iron garnet (TmIG)/YIG bilayer system. The ultralow dissipation rates in both YIG and TmIG, along with their significant spin-spin interaction at the interface, enable strong and coherent magnon-magnon coupling with a benchmarking cooperativity value larger than the conventional ferromagnetic metal-based heterostructures. The coupling strength can be tuned by varying the magnetic insulator layer thickness and magnon modes, which is consistent with analytical calculations and micromagnetic simulations. Our results demonstrate TmIG/YIG as a novel platform for investigating hybrid magnonic phenomena and open opportunities in magnon devices comprising all-insulator heterostructures. ",Kein DOI-Link verfügbar,2309.03116v1,Yes,potent(1)
0000-0002-3623-834X,Jiacheng Liu,The University of Hong Kong,Inverse Scaling: When Bigger Isn't Better,1970,"  Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models. ",Kein DOI-Link verfügbar,2306.09479v2,Yes,potent(1)
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,Viewport-Aware Dynamic 360° Video Segment Categorization,1970,"  Unlike conventional videos, 360{\deg} videos give freedom to users to turn their heads, watch and interact with the content owing to its immersive spherical environment. Although these movements are arbitrary, similarities can be observed between viewport patterns of different users and different videos. Identifying such patterns can assist both content and network providers to enhance the 360{\deg} video streaming process, eventually increasing the end-user Quality of Experience (QoE). But a study on how viewport patterns display similarities across different video content, and their potential applications has not yet been done. In this paper, we present a comprehensive analysis of a dataset of 88 360{\deg} videos and propose a novel video categorization algorithm that is based on similarities of viewports. First, we propose a novel viewport clustering algorithm that outperforms the existing algorithms in terms of clustering viewports with similar positioning and speed. Next, we develop a novel and unique dynamic video segment categorization algorithm that shows notable improvement in similarity for viewport distributions within the clusters when compared to that of existing static video categorizations. ",Kein DOI-Link verfügbar,2105.01701v2,Yes,"notable(1), potent(1)"
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,Virtualized Application Function Chaining: Maximizing the Wearable   System Lifetime,1970,"  The number of smart devices wear and carry by users is growing rapidly which is driven by innovative new smart wearables and interesting service o erings. This has led to applications that utilize multiple devices around the body to provide immersive environments such as mixed reality. These applications rely on a number of di erent types of functions such as sensing, communication and various types of processing, that require considerable resources. Thus one of the major challenges in supporting of these applications is dependent on the battery lifetime of the devices that provide the necessary functionality. The battery lifetime can be extended by either incorporating a battery with larger capacity and/or by utilizing the available resources e ciently. However, the increases in battery capacity are not keeping up with the demand and larger batteries add to both the weight and size of the device. Thus, the focus of this paper is to improve the battery e ciency through intelligent resources utilization. We show that, when the same resource is available on multiple devices that form part of the wearable system, and or is in close proximity, it is possible consider them as a resource pool and further utilize them intelligently to improve the system lifetime. Speci cally, we formulate the function allocation algorithm as a Mixed Integer Linear Programming (MILP) optimization problem and propose an e cient heuristic solution. The experimental data driven simulation results show that approximately 40-50% system battery life improvement can be achieved with proper function allocation and orchestration. ",Kein DOI-Link verfügbar,1804.00739v1,Yes,innovative(1)
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,TripletViNet: Mitigating Misinformation Video Spread Across Platforms,1970,"  There has been rampant propagation of fake news and misinformation videos on many platforms lately, and moderation of such content faces many challenges that must be overcome. Recent research has shown the feasibility of identifying video titles from encrypted network traffic within a single platform, for example, within YouTube or Facebook. However, there are no existing methods for cross-platform video recognition, a crucial gap that this works aims to address. Encrypted video traffic classification within a single platform, that is, classifying the video title of a traffic trace of a video on one platform by training on traffic traces of videos on the same platform, has significant limitations due to the large number of video platforms available to users to upload harmful content to. To attempt to address this limitation, we conduct a feasibility analysis into and attempt to solve the challenge of recognizing videos across multiple platforms by using the traffic traces of videos on one platform only. We propose TripletViNet, a framework that encompasses i) platform-wise pre-processing, ii) an encoder trained utilizing triplet learning for improved accuracy and iii) multiclass classifier for classifying the video title of a traffic trace. To evaluate the performance of TripletViNet, a comprehensive dataset with traffic traces for 100 videos on six major platforms with the potential for spreading misinformation such as YouTube, X, Instagram, Facebook, Rumble, and Tumblr was collected and used to test TripletViNet in both closed-set and open-set scenarios. TripletViNet achieves significant improvements in accuracy due to the correlation between video traffic and the video's VBR, with impressive final accuracies exceeding 90% in certain scenarios. ",https://doi.org/10.1145/3660512.3665519,2407.10644v1,Yes,potent(1)
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,MusicID: A Brainwave-based User Authentication System for Internet of   Things,1970,"  We propose MusicID, an authentication solution for smart devices that uses music-induced brainwave patterns as a behavioral biometric modality. We experimentally evaluate MusicID using data collected from real users whilst they are listening to two forms of music; a popular English song and individual's favorite song. We show that an accuracy over 98% for user identification and an accuracy over 97% for user verification can be achieved by using data collected from a 4-electrode commodity brainwave headset. We further show that a single electrode is able to provide an accuracy of approximately 85% and the use of two electrodes provides an accuracy of approximately 95%. As already shown by commodity brain-sensing headsets for meditation applications, we believe including dry EEG electrodes in smart-headsets is feasible and MusicID has the potential of providing an entry point and continuous authentication framework for upcoming surge of smart-devices mainly driven by Augmented Reality (AR)/Virtual Reality (VR) applications. ",Kein DOI-Link verfügbar,2006.01751v1,Yes,potent(1)
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,Characterizing and Detecting Money Laundering Activities on the Bitcoin   Network,1970,"  Bitcoin is by far the most popular crypto-currency solution enabling peer-to-peer payments. Despite some studies highlighting the network does not provide full anonymity, it is still being heavily used for a wide variety of dubious financial activities such as money laundering, ponzi schemes, and ransom-ware payments. In this paper, we explore the landscape of potential money laundering activities occurring across the Bitcoin network. Using data collected over three years, we create transaction graphs and provide an in-depth analysis on various graph characteristics to differentiate money laundering transactions from regular transactions. We found that the main difference between laundering and regular transactions lies in their output values and neighbourhood information. Then, we propose and evaluate a set of classifiers based on four types of graph features: immediate neighbours, curated features, deepwalk embeddings, and node2vec embeddings to classify money laundering and regular transactions. Results show that the node2vec-based classifier outperforms other classifiers in binary classification reaching an average accuracy of 92.29% and an F1-measure of 0.93 and high robustness over a 2.5-year time span. Finally, we demonstrate how effective our classifiers are in discovering unknown laundering services. The classifier performance dropped compared to binary classification, however, the prediction can be improved with simple ensemble techniques for some services. ",Kein DOI-Link verfügbar,1912.12060v1,Yes,potent(1)
0000-0003-4332-0082,Kanchana Thilakarathna,The University of Sydney,The Frontier of Data Erasure: Machine Unlearning for Large Language   Models,1970,"  Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI. ",Kein DOI-Link verfügbar,2403.15779v1,Yes,potent(1)
0000-0003-0189-0706,James Underwood,The University of Sydney,SimTreeLS: Simulating aerial and terrestrial laser scans of trees,1970,"  There are numerous emerging applications for digitizing trees using terrestrial and aerial laser scanning, particularly in the fields of agriculture and forestry. Interpretation of LiDAR point clouds is increasingly relying on data-driven methods (such as supervised machine learning) that rely on large quantities of hand-labelled data. As this data is potentially expensive to capture, and difficult to clearly visualise and label manually, a means of supplementing real LiDAR scans with simulated data is becoming a necessary step in realising the potential of these methods. We present an open source tool, SimTreeLS (Simulated Tree Laser Scans), for generating point clouds which simulate scanning with user-defined sensor, trajectory, tree shape and layout parameters. Upon simulation, material classification is kept in a pointwise fashion so leaf and woody matter are perfectly known, and unique identifiers separate individual trees, foregoing post-simulation labelling. This allows for an endless supply of procedurally generated data with similar characteristics to real LiDAR captures, which can then be used for development of data processing techniques or training of machine learning algorithms. To validate our method, we compare the characteristics of a simulated scan with a real scan using similar trees and the same sensor and trajectory parameters. Results suggest the simulated data is significantly more similar to real data than a sample-based control. We also demonstrate application of SimTreeLS on contexts beyond the real data available, simulating scans of new tree shapes, new trajectories and new layouts, with results presenting well. SimTreeLS is available as an open source resource built on publicly available libraries. ",Kein DOI-Link verfügbar,2011.11954v1,Yes,potent(2)
0000-0003-4212-1938,Xinyu Bai,The University of Sydney,SAR to Optical Image Translation with Color Supervised Diffusion Model,1970,"  Synthetic Aperture Radar (SAR) offers all-weather, high-resolution imaging capabilities, but its complex imaging mechanism often poses challenges for interpretation. In response to these limitations, this paper introduces an innovative generative model designed to transform SAR images into more intelligible optical images, thereby enhancing the interpretability of SAR images. Specifically, our model backbone is based on the recent diffusion models, which have powerful generative capabilities. We employ SAR images as conditional guides in the sampling process and integrate color supervision to counteract color shift issues effectively. We conducted experiments on the SEN12 dataset and employed quantitative evaluations using peak signal-to-noise ratio, structural similarity, and fr\'echet inception distance. The results demonstrate that our model not only surpasses previous methods in quantitative assessments but also significantly enhances the visual quality of the generated images. ",Kein DOI-Link verfügbar,2407.16921v1,Yes,innovative(1)
0000-0003-1219-2436,Yi Dai,Tsinghua University,Vision-based Traffic Flow Prediction using Dynamic Texture Model and   Gaussian Process,1970,"  In this paper, we describe work in progress towards a real-time vision-based traffic flow prediction (TFP) system. The proposed method consists of three elemental operators, that are dynamic texture model based motion segmentation, feature extraction and Gaussian process (GP) regression. The objective of motion segmentation is to recognize the target regions covering the moving vehicles in the sequence of visual processes. The feature extraction operator aims to extract useful features from the target regions. The extracted features are then mapped to the number of vehicles through the operator of GP regression. A training stage using historical visual data is required for determining the parameter values of the GP. Using a low-resolution visual data set, we performed preliminary evaluations on the performance of the proposed method. The results show that our method beats a benchmark solution based on Gaussian mixture model, and has the potential to be developed into qualified and practical solutions to real-time TFP. ",Kein DOI-Link verfügbar,1607.03991v2,Yes,potent(1)
0000-0003-4659-7060,Han Ge,"Tsinghua University, Zhejiang University",Magnetic structure and Ising-like antiferromagnetism in the bilayer   triangular lattice compound NdZnPO,1970,"  The complex interplay of spin frustration and quantum fluctuations in low-dimensional quantum materials leads to a variety of intriguing phenomena. This research focuses on a detailed analysis of the magnetic behavior exhibited by NdZnPO, a bilayer spin-1/2 triangular lattice antiferromagnet. The investigation employs magnetization, specific heat, and powder neutron scattering measurements. At zero field, a long-range magnetic order is observed at $T_{\rm N}=1.64~\rm K$. Powder neutron diffraction experiments show the Ising-like magnetic moments along the $c$-axis, revealing a stripe-like magnetic structure with three equivalent magnetic propagation vectors. Application of a magnetic field along the $c$-axis suppresses the antiferromagnetic order, leading to a fully polarized ferromagnetic state above $B_{\rm c}=4.5~\rm T$. This transition is accompanied by notable enhancements in the nuclear Schottky contribution. Moreover, the absence of spin frustration and expected field-induced plateau-like phases are remarkable observations. Detailed calculations of magnetic dipolar interactions revealed complex couplings reminiscent of a honeycomb lattice, suggesting the potential emergence of Kitaev-like physics within this system. This comprehensive study of the magnetic properties of NdZnPO highlights unresolved intricacies, underscoring the imperative for further exploration to unveil the underlying governing mechanisms. ",Kein DOI-Link verfügbar,2401.13318v1,Yes,"notable(1), potent(1)"
0000-0003-4659-7060,Han Ge,"Tsinghua University, Zhejiang University",Dzyaloshinskii-Moriya anisotropy effect on field-induced magnon   condensation in kagome antiferromagnet $α-Cu_3Mg(OH)_6Br_2$,1970,"  We performed a comprehensive electron spin resonance, magnetization and heat capacity study on the field-induced magnetic phase transitions in the kagome antiferromagnet $\alpha-Cu_3Mg(OH)_6Br_2$. With the successful preparation of single crystals, we mapped out the magnetic phase diagrams under the $c$-axis and $ab$-plane directional magnetic fields $B$. For $B\|c$, the three-dimensional (3D) magnon Bose-Einstein condensation (BEC) is evidenced by the power law scaling of the transition temperature, $T_c\propto (B_c-B)^{2/3}$. For $B\|ab$, the transition from the canted antiferromagetic (CAFM) state to the fully polarized (FP) state is a crossover rather than phase transition, and the characteristic temperature has a significant deviation from the 3D BEC scaling. The different behaviors of the field-induced magnetic transitions for $B\|c$ and $B\|ab$ could result from the Dzyaloshinkii-Moriya (DM) interaction with the DM vector along the $c$-axis, which preserves the $c$-axis directional spin rotation symmetry and breaks the spin rotation symmetry when $B\|ab$. The 3D magnon BEC scaling for $B\|c$ is immune to the off-stoichiometric disorder in our sample $\alpha-Cu_{3.26}Mg_{0.74}(OH)_6Br_2$. Our findings have the potential to shed light on the investigations of the magnetic anisotropy and disorder effects on the field-induced magnon BEC in the quantum antiferromagnet. ",https://doi.org/10.1103/PhysRevB.104.245107,2108.10020v1,Yes,potent(1)
0000-0002-9615-1879,Li Wang,"Tsinghua University, Tsinghua University Department of Chemistry",Deep JKO: time-implicit particle methods for general nonlinear gradient   flows,1970,"  We develop novel neural network-based implicit particle methods to compute high-dimensional Wasserstein-type gradient flows with linear and nonlinear mobility functions. The main idea is to use the Lagrangian formulation in the Jordan--Kinderlehrer--Otto (JKO) framework, where the velocity field is approximated using a neural network. We leverage the formulations from the neural ordinary differential equation (neural ODE) in the context of continuous normalizing flow for efficient density computation. Additionally, we make use of an explicit recurrence relation for computing derivatives, which greatly streamlines the backpropagation process. Our methodology demonstrates versatility in handling a wide range of gradient flows, accommodating various potential functions and nonlinear mobility scenarios. Extensive experiments demonstrate the efficacy of our approach, including an illustrative example from Bayesian inverse problems. This underscores that our scheme provides a viable alternative solver for the Kalman-Wasserstein gradient flow. ",Kein DOI-Link verfügbar,2311.06700v1,Yes,potent(1)
0000-0002-9615-1879,Li Wang,"Tsinghua University, Tsinghua University Department of Chemistry",Rogue wave formation and interactions in the defocusing nonlinear   Schrödinger equation with external potentials,1970,"  The defocusing nonlinear Schr\""odinger (NLS) equation has no the modulational instability, and was not found to possess the rogue wave (RW) phenomenon up to now. In this paper, we firstly investigate some novel nonlinear wave structures in the defocusing NLS equation with real-valued time-dependent and time-independent potentials such that the stable new RWs and W-shaped solitons are found, respectively. Moreover, the interactions of two or three RWs are explored such that the RWs with higher amplitudes are generated in the defocusing NLS equation with real-valued time-dependent potentials. Finally, we study the defocusing NLS equation with complex PT -symmetric potentials such that some RWs and W-shaped solitons are also found. These novel results will be useful to design the related physical experiments to generate the RW phenomena and W-shaped solitons in the case of defocusing nonlinear interactions, and to apply them in the related fields of nonlinear or even linear sciences. ",Kein DOI-Link verfügbar,2012.09983v1,Yes,potent(3)
0000-0002-9615-1879,Li Wang,"Tsinghua University, Tsinghua University Department of Chemistry",Data-driven rogue waves and parameter discovery in the defocusing NLS   equation with a potential using the PINN deep learning,1970,"  The physics-informed neural networks (PINNs) can be used to deep learn the nonlinear partial differential equations and other types of physical models. In this paper, we use the multi-layer PINN deep learning method to study the data-driven rogue wave solutions of the defocusing nonlinear Schr\""odinger (NLS) equation with the time-dependent potential by considering several initial conditions such as the rogue wave, Jacobi elliptic cosine function, two-Gaussian function, or three-hyperbolic-secant function, and periodic boundary conditions. Moreover, the multi-layer PINN algorithm can also be used to learn the parameter in the defocusing NLS equation with the time-dependent potential under the sense of the rogue wave solution. These results will be useful to further discuss the rogue wave solutions of the defocusing NLS equation with a potential in the study of deep learning neural networks. ",https://doi.org/10.1016/j.physleta.2021.127408,2012.09984v1,Yes,potent(3)
0000-0002-9615-1879,Li Wang,"Tsinghua University, Tsinghua University Department of Chemistry",Annealed Calderón-Zygmund estimates for elliptic operators with random   coefficients on $C^{1}$ domains,1970,"  Concerned with elliptic operators with stationary random coefficients governed by linear or nonlinear mixing conditions and bounded (or unbounded) $C^1$ domains, this paper mainly studies (weighted) annealed Calder\'on-Zygmund estimates, some of which are new even in a periodic setting. Stronger than some classical results derived by a perturbation argument in the deterministic case, our results own a scaling-invariant property, which additionally requires the non-perturbation method (based upon a quantitative homogenization theory and a set of functional analysis techniques) recently developed by M. Joisen and F. Otto \cite{Josien-Otto22}. To handle boundary estimates in certain UMD (unconditional martingale differences) spaces, we hand them over to Shen's real arguments \cite{Shen05, Shen23} instead of using Mikhlin's theorem. As a by-product, we also established ``resolvent estimates''. The potentially attractive part is to show how the two powerful kernel-free methods work together to make the results clean and robust. ",Kein DOI-Link verfügbar,2405.19102v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,MetaAnchor: Learning to Detect Objects with Customized Anchors,1970,"  We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios. ",Kein DOI-Link verfügbar,1807.00980v2,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution,1970,"  In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM. ",Kein DOI-Link verfügbar,2403.17927v2,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,OpenVIS: Open-vocabulary Video Instance Segmentation,1970,"  Open-vocabulary Video Instance Segmentation (OpenVIS) can simultaneously detect, segment, and track arbitrary object categories in a video, without being constrained to categories seen during training. In this work, we propose InstFormer, a carefully designed framework for the OpenVIS task that achieves powerful open-vocabulary capabilities through lightweight fine-tuning with limited-category data. InstFormer begins with the open-world mask proposal network, encouraged to propose all potential instance class-agnostic masks by the contrastive instance margin loss. Next, we introduce InstCLIP, adapted from pre-trained CLIP with Instance Guidance Attention, which encodes open-vocabulary instance tokens efficiently. These instance tokens not only enable open-vocabulary classification but also offer strong universal tracking capabilities. Furthermore, to prevent the tracking module from being constrained by the training data with limited categories, we propose the universal rollout association, which transforms the tracking problem into predicting the next frame's instance tracking token. The experimental results demonstrate the proposed InstFormer achieve state-of-the-art capabilities on a comprehensive OpenVIS evaluation benchmark, while also achieves competitive performance in fully supervised VIS task. ",Kein DOI-Link verfügbar,2305.16835v3,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Plug-and-Play Feature Generation for Few-Shot Medical Image   Classification,1970,"  Few-shot learning (FSL) presents immense potential in enhancing model generalization and practicality for medical image classification with limited training data; however, it still faces the challenge of severe overfitting in classifier training due to distribution bias caused by the scarce training samples. To address the issue, we propose MedMFG, a flexible and lightweight plug-and-play method designed to generate sufficient class-distinctive features from limited samples. Specifically, MedMFG first re-represents the limited prototypes to assign higher weights for more important information features. Then, the prototypes are variationally generated into abundant effective features. Finally, the generated features and prototypes are together to train a more generalized classifier. Experiments demonstrate that MedMFG outperforms the previous state-of-the-art methods on cross-domain benchmarks involving the transition from natural images to medical images, as well as medical images with different lesions. Notably, our method achieves over 10% performance improvement compared to several baselines. Fusion experiments further validate the adaptability of MedMFG, as it seamlessly integrates into various backbones and baselines, consistently yielding improvements of over 2.9% across all results. ",Kein DOI-Link verfügbar,2310.09471v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Towards Practical Certifiable Patch Defense with Vision Transformer,1970,"  Patch attacks, one of the most threatening forms of physical attack in adversarial examples, can lead networks to induce misclassification by modifying pixels arbitrarily in a continuous region. Certifiable patch defense can guarantee robustness that the classifier is not affected by patch attacks. Existing certifiable patch defenses sacrifice the clean accuracy of classifiers and only obtain a low certified accuracy on toy datasets. Furthermore, the clean and certified accuracy of these methods is still significantly lower than the accuracy of normal classification networks, which limits their application in practice. To move towards a practical certifiable patch defense, we introduce Vision Transformer (ViT) into the framework of Derandomized Smoothing (DS). Specifically, we propose a progressive smoothed image modeling task to train Vision Transformer, which can capture the more discriminable local context of an image while preserving the global semantic information. For efficient inference and deployment in the real world, we innovatively reconstruct the global self-attention structure of the original ViT into isolated band unit self-attention. On ImageNet, under 2% area patch attacks our method achieves 41.70% certified accuracy, a nearly 1-fold increase over the previous best method (26.00%). Simultaneously, our method achieves 78.58% clean accuracy, which is quite close to the normal ResNet-101 accuracy. Extensive experiments show that our method obtains state-of-the-art clean and certified accuracy with inferring efficiently on CIFAR-10 and ImageNet. ",Kein DOI-Link verfügbar,2203.08519v1,Yes,"innovative(1), innovatively(1)"
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,A User-Friendly Framework for Generating Model-Preferred Prompts in   Text-to-Image Synthesis,1970,"  Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics. ",Kein DOI-Link verfügbar,2402.12760v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,PG-Attack: A Precision-Guided Adversarial Attack Framework Against   Vision Foundation Models for Autonomous Driving,1970,"  Vision foundation models are increasingly employed in autonomous driving systems due to their advanced capabilities. However, these models are susceptible to adversarial attacks, posing significant risks to the reliability and safety of autonomous vehicles. Adversaries can exploit these vulnerabilities to manipulate the vehicle's perception of its surroundings, leading to erroneous decisions and potentially catastrophic consequences. To address this challenge, we propose a novel Precision-Guided Adversarial Attack (PG-Attack) framework that combines two techniques: Precision Mask Perturbation Attack (PMP-Attack) and Deceptive Text Patch Attack (DTP-Attack). PMP-Attack precisely targets the attack region to minimize the overall perturbation while maximizing its impact on the target object's representation in the model's feature space. DTP-Attack introduces deceptive text patches that disrupt the model's understanding of the scene, further enhancing the attack's effectiveness. Our experiments demonstrate that PG-Attack successfully deceives a variety of advanced multi-modal large models, including GPT-4V, Qwen-VL, and imp-V1. Additionally, we won First-Place in the CVPR 2024 Workshop Challenge: Black-box Adversarial Attacks on Vision Foundation Models and codes are available at https://github.com/fuhaha824/PG-Attack. ",Kein DOI-Link verfügbar,2407.13111v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,From Efficient Multimodal Models to World Models: A Survey,1970,"  Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field. ",Kein DOI-Link verfügbar,2407.00118v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Efficient Decision-based Black-box Patch Attacks on Video Recognition,1970,"  Although Deep Neural Networks (DNNs) have demonstrated excellent performance, they are vulnerable to adversarial patches that introduce perceptible and localized perturbations to the input. Generating adversarial patches on images has received much attention, while adversarial patches on videos have not been well investigated. Further, decision-based attacks, where attackers only access the predicted hard labels by querying threat models, have not been well explored on video models either, even if they are practical in real-world video recognition scenes. The absence of such studies leads to a huge gap in the robustness assessment for video models. To bridge this gap, this work first explores decision-based patch attacks on video models. We analyze that the huge parameter space brought by videos and the minimal information returned by decision-based models both greatly increase the attack difficulty and query burden. To achieve a query-efficient attack, we propose a spatial-temporal differential evolution (STDE) framework. First, STDE introduces target videos as patch textures and only adds patches on keyframes that are adaptively selected by temporal difference. Second, STDE takes minimizing the patch area as the optimization objective and adopts spatialtemporal mutation and crossover to search for the global optimum without falling into the local optimum. Experiments show STDE has demonstrated state-of-the-art performance in terms of threat, efficiency and imperceptibility. Hence, STDE has the potential to be a powerful tool for evaluating the robustness of video recognition models. ",Kein DOI-Link verfügbar,2303.11917v2,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,TagOOD: A Novel Approach to Out-of-Distribution Detection via   Vision-Language Representations and Class Center Learning,1970,"  Multimodal fusion, leveraging data like vision and language, is rapidly gaining traction. This enriched data representation improves performance across various tasks. Existing methods for out-of-distribution (OOD) detection, a critical area where AI models encounter unseen data in real-world scenarios, rely heavily on whole-image features. These image-level features can include irrelevant information that hinders the detection of OOD samples, ultimately limiting overall performance. In this paper, we propose \textbf{TagOOD}, a novel approach for OOD detection that leverages vision-language representations to achieve label-free object feature decoupling from whole images. This decomposition enables a more focused analysis of object semantics, enhancing OOD detection performance. Subsequently, TagOOD trains a lightweight network on the extracted object features to learn representative class centers. These centers capture the central tendencies of IND object classes, minimizing the influence of irrelevant image features during OOD detection. Finally, our approach efficiently detects OOD samples by calculating distance-based metrics as OOD scores between learned centers and test samples. We conduct extensive experiments to evaluate TagOOD on several benchmark datasets and demonstrate its superior performance compared to existing OOD detection methods. This work presents a novel perspective for further exploration of multimodal information utilization in OOD detection, with potential applications across various tasks. ",Kein DOI-Link verfügbar,2408.15566v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Hi-EF: Benchmarking Emotion Forecasting in Human-interaction,1970,"  Affective Forecasting, a research direction in psychology that predicts individuals future emotions, is often constrained by numerous external factors like social influence and temporal distance. To address this, we transform Affective Forecasting into a Deep Learning problem by designing an Emotion Forecasting paradigm based on two-party interactions. We propose a novel Emotion Forecasting (EF) task grounded in the theory that an individuals emotions are easily influenced by the emotions or other information conveyed during interactions with another person. To tackle this task, we have developed a specialized dataset, Human-interaction-based Emotion Forecasting (Hi-EF), which contains 3069 two-party Multilayered-Contextual Interaction Samples (MCIS) with abundant affective-relevant labels and three modalities. Hi-EF not only demonstrates the feasibility of the EF task but also highlights its potential. Additionally, we propose a methodology that establishes a foundational and referential baseline model for the EF task and extensive experiments are provided. The dataset and code is available at https://github.com/Anonymize-Author/Hi-EF. ",Kein DOI-Link verfügbar,2407.16406v1,Yes,potent(1)
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with   Diffusion Model for Blind Image Super-Resolution,1970,"  Pre-trained diffusion models utilized for image generation encapsulate a substantial reservoir of a priori knowledge pertaining to intricate textures. Harnessing the potential of leveraging this a priori knowledge in the context of image super-resolution presents a compelling avenue. Nonetheless, prevailing diffusion-based methodologies presently overlook the constraints imposed by degradation information on the diffusion process. Furthermore, these methods fail to consider the spatial variability inherent in the estimated blur kernel, stemming from factors such as motion jitter and out-of-focus elements in open-environment scenarios. This oversight results in a notable deviation of the image super-resolution effect from fundamental realities. To address these concerns, we introduce a framework known as Adaptive Multi-modal Fusion of \textbf{S}patially Variant Kernel Refinement with Diffusion Model for Blind Image \textbf{S}uper-\textbf{R}esolution (SSR). Within the SSR framework, we propose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a Depth-Informed Kernel, which takes the depth information into account and is spatially variant. Additionally, SVKR enhance the accuracy of depth information acquired from LR images, allowing for mutual enhancement between the depth map and blur kernel estimates. Finally, we introduce the Adaptive Multi-Modal Fusion (AMF) module to align the information from three modalities: low-resolution images, depth maps, and blur kernels. This alignment can constrain the diffusion model to generate more authentic SR results. ",Kein DOI-Link verfügbar,2403.05808v2,Yes,"intricate(1), notable(1), potent(1)"
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,Seeking Certainty In Uncertainty: Dual-Stage Unified Framework Solving   Uncertainty in Dynamic Facial Expression Recognition,1970,"  The contemporary state-of-the-art of Dynamic Facial Expression Recognition (DFER) technology facilitates remarkable progress by deriving emotional mappings of facial expressions from video content, underpinned by training on voluminous datasets. Yet, the DFER datasets encompass a substantial volume of noise data. Noise arises from low-quality captures that defy logical labeling, and instances that suffer from mislabeling due to annotation bias, engendering two principal types of uncertainty: the uncertainty regarding data usability and the uncertainty concerning label reliability. Addressing the two types of uncertainty, we have meticulously crafted a two-stage framework aiming at \textbf{S}eeking \textbf{C}ertain data \textbf{I}n extensive \textbf{U}ncertain data (SCIU). This initiative aims to purge the DFER datasets of these uncertainties, thereby ensuring that only clean, verified data is employed in training processes. To mitigate the issue of low-quality samples, we introduce the Coarse-Grained Pruning (CGP) stage, which assesses sample weights and prunes those deemed unusable due to their low weight. For samples with incorrect annotations, the Fine-Grained Correction (FGC) stage evaluates prediction stability to rectify mislabeled data. Moreover, SCIU is conceived as a universally compatible, plug-and-play framework, tailored to integrate seamlessly with prevailing DFER methodologies. Rigorous experiments across prevalent DFER datasets and against numerous benchmark methods substantiates SCIU's capacity to markedly elevate performance metrics. ",Kein DOI-Link verfügbar,2406.16473v1,Yes,"meticulous(1), meticulously(1)"
0000-0002-8364-0876,Wenqiang Zhang,Tsinghua University,A Survey on Facial Expression Recognition of Static and Dynamic Emotions,1970,"  Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at https://github.com/wangyanckxx/SurveyFER. ",Kein DOI-Link verfügbar,2408.15777v1,Yes,pivotal(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Correction to Solution of Dirac Equation,1970,"  Using the China unitary principle to test the Dirac theoryfor the hydrogen atomic spectrum shows that the standard Dirac function withthe Dirac energy levels is only one the formal solutions of theDirac-Coulomb equation, which conceals some pivotal mathematicalcontradictions. The theorem of existence of solution of the Dirac equationrequires an important modification to the Dirac angular momentum constantthat was defined by Dirac's algebra. It derives the modified radial Diracequation which has the consistency solution involving the quantum neutronradius and the neutron binding energy. The inevitable solution for otheratomic energy states is only equivalent to the Bohr solution. It concludesthat the Dirac equation is more suitable to describe the structure ofneutron. How to treat the difference between the unitary energy levels andthe result of the experimental observation of the atomic spectrums for thehydrogen atom needs to be solved urgently. ",Kein DOI-Link verfügbar,0908.4320v5,Yes,pivotal(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,On Generating Lagrangian Cuts for Two-Stage Stochastic Integer Programs,1970,"  We investigate new methods for generating Lagrangian cuts to solve two-stage stochastic integer programs. Lagrangian cuts can be added to a Benders reformulation, and are derived from solving single scenario integer programming subproblems identical to those used in the nonanticipative Lagrangian dual of a stochastic integer program. While Lagrangian cuts have the potential to significantly strengthen the Benders relaxation, generating Lagrangian cuts can be computationally demanding. We investigate new techniques for generating Lagrangian cuts with the goal of obtaining methods that provide significant improvements to the Benders relaxation quickly. Computational results demonstrate that our proposed method improves the Benders relaxation significantly faster than previous methods for generating Lagrangian cuts and, when used within a branch-and-cut algorithm, significantly reduces the size of the search tree for three classes of test problems. ",https://doi.org/10.1287/ijoc.2022.1185,2106.04023v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Mass behavior of hidden-charm pentaquarks with open-strange inspired by   these established $P_c$ molecular states,1970,"  Stimulated by the meson-baryon molecular interpretations of the $P_c$ states ($P_c(4312)/P_c(4440)/P_c(4457)$), we systematically study the interactions between an $S-$wave charm-strange baryon $\Xi_c^{(\prime,*)}$ and an anti-charmed meson $\bar{D}^{(*)}$ in a coupled channel analysis. Effective potentials for the $\Xi_c^{(\prime,*)}\bar{D}^{(*)}$ interactions in a one-boson-exchange model can be related to those in the $\Sigma_c^{(*)}\bar{D}^{(*)}$ systems by using the $SU(3)$ flavor symmetry and heavy quark symmetry. Our results can predict several promising hidden-charm molecular pentaquarks with strangeness $|S|=1$, which include the $\Xi_c^{\prime}\bar{D}$ states with $I(J^P)=0,1(1/2^-)$, the $\Xi_c^*\bar{D}$ states with $0,1(3/2^-)$, the $\Xi_c^{\prime}\bar{D}^*$ states with $0(1/2^-)$ and $0,1(3/2^-)$, and the $\Xi_c^*\bar{D}^*$ states with $0(1/2^-,3/2^-,5/2^-)$. ",https://doi.org/10.1103/PhysRevD.105.014029,2201.07603v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Non-line-of-sight reconstruction via structure sparsity regularization,1970,"  Non-line-of-sight (NLOS) imaging allows for the imaging of objects around a corner, which enables potential applications in various fields such as autonomous driving, robotic vision, medical imaging, security monitoring, etc. However, the quality of reconstruction is challenged by low signal-noise-ratio (SNR) measurements. In this study, we present a regularization method, referred to as structure sparsity (SS) regularization, for denoising in NLOS reconstruction. By exploiting the prior knowledge of structure sparseness, we incorporate nuclear norm penalization into the cost function of directional light-cone transform (DLCT) model for NLOS imaging system. This incorporation effectively integrates the neighborhood information associated with the directional albedo, thereby facilitating the denoising process. Subsequently, the reconstruction is achieved by optimizing a directional albedo model with SS regularization using fast iterative shrinkage-thresholding algorithm. Notably, the robust reconstruction of occluded objects is observed. Through comprehensive evaluations conducted on both synthetic and experimental datasets, we demonstrate that the proposed approach yields high-quality reconstructions, surpassing the state-of-the-art reconstruction algorithms, especially in scenarios involving short exposure and low SNR measurements. ",https://doi.org/10.1364/OL.501622,2308.02782v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Matryoshka Representation Learning for Recommendation,1970,"  Representation learning is essential for deep-neural-network-based recommender systems to capture user preferences and item features within fixed-dimensional user and item vectors. Unlike existing representation learning methods that either treat each user preference and item feature uniformly or categorize them into discrete clusters, we argue that in the real world, user preferences and item features are naturally expressed and organized in a hierarchical manner, leading to a new direction for representation learning. In this paper, we introduce a novel matryoshka representation learning method for recommendation (MRL4Rec), by which we restructure user and item vectors into matryoshka representations with incrementally dimensional and overlapping vector spaces to explicitly represent user preferences and item features at different hierarchical levels. We theoretically establish that constructing training triplets specific to each level is pivotal in guaranteeing accurate matryoshka representation learning. Subsequently, we propose the matryoshka negative sampling mechanism to construct training triplets, which further ensures the effectiveness of the matryoshka representation learning in capturing hierarchical user preferences and item features. The experiments demonstrate that MRL4Rec can consistently and substantially outperform a number of state-of-the-art competitors on several real-life datasets. Our code is publicly available at https://github.com/Riwei-HEU/MRL. ",Kein DOI-Link verfügbar,2406.07432v1,Yes,pivotal(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Searching for possible $Ω_c-$like molecular states from   $Ξ_{c}^{*}\bar{K}/Ξ_c\bar{K}^*/Ξ_c'\bar{K}^*$ interaction,1970,"  Stimulated by the observations of recent observation of $\Omega_c$ as well as former one of $P_c(4380)$ and $P_c(4450)$, we perform a coupled channel analysis of $\Xi_c^*\bar{K}/\Omega_c\eta/\Omega_c^*\eta/\Xi_c\bar{K}^*/\Xi_c'\bar{K}^*/\Omega_c\omega$ systems to search for possible $\Omega_c-$like molecular states by using a one-boson-exchange potential. Our results suggest there exists a loosely bound molecular state, a $\Xi_c^*\bar{K}/\Omega_c\eta/\Omega_c^*\eta/\Xi_c\bar{K}^*/\Xi_c'\bar{K}^*/\Omega_c\omega$ with $I(J^P)=0(3/2^-)$, it is mainly composed of the $\Xi_c^*\bar{K}$ system. Two-body strong decay width is also studied, where we find that $\Xi_c'\bar{K}$ is the dominant decay channel. ",https://doi.org/10.1103/PhysRevD.97.036016,1711.07650v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Recovering Dantzig-Wolfe Bounds by Cutting Planes,1970,"  Dantzig-Wolfe (DW) decomposition is a well-known technique in mixed-integer programming (MIP) for decomposing and convexifying constraints to obtain potentially strong dual bounds. We investigate cutting planes that can be derived using the DW decomposition algorithm and show that these cuts can provide the same dual bounds as DW decomposition. More precisely, we generate one cut for each DW block, and when combined with the constraints in the original formulation, these cuts imply the objective function cut one can simply write using the DW bound. This approach typically leads to a formulation with lower dual degeneracy that consequently has a better computational performance when solved by standard MIP solvers in the original space. We also discuss how to strengthen these cuts to improve the computational performance further. We test our approach on the Multiple Knapsack Assignment Problem and the Temporal Knapsack Problem, and show that the proposed cuts are helpful in accelerating the solution time without the need to implement branch and price. ",Kein DOI-Link verfügbar,2301.13149v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Tunable corner-like modes in generalized quadrupole topological   insulator,1970,"  Higher-order topological insulators harbor unique corner modes that hold immense potential for applications in information storage. However, the practical manipulation of these states has been constrained by the fixed positions and energies of conventional corner modes. In this work, we present a theoretical framework for generating topologically protected corner-like modes in higher-order topological insulators, exhibiting unprecedented tunability in their positions. These corner-like modes are characterized by a quantized generalized quadrupole moment, indicative of the presence of fractional charges. Moreover, we demonstrate the remarkable ability to modulate the energy of these topological corner modes. Our findings pave the way for the controlled manipulation of corner modes in higher-order topological insulators, opening up new avenues for their applications in advanced information technologies. ",Kein DOI-Link verfügbar,2406.19699v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Deep Multi-View Semi-Supervised Clustering with Sample Pairwise   Constraints,1970,"  Multi-view clustering has attracted much attention thanks to the capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally overlook the significance of weakly-supervised information and fail to preserve the feature properties of multiple views, thus resulting in unsatisfactory clustering performance. To address these issues, in this paper, we propose a novel Deep Multi-view Semi-supervised Clustering (DMSC) method, which jointly optimizes three kinds of losses during networks finetuning, including multi-view clustering loss, semi-supervised pairwise constraint loss and multiple autoencoders reconstruction loss. Specifically, a KL divergence based multi-view clustering loss is imposed on the common representation of multi-view data to perform heterogeneous feature optimization, multi-view weighting and clustering prediction simultaneously. Then, we innovatively propose to integrate pairwise constraints into the process of multi-view clustering by enforcing the learned multi-view representation of must-link samples (cannot-link samples) to be similar (dissimilar), such that the formed clustering architecture can be more credible. Moreover, unlike existing rivals that only preserve the encoders for each heterogeneous branch during networks finetuning, we further propose to tune the intact autoencoders frame that contains both encoders and decoders. In this way, the issue of serious corruption of view-specific and view-shared feature space could be alleviated, making the whole training procedure more stable. Through comprehensive experiments on eight popular image datasets, we demonstrate that our proposed approach performs better than the state-of-the-art multi-view and single-view competitors. ",Kein DOI-Link verfügbar,2206.04949v2,Yes,"innovative(1), innovatively(1)"
0000-0002-6249-6780,Rui Chen,Tsinghua University,Online Decision Making with Nonconvex Local and Convex Global   Constraints,1970,"  We study the online decision making problem (ODMP) as a natural generalization of online linear programming. In ODMP, a single decision maker undertakes a sequence of decisions over $T$ time steps. At each time step, the decision maker makes a locally feasible decision based on information available up to that point. The objective is to maximize the accumulated reward while satisfying some convex global constraints called goal constraints. The decision made at each step results in an $m$-dimensional vector that represents the contribution of this local decision to the goal constraints. In the online setting, these goal constraints are soft constraints that can be violated moderately. To handle potential nonconvexity and nonlinearity in ODMP, we propose a Fenchel dual-based online algorithm. At each time step, the algorithm requires solving a potentially nonconvex optimization problem over the local feasible set and a convex optimization problem over the goal set. Under certain stochastic input models, we show that the algorithm achieves $O(\sqrt{mT})$ goal constraint violation deterministically, and $\tilde{O}(\sqrt{mT})$ regret in expected reward. Numerical experiments on an online knapsack problem and an assortment optimization problem are conducted to demonstrate the potential of our proposed online algorithm. ",Kein DOI-Link verfügbar,2211.03997v2,Yes,potent(3)
0000-0002-6249-6780,Rui Chen,Tsinghua University,An Optimal Control Framework for Influencing Human Driving Behavior in   Mixed-Autonomy Traffic,1970,"  As autonomous vehicles (AVs) become increasingly prevalent, their interaction with human drivers presents a critical challenge. Current AVs lack social awareness, causing behavior that is often awkward or unsafe. To combat this, social AVs, which are proactive rather than reactive in their behavior, have been explored in recent years. With knowledge of robot-human interaction dynamics, a social AV can influence a human driver to exhibit desired behaviors by strategically altering its own behaviors. In this paper, we present a novel framework for achieving human influence. The foundation of our framework lies in an innovative use of control barrier functions to formulate the desired objectives of influence as constraints in an optimal control problem. The computed controls gradually push the system state toward satisfaction of the objectives, e.g. slowing the human down to some desired speed. We demonstrate the proposed framework's feasibility in a variety of scenarios related to car-following and lane changes, including multi-robot and multi-human configurations. In two case studies, we validate the framework's effectiveness when applied to the problems of traffic flow optimization and aggressive behavior mitigation. Given these results, the main contribution of our framework is its versatility in a wide spectrum of influence objectives and mixed-autonomy configurations. ",Kein DOI-Link verfügbar,2309.13456v2,Yes,"innovative(1), strategically(1)"
0000-0002-6249-6780,Rui Chen,Tsinghua University,"Decoupled, Energy Stable Scheme for Hydrodynamic Allen-Cahn Phase Field   Moving Contact Line Model",1970,"  In this paper, we present an efficient energy stable scheme to solve a phase field model incorporating contact line condition. Instead of the usually used Cahn-Hilliard type phase equation, we adopt the Allen-Cahn type phase field model with the static contact line boundary condition that coupled with incompressible Navier-Stokes equations with Navier boundary condition. The projection method is used to deal with the Navier-Stokes equa- tions and an auxiliary function is introduced for the non-convex Ginzburg-Landau bulk potential. We show that the scheme is linear, decoupled and energy stable. Moreover, we prove that fully discrete scheme is also energy stable. An efficient finite element spatial discretization method is implemented to verify the accuracy and efficiency of proposed schemes. Numerical results show that the proposed scheme is very efficient and accurate ",Kein DOI-Link verfügbar,1703.06780v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Disorder-induced topological phase transitions on Lieb lattices,1970,"  Motivated by the very recent experimental realization of electronic Lieb lattices and research interest on topological states of matter, we study the topological phase transitions driven by Anderson disorder on spin-orbit coupled Lieb lattices in the presence of spin-independent and dependent potentials. By combining the numerical transport and self-consistent Born approximation methods, we found that both time-reversal invariant and broken Lieb lattices can host disorder-induced gapful topological phases, including the quantum spin Hall insulator (QSHI) and quantum anomalous Hall insulator (QAHI) phases. For the time-reversal invariant case, this disorder can induce a topological phase transition directly from normal insulator (NI) to the QSHI. While for the time-reversal broken case, the disorder can induce either a QAHI-QSHI phase transition or a NI-QAHI-QSHI phase transition. Remarkably, the time-reversal broken QSHI phase can be induced by Anderson disorder on the spin-orbit coupled Lieb lattices without time-reversal symmetry. ",https://doi.org/10.1103/PhysRevB.96.205304,1708.02121v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Prediction of hidden-charm pentaquarks with double strangeness,1970,"  Inspired by the recent evidence of $P_{cs}(4459)$ reported by LHCb, we continue to perform the investigation of hidden-charm molecular pentaquarks with double strangeness, which are composed of an $S$-wave charmed baryon $\Xi_c^{(\prime,*)}$ and an $S$-wave anti-charmed-strange meson $\bar{D}_s^{(*)}$. Both the $S$-$D$ wave mixing effect and the coupled channel effect are taken into account in realistic calculation. A dynamics calculation shows that there may exist two types of hidden-charm molecular pentaquark with double strangeness, i.e., the $\Xi_{c}^{*}\bar D_s^*$ molecular state with $J^P={5}/{2}^{-}$ and the $\Xi_{c}^{\prime}\bar D_s^*$ molecular state with $J^P={3}/{2}^{-}$. According to this result, we strongly suggest the experimental exploration of hidden-charm molecular pentaquarks with double strangeness. Facing such opportunity, obviously the LHCb will have great potential to hunt for them, with the data accumulation at Run III and after High-Luminosity-LHC upgrade. ",https://doi.org/10.1103/PhysRevD.103.034014,2011.14296v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,1D self-healing beams in integrated silicon photonics,1970,"  Since the first experimental observation of optical Airy beams, various applications ranging from particle and cell micromanipulation to laser micromachining have exploited their non-diffracting and accelerating properties. The later discovery that Airy beams can self-heal after being blocked by an obstacle further proved their robustness to propagate under scattering and disordered environment. Here, we report the generation of Airy beams on an integrated silicon photonic chip and demonstrate that the on-chip 1D Airy beams preserve the same properties as the 2D beams. The 1D meta-optics used to create the Airy beam has the size of only 3 by 16 microns, at least three orders of magnitude smaller than the conventional optic. The on-chip self-healing beams demonstrated here could potentially enable diffraction-free light routing for on-chip optical networks and high-precision micromanipulation of bio-molecules on an integrated photonic chip. ",https://doi.org/10.1021/acsphotonics.1c00581,2103.12254v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Four-dimensional Floquet topological insulator with an emergent second   Chern number,1970,"  Floquet topological insulators have been widely investigated in lower-dimensional systems. However, Floquet topological insulators induced by time-periodic driving in higher-dimensional systems remain unexplored. In this work, we study the effects of time-periodic driving in a four-dimensional (4D) normal insulator, focusing on topological phase transitions at the resonant quasienergy gap. We consider two types of time-periodic driving, including a time-periodic onsite potential and a time-periodic vector potential. We reveal that both types of time-periodic driving can transform the 4D normal insulator into a 4D Floquet topological insulator characterized by an emergent second Chern number. Moreover, it is found that the topological phase of the 4D system can be modulated by tuning the strength and frequency of the time-periodic driving. Our work will be helpful for the future investigation of Floquet topological insulators in higher dimensions. ",https://doi.org/10.1103/PhysRevB.109.125303,2312.16013v2,Yes,potent(2)
0000-0002-6249-6780,Rui Chen,Tsinghua University,A Survey on Data-Centric Recommender Systems,1970,"  Recommender systems (RSs) have become an essential tool for mitigating information overload in a range of real-world applications. Recent trends in RSs have revealed a major paradigm shift, moving the spotlight from model-centric innovations to data-centric efforts (e.g., improving data quality and quantity). This evolution has given rise to the concept of data-centric recommender systems (Data-Centric RSs), marking a significant development in the field. This survey provides the first systematic overview of Data-Centric RSs, covering 1) the foundational concepts of recommendation data and Data-Centric RSs; 2) three primary issues of recommendation data; 3) recent research developed to address these issues; and 4) several potential future directions of Data-Centric RSs. ",Kein DOI-Link verfügbar,2401.17878v4,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft   Heads with Adversarial Learning,1970,"  Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature. While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored. In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head. By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs. Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding. We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads. ",Kein DOI-Link verfügbar,2408.08146v1,Yes,potent(2)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Light-induced half-quantized Hall effect and axion insulator,1970,"  Motivated by the recent experimental realization of the half-quantized Hall effect phase in a three-dimensional (3D) semi-magnetic topological insulator [M. Mogi et al., Nature Physics 18, 390 (2022)], we propose a scheme for realizing the half-quantized Hall effect and axion insulator in experimentally mature 3D topological insulator heterostructures. Our approach involves optically pumping and/or magnetically doping the topological insulator surface, such as to break time reversal and gap out the Dirac cones. By toggling between left and right circularly polarized optical pumping, the sign of the half-integer Hall conductance from each of the surface Dirac cones can be controlled, such as to yield half-quantized ($0+1/2$), axion ($-1/2+1/2=0$), and Chern ($1/2+1/2=1$) insulator phases. We substantiate our results based on detailed band structure and Berry curvature numerics on the Floquet Hamiltonian in the high-frequency limit. Our paper showcases how topological phases can be obtained through mature experimental approaches such as magnetic layer doping and circularly polarized laser pumping and opens up potential device applications such as a polarization chirality-controlled topological transistor. ",https://doi.org/10.1103/PhysRevB.108.075435,2306.03187v4,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Light-enhanced nonlinear Hall effect,1970,"  It is well known that a nontrivial Chern number results in quantized Hall conductance. What is less known is that, generically, the Hall response can be dramatically different from its quantized value in materials with broken inversion symmetry. This stems from the leading Hall contribution beyond the linear order, known as the Berry curvature dipole (BCD). While the BCD is in principle always present, it is typically very small outside of a narrow window close to a topological transition and is thus experimentally elusive without careful tuning of external fields, temperature, or impurities. In this work, we transcend this challenge by devising optical driving and quench protocols that enable practical and direct access to large BCD and nonlinear Hall responses. Varying the amplitude of an incident circularly polarized laser drives a topological transition between normal and Chern insulator phases, and importantly allows the precise unlocking of nonlinear Hall currents comparable to or larger than the linear Hall contributions. This strong BCD engineering is even more versatile with our two-parameter quench protocol, as demonstrated in our experimental proposal. Our predictions are expected to hold qualitatively across a broad range of Hall materials, thereby paving the way for the controlled engineering of nonlinear electronic properties in diverse media. ",Kein DOI-Link verfügbar,2401.18038v3,Yes,versatile(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Predicting another doubly charmed molecular resonance   $T_{cc}^{\prime+}(3876)$,1970,"  The isospin breaking effect plays an essential role in generating hadronic molecular states with a very tiny binding energy. Very recently, the LHCb Collaboration observed a very narrow doubly charmed tetraquark $T_{cc}^+$ in the $D^0D^0\pi$ mass spectrum, which lies just below the $D^0D^{*+}$ threshold around 273 keV. In this work, we study the $D^0D^{*+}/D^+D^{*0}$ interactions with the one-boson-exchange effective potentials and consider the isospin breaking effect carefully. We not only reproduce the mass of the newly observed $T_{cc}^+$ very well in the doubly charmed molecular tetraquark scenario, but also predict the other doubly charmed partner resonance $T_{cc}^{\prime+}$ with $m=3876~\text{MeV}$, and $\Gamma= 412~\text{keV}$. The prime decay modes of the $T_{cc}^{\prime+}$ are $D^0D^+\gamma$ and $D^+D^0\pi^0$. The peculiar characteristic mass spectrum of the $D^0D^{*+}/D^+D^{*0}$ molecular systems can be applied to identify the doubly charmed molecular states. ",https://doi.org/10.1103/PhysRevD.104.114042,2108.01911v4,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,State-wise Safe Reinforcement Learning: A Survey,1970,"  Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potential future directions. ",Kein DOI-Link verfügbar,2302.03122v3,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Finite-size effects in non-Hermitian topological systems,1970,"  We systematically investigate the finite-size effects in non-Hermitian one-dimensional (1D) Su-Schrieffer-Heeger (SSH) and two-dimensional (2D) Chern insulator models. Using a combination of analytical and numerical calculations, we show that the non-Hermitian intra-cell hoppings in the SSH model can modify the localization lengths of bulk and end states, giving rise to a complex finite-size energy gap that exhibits an oscillating exponential decay as the chain length grows. However, the imaginary staggered on-site potentials in the SSH model only change the end-state energy, leaving the localization lengths of the system unchanged. In this case, the finite-size energy gap can undergo a transition from real values to imaginary values. We observed similar phenomena for the finite-size effect in 2D Chern insulator systems. ",https://doi.org/10.1103/PhysRevB.99.155431,1901.06820v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,On the Spurious Interior Resonance Modes of Time Domain Integral   Equations for Analyzing Acoustic Scattering from Penetrable Objects,1970,"  The interior resonance problem of time domain integral equations (TDIEs) formulated to analyze acoustic field interactions on penetrable objects is investigated. Two types of TDIEs are considered: The first equation, which is termed the time domain potential integral equation (TDPIE) (in unknowns velocity potential and its normal derivative), suffers from the interior resonance problem, i.e., its solution is replete with spurious modes that are excited at the resonance frequencies of the acoustic cavity in the shape of the scatterer. Numerical experiments demonstrate that, unlike the frequency-domain integral equations, the amplitude of these modes in the time domain could be suppressed to a level that does not significantly affect the solution. The second equation is obtained by linearly combining TDPIE with its normal derivative. Weights of the combination are carefully selected to enable the numerical computation of the singular integrals. The solution of this equation, which is termed the time domain combined potential integral equation (TDCPIE), does not involve any spurious interior resonance modes. ",https://doi.org/10.1121/10.0009401,2108.04889v1,Yes,potent(3)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Doubly charmed molecular pentaquarks,1970,"  We perform a systematic exploration of the possible doubly charmed molecular pentaquarks composed of $\Sigma_c^{(*)}D^{(*)}$ with the one-boson-exchange potential model. After taking into account the $S-D$ wave mixing and the coupled channel effects, we predict several possible doubly charmed molecular pentaquarks, which include the $\Sigma_cD$ with $I(J^P) = 1/2(1/2^-)$, $\Sigma_c^*D$ with $1/2(3/2^-)$, and $\Sigma_cD^*$ with $1/2(1/2^-)$, $1/2(3/2^-)$. The $\Sigma_cD$ state with $3/2(1/2^-)$ and $\Sigma_cD^*$ state with $3/2(1/2^-)$ may also be suggested as candidates of doubly charmed molecular pentaquarks. The $\Sigma_cD$ and $\Sigma_c^*D$ states can be searched for by analyzing the $\Lambda_cD\pi$ invariant mass spectrum of the bottom baryon and $B$ meson decays. The $\Sigma_cD^*$ states can be searched for in the invariant mass spectrum of $\Lambda_cD^*\pi$, $\Lambda_cD\pi\pi$ and $\Lambda_cD\pi\gamma$. Since the width of $\Sigma_c^*$ is much larger than that of $D^*$, $\Sigma_c^*D\rightarrow \Lambda_cD\pi$ will be the dominant decay mode. We sincerely hope these candidates for the doubly charmed molecular pentaqurks will be searched by LHCb or BelleII collaboration in the near future. ",https://doi.org/10.1016/j.physletb.2021.136693,2108.12730v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Broadband nonvolatile electrically programmable silicon photonic   switches,1970,"  Programmable photonic integrated circuits (PICs) have recently gained significant interest due to their potential in creating next-generation technologies ranging from artificial neural networks and microwave photonics to quantum information processing. The fundamental building block of such programmable PICs is a tunable 2 x 2 switch, traditionally controlled by the thermo-optic or free-carrier dispersion. Yet, these implementations are power-hungry, volatile, and have a large footprint (typically > 100 um). Therefore, a truly 'set-and-forget' type 2 x 2 switch with zero static power consumption is highly desirable for large-scale PICs. Here, we report a broadband nonvolatile electrically programmable 2 x 2 silicon photonic switch based on the phase-change material Ge2Sb2Te5. The directional coupler switch exhibits a compact coupling length (64 um), small insertion loss (<2 dB), and minimal crosstalk (<-8 dB) across the entire telecommunication C-band while maintaining a record-high endurance of over 2,800 switching cycles. This demonstrated switch constitutes a critical component for realizing future generic programmable silicon photonic systems. ",https://doi.org/10.1021/acsphotonics.2c00452,2201.05439v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Nonlinear Hall effect on a disordered lattice,1970,"  The nonlinear Hall effect has recently attracted significant interest due to its potential as a promising spectral tool and device applications. A theory of the nonlinear Hall effect on a disordered lattice is a crucial step towards explorations in realistic devices, but has not been addressed. We study the nonlinear Hall response on a lattice, which allows us to introduce strong disorder numerically. We reveal a disorder-induced fluctuation of the Berry curvature that was not discovered in the previous perturbation theories. The fluctuating Berry curvature induces a fluctuation of the nonlinear Hall conductivity, which anomalously increases as the Fermi energy moves from the band edges to higher energies. More importantly, the fluctuation may explain those observations in the recent experiments. We also discover an ""Anderson localization"" of the nonlinear Hall effect. This work shows a territory of the nonlinear Hall effect yet to be explored. ",Kein DOI-Link verfügbar,2309.07000v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,"The $Ξ_c^{(\prime,*)}N$ interactions and the corresponding bound   states",1970,"  In the framework of a one-boson-exchange model, we conduct a comprehensive investigation into the interactions between an $S-$wave charm-strange baryon $\Xi_c^{(\prime,*)}$ and a nucleon by considering the $S-D$ wave mixing effects and the coupled channel effects. Our results can predict a serial of possible $S-$wave $\Xi_c^{(\prime,*)}N$ molecular candidates. The one-pion-exchange model together with the coupled channel effects play an important role in generating the $\Xi_cN$ bound states with $I(J^P)=0(0^+)$ and $0(1^+)$, the one-pion-exchange effective potentials are strong enough to bind the $\Xi_c^{\prime}N$ states with $I(J^P)=0(0^+)$ and $0(1^+)$ and $\Xi_c^*N$ states with $I(J^P)=0(1^+)$ and $0(2^+)$, and the intermediate range and short range force originating from the scalar meson and the vector mesons exchanges interactions are essential in formative to the isovector $S-$wave $\Xi_c^{(\prime,*)}N$. We expect the experiments looking for these possible molecular candidates. ",Kein DOI-Link verfügbar,2404.17935v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Service Delay Minimization for Federated Learning over Mobile Devices,1970,"  Federated learning (FL) over mobile devices has fostered numerous intriguing applications/services, many of which are delay-sensitive. In this paper, we propose a service delay efficient FL (SDEFL) scheme over mobile devices. Unlike traditional communication efficient FL, which regards wireless communications as the bottleneck, we find that under many situations, the local computing delay is comparable to the communication delay during the FL training process, given the development of high-speed wireless transmission techniques. Thus, the service delay in FL should be computing delay + communication delay over training rounds. To minimize the service delay of FL, simply reducing local computing/communication delay independently is not enough. The delay trade-off between local computing and wireless communications must be considered. Besides, we empirically study the impacts of local computing control and compression strategies (i.e., the number of local updates, weight quantization, and gradient quantization) on computing, communication and service delays. Based on those trade-off observation and empirical studies, we develop an optimization scheme to minimize the service delay of FL over heterogeneous devices. We establish testbeds and conduct extensive emulations/experiments to verify our theoretical analysis. The results show that SDEFL reduces notable service delay with a small accuracy drop compared to peer designs. ",Kein DOI-Link verfügbar,2205.09868v1,Yes,notable(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Taming Diffusion Probabilistic Models for Character Control,1970,"  We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. Project page and source codes: https://aiganimation.github.io/CAMDM/ ",Kein DOI-Link verfügbar,2404.15121v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Correlation of the hidden-charm molecular tetraquarks and the   charmoniumlike structures existing in the $B\to XYZ+K$,1970,"  The molecular assignments to the three $P_c$ states and the similar production mechanism between the $\Lambda_b\to P_c+K$ and $B\to XYZ+K$ convince us the $B$ decaying to a charmonium state plus light mesons could be the appropriate production process to search for the charmoniumlike molecular tetraquarks. In this work, we systematically study the interactions between a charmed (charmed-strange) meson and an anti-charmed (anti-charm-strange) meson, which include the $D^{(*)}\bar{D}^{(*)}$, $\bar{D}^{(*)}\bar{D}_1$, $D^{(*)}\bar{D}_2^*$, $D_s^{(*)}\bar{D}_s^{(*)}$, ${D}_s^{(*)}\bar{D}_{s0}^*$, $D_s^{(*)}\bar{D}_{s1}^{\prime}$, ${D}_s^{(*)}\bar{D}_{s1}$, $D_s^{(*)}\bar{D}_{s2}^*$ systems. After adopting the one-boson-exchange effective potentials, our numerical results indicate that, on one hand, there can exist a serial of isoscalar charmoniumlike $\mathcal{D}\bar{\mathcal{D}}$ and $\mathcal{D}_s\bar{\mathcal{D}}_s$ molecular states, on the other hand, we can fully exclude the charged charmoniumlike states as the isovector charmoniumlike molecules. Meanwhile, we discuss the two-body hidden-charm decay channels for the obtained $\mathcal{D}\bar{\mathcal{D}}$ and $\mathcal{D}_s\bar{\mathcal{D}}_s$ molecules, especially the $D^{*}\bar{D}^{*}$ molecular tetraquarks. By analyzing the experimental data collected from the $B\to XYZ+K$ and the mass spectrum and two-body hidden-charm decay channels for the obtained $\mathcal{D}\bar{\mathcal{D}}$ and $\mathcal{D}_s\bar{\mathcal{D}}_s$ molecules, we find several possible hints of the existence of the charmoniumlike molecular tetraquarks, i.e., a peculiar characteristic mass spectrum of the isoscalar $D^*\bar{D}^*$ molecular systems can be applied to identify the charmoniumlike molecule. We look forward to the future experiments like the LHCb, Belle II, and BESIII Collaborations can test our results with more precise experimental data. ",https://doi.org/10.1103/PhysRevD.104.094010,2103.04698v3,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Predicting exotic molecular states composed of nucleon and P-wave   charmed meson,1970,"  In this work, we study the interaction between a nucleon and a $P$-wave charmed meson in the $T$ doublet by exchanging a pion. Our calculations indicate that a nucleon and a $P$-wave charmed meson with $J^P=0^+$ or $J^P=1^+$ in the $T$ doublet can form bound states. We propose the experimental search for these exotic molecular states near the $D_1(2420)N$ and $D_2^*(2460)N$ thresholds, where Belle, LHCb and the forthcoming Belle II have the discovery potential for them. ",https://doi.org/10.1103/PhysRevD.90.034011,1406.7481v3,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Predictions of the hidden-charm molecular states with four-quark   component,1970,"  In this work, we study the $T\bar{T}$-type molecular systems systematically via one pion exchange model, where $T$ denotes the narrow $J^P=1^+$ $D_1$ meson or $2^+$ $D_2^*$ meson and $\bar{T}$ is its antiparticle. With the effective potentials, we try to find the bound state solutions of the corresponding systems, which provide crucial information of whether there exist the $T\bar{T}$-type molecular states. According to our analysis, we predict some $T\bar{T}$-type molecular states which may be accessible at future experiments like LHCb and forthcoming BelleII. ",https://doi.org/10.1140/epjc/s10052-016-4166-x,1511.03439v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,"Orbital Angular Momentum Waves: Generation, Detection and Emerging   Applications",1970,"  Orbital angular momentum (OAM) has aroused a widespread interest in many fields, especially in telecommunications due to its potential for unleashing new capacity in the severely congested spectrum of commercial communication systems. Beams carrying OAM have a helical phase front and a field strength with a singularity along the axial center, which can be used for information transmission, imaging and particle manipulation. The number of orthogonal OAM modes in a single beam is theoretically infinite and each mode is an element of a complete orthogonal basis that can be employed for multiplexing different signals, thus greatly improving the spectrum efficiency. In this paper, we comprehensively summarize and compare the methods for generation and detection of optical OAM, radio OAM and acoustic OAM. Then, we represent the applications and technical challenges of OAM in communications, including free-space optical communications, optical fiber communications, radio communications and acoustic communications. To complete our survey, we also discuss the state of art of particle manipulation and target imaging with OAM beams. ",https://doi.org/10.1109/COMST.2019.2952453,1903.07818v3,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Growth and Properties of Dislocated Two-dimensional Layered Materials,1970,"  Two-dimensional (2D) layered materials hosting dislocations have attracted considerable research attention in recent years. In particular, screw dislocations can result in a spiral topology and an interlayer twist in the layered materials, significantly impacting the stacking order and symmetry of the layers. Moreover, the dislocations with large strain and heavily distorted atomic registry can result in a local modification of the structures around the dislocation. The dislocations thus provide a useful route to engineering optical, electrical, thermal, mechanical and catalytic properties of the 2D layered materials, which show great potential to bring new functionalities. This article presents a comprehensive review of the experimental and theoretical progress on the growth and properties of the dislocated 2D layered materials. It also offers an outlook on the future works in this promising research field. ",https://doi.org/10.1557/adv.2020.334,2008.00071v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered   Scenes,1970,"  Grasping is among the most fundamental and long-lasting problems in robotics study. This paper studies the problem of 6-DoF(degree of freedom) grasping by a parallel gripper in a cluttered scene captured using a commodity depth sensor from a single viewpoint. We address the problem in a learning-based framework. At the high level, we rely on a single-shot grasp proposal network, trained with synthetic data and tested in real-world scenarios. Our single-shot neural network architecture can predict amodal grasp proposal efficiently and effectively. Our training data synthesis pipeline can generate scenes of complex object configuration and leverage an innovative gripper contact model to create dense and high-quality grasp annotations. Experiments in synthetic and real environments have demonstrated that the proposed approach can outperform state-of-the-arts by a large margin. ",Kein DOI-Link verfügbar,1910.14218v1,Yes,innovative(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Towards Energy Efficient Federated Learning over 5G+ Mobile Devices,1970,"  The continuous convergence of machine learning algorithms, 5G and beyond (5G+) wireless communications, and artificial intelligence (AI) hardware implementation hastens the birth of federated learning (FL) over 5G+ mobile devices, which pushes AI functions to mobile devices and initiates a new era of on-device AI applications. Despite the remarkable progress made in FL, huge energy consumption is one of the most significant obstacles restricting the development of FL over battery-constrained 5G+ mobile devices. To address this issue, in this paper, we investigate how to develop energy efficient FL over 5G+ mobile devices by making a trade-off between energy consumption for ""working"" (i.e., local computing) and that for ""talking"" (i.e., wireless communications) in order to boost the overall energy efficiency. Specifically, we first examine energy consumption models for graphics processing unit (GPU) computation and wireless transmissions. Then, we overview the state of the art of integrating FL procedure with energy-efficient learning techniques (e.g., gradient sparsification, weight quantization, pruning, etc.). Finally, we present several potential future research directions for FL over 5G+ mobile devices from the perspective of energy efficiency. ",Kein DOI-Link verfügbar,2101.04866v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Rewritable Photonic Integrated Circuits Using Dielectric-assisted   Phase-change Material Waveguides,1970,"  Photonic integrated circuits (PICs) have the potential to drastically expand the capabilities of optical communications, sensing, and quantum information science and engineering. However, PICs are commonly fabricated using selective material etching, a subtractive process. Thus, the chip's functionality cannot be substantially altered once fabricated. Here, we propose to exploit wide-bandgap non-volatile phase-change materials (PCMs) to create a rewritable PIC platform. A PCM-based PIC can be written using a nano-second pulsed laser without removing any material, akin to rewritable compact disks. The whole circuit can then be erased by heating, and a completely new circuit can be rewritten. We designed a dielectric-assisted PCM waveguide consisting of a thick dielectric layer on top of a thin layer of wide-bandgap PCMs Sb2S3 and Sb2Se3. The low-loss PCMs and our engineered waveguiding structure lead to a negligible optical loss. Furthermore, we analyzed and specified the spatio-temporal laser pulse shape to write the PCMs. Our proposed platform will enable low-cost manufacturing and have a far-reaching impact on the rapid prototyping of PICs, validation of new designs, and photonic education. ",https://doi.org/10.1364/OL.486403,2301.10296v2,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,"Nonlocal, Flat Band Meta-optics for Monolithic, High Efficiency, Compact   Photodetectors",1970,"  Miniaturized photodetectors are becoming increasingly sought-after components for a range of next generation technologies, such as autonomous vehicles, integrated wearable devices, or gadgets embedded in the Internet of Things. A major challenge, however, lies in shrinking the device footprint, while maintaining high efficiency. This conundrum can be solved by realizing non-trivial relation between the energy and momentum of photons, such as dispersion-free angle-independent devices, known as flat bands. Here, we leverage flat band meta-optics to simultaneously achieve critical absorption over a wide range of incidence angles. For a monolithic silicon meta-optical photodiode, we achieved ~10-fold enhancement in the photon-to-electron conversion efficiency. Such enhancement over a large angular range of ~36 degrees allows incoming light to be collected via a large aperture lens and focused on a compact photodiode, potentially enabling high-speed and low-light operation. Our research unveils new possibilities for creating compact and efficient optoelectronic devices with far-reaching impact on various applications, including augmented reality and light detection and ranging. ",https://doi.org/10.1021/acs.nanolett.3c05139,2312.05395v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,A Promising Technology for 6G Wireless Networks: Intelligent Reflecting   Surface,1970,"  The intelligent information society, which is highly digitized, intelligence inspired and globally data driven, will be deployed in the next decade. The next 6G wireless communication networks are the key to achieve this grand blueprint, which is expected to connect everything, provide full dimensional wireless coverage and integrate all functions to support full-vertical applications. Recent research reveals that intelligent reflecting surface (IRS) with wireless environment control capability is a promising technology for 6G networks. Specifically, IRS can intelligently control the wavefront, e.g., the phase, amplitude, frequency, and even polarization by massive tunable elements, thus achieving fine-grained 3-D passive beamforming. In this paper, we first give a blueprint of the next 6G networks including the vision, typical scenarios and key performance indicators (KPIs). Then, we provide an overview of IRS including the new signal model, hardware architecture and competitive advantages in 6G networks. Besides, we discuss the potential application of IRS in the connectivity of 6G networks in detail, including intelligent and controllable wireless environment, ubiquitous connectivity, deep connectivity and holographic connectivity. At last, we summarize the challenges of IRS application and deployment in 6G networks. As a timely review of IRS, our summary will be of interest to both researchers and practitioners engaging in IRS for 6G networks. ",Kein DOI-Link verfügbar,2110.09114v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Probing new types of $P_c$ states inspired by the interaction between an   $S$-wave charmed baryon and an anticharmed meson in a $\bar T$ doublet state,1970,"  Inspired by the observations of three $P_c$ states, we systematically investigate interactions between an $S$-wave charmed baryon $\mathcal{B}_{c}^{(*)}=\Lambda_c/\Sigma_c/\Sigma_c^{*}$ and an anticharmed meson $\bar T=\bar D_1/\bar D_2^*$ with the one-pion-exchange potential model and the one-boson-exchange potential model, and search for possible new types of $P_c$ states with the structures of $\mathcal{B}_{c}^{(*)}\bar T$. Both $S$-$D$ wave mixing and coupled channel effects are considered. Our results suggest that in some $\mathcal{B}_{c}^{(*)}\bar T$ systems there are ideal candidates of new types of $P_c$ states, i.e., the $\Sigma_c\bar{D}_1$ state with $I(J^P)=1/2(1/2^+)$, the $\Sigma_c\bar{D}_2^*$ state with $I(J^P)=1/2(3/2^+)$, the $\Sigma_c^*\bar{D}_1$ state with $I(J^P)=1/2(1/2^+)$, and the $\Sigma_c^*\bar{D}_2^*$ states with $I(J^P)=1/2(1/2^+, 3/2^+)$, and we suggest that these predicted new types of $P_c$ states can be detected in the process $\Lambda_b^0 \to \psi(2S) p \pi^{-}$. Meanwhile, we also extend our study to the interactions between an $S$-wave charmed baryon and a charmed meson in a $T$ doublet, and we predict a series of double-charm molecular pentaquarks. ",https://doi.org/10.1103/PhysRevC.101.025201,1905.03636v4,Yes,potent(2)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Hidden-charm pentaquarks with triple strangeness due to the   $Ω_{c}^{(*)}\bar{D}_s^{(*)}$ interactions,1970,"  Motivated by the successful interpretation of these observed $P_c$ and $P_{cs}$ states under the meson-baryon molecular picture, we systematically investigate the possible hidden-charm molecular pentaquark states with triple strangeness which is due to the $\Omega_{c}^{(*)}\bar{D}_s^{(*)}$ interactions. We perform a dynamical calculation of the possible hidden-charm molecular pentaquarks with triple strangeness by the one-boson-exchange model, where the $S$-$D$ wave mixing effect and the coupled channel effect are taken into account in our calculation. Our results suggest that the $\Omega_{c}\bar D_s^*$ state with $J^P={3}/{2}^{-}$ and the $\Omega_{c}^{*}\bar D_s^*$ state with $J^P={5}/{2}^{-}$ can be recommended as the candidates of the hidden-charm molecular pentaquark with triple strangeness. Furthermore, we discuss the two-body hidden-charm strong decay behaviors of these possible hidden-charm molecular pentaquarks with triple strangeness by adopting the quark-interchange model. These predictions are expected to be tested at the LHCb, which can be as a potential research issue with more accumulated experimental data in near future. ",https://doi.org/10.1103/PhysRevD.103.054025,2101.11200v3,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,"Smart Roads: Roadside Perception, Vehicle-Road Cooperation and Business   Model",1970,"  Smart roads have become an essential component of intelligent transportation systems (ITS). The roadside perception technology, a critical aspect of smart roads, utilizes various sensors, roadside units (RSUs), and edge computing devices to gather real-time traffic data for vehicle-road cooperation. However, the full potential of smart roads in improving the safety and efficiency of autonomous vehicles only can be realized through the mass deployment of roadside perception and communication devices. On the one hand, roadside devices require significant investment but can only achieve monitoring function currently, resulting in no profitability for investors. On the other hand, drivers lack trust in the safety of autonomous driving technology, making it difficult to promote large-scale commercial applications. To deal with the dilemma of mass deployment, we propose a novel smart-road vehicle-guiding architecture for vehicle-road cooperative autonomous driving, based on which we then propose the corresponding business model and analyze its benefits from both operator and driver perspectives. The numerical simulations validate that our proposed smart road solution can enhance driving safety and traffic efficiency. Moreover, we utilize the cost-benefit analysis (CBA) model to assess the economic advantages of the proposed business model which indicates that the smart highway that can provide vehicle-guided-driving services for autonomous vehicles yields more profit than the regular highway. ",Kein DOI-Link verfügbar,2312.09439v1,Yes,potent(1)
0000-0002-6249-6780,Rui Chen,Tsinghua University,Part-Guided 3D RL for Sim2Real Articulated Object Manipulation,1970,"  Manipulating unseen articulated objects through visual feedback is a critical but challenging task for real robots. Existing learning-based solutions mainly focus on visual affordance learning or other pre-trained visual models to guide manipulation policies, which face challenges for novel instances in real-world scenarios. In this paper, we propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations. We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training. To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation. In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances. Experimental results demonstrate the effectiveness of our framework in both simulation and real-world settings. Our code is available at https://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation. ",Kein DOI-Link verfügbar,2404.17302v1,Yes,versatile(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Modeling Cognitive-Affective Processes with Appraisal and Reinforcement   Learning,1970,"  Computational models can advance affective science by shedding light onto the interplay between cognition and emotion from an information processing point of view. We propose a computational model of emotion that integrates reinforcement learning (RL) and appraisal theory, establishing a formal relationship between reward processing, goal-directed task learning, cognitive appraisal and emotional experiences. The model achieves this by formalizing evaluative checks from the component process model (CPM) in terms of temporal difference learning updates. We formalized novelty, goal relevance, goal conduciveness, and power. The formalization is task independent and can be applied to any task that can be represented as a Markov decision problem (MDP) and solved using RL. We investigated to what extent CPM-RL enables simulation of emotional responses cased by interactive task events. We evaluate the model by predicting a range of human emotions based on a series of vignette studies, highlighting its potential in improving our understanding of the role of reward processing in affective experiences. ",Kein DOI-Link verfügbar,2309.06367v2,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Non-Orthogonal Random Access (NORA) for 5G Networks,1970,"  The massive amounts of machine-type user equipments (UEs) will be supported in the future fifth generation (5G) networks. However, the potential large random access (RA) delay calls for a new RA scheme and for a detailed assessment of its performance. Motivated by the key idea of non-orthogonal multiple access, the non-orthogonal random access (NORA) scheme based on successive interference cancellation (SIC) is proposed in this paper to alleviate the access congestion problem. Specifically, NORA utilizes the difference of time of arrival to identify multiple UEs with the identical preamble, and enables power domain multiplexing of collided UEs in the following access process, while the base station performs SIC based on the channel conditions obtained through preamble detection. Our analysis show that the performance of NORA is superior to the conventional orthogonal random access (ORA) scheme in terms of the preamble collision probability, access success probability and throughput of random access. Simulation results verify our analysis and further show that our NORA scheme can improve the number of the supported UEs by more than 30%. Moreover, the number of preamble transmissions and the access delay for successfully accessed UEs are also reduced significantly by using the proposed random access scheme. ",Kein DOI-Link verfügbar,1705.01235v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Unified Framework for the Effective Rate Analysis of Wireless   Communication Systems over MISO Fading Channels,1970,"  This paper proposes a unified framework for the effective rate analysis over arbitrary correlated and not necessarily identical multiple inputs single output (MISO) fading channels, which uses moment generating function (MGF) based approach and H transform representation. The proposed framework has the potential to simplify the cumbersome analysis procedure compared to the probability density function (PDF) based approach. Moreover, the effective rates over two specific fading scenarios are investigated, namely independent but not necessarily identical distributed (i.n.i.d.) MISO hyper Fox's H fading channels and arbitrary correlated generalized K fading channels. The exact analytical representations for these two scenarios are also presented. By substituting corresponding parameters, the effective rates in various practical fading scenarios, such as Rayleigh, Nakagami-m, Weibull/Gamma and generalized K fading channels, are readily available. In addition, asymptotic approximations are provided for the proposed H transform and MGF based approach as well as for the effective rate over i.n.i.d. MISO hyper Fox's H fading channels. Simulations under various fading scenarios are also presented, which support the validity of the proposed method. ",Kein DOI-Link verfügbar,1612.03882v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Coverage Probability Analysis of IRS-Aided Communication Systems,1970,"  The intelligent reflective surface (IRS) technology has received many interests in recent years, thanks to its potential uses in future wireless communications, in which one of the promising use cases is to widen coverage, especially in the line-of-sight-blocked scenarios. Therefore, it is critical to analyze the corresponding coverage probability of IRS-aided communication systems. To our best knowledge, however, previous works focusing on this issue are very limited. In this paper, we analyze the coverage probability under the Rayleigh fading channel, taking the number and size of the array elements into consideration. We first derive the exact closed-form of coverage probability for the unit element. Afterward, with the method of moment matching, the approximation of the coverage probability can be formulated as the ratio of upper incomplete Gamma function and Gamma function, allowing an arbitrary number of elements. Finally, we comprehensively evaluate the impacts of essential factors on the coverage probability, such as the coefficient of fading channel, the number and size of the element, and the angle of incidence. Overall, the paper provides a succinct and general expression of coverage probability, which can be helpful in the performance evaluation and practical implementation of the IRS. ",https://doi.org/10.1109/TVT.2021.3063408,2012.03171v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,On the Multivariate Gamma-Gamma ($ΓΓ$) Distribution with   Arbitrary Correlation and Applications in Wireless Communications,1970,"  The statistical properties of the multivariate Gamma-Gamma ($\Gamma \Gamma$) distribution with arbitrary correlation have remained unknown. In this paper, we provide analytical expressions for the joint probability density function (PDF), cumulative distribution function (CDF) and moment generation function of the multivariate $\Gamma \Gamma$ distribution with arbitrary correlation. Furthermore, we present novel approximating expressions for the PDF and CDF of the sum of $\Gamma \Gamma$ random variables with arbitrary correlation. Based on this statistical analysis, we investigate the performance of radio frequency and optical wireless communication systems. It is noteworthy that the presented expressions include several previous results in the literature as special cases. ",https://doi.org/10.1109/TVT.2015.2438192,1505.06685v1,Yes,noteworthy(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,A Survey on User-Centric Cell-Free Massive MIMO Systems,1970,"  The mobile data traffic has been exponentially growing during the last decades, which has been enabled by the densification of the network infrastructure in terms of increased cell density (i.e., ultra-dense network (UDN)) and/or increased number of active antennas per access point (AP) (i.e., massive multiple-input multiple-output (mMIMO)). However, neither UDN nor mMIMO will meet the increasing data rate demands of the sixth generation (6G) wireless communications due to the inter-cell interference and large quality-of-service variations, respectively. Cell-free (CF) mMIMO, which combines the best aspects of UDN and mMIMO, is viewed as a key solution to this issue. In such systems, each user equipment (UE) is served by a preferred set of surrounding APs cooperatively. In this paper, we provide a survey of the state-of-the-art literature on CF mMIMO. As a starting point, the significance and the basic properties of CF mMIMO are highlighted. We then present the canonical framework, where the essential details (i.e., transmission procedure and mathematical system model) are discussed. Next, we provide a deep look at the resource allocation and signal processing problems related to CF mMIMO and survey the up-to-date schemes and algorithms. After that, we discuss the practical issues when implementing CF mMIMO. Potential future directions are then pointed out. Finally, we conclude this paper with a summary of the key lessons learned in this field. This paper aims to provide a starting point for anyone who wants to conduct research on CF mMIMO for future wireless networks. ",Kein DOI-Link verfügbar,2104.13667v2,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Double-Layer Power Control for Mobile Cell-Free XL-MIMO with Multi-Agent   Reinforcement Learning,1970,"  Cell-free (CF) extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as a promising technology for enabling future wireless communication systems. Significant attention has been generated by its considerable advantages in augmenting degrees of freedom. In this paper, we first investigate a CF XL-MIMO system with base stations equipped with XL-MIMO panels under a dynamic environment. Then, we propose an innovative multi-agent reinforcement learning (MARL)-based power control algorithm that incorporates predictive management and distributed optimization architecture, which provides a dynamic strategy for addressing high-dimension signal processing problems. Specifically, we compare various MARL-based algorithms, which shows that the proposed MARL-based algorithm effectively strikes a balance between spectral efficiency (SE) performance and convergence time. Moreover, we consider a double-layer power control architecture based on the large-scale fading coefficients between antennas to suppress interference within dynamic systems. Compared to the single-layer architecture, the results obtained unveil that the proposed double-layer architecture has a nearly24% SE performance improvement, especially with massive antennas and smaller antenna spacing. ",Kein DOI-Link verfügbar,2309.17079v1,Yes,innovative(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Cell-Free Massive MIMO-OFDM for High-Speed Train Communications,1970,"  Cell-free (CF) massive multiple-input multiple-output (MIMO) systems show great potentials in low-mobility scenarios, due to cell boundary disappearance and strong macro diversity. However, the great Doppler frequency offset (DFO) leads to serious inter-carrier interference in orthogonal frequency division multiplexing (OFDM) technology, which makes it difficult to provide high-quality transmissions for both high-speed train (HST) operation control systems and passengers. In this paper, we focus on the performance of CF massive MIMO-OFDM systems with both fully centralized and local minimum mean square error (MMSE) combining in HST communications. Considering the local maximum ratio (MR) combining, the large-scale fading decoding (LSFD) cooperation and the practical effect of DFO on system performance, exact closed-form expressions for uplink spectral efficiency (SE) expressions are derived. We observe that cooperative MMSE combining achieves better SE performance than uncooperative MR combining. In addition, HST communications with small cell and cellular massive MIMO-OFDM systems are compared in terms of SE. Numerical results reveal that the CF massive MIMO-OFDM system achieves a larger and more uniform SE than the other systems. Finally, the train antenna centric (TA-centric) CF massive MIMO-OFDM system is designed for practical implementation in HST communications, and three power control schemes are adopted to optimize the propagation of TAs for reducing the impact of the DFO. ",Kein DOI-Link verfügbar,2203.00900v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Novel Device-to-Device Discovery Scheme based on Random Backoff in   LTE-Advanced Networks,1970,"  Device-to-Device (D2D) discovery is a key enabler of D2D communications for the direct exchange of local area traffic between proximity users (UEs) to improve spectral efficiency. The direct D2D discovery relies on the capabilities of the D2D UEs to autonomously indicate their presence to proximity D2D UEs. {Despite} its potential of reducing energy and signalling burden, the direct D2D discovery {has not drawn adequate attention}. In this paper, we propose a direct D2D discovery scheme based on the random backoff procedure, where D2D UEs randomly choose a backoff interval and retransmit a beacon. Compared with existing schemes, the performance of the proposed scheme can be significantly enhanced in terms of the discovery probability and the discovery delay. Several useful guidelines for its design are proposed based on our analysis. Finally, numerical results provide valuable insights on the performance tradeoff inherent to the proposed D2D discovery scheme. ",Kein DOI-Link verfügbar,1707.03576v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,On Low-Resolution ADCs in Practical 5G Millimeter-Wave Massive MIMO   Systems,1970,"  Nowadays, millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems is a favorable candidate for the fifth generation (5G) cellular systems. However, a key challenge is the high power consumption imposed by its numerous radio frequency (RF) chains, which may be mitigated by opting for low-resolution analog-to-digital converters (ADCs), whilst tolerating a moderate performance loss. In this article, we discuss several important issues based on the most recent research on mmWave massive MIMO systems relying on low-resolution ADCs. We discuss the key transceiver design challenges including channel estimation, signal detector, channel information feedback and transmit precoding. Furthermore, we introduce a mixed-ADC architecture as an alternative technique of improving the overall system performance. Finally, the associated challenges and potential implementations of the practical 5G mmWave massive MIMO system {with ADC quantizers} are discussed. ",https://doi.org/10.1109/MCOM.2018.1600731,1803.07384v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Joint Cooperative Clustering and Power Control for Energy-Efficient   Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning,1970,"  In this paper, we investigate the amalgamation of cell-free (CF) and extremely large-scale multiple-input multiple-output (XL-MIMO) technologies, referred to as a CF XL-MIMO, as a promising advancement for enabling future mobile networks. To address the computational complexity and communication power consumption associated with conventional centralized optimization, we focus on user-centric dynamic networks in which each user is served by an adaptive subset of access points (AP) rather than all of them. We begin our research by analyzing a joint resource allocation problem for energy-efficient CF XL-MIMO systems, encompassing cooperative clustering and power control design, where all clusters are adaptively adjustable. Then, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme, which offers an effective strategy to tackle the challenges of high-dimensional signal processing. In the section of numerical results, we compare various algorithms with different network architectures. These comparisons reveal that the proposed MARL-based cooperative architecture can effectively strike a balance between system performance and communication overhead, thereby improving energy efficiency performance. It is important to note that increasing the number of user equipments participating in information sharing can effectively enhance SE performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the number of participants and EE performance. ",Kein DOI-Link verfügbar,2406.05481v2,Yes,innovative(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,C3KG: A Chinese Commonsense Conversation Knowledge Graph,1970,"  Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense knowledge and dialog flow information. To show the potential of our graph, we develop a graph-conversation matching approach, and benchmark two graph-grounded conversational tasks. ",Kein DOI-Link verfügbar,2204.02549v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Wireless Energy Transfer in RIS-Aided Cell-Free Massive MIMO Systems:   Opportunities and Challenges,1970,"  In future sixth-generation (6G) mobile networks, the Internet-of-Everything (IoE) is expected to provide extremely massive connectivity for small battery-powered devices. Indeed, massive devices with limited energy storage capacity impose persistent energy demand hindering the lifetime of communication networks. As a remedy, wireless energy transfer (WET) is a key technology to address these critical energy supply issues. On the other hand, cell-free (CF) massive multiple-input multiple-output (MIMO) systems offer an efficient network architecture to realize the roll-out of the IoE. In this article, we first propose the paradigm of reconfigurable intelligent surface (RIS)-aided CF massive MIMO systems for WET, including its potential application scenarios and system architecture. The four-stage transmission procedure is discussed and analyzed to illustrate the practicality of the architecture. Then we put forward and analyze the hardware design of RIS. Particularly, we discuss the three corresponding operating modes and the amalgamation of WET technology and RIS-aided CF massive MIMO. Representative simulation results are given to confirm the superior performance achieved by our proposed schemes. Also, we investigate the optimal location of deploying multiple RISs to achieve the best system performance. Finally, several important research directions of RIS-aided CF massive MIMO systems with WET are presented to inspire further potential investigation. ",Kein DOI-Link verfügbar,2201.11302v2,Yes,potent(2)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Channel Estimation for XL-MIMO Systems with Polar-Domain Multi-Scale   Residual Dense Network,1970,"  Extremely large-scale multiple-input multiple-output (XL-MIMO) is a promising technique to enable versatile applications for future wireless communications.To realize the huge potential performance gain, accurate channel state information is a fundamental technical prerequisite. In conventional massive MIMO, the channel is often modeled by the far-field planar-wavefront with rich sparsity in the angular domain that facilitates the design of low-complexity channel estimation. However, this sparsity is not conspicuous in XL-MIMO systems due to the non-negligible near-field spherical-wavefront. To address the inherent performance loss of the angular-domain channel estimation schemes, we first propose the polar-domain multiple residual dense network (P-MRDN) for XL-MIMO systems based on the polar-domain sparsity of the near-field channel by improving the existing MRDN scheme. Furthermore, a polar-domain multi-scale residual dense network (P-MSRDN) is designed to improve the channel estimation accuracy. Finally, simulation results reveal the superior performance of the proposed schemes compared with existing benchmark schemes and the minimal influence of the channel sparsity on the proposed schemes. ",Kein DOI-Link verfügbar,2308.16400v2,Yes,"versatile(1), potent(1)"
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,"Generative AI Agent for Next-Generation MIMO Design: Fundamentals,   Challenges, and Vision",1970,"  Next-generation multiple input multiple output (MIMO) is expected to be intelligent and scalable. In this paper, we study generative artificial intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we provide an overview of the development, fundamentals, and challenges of the next-generation MIMO. Then, we propose the concept of the generative AI agent, which is capable of generating tailored and specialized contents with the aid of large language model (LLM) and retrieval augmented generation (RAG). Next, we comprehensively discuss the features and advantages of the generative AI agent framework. More importantly, to tackle existing challenges of next-generation MIMO, we discuss generative AI agent-enabled next-generation MIMO design, from the perspective of performance analysis, signal processing, and resource allocation. Furthermore, we present two compelling case studies that demonstrate the effectiveness of leveraging the generative AI agent for performance analysis in complex configuration scenarios. These examples highlight how the integration of generative AI agents can significantly enhance the analysis and design of next-generation MIMO systems. Finally, we discuss important potential research future directions. ",Kein DOI-Link verfügbar,2404.08878v1,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,"Flexible-Position MIMO for Wireless Communications: Fundamentals,   Challenges, and Future Directions",1970,"  The flexible-position multiple-input multiple-output (FLP-MIMO), such as fluid antennas and movable antennas, is a promising technology for future wireless communications. This is due to the fact that the positions of antennas at the transceiver and reflector can be dynamically optimized to achieve better channel conditions and, as such, can provide high spectral efficiency (SE) and energy efficiency (EE) gains with fewer antennas. In this article, we introduce the fundamentals of FLP-MIMO systems, including hardware design, structure design, and potential applications. We shall demonstrate that FLP-MIMO, using fewer flexible antennas, can match the channel hardening achieved by a large number of fixed antennas. We will then analyze the SE-EE relationship for FLP-MIMO and fixed-position MIMO. Furthermore, we will design the optimal trajectory of flexible antennas to maximize system sum SE or total EE at a fixed travel distance of each antenna. Finally, several important research directions regarding FLP-MIMO communications are presented to facilitate further investigation. ",Kein DOI-Link verfügbar,2308.14578v2,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Joint SIM Configuration and Power Allocation for Stacked Intelligent   Metasurface-assisted MU-MISO Systems with TD3,1970,"  The stacked intelligent metasurface (SIM) emerges as an innovative technology with the ability to directly manipulate electromagnetic (EM) wave signals, drawing parallels to the operational principles of artificial neural networks (ANN). Leveraging its structure for direct EM signal processing alongside its low-power consumption, SIM holds promise for enhancing system performance within wireless communication systems. In this paper, we focus on SIM-assisted multi-user multi-input and single-output (MU-MISO) system downlink scenarios in the transmitter. We proposed a joint optimization method for SIM phase shift configuration and antenna power allocation based on the twin delayed deep deterministic policy gradient (TD3) algorithm to efficiently improve the sum rate. The results show that the proposed algorithm outperforms both deep deterministic policy gradient (DDPG) and alternating optimization (AO) algorithms. Furthermore, increasing the number of meta-atoms per layer of the SIM is always beneficial. However, continuously increasing the number of layers of SIM does not lead to sustained performance improvement. ",Kein DOI-Link verfügbar,2408.05756v1,Yes,innovative(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Generative AI-aided Joint Training-free Secure Semantic Communications   via Multi-modal Prompts,1970,"  Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decoding. Moreover, in response to security concerns, we introduce the application of covert communications aided by a friendly jammer. The system jointly optimizes the diffusion step, jamming, and transmitting power with the aid of the generative diffusion models, enabling successful and secure transmission of the source messages. ",Kein DOI-Link verfügbar,2309.02616v1,Yes,"notable(1), potent(1)"
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,The VEP Booster: A Closed-Loop AI System for Visual EEG Biomarker   Auto-generation,1970,"  Effective visual brain-machine interfaces (BMI) is based on reliable and stable EEG biomarkers. However, traditional adaptive filter-based approaches may suffer from individual variations in EEG signals, while deep neural network-based approaches may be hindered by the non-stationarity of EEG signals caused by biomarker attenuation and background oscillations. To address these challenges, we propose the Visual Evoked Potential Booster (VEP Booster), a novel closed-loop AI framework that generates reliable and stable EEG biomarkers under visual stimulation protocols. Our system leverages an image generator to refine stimulus images based on real-time feedback from human EEG signals, generating visual stimuli tailored to the preferences of primary visual cortex (V1) neurons and enabling effective targeting of neurons most responsive to stimuli. We validated our approach by implementing a system and employing steady-state visual evoked potential (SSVEP) visual protocols in five human subjects. Our results show significant enhancements in the reliability and utility of EEG biomarkers for all individuals, with the largest improvement in SSVEP response being 105%, the smallest being 28%, and the average increase being 76.5%. These promising results have implications for both clinical and technological applications ",Kein DOI-Link verfügbar,2407.15167v1,Yes,potent(2)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Learning Audio-Visual Source Localization via False Negative Aware   Contrastive Learning,1970,"  Self-supervised audio-visual source localization aims to locate sound-source objects in video frames without extra annotations. Recent methods often approach this goal with the help of contrastive learning, which assumes only the audio and visual contents from the same video are positive samples for each other. However, this assumption would suffer from false negative samples in real-world training. For example, for an audio sample, treating the frames from the same audio class as negative samples may mislead the model and therefore harm the learned representations e.g., the audio of a siren wailing may reasonably correspond to the ambulances in multiple images). Based on this observation, we propose a new learning strategy named False Negative Aware Contrastive (FNAC) to mitigate the problem of misleading the training with such false negative samples. Specifically, we utilize the intra-modal similarities to identify potentially similar samples and construct corresponding adjacency matrices to guide contrastive learning. Further, we propose to strengthen the role of true negative samples by explicitly leveraging the visual features of sound sources to facilitate the differentiation of authentic sounding source regions. FNAC achieves state-of-the-art performances on Flickr-SoundNet, VGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in mitigating the false negative issue. The code is available at \url{https://github.com/OpenNLPLab/FNAC_AVL}. ",Kein DOI-Link verfügbar,2303.11302v2,Yes,potent(1)
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,"A Tutorial on Extremely Large-Scale MIMO for 6G: Fundamentals, Signal   Processing, and Applications",1970,"  Extremely large-scale multiple-input-multiple-output (XL-MIMO), which offers vast spatial degrees of freedom, has emerged as a potentially pivotal enabling technology for the sixth generation (6G) of wireless mobile networks. With its growing significance, both opportunities and challenges are concurrently manifesting. This paper presents a comprehensive survey of research on XL-MIMO wireless systems. In particular, we introduce four XL-MIMO hardware architectures: uniform linear array (ULA)-based XL-MIMO, uniform planar array (UPA)-based XL-MIMO utilizing either patch antennas or point antennas, and continuous aperture (CAP)-based XL-MIMO. We comprehensively analyze and discuss their characteristics and interrelationships. Following this, we introduce several electromagnetic characteristics and general distance boundaries in XL-MIMO. Given the distinct electromagnetic properties of near-field communications, we present a range of channel models to demonstrate the benefits of XL-MIMO. We further discuss and summarize signal processing schemes for XL-MIMO. It is worth noting that the low-complexity signal processing schemes and deep learning empowered signal processing schemes are reviewed and highlighted to promote the practical implementation of XL-MIMO. Furthermore, we explore the interplay between XL-MIMO and other emergent 6G technologies. Finally, we outline several compelling research directions for future XL-MIMO wireless communication systems. ",Kein DOI-Link verfügbar,2307.07340v2,Yes,"pivotal(1), potent(1)"
0000-0001-9416-1523,Jiayi Zhang,Tsinghua University,Data Interpreter: An LLM Agent For Data Science,1970,"  Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT. ",Kein DOI-Link verfügbar,2402.18679v3,Yes,pivotal(1)
0000-0002-0918-4630,Dan Nicolae,"The University of Chicago, University of Chicago",A functional data analysis approach for genetic association studies,1970,"  We present a new method based on Functional Data Analysis (FDA) for detecting associations between one or more scalar covariates and a longitudinal response, while correcting for other variables. Our methods exploit the temporal structure of longitudinal data in ways that are otherwise difficult with a multivariate approach. Our procedure, from an FDA perspective, is a departure from more established methods in two key aspects. First, the raw longitudinal phenotypes are assembled into functional trajectories prior to analysis. Second, we explore an association test that is not directly based on principal components. We instead focus on quantifying the reduction in $L^2$ variability as a means of detecting associations. Our procedure is motivated by longitudinal genome wide association studies and, in particular, the childhood asthma management program (CAMP) which explores the long term effects of daily asthma treatments. We conduct a simulation study to better understand the advantages (and/or disadvantages) of an FDA approach compared to a traditional multivariate one. We then apply our methodology to data coming from CAMP. We find a potentially new association with a SNP negatively affecting lung function. Furthermore, this SNP seems to have an interaction effect with one of the treatments. ",https://doi.org/10.1214/13-AOAS692,1404.7301v1,Yes,potent(1)
0000-0002-1956-1807,Supriya Ghosh,"The University of Chicago, University of Chicago",Challenges to magnetic doping of thin films of the Dirac semimetal   Cd$_3$As$_2$,1970,"  Magnetic doping of topological quantum materials provides an attractive route for studying the effects of time-reversal symmetry breaking. Thus motivated, we explore the introduction of the transition metal Mn into thin films of the Dirac semimetal Cd3As2 during growth by molecular beam epitaxy. Scanning transmission electron microscopy measurements show the formation of a Mn-rich phase at the top surface of Mn-doped Cd3As2 thin films grown using both uniform doping and delta doping. This suggests that Mn acts as a surfactant during epitaxial growth of Cd3As2, resulting in phase separation. Magnetometry measurements of such samples indicate a ferromagnetic phase with out-of-plane magnetic anisotropy. Electrical magneto-transport measurements of these films as a function of temperature, magnetic field, and chemical potential reveal a lower carrier density and higher electron mobility compared to pristine Cd3As2 films grown under similar conditions. This suggests that the surfactant effect might also serve to getter impurities. We observe robust quantum transport (Shubnikov-de Haas oscillations and an incipient integer quantum Hall effect) in very thin (7 nm) Cd3As2 films despite being in direct contact with a structurally disordered surface ferromagnetic overlayer. ",https://doi.org/10.1103/PhysRevMaterials.6.024203,2112.12117v1,Yes,potent(1)
0000-0002-1956-1807,Supriya Ghosh,"The University of Chicago, University of Chicago",Rashba spin splitting-induced topological Hall effect in a Dirac   semimetal-ferromagnetic semiconductor heterostructure,1970,"  We use a concerted theory-experiment effort to investigate the formation of chiral real space spin texture when the archetypal Dirac semimetal Cd$_3$As$_2$ is interfaced with In$_{1-x}$Mn$_x$As, a ferromagnetic semiconductor with perpendicular magnetic anisotropy. Our calculations reveal a nonzero off-diagonal spin susceptibility in the Cd$_3$As$_2$ layer due to the Rashba spin-orbit coupling from broken inversion symmetry. This implies the presence of a Dzyaloshinskii-Moriya interaction between local moments in the In$_{1-x}$Mn$_x$As layer, mediated by Dirac electrons in the vicinal Cd$_3$As$_2$ layer, potentially creating the conditions for a real space chiral spin texture. Using electrical magnetoresistance measurements at low temperature, we observe an emergent excess contribution to the transverse magneto-resistance whose behavior is consistent with a topological Hall effect arising from the formation of an interfacial chiral spin texture. This excess Hall voltage varies with gate voltage, indicating a promising electrostatically-tunable platform for understanding the interplay between the helical momentum space states of a Dirac semimetal and chiral real space spin textures in a ferromagnet. ",Kein DOI-Link verfügbar,2403.18485v1,Yes,potent(1)
0000-0002-1956-1807,Supriya Ghosh,"The University of Chicago, University of Chicago",Room-Temperature Valence Transition in a Strain-Tuned Perovskite Oxide,1970,"  Cobalt oxides have long been understood to display intriguing phenomena known as spin-state crossovers, where the cobalt ion spin changes vs. temperature, pressure, etc. A very different situation was recently uncovered in praseodymium-containing cobalt oxides, where a first-order coupled spin-state/structural/metal-insulator transition occurs, driven by a remarkable praseodymium valence transition. Such valence transitions, particularly when triggering spin-state and metal-insulator transitions, offer highly appealing functionality, but have thus far been confined to cryogenic temperatures in bulk materials (e.g., 90 K in Pr1-xCaxCoO3). Here, we show that in thin films of the complex perovskite (Pr1-yYy)1-xCaxCoO3-{\delta}, heteroepitaxial strain tuning enables stabilization of valence-driven spin-state/structural/metal-insulator transitions to at least 291 K, i.e., around room temperature. The technological implications of this result are accompanied by fundamental prospects, as complete strain control of the electronic ground state is demonstrated, from ferromagnetic metal under tension to nonmagnetic insulator under compression, thereby exposing a potential novel quantum critical point. ",https://doi.org/10.1038/s41467-022-35024-8,2112.10917v2,Yes,potent(1)
0000-0003-4075-6139,Jessica Lu,"The University of Chicago, University of Chicago Medicine",US Adaptive Optics Roadmap to Achieve Astro2020,1970,"  In the recent Astro2020 Decadal Report, ''Pathways to Discovery in Astronomy and Astrophysics for the 2020s'' Adaptive Optics (AO) was identified as a crucial technology for a variety of reasons. These included an emphasis on high-contrast imaging and AO systems as being part of future technology development especially with application to the two US ELT projects. Instrument upgrades were also identified for existing 4m to 10m class telescopes which would incorporate upgrades to existing AO systems. As noted in the Report: (1) ''the central role of AO instrumentation and the importance of further development are rapidly growing, with novel concepts pushing toward wider area'', (2) ''Visible AO has high potential scientific return by opening up an entire wavelength regime to high angular resolution studies. The goal is to exploit the smaller diffraction limit of telescopes in the optical, yet both the coherence length and time decrease at shorter wavelengths requiring wavefront sensing at high spatial and temporal frequencies that are currently technologically challenging. This is an important developing area for the 2020s - 2030s.'', and (3) ''Such investments in AO systems development is a key risk mitigation strategy for ELTs, whose full resolution and sensitivity potential can only be realized with AO, and which is recognized as the most important technical risk for both GMT and TMT''.   A workshop was held in May, 2023 to develop a Community Response document (this document) to provide feedback and suggested priorities to various funding agencies, such as NSF, NASA, and DoE, as to the AO Research and Development priorities to meet the technical and science objectives outlined in Astro2020 for ground-based AO, both stand-alone and in support of space missions. ",Kein DOI-Link verfügbar,2402.06743v1,Yes,potent(2)
0000-0002-2646-7042,Rohan Shah,"University of Chicago, University of Chicago College, University of Chicago Pritzker School of Medicine",ExoMol molecular line lists - XXVI: spectra of SH and NS,1970,"  Line lists for the sulphur-containing molecules SH (the mercapto radical) and NS are computed as part of the ExoMol project. These line lists consider transitions within the $X$ ${}^2\Pi$ ground state for $^{32}$SH, $^{33}$SH, $^{34}$SH and $^{\text{32}}$SD, and $^{14}$N$^{32}$S, $^{14}$N$^{33}$S, $^{14}$N$^{34}$S, $^{14}$N$^{36}$S and $^{15}$N$^{32}$S. Ab initio potential energy (PEC) and spin-orbit coupling (SOC) curves are computed and then improved by fitting to experimentally observed transitions. Fully ab initio dipole moment curves (DMCs) computed at high level of theory are used to produce the final line lists. For SH, our fit gives a root-mean-square (rms) error of 0.03 cm$^{-1}$ between the observed ($v_{\rm max}=4$, $J_{\rm max} = 34.5$) and calculated transitions wavenumbers; this is extrapolated such that all $X$ $^2\Pi$ rotational-vibrational-electronic (rovibronic) bound states are considered. For $^{\text{32}}$SH the resulting line list contains about 81000 transitions and 2300 rovibronic states, considering levels up to $v_{\rm max} = 14$ and $J_{\rm max} = 60.5$. For NS the refinement used a combination of experimentally determined frequencies and energy levels and led to an rms fitting error of 0.002 cm$^{-1}$. Each NS calculated line list includes around 2.8 million transitions and 31000 rovibronic states with a vibrational range up to $v=53$ and rotational range to $J=235.5$, which covers up to 23000 cm$^{-1}$. Both line lists should be complete for temperatures up to 5000 K. Example spectra simulated using this line list are shown and comparisons made to the existing data in the CDMS database. The line lists are available from the CDS (http://cdsarc.u-strasbg.fr) and ExoMol (www.exomol.com) data bases. ",https://doi.org/10.1093/mnras/sty939,1803.09724v1,Yes,potent(1)
0000-0003-2278-2835,Grace Su,"University of Chicago, University of Chicago Consortium on School Research, University of Chicago Inclusive Economy Lab",DeepScribe: Localization and Classification of Elamite Cuneiform Signs   Via Deep Learning,1970,"  Twenty-five hundred years ago, the paperwork of the Achaemenid Empire was recorded on clay tablets. In 1933, archaeologists from the University of Chicago's Oriental Institute (OI) found tens of thousands of these tablets and fragments during the excavation of Persepolis. Many of these tablets have been painstakingly photographed and annotated by expert cuneiformists, and now provide a rich dataset consisting of over 5,000 annotated tablet images and 100,000 cuneiform sign bounding boxes. We leverage this dataset to develop DeepScribe, a modular computer vision pipeline capable of localizing cuneiform signs and providing suggestions for the identity of each sign. We investigate the difficulty of learning subtasks relevant to cuneiform tablet transcription on ground-truth data, finding that a RetinaNet object detector can achieve a localization mAP of 0.78 and a ResNet classifier can achieve a top-5 sign classification accuracy of 0.89. The end-to-end pipeline achieves a top-5 classification accuracy of 0.80. As part of the classification module, DeepScribe groups cuneiform signs into morphological clusters. We consider how this automatic clustering approach differs from the organization of standard, printed sign lists and what we may learn from it. These components, trained individually, are sufficient to produce a system that can analyze photos of cuneiform tablets from the Achaemenid period and provide useful transliteration suggestions to researchers. We evaluate the model's end-to-end performance on locating and classifying signs, providing a roadmap to a linguistically-aware transliteration system, then consider the model's potential utility when applied to other periods of cuneiform writing. ",Kein DOI-Link verfügbar,2306.01268v1,Yes,potent(1)
0000-0003-2767-2379,Dylan Hatt,University of Chicago,Velocity and mass bias in the distribution of dark matter halos,1970,"  The non-linear, scale-dependent bias in the mass distribution of galaxies and the underlying dark matter is a key systematic affecting the extraction of cosmological parameters from galaxy clustering. Using 95 million halos from the Millennium-XXL N-body simulation, we find that the mass bias is scale independent only for $k<0.1 h{\rm Mpc}^{-1}$ today ($z=0$) and for $k<0.2 h{\rm Mpc}^{-1}$ at $z=0.7$. We test analytic halo bias models against our simulation measurements and find that the model of Tinker et al. 2005 is accurate to better then 5% at $z=0$. However, the simulation results are better fit by an ellipsoidal collapse model at $z=0.7$. We highlight, for the first time, another potentially serious systematic due to a sampling bias in the halo velocity divergence power spectra which will affect the comparison between observations and any redshift space distortion model which assumes dark matter velocity statistics with no velocity bias. By measuring the velocity divergence power spectra for different sized halo samples, we find that there is a significant bias which increases with decreasing number density. This bias is approximately 20% at $k=0.1h$Mpc$^{-1}$ for a halo sample of number density $\bar{n} = 10^{-3} (h/$Mpc$)^3$ at both $z=0$ and $z=0.7$ for the velocity divergence auto power spectrum. Given the importance of redshift space distortions as a probe of dark energy and the on-going major effort to advance models for the clustering signal in redshift space, our results show this velocity bias introduces another systematic, alongside scale-dependent halo mass bias, which cannot be neglected. ",https://doi.org/10.1093/mnras/stu2043,1407.7296v1,Yes,potent(1)
0000-0003-2767-2379,Dylan Hatt,University of Chicago,The Carnegie-Chicago Hubble Program. II. The Distance to IC 1613: The   Tip of the Red Giant Branch and RR Lyrae Period-Luminosity Relations,1970,"  IC 1613 is an isolated dwarf galaxy within the Local Group. Low foreground and internal extinction, low metallicity, and low crowding make it an invaluable testbed for the calibration of the local distance ladder. We present new, high-fidelity distance estimates to IC 1613 via its Tip of the Red Giant Branch (TRGB) and its RR Lyrae (RRL) variables as part of the Carnegie-Chicago Hubble Program, which seeks an alternate local route to \ho using Population II stars. We have measured a TRGB magnitude I=20.35+/-0.01 (statistical)+/-0.01 (systematic) using wide-field observations obtained from the IMACS camera on the Magellan-Baade telescope. We have further constructed optical and near-infrared RRL light curves using archival BI- and new H- band observations from the ACS/WFC and WFC3/IR instruments aboard the Hubble Space Telescope (HST). In advance of future Gaia data releases, we set provisional values for the TRGB luminosity via the Large Magellanic Cloud and Galactic RRL zero-points via HST parallaxes. We find corresponding true distance moduli \mu(TRGB)=24.30+/-0.03 (statistical)+/-0.05 (systematic) and \mu(RRL)=24.28+/-0.04 (statistical+systematic). We compare our results to a body of recent publications on IC 1613 and find no statistically significant difference between the distances derived from stars of Population I and II. ",https://doi.org/10.3847/1538-4357/aa7f73,1703.06468v2,Yes,invaluable(1)
0000-0003-2767-2379,Dylan Hatt,University of Chicago,The Carnegie-Chicago Hubble Program. VIII. An Independent Determination   of the Hubble Constant Based on the Tip of the Red Giant Branch,1970,"  We present a new and independent determination of the local value of the Hubble constant based on a calibration of the Tip of the Red Giant Branch (TRGB) applied to Type Ia supernovae (SNeIa). We find a value of Ho = 69.8 +/- 0.8 (+/-1.1\% stat) +/- 1.7 (+/-2.4\% sys) km/sec/Mpc. The TRGB method is both precise and accurate, and is parallel to, but independent of the Cepheid distance scale. Our value sits midway in the range defined by the current Hubble tension. It agrees at the 1.2-sigma level with that of the Planck 2018 estimate, and at the 1.7-sigma level with the SHoES measurement of Ho based on the Cepheid distance scale. The TRGB distances have been measured using deep Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) imaging of galaxy halos. The zero point of the TRGB calibration is set with a distance modulus to the Large Magellanic Cloud of 18.477 +/- 0.004 (stat) +/-0.020 (sys) mag, based on measurement of 20 late-type detached eclipsing binary (DEB) stars, combined with an HST parallax calibration of a 3.6 micron Cepheid Leavitt law based on Spitzer observations. We anchor the TRGB distances to galaxies that extend our measurement into the Hubble flow using the recently completed Carnegie Supernova Project I sample containing about 100 well-observed SNeIa. There are several advantages of halo TRGB distance measurements relative to Cepheid variables: these include low halo reddening, minimal effects of crowding or blending of the photometry, only a shallow (calibrated) sensitivity to metallicity in the I-band, and no need for multiple epochs of observations or concerns of different slopes with period. In addition, the host masses of our TRGB host-galaxy sample are higher on average than the Cepheid sample, better matching the range of host-galaxy masses in the CSP distant sample, and reducing potential systematic effects in the SNeIa measurements. ",https://doi.org/10.3847/1538-4357/ab2f73,1907.05922v1,Yes,potent(1)
0000-0003-4571-9046,Yang Ren,"Princeton University, Princeton University Plasma Physics Laboratory",Dual-Tap Optical-Digital Feedforward Equalization Enabling High-Speed   Optical Transmission in IM/DD Systems,1970,"  Intensity-modulation and direct-detection (IM/DD) transmission is widely adopted for high-speed optical transmission scenarios due to its cost-effectiveness and simplicity. However, as the data rate increases, the fiber chromatic dispersion (CD) would induce a serious power fading effect, and direct detection could generate inter-symbol interference (ISI). Moreover, the ISI becomes more severe with the increase of fiber length, thereby highly restricting the transmission distance of IM/DD systems. This paper proposes a dual-tap optical-digital feedforward equalization (DT-ODFE) scheme, which could effectively compensate for CD-induced power fading while maintaining low cost and simplicity. A theoretical channel response is formulated for IM/DD transmission, incorporating a dual-tap optical equalizer, and the theoretical analysis reveals that for an IM/DD transmission using 1371nm over 10km standard single-mode fiber (SSMF), frequency notch is removed from 33.7GHz to 46GHz. Simulation results show that the DT- ODFE achieves an SNR gain of 2.3dB over IM/DD systems with symbol-space feedforward equalizer (FFE) alone. As the fiber length increases to 15 km, DT- ODFE performs well, while FFE, decision-feedback equalizer (DFE) and Volterra nonlinear equalizers (VNLE) all fail to compensate for the power fading and the 7% hard-decision FEC limit is not satisfied. For 200 Gb/s/$\lambda$ PAM-4 over 15km SSMF, results show that the signal-to-noise ratio (SNR) of the proposed DT- ODFE with optimal coefficients satisfies the 7% hard-decision FEC limit, which uncovers the great potential of the DT- ODFE for high-speed IM/DD systems in LR/FR scenarios. ",Kein DOI-Link verfügbar,2402.00616v2,Yes,potent(1)
0000-0003-4571-9046,Yang Ren,"Princeton University, Princeton University Plasma Physics Laboratory",Realization of Anomalous Floquet Insulators in Strongly-Coupled   Nanophotonic Lattices,1970,"  We experimentally realized Floquet topological photonic insulators using a square lattice of direct-coupled octagonal resonators. Unlike previously reported topological insulator systems based on microring lattices, the nontrivial topological behaviors of our system arise directly from the periodic evolution of light around each octagon to emulate a periodically-driven system. By exploiting asynchronism in the evanescent coupling between adjacent octagonal resonators, we could achieve strong and asymmetric couplings in each unit cell, which are necessary for observing Anomalous Floquet Insulator behaviors. Direct imaging of scattered light from fabricated samples confirmed the existence of chiral edge states as predicted by the topological phase map of the lattice. In addition, by exploiting the frequency dispersion of the coupling coefficients, we could also observe topological phase changes of the lattice from normal insulator to Chern and Floquet insulators. Our lattice thus provides a versatile nanophotonic system for investigating 2D Floquet topological insulators. ",https://doi.org/10.1103/PhysRevLett.124.253601,1912.10126v1,Yes,versatile(1)
0000-0003-4571-9046,Yang Ren,"Princeton University, Princeton University Plasma Physics Laboratory",Diffusion-controlled Alloying of Single-phase Multi-principal Covalent   Transition Metal Carbides with Enhanced Damage tolerance and Exceptional   Thermal Properties,1970,"  Multicomponent alloying has displayed extraordinary potential for producing exceptional structural and functional materials. However, the synthesis of single-phase, multi-principal covalent compounds remains a challenge. Here we present a diffusion-controlled alloying strategy for the successful realization of covalent multi-principal transition metal carbides (MPTMCs) with a single face-centered cubic (FCC) phase. The increased interfacial diffusion promoted by the addition of a nonstoichiometric compound leads to rapid formation of the new single phase at much lower sintering temperature. Direct atomic-level observations via scanning transmission electron microscopy demonstrate that MPTMCs are composed of a single phase with a random distribution of all cations, which holds the key to the unique combinations of improved fracture toughness, superior Vickers hardness, and extremely lower thermal diffusivity achieved in MPTMCs. The present discovery provides a promising approach toward the design and synthesis of next-generation high-performance materials. ",Kein DOI-Link verfügbar,1810.01944v1,Yes,potent(1)
0000-0002-4005-0171,Andrew D. Jackson,Princeton University,On the action potential as a propagating density pulse and the role of   anesthetics,1970,"  The Hodgkin-Huxley model of nerve pulse propagation relies on ion currents through specific resistors called ion channels. We discuss a number of classical thermodynamic findings on nerves that are not contained in this classical theory. Particularly striking is the finding of reversible heat changes, thickness and phase changes of the membrane during the action potential. Data on various nerves rather suggest that a reversible density pulse accompanies the action potential of nerves. Here, we attempted to explain these phenomena by propagating solitons that depend on the presence of cooperative phase transitions in the nerve membrane. These transitions are, however, strongly influenced by the presence of anesthetics. Therefore, the thermodynamic theory of nerve pulses suggests a explanation for the famous Meyer-Overton rule that states that the critical anesthetic dose is linearly related to the solubility of the drug in the membranes. ",https://doi.org/10.1142/S179304800700043X,physics/0610117v2,Yes,potent(2)
0000-0002-4005-0171,Andrew D. Jackson,Princeton University,Skewness and Kurtosis as Indicators of Non-Gaussianity in Galactic   Foreground Maps,1970,"  Observational cosmology is entering an era in which high precision will be required in both measurement and data analysis. Accuracy, however, can only be achieved with a thorough understanding of potential sources of contamination from foreground effects. Our primary focus will be on non- Gaussian effects in foregrounds. This issue will be crucial for coming experiments to determine B-mode polarization. We propose a novel method for investigating a data set in terms of skewness and kurtosis in locally defined regions that collectively cover the entire sky. The method is demonstrated on two sky maps: (i) the SMICA map of Cosmic Microwave Background fluctuations provided by the Planck Collaboration and (ii) a version of the Haslam map at 408 MHz that describes synchrotron radiation. We find that skewness and kurtosis can be evaluated in combination to reveal local physical information. In the present case, we demonstrate that the local properties of both maps are predominantly Gaussian. This result was expected for the SMICA map; that it also applies for the Haslam map is surprising. The approach described here has a generality and flexibility that should make it useful in a variety of astrophysical and cosmological contexts. ",https://doi.org/10.1088/1475-7516/2015/11/019,1509.03100v2,Yes,potent(1)
0000-0002-4005-0171,Andrew D. Jackson,Princeton University,Solitary electromechanical pulses in Lobster neurons,1970,"  Investigations of nerve activity have focused predominantly on electrical phenomena. Nerves, however, are thermodynamic systems, and changes in temperature and in the dimensions of the nerve can also be observed during the action potential. Measurements of heat changes during the action potential suggest that the nerve pulse shares many characteristics with an adiabatic pulse. First experiments in the 1980s suggested small changes in nerve thickness and length during the action potential. Such findings have led to the suggestion that the action potential may be related to electromechanical solitons traveling without dissipation. However, they have been no modern attempts to study mechanical phenomena in nerves. Here, we present ultrasensitive AFM recordings of mechanical changes on the order of 2 - 12 {\AA} in the giant axons of the lobster. We show that the nerve thickness changes in phase with voltage change. When stimulated at opposite ends of the same axon, colliding action potentials pass through one another and do not annihilate. These observations are consistent with a mechanical interpretation of the nervous impulse. ",https://doi.org/10.1016/j.bpc.2016.06.005,1502.07166v2,Yes,potent(5)
0000-0002-4005-0171,Andrew D. Jackson,Princeton University,Periodic solutions and refractory periods in the soliton theory for   nerves and the locust femoral nerve,1970,"  Close to melting transitions it is possible to propagate solitary electromechanical pulses which reflect many of the experimental features of the nerve pulse including mechanical dislocations and reversible heat production. Here we show that one also obtains the possibility of periodic pulse generation when the boundary condition for the nerve is the conservation of the overall length of the nerve. This condition generates an undershoot beneath the baseline (`hyperpolarization') and a `refractory period', i.e., a minimum distance between pulses. In this paper, we outline the theory for periodic solutions to the wave equation and compare these results to action potentials from the femoral nerve of the locust (locusta migratoria). In particular, we describe the frequently occurring minimum-distance doublet pulses seen in these neurons and compare them to the periodic pulse solutions. ",https://doi.org/10.1016/j.bpc.2010.11.001,1006.3281v1,Yes,potent(1)
0000-0002-6328-152X,Peter Green,Princeton University,Bayesian Protein Sequence and Structure Alignment,1970,"  The structure of a protein is crucial in determining its functionality, and is much more conserved than sequence during evolution. A key task in structural biology is to compare protein structures in order to determine evolutionary relationships, estimate the function of newly-discovered structures, and predict unknown structures. We propose a Bayesian method for protein structure alignment, with the prior on alignments based on functions which penalise ``gaps'' in the aligned sequences. We show how a broad class of penalty functions fits into this framework, and how the resulting posterior distribution can be efficiently sampled. A commonly-used gap penalty function is shown to be a special case, and we propose a new penalty function which alleviates an undesirable feature of the commonly-used penalty. We illustrate our method on benchmark data sets, and find it competes well with popular tools from computational biology. Our method has the benefit of being able to potentially explore multiple competing alignments and quantify their merits probabilistically. The framework naturally allows for further information such as amino acid sequence to be included, and could be adapted to other situations such as flexible proteins or domain swaps. ",Kein DOI-Link verfügbar,1404.1556v3,Yes,potent(1)
0009-0008-1656-2258,Ran Raz,Princeton University,Block Rigidity: Strong Multiplayer Parallel Repetition implies   Super-Linear Lower Bounds for Turing Machines,1970,"  We prove that a sufficiently strong parallel repetition theorem for a special case of multiplayer (multiprover) games implies super-linear lower bounds for multi-tape Turing machines with advice. To the best of our knowledge, this is the first connection between parallel repetition and lower bounds for time complexity and the first major potential implication of a parallel repetition theorem with more than two players.   Along the way to proving this result, we define and initiate a study of block rigidity, a weakening of Valiant's notion of rigidity. While rigidity was originally defined for matrices, or, equivalently, for (multi-output) linear functions, we extend and study both rigidity and block rigidity for general (multi-output) functions. Using techniques of Paul, Pippenger, Szemer\'edi and Trotter, we show that a block-rigid function cannot be computed by multi-tape Turing machines that run in linear (or slightly super-linear) time, even in the non-uniform setting, where the machine gets an arbitrary advice tape.   We then describe a class of multiplayer games, such that, a sufficiently strong parallel repetition theorem for that class of games implies an explicit block-rigid function. The games in that class have the following property that may be of independent interest: for every random string for the verifier (which, in particular, determines the vector of queries to the players), there is a unique correct answer for each of the players, and the verifier accepts if and only if all answers are correct. We refer to such games as independent games. The theorem that we need is that parallel repetition reduces the value of games in this class from $v$ to $v^{\Omega(n)}$, where $n$ is the number of repetitions.   As another application of block rigidity, we show conditional size-depth tradeoffs for boolean circuits, where the gates compute arbitrary functions over large sets. ",Kein DOI-Link verfügbar,2011.09093v2,Yes,potent(1)
0000-0003-1498-8980,Ricardo Shousha,Princeton University,Highest Fusion Performance without Harmful Edge Energy Bursts in Tokamak,1970,"  The path of tokamak fusion and ITER is maintaining high-performance plasma to produce sufficient fusion power. This effort is hindered by the transient energy burst arising from the instabilities at the boundary of high-confinement plasmas. The application of 3D magnetic perturbations is the method in ITER and possibly in future fusion power plants to suppress this instability and avoid energy busts damaging the device. Unfortunately, the conventional use of the 3D field in tokamaks typically leads to degraded fusion performance and an increased risk of other plasma instabilities, two severe issues for reactor implementation. In this work, we present an innovative 3D field optimization, exploiting machine learning, real-time adaptability, and multi-device capabilities to overcome these limitations. This integrated scheme is successfully deployed on DIII-D and KSTAR tokamaks, consistently achieving reactor-relevant core confinement and the highest fusion performance without triggering damaging instabilities or bursts while demonstrating ITER-relevant automated 3D optimization for the first time. This is enabled both by advances in the physics understanding of self-organized transport in the plasma edge and by advances in machine-learning technology, which is used to optimize the 3D field spectrum for automated management of a volatile and complex system. These findings establish real-time adaptive 3D field optimization as a crucial tool for ITER and future reactors to maximize fusion performance while simultaneously minimizing damage to machine components. ",Kein DOI-Link verfügbar,2405.05452v1,Yes,innovative(1)
0009-0006-6741-7048,David Brown,Princeton University,Challenges of Designing and Developing Tangible Interfaces for Mental   Well-being,1970,"  Mental well-being technologies possess many qualities that give them the potential to help people receive assessment and treatment who may otherwise not receive help due to fear of stigma or lack of resources. The combination of advances in sensors, microcontrollers and machine learning is leading to the emergence of dedicated tangible interfaces to monitor and promote positive mental well-being. However, there are key technical, ergonomic and aesthetic challenges to be overcome in order to make these interfaces effective and respond to users' needs. In this paper, the barriers to develop mental well-being tangible interfaces are discussed by identifying and examining the recent technological challenges machine learning, sensors, microcontrollers and batteries create.User-oriented challenges that face the development of mental well-being technologies are then considered ranging from user engagement during co-design and trials to ethical and privacy concerns. ",Kein DOI-Link verfügbar,1909.11752v1,Yes,potent(1)
0000-0003-2591-3888,Laurent Binet,Université PSL,"Structure, composition, and location of organic matter in the enstatite   chondrite Sahara 97096 (EH3)",1970,"  The insoluble organic matter (IOM) of an unequilibrated enstatite chondrite Sahara (SAH) 97096 has been investigated using a battery of analytical techniques. As the enstatite chondrites are thought to have formed in a reduced environment at higher temperatures than carbonaceous chondrites, they constitute an interesting comparative material to test the heterogeneities of the IOM in the solar system and to constrain the processes that could affect IOM during solar system evolution. The SAH 97096 IOM is found in situ: as submicrometer grains in the network of fine-grained matrix occurring mostly around chondrules and as inclusions in metallic nodules, where the carbonaceous matter appears to be more graphitized. IOM in these two settings has very similar $\delta^{15}N$ and $\delta^{13}C$; this supports the idea that graphitized inclusions in metal could be formed by metal catalytic graphitization of matrix IOM. A detailed comparison between the IOM extracted from a fresh part and a terrestrially weathered part of SAH 97096 shows the similarity between both IOM samples in spite of the high degree of mineral alteration in the latter. The isolated IOM exhibits a heterogeneous polyaromatic macromolecular structure, sometimes highly graphitized, without any detectable free radicals and deuterium-heterogeneity and having mean H- and N-isotopic compositions in the range of values observed for carbonaceous chondrites. It contains some submicrometer-sized areas highly enriched in $^{15}N$ ($\delta^{15}N$ up to 1600 permil). These observations reinforce the idea that the IOM found in carbonaceous chondrites is a common component widespread in the solar system. Most of the features of SAH 97096 IOM could be explained by the thermal modification of this main component. ",https://doi.org/10.1111/j.1945-5100.2011.01306.x,1502.00216v1,Yes,fresh(1)
0000-0002-5480-1386,David Armstrong,"University of Toronto, University of Toronto - Mississauga, University of Toronto Mississauga",Sandwiched planet formation: restricting the mass of a middle planet,1970,"  We conduct gas and dust hydrodynamical simulations of protoplanetary discs with one and two embedded planets to determine the impact that a second planet located further out in the disc has on the potential for subsequent planet formation in the region locally exterior to the inner planet. We show how the presence of a second planet has a strong influence on the collection of solid material near the inner planet, particularly when the outer planet is massive enough to generate a maximum in the disc's pressure profile. This effect in general acts to reduce the amount of material that can collect in a pressure bump generated by the inner planet. When viewing the inner pressure bump as a location for potential subsequent planet formation of a third planet, we therefore expect that the mass of such a planet will be smaller than it would be in the case without the outer planet, resulting in a small planet being sandwiched between its neighbours - this is in contrast to the expected trend of increasing planet mass with radial distance from the host star. We show that several planetary systems have been observed that do not show this trend but instead have a smaller planet sandwiched in between two more massive planets. We present the idea that such an architecture could be the result of the subsequent formation of a middle planet after its two neighbours formed at some earlier stage. ",Kein DOI-Link verfügbar,2310.01488v2,Yes,potent(2)
0000-0002-5480-1386,David Armstrong,"University of Toronto, University of Toronto - Mississauga, University of Toronto Mississauga",DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection,1970,"  Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the automated detection of diabetic foot ulcers using machine learning techniques, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms. These can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves at home to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4,000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research. This grand challenge takes on even greater urgency in a peri and post-pandemic period, where stresses on resource utilization will increase the need for technology that allows people to remain active, healthy and intact in their home. ",https://doi.org/10.17925/EE.2021.1.1.5,2004.11853v3,Yes,innovative(1)
0000-0002-6229-9103,Alireza Amirshahi,EPFL,Predicting Survey Response with Quotation-based Modeling: A Case Study   on Favorability towards the United States,1970,"  The acquisition of survey responses is a crucial component in conducting research aimed at comprehending public opinion. However, survey data collection can be arduous, time-consuming, and expensive, with no assurance of an adequate response rate. In this paper, we propose a pioneering approach for predicting survey responses by examining quotations using machine learning. Our investigation focuses on evaluating the degree of favorability towards the United States, a topic of interest to many organizations and governments. We leverage a vast corpus of quotations from individuals across different nationalities and time periods to extract their level of favorability. We employ a combination of natural language processing techniques and machine learning algorithms to construct a predictive model for survey responses. We investigate two scenarios: first, when no surveys have been conducted in a country, and second when surveys have been conducted but in specific years and do not cover all the years. Our experimental results demonstrate that our proposed approach can predict survey responses with high accuracy. Furthermore, we provide an exhaustive analysis of the crucial features that contributed to the model's performance. This study has the potential to impact survey research in the field of data science by substantially decreasing the cost and time required to conduct surveys while simultaneously providing accurate predictions of public opinion. ",Kein DOI-Link verfügbar,2305.14086v2,Yes,potent(1)
0000-0002-6229-9103,Alireza Amirshahi,EPFL,MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few   Shots,1970,"  Wearable systems provide continuous health monitoring and can lead to early detection of potential health issues. However, the lifecycle of wearable systems faces several challenges. First, effective model training for new wearable devices requires substantial labeled data from various subjects collected directly by the wearable. Second, subsequent model updates require further extensive labeled data for retraining. Finally, frequent model updating on the wearable device can decrease the battery life in long-term data monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a meta-learning method to reduce the amount of initial data collection required. Moreover, our approach incorporates a prototypical updating mechanism, simplifying the update process by modifying the class prototype rather than retraining the entire model. We explore the performance of MetaWearS in two case studies, namely, the detection of epileptic seizures and the detection of atrial fibrillation. We show that by fine-tuning with just a few samples, we achieve 70% and 82% AUC for the detection of epileptic seizures and the detection of atrial fibrillation, respectively. Compared to a conventional approach, our proposed method performs better with up to 45% AUC. Furthermore, updating the model with only 16 minutes of additional labeled data increases the AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for model updates by 456x and 418x for epileptic seizure and AF detection, respectively. ",Kein DOI-Link verfügbar,2408.01988v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Conduction of Ultracold Fermions Through a Mesoscopic Channel,1970,"  In a mesoscopic conductor electric resistance is detected even if the device is defect-free. We engineer and study a cold-atom analog of a mesoscopic conductor. It consists of a narrow channel connecting two macroscopic reservoirs of fermions that can be switched from ballistic to diffusive. We induce a current through the channel and find ohmic conduction, even for a ballistic channel. An analysis of in-situ density distributions shows that in the ballistic case the chemical potential drop occurs at the entrance and exit of the channel, revealing the presence of contact resistance. In contrast, a diffusive channel with disorder displays a chemical potential drop spread over the whole channel. Our approach opens the way towards quantum simulation of mesoscopic devices with quantum gases. ",https://doi.org/10.1126/science.1223175,1203.1927v2,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Superfluidity with disorder in a quantum gas thin film,1970,"  We investigate the properties of a strongly interacting, superfluid gas of 6Li2 Feshbach molecules forming a thin film confined in a quasi two-dimensional channel with a tunable random potential, creating a microscopic disorder. We measure the atomic current and extract the resistance of the film in a two-terminal configuration, and identify a superfluid state at low disorder strength, which evolves into a normal, poorly conducting state for strong disorder. The transition takes place when the chemical potential reaches the percolation threshold of the disorder. The evolution of the conduction properties contrasts with the smooth behavior of the density and compressibility across the transition, measured in-situ at equilibrium. These features suggest the emergence of a glass-like phase at strong disorder. ",Kein DOI-Link verfügbar,1211.7272v1,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Two-terminal transport measurements with cold atoms,1970,"  In the last years, the ability of cold atoms experiments to explore condensed- matter related questions has dramatically progressed. Transport experiments, in particular, have expanded to the point that conductances and other transport coefficients can now be measured in a way directly analogous to solid state physics, extending cold atoms based quantum simulations into the domain of quantum electronic devices. In this topical review, we describe the transport experiments performed with cold gases in the two terminals configuration, with an emphasis on the specific features of cold atomic gases compared to solid state physics. We present the experimental techniques and the main experimental findings, focusing on but not restricted to the recent experiments performed in our group. We eventually discuss the perspectives opened by this approach, the main technical and conceptual challenges for future developments, and the potential applications as a quantum simulator for transport phenomena and mesoscopic physics problems. ",https://doi.org/10.1088/1361-648X/aa74a1,1706.01085v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Light-shift tomography in an optical-dipole trap for neutral atoms,1970,"  We report on light-shift tomography of a cloud of 87 Rb atoms in a far-detuned optical-dipole trap at 1565 nm. Our method is based on standard absorption imaging, but takes advantage of the strong light-shift of the excited state of the imaging transition, which is due to a quasi-resonance of the trapping laser with a higher excited level. We use this method to (i) map the equipotentials of a crossed optical-dipole trap, and (ii) study the thermalisation of an atomic cloud by following the evolution of the potential-energy of atoms during the free-evaporation process. ",https://doi.org/10.1103/PhysRevA.78.031401,0807.3672v1,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Observing the Drop of Resistance in the Flow of a Superfluid Fermi Gas,1970,"  In this work, we investigate the conduction properties of strongly interacting fermions flowing through a quasi two-dimensional, multimode channel, which connects two atomic reservoirs. The atomic current in the channel is controlled using a repulsive potential created by an off-resonant laser beam. In analogy with an electronic field-effect transistor, this gate potential controls the chemical potential in the channel while keeping the temperature imposed by the reservoirs unchanged. With the gate potential as a control parameter, we measure the current through the channel over a large dynamic range and determine the density distribution in the channel region. This allows us to observe the onset of superfluid flow of strongly interacting fermions. These measurements are compared to the case of a weakly interacting Fermi gas. ",https://doi.org/10.1038/nature11613,1210.1426v1,Yes,potent(4)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,"Direct Observation of Fragmentation in a Disordered, Strongly   Interacting Fermi Gas",1970,"  Describing the behaviour of strongly interacting particles in the presence of disorder is among the most challenging problems in quantum many-body physics. The controlled setting of cold atom experiments provides a new avenue to address these challenges [1], complementing studies in solid state physics, where a number of puzzling findings have emerged in experiments using superconducting thin films [2,3]. Here we investigate a strongly interacting thin film of an atomic Fermi gas subject to a random potential. We use high-resolution in-situ imaging [4-7] to resolve the atomic density at the length scale of a single impurity, which would require scanning probe techniques in solid state physics [8]. This allows us to directly observe the fragmentation of the density profile and to extract its percolation properties. Transport measurements in a two-terminal configuration indicate that the fragmentation process is accompanied by a breakdown of superfluidity. Our results suggest that percolation of paired atoms is responsible for the loss of superfluidity, and that disorder is able to increase the binding energy of pairs. ",https://doi.org/10.1103/PhysRevLett.115.045302,1311.5174v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,A cavity quantum electrodynamics implementation of the   Sachdev--Ye--Kitaev model,1970,"  The search for a quantum theory of gravity has led to the discovery of quantum many-body systems that are dual to gravitational models with quantum properties. The perhaps most famous of these systems is the Sachdev-Ye-Kitaev (SYK) model. It features maximal scrambling of quantum information, and opens a potential inroad to experimentally investigating aspects of quantum gravity. A scalable laboratory realisation of this model, however, remains outstanding. Here, we propose a feasible implementation of the SYK model in cavity quantum electrodynamics platforms. Through detailed analytical and numerical demonstrations, we show how driving a cloud of fermionic atoms trapped in a multi-mode optical cavity, and subjecting it to a spatially disordered AC-Stark shift retrieves the physics of the SYK model, with random all-to-all interactions and fast scrambling. Our work provides a blueprint for realising the SYK model in a scalable system, with the prospect of studying holographic quantum matter in the laboratory. ",Kein DOI-Link verfügbar,2303.11343v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Observation of Quantized Conductance in Neutral Matter,1970,"  In transport experiments the quantum nature of matter becomes directly evident when changes in conductance occur only in discrete steps, with a size determined solely by Planck's constant h. The observations of quantized steps in the electric conductance have provided important insights into the physics of mesoscopic systems and allowed for the development of quantum electronic devices. Even though quantized conductance should not rely on the presence of electric charges, it has never been observed for neutral, massive particles. In its most fundamental form, the phenomenon requires a quantum degenerate Fermi gas, a ballistic and adiabatic transport channel, and a constriction with dimensions comparable to the Fermi wavelength. Here we report on the observation of quantized conductance in the transport of neutral atoms. We employ high resolution lithography to shape light potentials that realize either a quantum point contact or a quantum wire for atoms. These constrictions are imprinted on a quasi two-dimensional ballistic channel connecting two adjustable reservoirs of quantum degenerate fermionic lithium atoms. By tuning either a gate potential or the transverse confinement of the constrictions, we observe distinct plateaus in the conductance for atoms. The conductance in the first plateau is found to be equal to 1/h, the universal conductance quantum. For low gate potentials we find good agreement between the experimental data and the Landauer formula, with all parameters determined a priori. Our experiment constitutes the cold atom version of a mesoscopic device and can be readily extended to more complex geometries and interacting quantum gases. ",https://doi.org/10.1038/nature14049,1404.6400v1,Yes,potent(3)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Mapping out spin and particle conductances in a quantum point contact,1970,"  We study particle and spin transport in a single mode quantum point contact using a charge neutral, quantum degenerate Fermi gas with tunable, attractive interactions. This yields the spin and particle conductance of the point contact as a function of chemical potential or confinement. The measurements cover a regime from weak attraction, where quantized conductance is observed, to the resonantly interacting superfluid. Spin conductance exhibits a broad maximum when varying the chemical potential at moderate interactions, which signals the emergence of Cooper pairing. In contrast, the particle conductance is unexpectedly enhanced even before the gas is expected to turn into a superfluid, continuously rising from the plateau at 1/h for weak interactions to plateaux-like features at non-universal values as high as 4/h for intermediate interactions. For strong interactions, the particle conductance plateaux disappear and the spin conductance gets suppressed, confirming the spin-insulating character of a superfluid. Our observations document the breakdown of universal conductance quantization as many-body correlations appear. The observed anomalous quantization challenges a Fermi liquid description of the normal phase, shedding new light on the nature of the strongly attractive Fermi gases. ",https://doi.org/10.1073/pnas.1601812113,1511.05961v2,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Breakdown of the Wiedemann-Franz law in a unitary Fermi gas,1970,"  We report on coupled heat and particle transport measurements through a quantum point contact (QPC) connecting two reservoirs of resonantly interacting, finite temperature Fermi gases. After heating one of them, we observe a particle current flowing from cold to hot. We monitor the temperature evolution of the reservoirs and find that the system evolves after an initial response into a non-equilibrium steady state with finite temperature and chemical potential differences across the QPC. In this state any relaxation in the form of heat and particle currents vanishes. From our measurements we extract the transport coefficients of the QPC and deduce a Lorenz number violating the Wiedemann-Franz law by one order of magnitude, a characteristic persisting even for a wide contact. In contrast, the Seebeck coefficient takes a value close to that expected for a non-interacting Fermi gas and shows a smooth decrease as the atom density close to the QPC is increased beyond the superfluid transition. Our work represents a fermionic analog of the fountain effect observed with superfluid helium and poses new challenges for microscopic modeling of the finite temperature dynamics of the unitary Fermi gas. ",https://doi.org/10.1073/pnas.1803336115,1803.00935v1,Yes,potent(1)
0000-0002-8386-0078,Sho Watanabe,EPFL,Ni$_{80}$Fe$_{20}$ Nanotubes with Optimized Spintronic Functionalities   Prepared by Atomic Layer Deposition,1970,"  Permalloy Ni$_{80}$Fe$_{20}$ is one of the key magnetic materials in the field of magnonics. Its potential would be further unveiled if it could be deposited in three dimensional (3D) architectures of sizes down to the nanometer. Atomic Layer Deposition, ALD, is the technique of choice for covering arbitrary shapes with homogeneous thin films. Early successes with ferromagnetic materials include nickel and cobalt. Still, challenges in depositing ferromagnetic alloys reside in the synthesis via decomposing the consituent elements at the same temperature and homogeneously. We report plasma-enhanced ALD to prepare permalloy Ni$_{80}$Fe$_{20}$ thin films and nanotubes using nickelocene and iron(III) tert-butoxide as metal precursors, water as the oxidant agent and an in-cycle plasma enhanced reduction step with hydrogen. We have optimized the ALD cycle in terms of Ni:Fe atomic ratio and functional properties. We obtained a Gilbert damping of 0.013, a resistivity of 28 $\mu\Omega$cm and an anisotropic magnetoresistance effect of 5.6 $\%$ in the planar thin film geometry. We demonstrate that the process also works for covering GaAs nanowires, resulting in permalloy nanotubes with high aspect ratios and diameters of about 150 nm. Individual nanotubes were investigated in terms of crystal phase, composition and spin-dynamic response by microfocused Brillouin Light Scattering. Our results enable NiFe-based 3D spintronics and magnonic devices in curved and complex topology operated in the GHz frequency regime. ",https://doi.org/10.1039/d1nr02291a,2105.01969v1,Yes,potent(1)
0000-0002-5586-5488,David Heim,EPFL,Piezo-optomechanical cantilever modulators for VLSI visible photonics,1970,"  Visible-wavelength very large-scale integration (VLSI) photonic circuits have potential to play important roles in quantum information and sensing technologies. The realization of scalable, high-speed, and low-loss photonic mesh circuits depends on reliable and well-engineered visible photonic components. Here we report a low-voltage optical phase shifter based on piezo-actuated mechanical cantilevers, fabricated on a CMOS compatible, 200 mm wafer-based visible photonics platform. We show linear phase and amplitude modulation with 6 V$_{\pi}$-cm in differential operation, -1.5 dB to -2 dB insertion loss, and up to 40 dB contrast in the 700 nm - 780 nm range. By adjusting selected cantilever parameters, we demonstrate a low-displacement and a high-displacement device, both exhibiting a nearly flat frequency response from DC to a peak mechanical resonance at 23 MHz and 6.8 MHz respectively, which through resonant enhancement of Q~40, further decreases the operating voltage down to 0.15 V$_{\pi}$-cm. ",https://doi.org/10.1063/5.0088424,2201.12447v1,Yes,potent(1)
0000-0002-9870-9477,Kirell Benzi,EPFL,Song Recommendation with Non-Negative Matrix Factorization and Graph   Total Variation,1970,"  This work formulates a novel song recommender system as a matrix completion problem that benefits from collaborative filtering through Non-negative Matrix Factorization (NMF) and content-based filtering via total variation (TV) on graphs. The graphs encode both playlist proximity information and song similarity, using a rich combination of audio, meta-data and social features. As we demonstrate, our hybrid recommendation system is very versatile and incorporates several well-known methods while outperforming them. Particularly, we show on real-world data that our model overcomes w.r.t. two evaluation metrics the recommendation of models solely based on low-rank information, graph-based information or a combination of both. ",Kein DOI-Link verfügbar,1601.01892v2,Yes,versatile(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Gravitational lensing in low-redshift clusters of galaxies: the arc-like   object in Abell 3408 and its lensing interpretation,1970,"  We analyze the seldomly discussed lensing effects expected in low-z clusters (z = 0.05-0.15), using as an example the bright arc (z=0.073) discovered by Campusano and Hardy(1996) near the dominant cD galaxy of the cluster Abell 3408 (z=0.042). We present photometric and spectroscopic observations for both the dominant galaxy and the arc. The mass distribution in A3408 is modeled by scaled versions of the representative distributions derived from studies of clusters at higher redshifts. The two gravitational potentials considered are: i) a ``minimum'' mass case where the mass distribution follows the light profile of the central elliptical galaxy and, ii) a ``maximum'' mass case where a typical massive dark halo is added to the previous case. The observed arc is well reproduced by both models, but rather small magnifications of the source galaxy are implied. The source galaxy is tentatively identified in both the lensing and non-lensing scenarios as being a spiral. The smaller lensed spiral (14.6 h_50^{-1} kpc, M_B=-18.2) predicted by the dark halo model appears to fit the observations marginally better. Furthermore, we found that only the dark halo model predicts a measurable amount of weak shear in the images of faint background galaxies. We conclude that observations, under very good seeing conditions, of week shear in faint background galaxies in the direction of low-redshift galaxy clusters are possible. When the latter are combined with X-ray data, a powerful tool to probe the mass distribution in the very central region of galaxy clusters emerges. ",https://doi.org/10.1086/311252,astro-ph/9712069v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Submillimeter-selected galaxies,1970,"  The first generation of submillimeter(submm)-wave surveys are being carried out using the 450/850-micron SCUBA camera at the JCMT on Mauna Kea. These surveys are potentially sensitive to galaxies at very high redshift, and the galaxies that have been detected so far appear to contribute the greater fraction of the mm/submm-wave background radiation intensity measured by COBE. In order to understand this new population of galaxies, individual examples must be studied in detail across many wavebands; in particular their redshifts must be determined. We discuss the potential selection effects at work in submm-wave surveys and describe the spectral energy distributions of galaxies selected or luminous in the submm waveband. We also describe the general procedure for, and emphasize the difficulty of, identifying optical counterparts to submm-selected galaxies. Finally, we summarize what is known about the redshifts of these galaxies and the source of their luminosity. ",Kein DOI-Link verfügbar,astro-ph/9908111v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Detecting compact dark matter in galaxy clusters via gravitational   microlensing: A2218 & A370,1970,"  After decades of searching, the true nature of dark matter still eludes us. One potential probe of the form of dark matter in galaxy clusters is to search for microlensing variability in the giant arcs and arclets. In this paper, a simple method is introduced to characterize pixel variability in the limit of high optical depth to microlensing. Expanding on earlier work, the expected microlensing signal for two massive clusters, A2218 & A370 is calculated. It is found that the microlensing signal depends sensitively upon the mix of smooth and compact dark matter in the cluster. Comparison of two deep exposures taken with James Webb Space Telescope or two hour long exposures taken with a 30-metre class telescope in two epochs separated by a few years will possibly detect about a few dozen pixels which show strong variability due to microlensing at five sigma level, revealing wealth of information on the microlensing population. ",https://doi.org/10.1111/j.1365-2966.2004.08115.x,astro-ph/0406282v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Complete coordination of robotic fiber positioners for massive   spectroscopic surveys,1970,"  Robotic fiber positioners play a vital role in the generation of massive spectroscopic surveys. The more complete a positioners set is coordinated, the more information its corresponding spectrograph receives during an observation. The complete coordination problem of positioners sets is studied in this paper. We first define the local and the global completeness problems and determine their relationship. We then propose a new artificial potential field according to which the convergences of a positioner and its neighboring positioners are cooperatively taken into account. We also discover the required condition for a complete coordination. We finally explain how the modifications of some of the parameters of a positioners set may resolve its incompleteness coordination scenarios. We verify our accomplishments using simulations. ",https://doi.org/10.1117/1.JATIS.5.4.045002,2005.10448v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Astrobotics: Swarm Robotics for Astrophysical Studies,1970,"  This paper introduces the emerging field of astrobotics, that is, a recently-established branch of robotics to be of service to astrophysics and observational astronomy. We first describe a modern requirement of dark matter studies, i.e., the generation of the map of the observable universe, using astrobots. Astrobots differ from conventional two-degree-of-freedom robotic manipulators in two respects. First, the dense formation of astrobots give rise to the extremely overlapping dynamics of neighboring astrobots which make them severely subject to collisions. Second, the structure of astrobots and their mechanical specifications are specialized due to the embedded optical fibers passed through them. We focus on the coordination problem of astrobots whose solutions shall be collision-free, fast execution, and complete in terms of the astrobots' convergence rates. We also illustrate the significant impact of astrobots assignments to observational targets on the quality of coordination solutions To present the current state of the field, we elaborate the open problems including next-generation astrophysical projects including 20,000 astrobots, and other fields, such as space debris tracking, in which astrobots may be potentially used ",https://doi.org/10.1109/MRA.2020.3044911,2210.02587v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Ultra-deep mid-IR survey of a lensing cluster,1970,"  We present the first results of mid-infrared (MIR) ultra-deep observations towards the lensing cluster Abell 2390 using the ISOCAM infrared camera on-board ESA's Infrared Space Observatory (ISO) satellite. They reveal a large number of luminous MIR sources. Optical and near-infrared (NIR) cross-identification suggests that almost all 15 microns sources and about half of the 7 microns are identified with distant lensed galaxies. Thanks to the gravitational amplification these sources constitute the faintest MIR sources detected. We confirm that the number counts derived at 15 microns show a clear excess of sources with respect to the predictions of a no-evolution model.   The possible extension of the NGST instrumentation from the near-IR (1-5 microns) to the thermal infrared, up to 20 microns (as suggested by the NGST task group report, October 1997) would permit study of this new population of dust-enshrouded AGN/starburst galaxies detected by ISOCAM, up to very high redshifts and with vastly improved spatial resolution. The existence of this population demonstrats that the discrimination of dust contributions, possible in the MIR, must be an important consideration in reaching an understanding of the Universe at high redshift. Therefore we stress that the access of NGST to the thermal infrared would increase tremendously its scientific potential to study the early universe. ",Kein DOI-Link verfügbar,astro-ph/9808131v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Evidence for tidal stripping of dark matter halos in massive   cluster-lenses,1970,"  In this letter, we present the results of our study of galaxy-galaxy lensing in massive cluster-lenses spanning $z = 0.17$ to 0.58, utilizing high-quality archival {\it Hubble Space Telescope} ({\it HST}) data. Local anisotropies in the shear maps are assumed to arise from dark matter substructure within these clusters. Associating the substructure with bright early-type cluster galaxies, we quantify the properties of typical $L^*$ cluster members in a statistical fashion. The fraction of total mass associated with individual galaxies within the inner regions of these clusters ranges from 10--20% implying that the bulk of the dark matter in massive lensing clusters is smoothly distributed. Looking at the properties of the cluster galaxies, we find strong evidence ($>3$-$\sigma$ significance) that a fiducial early-type $L^\ast$ galaxy in these clusters has a mass distribution that is tidally truncated compared to equivalent luminosity galaxies in the field. In fact, we exclude field galaxy scale dark halos for these cluster early-types at $>10$-$\sigma$ significance. We compare the tidal radii obtained from this lensing analysis with the central density of the cluster potentials and find a correlation which is in excellent agreement with theoretical expectations of tidal truncation: $\log [r_t*] \propto (-0.6\pm 0.2) \log [\rho_0]$. ",https://doi.org/10.1086/345399,astro-ph/0207049v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Multi-scale cluster lens mass mapping I. Strong Lensing modelling,1970,"  We propose a novel technique to refine the modelling of galaxy clusters mass distribution using gravitational lensing. The idea is to combine the strengths of both ""parametric"" and ""non-parametric"" methods to improve the quality of the fit. We develop a multi-scale model that allows sharper contrast in regions of higher density where the number of constraints is generally higher. Our model consists of (i) a multi-scale grid of radial basis functions with physically motivated profiles and (ii) a list of galaxy-scale potentials at the location of the cluster member galaxies. This arrangement of potentials of different sizes allows to reach a high resolution for the model with a minimum number of parameters. We apply our model to the well studied cluster Abell 1689. We estimate the quality of our mass reconstruction with a Bayesian MCMC sampler. For a selected subset of multiple images, we manage to halve the errors between the predicted and observed image positions compared to previous studies. This owes to the flexibility of multi-scale models at intermediate scale between cluster and galaxy scale. The software developed for this paper is part of the public lenstool package which can be found at www.oamp.fr/cosmology/lenstool. ",https://doi.org/10.1111/j.1365-2966.2009.14654.x,0901.3792v3,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Cosmological Constraints From Weak Lensing Peak Statistics With CFHT   Stripe 82 Survey,1970,"  We derived constraints on cosmological parameters using weak lensing peak statistics measured on the $\sim130~{\rm deg}^2$ of the Canada-France-Hawaii Telescope Stripe 82 Survey (CS82). This analysis demonstrates the feasibility of using peak statistics in cosmological studies. For our measurements, we considered peaks with signal-to-noise ratio in the range of $\nu=[3,6]$. For a flat $\Lambda$CDM model with only $(\Omega_{\rm m}, \sigma_8)$ as free parameters, we constrained the parameters of the following relation $\Sigma_8=\sigma_8(\Omega_{\rm m}/0.27)^{\alpha}$ to be: $\Sigma_8=0.82 \pm 0.03 $ and $\alpha=0.43\pm 0.02$. The $\alpha$ value found is considerably smaller than the one measured in two-point and three-point cosmic shear correlation analyses, showing a significant complement of peak statistics to standard weak lensing cosmological studies. The derived constraints on $(\Omega_{\rm m}, \sigma_8)$ are fully consistent with the ones from either WMAP9 or Planck. From the weak lensing peak abundances alone, we obtained marginalised mean values of $\Omega_{\rm m}=0.38^{+0.27}_{-0.24}$ and $\sigma_8=0.81\pm 0.26$. Finally, we also explored the potential of using weak lensing peak statistics to constrain the mass-concentration relation of dark matter halos simultaneously with cosmological parameters. ",https://doi.org/10.1093/mnras/stv784,1412.3683v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Systematic or Signal? How dark matter misalignments can bias strong   lensing models of galaxy clusters,1970,"  We explore how assuming that mass traces light in strong gravitational lensing models can lead to systematic errors in the predicted position of multiple images. Using a model based on the galaxy cluster MACSJ0416 (z = 0.397) from the Hubble Frontier Fields, we split each galactic halo into a baryonic and dark matter component. We then shift the dark matter halo such that it no longer aligns with the baryonic halo and investigate how this affects the resulting position of multiple images. We find for physically motivated misalignments in dark halo position, ellipticity, position angle and density profile, that multiple images can move on average by more than 0.2"" with individual images moving greater than 1"". We finally estimate the full error induced by assuming that light traces mass and find that this assumption leads to an expected RMS error of 0.5"", almost the entire error budget observed in the Frontier Fields. Given the large potential contribution from the assumption that light traces mass to the error budget in mass reconstructions, we predict that it should be possible to make a first significant detection and characterisation of dark halo misalignments in the Hubble Frontier Fields with strong lensing. Finally, we find that it may be possible to detect ~1kpc offsets between dark matter and baryons, the smoking gun for self-interacting dark matter, should the correct alignment of multiple images be observed. ",https://doi.org/10.1093/mnras/stw295,1601.06793v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Experimental evaluation of complete safe coordination of astrobots for   Sloan Digital Sky Survey V,1970,"  The data throughput of massive spectroscopic surveys in the course of each observation is directly coordinated with the number of optical fibers which reach their target. In this paper, we evaluate the safety and the performance of the astrobots coordination in SDSS-V by conducting various experimental and simulated tests. We illustrate that our strategy provides a complete coordination condition which depends on the operational characteristics of astrobots, their configurations, and their targets. Namely, a coordination method based on the notion of cooperative artificial potential fields is used to generate safe and complete trajectories for astrobots. Optimal target assignment further improves the performance of the used algorithm in terms of faster convergences and less oscillatory movements. Both random targets and galaxy catalog targets are employed to observe the coordination success of the algorithm in various target distributions. The proposed method is capable of handling all potential collisions in the course of coordination. Once the completeness condition is fulfilled according to initial configuration of astrobots and their targets, the algorithm reaches full convergence of astrobots. Should one assign targets to astrobots using efficient strategies, convergence time as well as the number of oscillations decrease in the course of coordination. Rare incomplete scenarios are simply resolved by trivial modifications of astrobots swarms' parameters. ",https://doi.org/10.1007/s10686-020-09687-4,2012.10656v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,PINION: Physics-informed neural network for accelerating radiative   transfer simulations for cosmic reionization,1970,"  With the advent of the Square Kilometre Array Observatory (SKAO), scientists will be able to directly observe the Epoch of Reionization by mapping the distribution of neutral hydrogen at different redshifts. While physically motivated results can be simulated with radiative transfer codes, these simulations are computationally expensive and can not readily produce the required scale and resolution simultaneously. Here we introduce the Physics-Informed neural Network for reIONization (PINION), which can accurately and swiftly predict the complete 4-D hydrogen fraction evolution from the smoothed gas and mass density fields from pre-computed N-body simulation. We trained PINION on the C$^2$-Ray simulation outputs and a physics constraint on the reionization chemistry equation is enforced. With only five redshift snapshots and a propagation mask as a simplistic approximation of the ionizing photon mean free path, PINION can accurately predict the entire reionization history between $z=6$ and $12$. We evaluate the accuracy of our predictions by analysing the dimensionless power spectra and morphology statistics estimations against C$^2$-Ray results. We show that while the network's predictions are in good agreement with simulation to redshift $z>7$, the network's accuracy suffers for $z<7$ primarily due to the oversimplified propagation mask. We motivate how PINION performance can be drastically improved and potentially generalized to large-scale simulations. ",https://doi.org/10.1093/mnras/stad615,2208.13803v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,An HST Lensing Survey of X-ray Luminous Galaxy Clusters: I. A383,1970,"  We analyse the mass distribution in the core of A383 (z=0.188), one of 12 X-ray luminous clusters at z~0.2 selected for a comprehensive and unbiased study of the mass distribution in massive clusters. Deep HST imaging reveals a wide variety of gravitationally lensed features in A383, including a giant arc formed from the strongly-lensed images of 2 background galaxies, 2 radial arcs, several multiply-imaged arcs and numerous arclets. Based upon the constraints from the various lensed features, we construct a detailed mass model for the central regions of the cluster, taking into account both the cluster-scale potential and perturbations from individual cluster galaxies. Keck spectroscopy of one component of the giant arc identifies it as a star-forming galaxy at z=1.01 and provides an accurate measurement of the cluster mass within the radius of the giant arc (65kpc) of (3.5+/-0.1)*10^13 Mo. Using the weak shear measured from our HST observations we extend our mass model to determine a mass of (1.8+/-0.2)*10^14 Mo within a radius of 250kpc. On smaller scales we employ the radial arcs as probes of the shape of the mass distribution in the cluster core (r<20kpc), and find that the mass profile is more peaked than a single NFW profile. The optical and X-ray properties of A383 indicate the presence of a central cooling flow, for which we derive a mass deposition rate of >200 Mo/yr. We also use the X-ray emission from A383 to obtain independent estimates of the total mass within projected radii of 65 and 250kpc: (4.0+/-1.4)*10^13 Mo and (1.2+/-0.5)*10^14 Mo, which are consistent with the lensing measurements. [Abridged] ",https://doi.org/10.1086/320557,astro-ph/0008315v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,COSMOS: 3D weak lensing and the growth of structure,1970,"  We present a three dimensional cosmic shear analysis of the Hubble Space Telescope COSMOS survey, the largest ever optical imaging program performed in space. We have measured the shapes of galaxies for the tell-tale distortions caused by weak gravitational lensing, and traced the growth of that signal as a function of redshift. Using both 2D and 3D analyses, we measure cosmological parameters Omega_m, the density of matter in the universe, and sigma_8, the normalization of the matter power spectrum. The introduction of redshift information tightens the constraints by a factor of three, and also reduces the relative sampling (or ""cosmic"") variance compared to recent surveys that may be larger but are only two dimensional. From the 3D analysis, we find sigma_8*(Omega_m/0.3)^0.44=0.866+^0.085_-0.068 at 68% confidence limits, including both statistical and potential systematic sources of error in the total budget. Indeed, the absolute calibration of shear measurement methods is now the dominant source of uncertainty. Assuming instead a baseline cosmology to fix the geometry of the universe, we have measured the growth of structure on both linear and non-linear physical scales. Our results thus demonstrate a proof of concept for tomographic analysis techniques that have been proposed for future weak lensing surveys by a dedicated wide-field telescope in space. ",https://doi.org/10.1086/516599,astro-ph/0701480v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Handbook for the GREAT08 Challenge: An image analysis competition for   cosmological lensing,1970,"  The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses on a problem that is of crucial importance for future observations in cosmology. The shapes of distant galaxies can be used to determine the properties of dark energy and the nature of gravity, because light from those galaxies is bent by gravity from the intervening dark matter. The observed galaxy images appear distorted, although only slightly, and their shapes must be precisely disentangled from the effects of pixelisation, convolution and noise. The worldwide gravitational lensing community has made significant progress in techniques to measure these distortions via the Shear TEsting Program (STEP). Via STEP, we have run challenges within our own community, and come to recognise that this particular image analysis problem is ideally matched to experts in statistical inference, inverse problems and computational learning. Thus, in order to continue the progress seen in recent years, we are seeking an infusion of new ideas from these communities. This document details the GREAT08 Challenge for potential participants. Please visit http://www.great08challenge.info for the latest information. ",https://doi.org/10.1214/08-AOAS222,0802.1214v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,A Hubble & Spitzer Space Telescope Survey for Gravitationally-Lensed   Galaxies: Further Evidence for a Significant Population of Low Luminosity   Galaxies beyond Redshift Seven,1970,"  We present the results of a systematic search for gravitationally-lensed continuum Lyman break `drop-outs' beyond a redshift 7 conducted via very deep imaging through six foreground clusters undertaken with the Hubble and Spitzer Space Telescopes. The survey has yielded 10 z-band and 2 J-band drop-out candidates to photometric limits of J_110~=26.2 AB (5sigma). Taking into account the magnifications afforded by our clusters (1-4 magnitudes), we probe the presence of z>7 sources to unlensed limits of J_{110}~=30 AB, fainter than those charted in the Hubble Ultradeep Field. To verify the fidelity of our candidates we conduct a number of tests for instrumental effects which would lead to spurious detections, and carefully evaluate the likelihood of foreground contamination by considering photometric uncertainties in the drop-out signature, the upper limits from stacked IRAC data and the statistics of multiply-imaged sources. Overall, we conclude that we can expect about half of our sample of z-band drop-outs are likely to be at high redshift. An ambitious infrared spectroscopic campaign undertaken with the NIRSPEC spectrograph at the WM Keck Observatory for seven of the most promising candidates failed to detect any Lyman-alpha emission highlighting the challenge of making further progress in this field. While the volume density of high redshift sources will likely remain uncertain until more powerful facilities are available, our data provides the first potentially interesting constraints on the UV luminosity function at z~=7.5 at intrinsically faint limits. We discuss the implications of our results in the context of the hypothesis that the bulk of the reionizing photons in the era 7<z<12 arise in low luminosity galaxies undetected by conventional surveys. ",https://doi.org/10.1086/591312,0803.4391v4,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,"Strong Lensing as a Probe of the Mass Distribution Beyond the Einstein   Radius. Mass & Light in SL2S J08544-0121, a Galaxy Group at z=0.35",1970,"  Precise modelling of strong lensing systems can be affected by external mass distributions, e.g. the group or cluster within which the lens is embedded. In this article, we propose to turn this limitation to our advantage and to use precise strong lensing modelling to probe external mass distributions surrounding the lens. We consider SL2S J08544-0121, a galaxy group at z=0.35 that contains a strong lensing system. A simple elliptical isothermal potential cannot reproduce satisfactorily the strong lensing constraints. We include an external mass perturbation corresponding to the group within which the lens is embedded. The lensing properties of this perturbation are parametrised by its total mass M and a smoothing scale s that quantifies the characteristic scale over which M is distributed. For a range of these parameters, we are able to reproduce accurately the observations. This suggests that light is a good tracer of mass. Interestingly, this also shows that a localised strong lensing analysis (on scales of ~10"") allows us to constrain global properties of the group as a whole (on scales of ~100). Indeed, we constrain the group mass-to-light ratio to be M/L=98+-27 (i band, solar units, not corrected for evolution) and s=20"" +- 9 (2sigma confidence level). We demonstrate that these strong lensing only constraints are due to the perturbed strong lensing configuration, where the main arc is located at ~5"" from the galaxy, whereas its counter-image is found at ~8"". To test independently our resulting strong lensing model, we pursue an independent weak lensing analysis of the group and find a mass-to-light ratio in the range 66-146 (1sigma confidence level). ",https://doi.org/10.1051/0004-6361/200912747,0906.4118v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Type Ia Supernova Rate in Redshift 0.5--0.9 Galaxy Clusters,1970,"  Supernova (SN) rates are potentially powerful diagnostics of metal enrichment and SN physics, particularly in galaxy clusters with their deep, metal-retaining potentials and relatively simple star-formation histories. We have carried out a survey for supernovae (SNe) in galaxy clusters, at a redshift range 0.5<z<0.9, using the Advanced Camera for Surveys (ACS) on the Hubble Space Telescope. We reimaged a sample of 15 clusters that were previously imaged by ACS, thus obtaining two to three epochs per cluster, in which we discovered five likely cluster SNe, six possible cluster SNe Ia, two hostless SN candidates, and several background and foreground events. Keck spectra of the host galaxies were obtained to establish cluster membership. We conducted detailed efficiency simulations, and measured the stellar luminosities of the clusters using Subaru images. We derive a cluster SN rate of 0.35 SNuB +0.17/-0.12 (statistical) \pm0.13 (classification) \pm0.01 (systematic) [where SNuB = SNe (100 yr 10^10 L_B_sun)^-1] and 0.112 SNuM +0.055/-0.039 (statistical) \pm0.042 (classification) \pm0.005 (systematic) [where SNuM = SNe (100 yr 10^10 M_sun)^-1]. As in previous measurements of cluster SN rates, the uncertainties are dominated by small-number statistics. The SN rate in this redshift bin is consistent with the SN rate in clusters at lower redshifts (to within the uncertainties), and shows that there is, at most, only a slight increase of cluster SN rate with increasing redshift. The low and fairly constant SN Ia rate out to z~1 implies that the bulk of the iron mass in clusters was already in place by z~1. The recently observed doubling of iron abundances in the intracluster medium between z=1 and 0, if real, is likely the result of redistribution of existing iron, rather than new production of iron. ",https://doi.org/10.1088/0004-637X/718/2/876,1006.3757v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Hubble Frontier Fields : A High Precision Strong Lensing Analysis of   Galaxy Cluster MACSJ0416.1-2403 using ~200 Multiple Images,1970,"  We present a high-precision mass model of the galaxy cluster MACSJ0416.1-2403, based on a strong-gravitational-lensing analysis of the recently acquired Hubble Space Telescope Frontier Fields (HFF) imaging data. Taking advantage of the unprecedented depth provided by HST/ACS observations in three passbands, we identify 51 new multiply imaged galaxies, quadrupling the previous census and bringing the grand total to 68, comprising 194 individual lensed images. Having selected a subset of the 57 most securely identified multiply imaged galaxies, we use the Lenstool software package to constrain a lens model comprised of two cluster-scale dark-matter halos and 98 galaxy-scale halos. Our best-fit model predicts image positions with an $RMS$ error of 0.68'', which constitutes an improvement of almost a factor of two over previous, pre-HFF models of this cluster. We find the total projected mass inside a 200~kpc aperture to be $(1.60\pm0.01)\times 10^{14}\ M_\odot$, a measurement that offers a three-fold improvement in precision, reaching the percent level for the first time in any cluster. Finally, we quantify the increase in precision of the derived gravitational magnification of high-redshift galaxies and find an improvement by a factor of $\sim$2.5 in the statistical uncertainty. Our findings impressively confirm that HFF imaging has indeed opened the domain of high-precision mass measurements for massive clusters of galaxies. ",https://doi.org/10.1093/mnras/stu1355,1405.3582v3,Yes,impressively(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,SPIDERS: the spectroscopic follow-up of X-ray selected clusters of   galaxies in SDSS-IV,1970,"  SPIDERS (The SPectroscopic IDentification of eROSITA Sources) is a program dedicated to the homogeneous and complete spectroscopic follow-up of X-ray AGN and galaxy clusters over a large area ($\sim$7500 deg$^2$) of the extragalactic sky. SPIDERS is part of the SDSS-IV project, together with the Extended Baryon Oscillation Spectroscopic Survey (eBOSS) and the Time-Domain Spectroscopic Survey (TDSS). This paper describes the largest project within SPIDERS before the launch of eROSITA: an optical spectroscopic survey of X-ray selected, massive ($\sim 10^{14}$ to $10^{15}~M_{\odot}$) galaxy clusters discovered in ROSAT and XMM-Newton imaging. The immediate aim is to determine precise ($\Delta_z \sim 0.001$) redshifts for 4,000-5,000 of these systems out to $z \sim 0.6$. The scientific goal of the program is precision cosmology, using clusters as probes of large-scale structure in the expanding Universe. We present the cluster samples, target selection algorithms and observation strategies. We demonstrate the efficiency of selecting targets using a combination of SDSS imaging data, a robust red-sequence finder and a dedicated prioritization scheme. We describe a set of algorithms and work-flow developed to collate spectra and assign cluster membership, and to deliver catalogues of spectroscopically confirmed clusters. We discuss the relevance of line-of-sight velocity dispersion estimators for the richer systems. We illustrate our techniques by constructing a catalogue of 230 spectroscopically validated clusters ($0.031 < z < 0.658$), found in pilot observations. We discuss two potential science applications of the SPIDERS sample: the study of the X-ray luminosity-velocity dispersion ($L_X-\sigma$) relation and the building of stacked phase-space diagrams. ",https://doi.org/10.1093/mnras/stw2214,1608.08963v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Quasi-Stellar Objects acting as potential Strong Gravitational Lenses in   the SDSS-III BOSS survey,1970,"  We present a sample of 12 Quasi-Stellar Objects (QSOs) potentially acting as strong gravitational lenses on background Emission Line Galaxies (ELG) or Lyman-$\alpha$ Emitters (LAEs) selected selected through a systematic search of the 297301 QSOs in the Sloan Digital Sky Survey (SDSS)-III Data Release 12. Candidates are identified by looking for compound spectra, where emission lines at a redshift larger than that of the quasar can be identified in the residuals after a QSO spectral template is subtracted from the observed spectra. The narrow diameter of BOSS fibers (2"") then ensures that the object responsible for the additional emission lines must lie close to the line of sight of the QSO and hence provides a high probability of lensing. Among the 12 candidates identified, 9 have definite evidence for the presence of a background ELG identified by at least 4 higher-redshift nebular emission lines. The remaining $3$ probable candidates present a strong asymmetrical emission line attributed to a background Lyman-$\alpha$ emitter (LAE). The QSO-ELG (QSO-LAE) lens candidates have QSO lens redshifts in the range $0.24\lesssim z_{QSO} \lesssim 0.66$ ($0.75 \lesssim z_{QSO} \lesssim 1.23$ ) and background galaxy redshifts in the range $0.48 \lesssim z_{S,ELG} \lesssim 0.94$ ($2.17 \lesssim z_{S,LAE} \lesssim 4.48$). We show that the algorithmic search is complete at >90% for QSO-ELG systems, whereas it falls at 40-60% for QSO-LAE, depending on the redshift of the source. Upon confirmation of the lensing nature of the systems, this sample may quadruple the number of known QSOs acting as strong lenses. We have determined the completeness of our search, which allows future studies to compute lensing probabilities of galaxies by QSOs and differentiate between different QSO models. (Abridged) ",https://doi.org/10.1051/0004-6361/201834978,1711.01184v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,High Performance Computing for gravitational lens modeling: single vs   double precision on GPUs and CPUs,1970,"  Strong gravitational lensing is a powerful probe of cosmology and the dark matter distribution. Efficient lensing software is already a necessity to fully use its potential and the performance demands will only increase with the upcoming generation of telescopes. In this paper, we study the possible impact of High Performance Computing techniques on a performance-critical part of the widely used lens modeling software LENSTOOL. We implement the algorithm once as a highly optimized CPU version and once with graphics card acceleration for a simple parametric lens model. In addition, we study the impact of finite machine precision on the lensing algorithm. While double precision is the default choice for scientific applications, we find that single precision can be sufficiently accurate for our purposes and lead to a big speedup. Therefore we develop and present a mixed precision algorithm which only uses double precision when necessary. We measure the performance of the different implementations and find that the use of High Performance Computing Techniques dramatically improves the code performance both on CPUs and GPUs. Compared to the current LENSTOOL implementation on 12 CPU cores, we obtain speedup factors of up to 170. We achieve this optimal performance by using our mixed precision algorithm on a high-end GPU which is common in modern supercomputers. We also show that these techniques reduce the energy consumption by up to 98%. Furthermore, we demonstrate that a highly competitive speedup can be reached with consumer GPUs. While they are an order of magnitude cheaper than the high-end graphics cards, they are rarely used for scientific computations due to their low double precision performance. Our mixed precision algorithm unlocks their full potential. The consumer GPU delivers a speedup which is only a factor of four lower than the best speedup achieved by a high-end GPU. ",Kein DOI-Link verfügbar,1902.03252v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Model BOSS & eBOSS Luminous Red Galaxies at 0.2 < z < 1.0 using SubHalo   Abundance Matching with 3 parameters,1970,"  SubHalo Abundance Matching (SHAM) is an empirical method for constructing galaxy catalogues based on high-resolution $N$-body simulations. We apply SHAM on the UNIT simulation to simulate SDSS BOSS/eBOSS Luminous Red Galaxies (LRGs) within a wide redshift range of $0.2 < z < 1.0$. Besides the typical SHAM scatter parameter $\sigma$, we include $v_{\rm smear}$ and $V_{\rm ceil}$ to take into account the redshift uncertainty and the galaxy incompleteness respectively. These two additional parameters are critical for reproducing the observed 2PCF multipoles on 5--25$\,h^{-1}\,{\rm Mpc}$. The redshift uncertainties obtained from the best-fitting $v_{\rm smear}$ agree with those measured from repeat observations for all SDSS LRGs except for the LOWZ sample. We explore several potential systematics but none of them can explain the discrepancy found in LOWZ. Our explanation is that the LOWZ galaxies might contain another type of galaxies which needs to be treated differently. The evolution of the measured $\sigma$ and $V_{\rm ceil}$ also reveals that the incompleteness of eBOSS galaxies decreases with the redshift. This is the consequence of the magnitude lower limit applied in eBOSS LRG target selection. Our SHAM also set upper limits for the intrinsic scatter of the galaxy--halo relation given a complete galaxy sample: $\sigma_{\rm int}<0.31$ for LOWZ at $0.2<z<0.33$, $\sigma_{\rm int}<0.36$ for LOWZ at $0.33<z<0.43$, and $\sigma_{\rm int}<0.46$ for CMASS at $0.43<z<0.51$. The projected 2PCFs of our SHAM galaxies also agree with the observational ones on the 2PCF fitting range. ",https://doi.org/10.1093/mnras/stac2176,2203.11069v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,An unbiased method of measuring the ratio of two data sets,1970,"  In certain cases of astronomical data analysis, the meaningful physical quantity to extract is the ratio $R$ between two data sets. Examples include the lensing ratio, the interloper rate in spectroscopic redshift samples, the decay rate of gravitational potential and $E_G$ to test gravity. However, simply taking the ratio of the two data sets is biased, since it renders (even statistical) errors in the denominator into systematic errors in $R$. Furthermore, it is not optimal in minimizing statistical errors of $R$. Based on Bayesian analysis and the usual assumption of Gaussian error in the data, we derive an analytical expression of the posterior PDF $P(R)$. This result enables fast and unbiased $R$ measurement, with minimal statistical errors. Furthermore, it relies on no underlying model other than the proportionality relation between the two data sets. Even more generally, it applies to the cases where the proportionality relation holds for the underlying physics/statistics instead of the two data sets directly. It also applies to the case of multiple ratios ($R\rightarrow {\bf R}=(R_1,R_2,\cdots)$). We take the lensing ratio as an example to demonstrate our method. We take lenses as DESI imaging survey galaxies, and sources as DECaLS cosmic shear and \emph{Planck} CMB lensing. We restrict the analysis to the ratio between CMB lensing and cosmic shear. The resulting $P(R)$, for multiple lens-shear pairs, are all nearly Gaussian. The S/N of measured $R$ ranges from $4.9$ to $8.4$. We perform several tests to verify the robustness of the above result. ",https://doi.org/10.3847/1538-4365/acda2a,2210.13717v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Cosmic void exclusion models and their impact on the distance scale   measurements from large scale structure,1970,"  Baryonic Acoustic Oscillations (BAOs) studies based on the clustering of voids and matter tracers provide important constraints on cosmological parameters related to the expansion of the Universe. However, modelling the void exclusion effect is an important challenge for fully exploiting the potential of this kind of analyses. We thus develop two numerical methods to describe the clustering of cosmic voids. Neither model requires additional cosmological information beyond that assumed within the galaxy de-wiggled model. The models consist in power spectra whose performance we assess in comparison to a parabolic model on Patchy cubic and light-cone mocks. Moreover, we test their robustness against systematic effects and the reconstruction technique. The void model power spectra and the parabolic model with a fixed parameter provide strongly correlated values for the Alcock-Paczynski ($\alpha$) parameter, for boxes and light-cones likewise. The resulting $\alpha$ values -- for all three models -- are unbiased and their uncertainties are correctly estimated. However, the numerical models show less variation with the fitting range compared to the parabolic one. The Bayesian evidence suggests that the numerical techniques are often favoured compared to the parabolic model. Moreover, the void model power spectra computed on boxes can describe the void clustering from light-cones as well as from boxes. The same void model power spectra can be used for the study of pre- and post-reconstructed data-sets. Lastly, the two numerical techniques are resilient against the studied systematic effects. Consequently, using either of the two new void models, one can more robustly measure cosmological parameters. ",https://doi.org/10.1093/mnras/stad813,2211.04328v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,DESI and DECaLS (D&D): galaxy-galaxy lensing measurements with 1% survey   and its forecast,1970,"  The shear measurement from DECaLS (Dark Energy Camera Legacy Survey) provides an excellent opportunity for galaxy-galaxy lensing study with DESI (Dark Energy Spectroscopic Instrument) galaxies, given the large ($\sim 9000$ deg$^2$) sky overlap. We explore this potential by combining the DESI 1\% survey and DECaLS DR8. With $\sim 106$ deg$^2$ sky overlap, we achieve significant detection of galaxy-galaxy lensing for BGS and LRG as lenses. Scaled to the full BGS sample, we expect the statistical errors to improve from $18(12)\%$ to a promising level of $2(1.3)\%$ at $\theta>8^{'}(<8^{'})$. This brings stronger requirements for future systematics control. To fully realize such potential, we need to control the residual multiplicative shear bias $|m|<0.01$ and the bias in the mean redshift $|\Delta z|<0.015$. We also expect significant detection of galaxy-galaxy lensing with DESI LRG/ELG full samples as lenses, and cosmic magnification of ELG through cross-correlation with low-redshift DECaLS shear. {If such systematical error control can be achieved,} we find the advantages of DECaLS, comparing with KiDS (Kilo Degree Survey) and HSC (Hyper-Suprime Cam), are at low redshift, large-scale, and in measuring the shear-ratio (to $\sigma_R\sim 0.04$) and cosmic magnification. ",https://doi.org/10.1093/mnras/stad2221,2301.13434v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,CURLING - I. The Influence of Point-like Image Approximation on the   Outcomes of Cluster Strong Lens Modeling,1970,"  Cluster-scale strong lensing is a powerful tool for exploring the properties of dark matter and constraining cosmological models. However, due to the complex parameter space, pixelized strong lens modeling in galaxy clusters is computationally expensive, leading to the point-source approximation of strongly lensed extended images, potentially introducing systematic biases. Herein, as the first paper of the ClUsteR strong Lens modelIng for the Next-Generation observations (CURLING) program, we use lensing ray-tracing simulations to quantify the biases and uncertainties arising from the point-like image approximation for JWST-like observations. Our results indicate that the approximation works well for reconstructing the total cluster mass distribution, but can bias the magnification measurements near critical curves and the constraints on the cosmological parameters, the total matter density of the Universe $\Omega_{\rm m}$, and dark energy equation of state parameter $w$. To mitigate the biases, we propose incorporating the extended surface brightness distribution of lensed sources into the modeling. This approach reduces the bias in magnification from 46.2 per cent to 0.09 per cent for $\mu \sim 1000$. Furthermore, the median values of cosmological parameters align more closely with the fiducial model. In addition to the improved accuracy, we also demonstrate that the constraining power can be substantially enhanced. In conclusion, it is necessary to model cluster-scale strong lenses with pixelized multiple images, especially for estimating the intrinsic luminosity of highly magnified sources and accurate cosmography in the era of high-precision observations. ",Kein DOI-Link verfügbar,2405.03135v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Strong gravitational lensing probes of the particle nature of dark   matter,1970,"  There is a vast menagerie of plausible candidates for the constituents of dark matter, both within and beyond extensions of the Standard Model of particle physics. Each of these candidates may have scattering (and other) cross section properties that are consistent with the dark matter abundance, BBN, and the most scales in the matter power spectrum; but which may have vastly different behavior at sub-galactic ""cutoff"" scales, below which dark matter density fluctuations are smoothed out. The only way to quantitatively measure the power spectrum behavior at sub-galactic scales at distances beyond the local universe, and indeed over cosmic time, is through probes available in multiply imaged strong gravitational lenses. Gravitational potential perturbations by dark matter substructure encode information in the observed relative magnifications, positions, and time delays in a strong lens. Each of these is sensitive to a different moment of the substructure mass function and to different effective mass ranges of the substructure. The time delay perturbations, in particular, are proving to be largely immune to the degeneracies and systematic uncertainties that have impacted exploitation of strong lenses for such studies. There is great potential for a coordinated theoretical and observational effort to enable a sophisticated exploitation of strong gravitational lenses as direct probes of dark matter properties. This opportunity motivates this white paper, and drives the need for: a) strong support of the theoretical work necessary to understand all astrophysical consequences for different dark matter candidates; and b) tailored observational campaigns, and even a fully dedicated mission, to obtain the requisite data. ",Kein DOI-Link verfügbar,0902.3219v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,"A Spectroscopic Road Map for Cosmic Frontier: DESI, DESI-II, Stage-5",1970,"  In this white paper, we present an experimental road map for spectroscopic experiments beyond DESI. DESI will be a transformative cosmological survey in the 2020s, mapping 40 million galaxies and quasars and capturing a significant fraction of the available linear modes up to z=1.2. DESI-II will pilot observations of galaxies both at much higher densities and extending to higher redshifts. A Stage-5 experiment would build out those high-density and high-redshift observations, mapping hundreds of millions of stars and galaxies in three dimensions, to address the problems of inflation, dark energy, light relativistic species, and dark matter. These spectroscopic data will also complement the next generation of weak lensing, line intensity mapping and CMB experiments and allow them to reach their full potential. ",Kein DOI-Link verfügbar,2209.03585v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The MegaMapper: A Stage-5 Spectroscopic Instrument Concept for the Study   of Inflation and Dark Energy,1970,"  In this white paper, we present the MegaMapper concept. The MegaMapper is a proposed ground-based experiment to measure Inflation parameters and Dark Energy from galaxy redshifts at $2<z<5$. In order to achieve path-breaking results with a mid-scale investment, the MegaMapper combines existing technologies for critical path elements and pushes innovative development in other design areas. To this aim, we envision a 6.5-m Magellan-like telescope, with a newly designed wide field, coupled with DESI spectrographs, and small-pitch robots to achieve multiplexing of at least 26,000. This will match the expected achievable target density in the redshift range of interest and provide a 10x capability over the existing state-of the art, without a 10x increase in project budget. ",Kein DOI-Link verfügbar,2209.04322v1,Yes,innovative(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Fourteenth Data Release of the Sloan Digital Sky Survey: First   Spectroscopic Data from the extended Baryon Oscillation Spectroscopic Survey   and from the second phase of the Apache Point Observatory Galactic Evolution   Experiment,1970,"  The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been in operation since July 2014. This paper describes the second data release from this phase, and the fourteenth from SDSS overall (making this, Data Release Fourteen or DR14). This release makes public data taken by SDSS-IV in its first two years of operation (July 2014-2016). Like all previous SDSS releases, DR14 is cumulative, including the most recent reductions and calibrations of all data taken by SDSS since the first phase began operations in 2000. New in DR14 is the first public release of data from the extended Baryon Oscillation Spectroscopic Survey (eBOSS); the first data from the second phase of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2), including stellar parameter estimates from an innovative data driven machine learning algorithm known as ""The Cannon""; and almost twice as many data cubes from the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previous release (N = 2812 in total). This paper describes the location and format of the publicly available data from SDSS-IV surveys. We provide references to the important technical papers describing how these data have been taken (both targeting and observation details) and processed for scientific use. The SDSS website (www.sdss.org) has been updated for this release, and provides links to data downloads, as well as tutorials and examples of data use. SDSS-IV is planning to continue to collect astronomical data until 2020, and will be followed by SDSS-V. ",https://doi.org/10.3847/1538-4365/aa9e8a,1707.09322v3,Yes,innovative(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Sixteenth Data Release of the Sloan Digital Sky Surveys: First   Release from the APOGEE-2 Southern Survey and Full Release of eBOSS Spectra,1970,"  This paper documents the sixteenth data release (DR16) from the Sloan Digital Sky Surveys; the fourth and penultimate from the fourth phase (SDSS-IV). This is the first release of data from the southern hemisphere survey of the Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2); new data from APOGEE-2 North are also included. DR16 is also notable as the final data release for the main cosmological program of the Extended Baryon Oscillation Spectroscopic Survey (eBOSS), and all raw and reduced spectra from that project are released here. DR16 also includes all the data from the Time Domain Spectroscopic Survey (TDSS) and new data from the SPectroscopic IDentification of ERosita Survey (SPIDERS) programs, both of which were co-observed on eBOSS plates. DR16 has no new data from the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey (or the MaNGA Stellar Library ""MaStar""). We also preview future SDSS-V operations (due to start in 2020), and summarize plans for the final SDSS-IV data release (DR17). ",https://doi.org/10.3847/1538-4365/ab929e,1912.02905v2,Yes,notable(1)
0000-0002-3223-6320,Adélie Garin,EPFL,A Lattice-Theoretic Perspective on the Persistence Map,1970,"  We provide a naturally isomorphic description of the persistence map from merge trees to barcodes in terms of a monotone map from the partition lattice to the subset lattice. Our description is local, which offers the potential to speed up inverse computations, and brings classical tools in combinatorics to bear on an active area of research in topological data analysis (TDA). ",Kein DOI-Link verfügbar,2203.00643v1,Yes,potent(1)
0009-0001-9739-8849,Luca Arnaboldi,EPFL,Quantitative Analysis of DoS Attacks and Client Puzzles in IoT Systems,1970,"  Denial of Service (DoS) attacks constitute a major security threat to today's Internet. This challenge is especially pertinent to the Internet of Things (IoT) as devices have less computing power, memory and security mechanisms to mitigate DoS attacks. This paper presents a model that mimics the unique characteristics of a network of IoT devices, including components of the system implementing `Crypto Puzzles' - a DoS mitigation technique. We created an imitation of a DoS attack on the system, and conducted a quantitative analysis to simulate the impact such an attack may potentially exert upon the system, assessing the trade off between security and throughput in the IoT system. We model this through stochastic model checking in PRISM and provide evidence that supports this as a valuable method to compare the efficiency of different implementations of IoT systems, exemplified by a case study. ",https://doi.org/10.1007/978-3-319-68063-7_16,1710.11021v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Mathematical Optimization of Resolution Improvement in Structured Light   data by Periodic Scanning Motion: Application for Feedback during Lunar   Landing,1970,"  This research explores the enhancement of lunar landing precision through an advanced structured light system, integrating machine learning, Iterative Learning Control (ILC) and Structured Illumination Microscopy (SIM) techniques. By employing Moire fringe patterns for high-precision scanning maneuvers, the study addresses the limitations of conventional structured light systems. A nonlinear mathematical optimization model is developed to refine the world model, optimizing oscillation frequency and amplitude to improve resolution. The findings suggest that this approach can double the conventional resolution, promising significant advancements in the accuracy of lunar landings, with potential real-time application. ",Kein DOI-Link verfügbar,2408.06628v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based   Online Intelligent Education Systems,1970,"  Cognitive diagnosis aims to gauge students' mastery levels based on their response logs. Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing. WOIESs are open learning environment where numerous new students constantly register and complete exercises. In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning. However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training. To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. Specifically, in ICDM, we propose a novel student-centered graph (SCG). Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG. Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining. To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts. Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students. ",https://doi.org/10.1145/3589334.3645589,2404.11290v1,Yes,pivotal(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Autler-Townes doublet in single-photon Rydberg spectra of Cesium atomic   vapor with a 319 nm UV laser,1970,"  We demonstrate the single-photon excitation spectra of cesium Rydberg atoms by means of a Doppler-free purely all-optical detection with a room-temperature vapor cell and a 319 nm ultra-violet (UV) laser. We excite atoms directly from 6S1/2 ground state to 71P3/2 Rydberg state with a narrow-linewidth 319 nm UV laser. The detection of Rydberg states is performed by monitoring the absorption of an 852 nm probe beam in a V-type three-level system. With a strong coupling light, we observe the Autler-Townes doublet and investigate experimentally the dependence of the separation and linewidth on the coupling intensity, which is consistent with the prediction based on the dressed state theory. We further investigate the Rydberg spectra with an external magnetic field. The existence of non-degenerate Zeeman sub-levels results in the broadening and shift of the spectra. It has potential application in sensing magnetic field. ",https://doi.org/10.1007/s00340-019-7151-x,1806.00169v3,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Towards implementation of a magic optical-dipole trap for confining   ground-state and Rydberg-state cesium cold atoms,1970,"  Long ground-Rydberg coherence lifetime is interesting for implementing high-fidelity quantum logic gates, many-body physics, and other quantum information protocols. However, the potential formed by a conventional far-off-resonance red-detuned optical-dipole trap (ODT) is usually repulsive for Rydberg atoms, which will result in fast atom loss and low repetition rate of the experimental sequence. These issues can be addressed by a magic ODT. We performed the calculation of ODT's magic detuning for confinement of cesium ground state and Rydberg state with the same potential well. We used a sum-over-states method to calculate the dynamic polarizabilities of $6S_{1/2}$ ground state and highly-excited ($nS_{1/2}$ and $nP_{3/2}$) Rydberg state of cesium atoms, and identify corresponding magic detuning for optical wavelengths in the range of $850 - 2000$ nm. We estimated the trapping lifetime of cesium Rydberg atoms confined in the magic ODT by including different dissipative mechanisms. Furthermore, we have experimentally realized an 1879.43-nm single-frequency laser system with a watt-level output power for setting up the magic ODT for $6S_{1/2}$ ground-state and $84P_{3/2}$ Rydberg-state cesium cold atoms. ",https://doi.org/10.1088/1361-6455/ab91de,2001.05698v2,Yes,potent(2)
0000-0002-8947-1954,Shuo Liu,EPFL,Training variational quantum algorithms with random gate activation,1970,"  Variational quantum algorithms (VQAs) hold great potentials for near-term applications and are promising to achieve quantum advantage on practical tasks. However, VQAs suffer from severe barren plateau problem as well as have a large probability of being trapped in local minima. In this Letter, we propose a novel training algorithm with random quantum gate activation for VQAs to efficiently address these two issues. This new algorithm processes effectively much fewer training parameters than the conventional plain optimization strategy, which efficiently mitigates barren plateaus with the same expressive capability. Additionally, by randomly adding two-qubit gates to the circuit ansatz, the optimization trajectories can escape from local minima and reach the global minimum more frequently due to more sources of randomness. In real quantum experiments, the new training algorithm can also reduce the quantum computational resources required and be more quantum noise resilient. We apply our training algorithm to solve variational quantum simulation problems for ground states and present convincing results that showcase the advantages of our novel strategy where better performance is achieved by the combination of mitigating barren plateaus, escaping from local minima, and reducing the effect of quantum noises. We further propose that the entanglement phase transition could be one underlying reason why our RA training is so effective. ",https://doi.org/10.1103/PhysRevResearch.5.L032040,2303.08154v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Multistage linguistic conditioning of convolutional layers for speech   emotion recognition,1970,"  In this contribution, we investigate the effectiveness of deep fusion of text and audio features for categorical and dimensional speech emotion recognition (SER). We propose a novel, multistage fusion method where the two information streams are integrated in several layers of a deep neural network (DNN), and contrast it with a single-stage one where the streams are merged in a single point. Both methods depend on extracting summary linguistic embeddings from a pre-trained BERT model, and conditioning one or more intermediate representations of a convolutional model operating on log-Mel spectrograms. Experiments on the MSP-Podcast and IEMOCAP datasets demonstrate that the two fusion methods clearly outperform a shallow (late) fusion baseline and their unimodal constituents, both in terms of quantitative performance and qualitative behaviour. Overall, our multistage fusion shows better quantitative performance, surpassing alternatives on most of our evaluations. This illustrates the potential of multistage fusion in better assimilating text and audio information. ",https://doi.org/10.3389/fcomp.2023.1072479,2110.06650v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,CloudChain: A Cloud Blockchain Using Shared Memory Consensus and RDMA,1970,"  Blockchain technologies can enable secure computing environments among mistrusting parties. Permissioned blockchains are particularly enlightened by companies, enterprises, and government agencies due to their efficiency, customizability, and governance-friendly features. Obviously, seamlessly fusing blockchain and cloud computing can significantly benefit permissioned blockchains; nevertheless, most blockchains implemented on clouds are originally designed for loosely-coupled networks where nodes communicate asynchronously, failing to take advantages of the closely-coupled nature of cloud servers. In this paper, we propose an innovative cloud-oriented blockchain -- CloudChain, which is a modularized three-layer system composed of the network layer, consensus layer, and blockchain layer. CloudChain is based on a shared-memory model where nodes communicate synchronously by direct memory accesses. We realize the shared-memory model with the Remote Direct Memory Access technology, based on which we propose a shared-memory consensus algorithm to ensure presistence and liveness, the two crucial blockchain security properties countering Byzantine nodes. We also implement a CloudChain prototype based on a RoCEv2-based testbed to experimentally validate our design, and the results verify the feasibility and efficiency of CloudChain. ",Kein DOI-Link verfügbar,2106.04122v1,Yes,innovative(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Sensor Misalignment-tolerant AUV Navigation with Passive DoA and Doppler   Measurements,1970,"  We present a sensor misalignment-tolerant AUV navigation method that leverages measurements from an acoustic array and dead reckoned information. Recent studies have demonstrated the potential use of passive acoustic Direction of Arrival (DoA) measurements for AUV navigation without requiring ranging measurements. However, the sensor misalignment between the acoustic array and the attitude sensor was not accounted for. Such misalignment may deteriorate the navigation accuracy. This paper proposes a novel approach that allows simultaneous AUV navigation, beacon localization, and sensor alignment. An Unscented Kalman Filter (UKF) that enables the necessary calculations to be completed at an affordable computational load is developed. A Nonlinear Least Squares (NLS)-based technique is employed to find an initial solution for beacon localization and sensor alignment as early as possible using a short-term window of measurements. Experimental results demonstrate the performance of the proposed method. ",Kein DOI-Link verfügbar,2402.07218v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,LVLM-eHub: A Comprehensive Evaluation Benchmark for Large   Vision-Language Models,1970,"  Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena ",Kein DOI-Link verfügbar,2306.09265v1,Yes,innovative(2)
0000-0002-8947-1954,Shuo Liu,EPFL,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with   Hierarchical Capability for Large Vision-Language Models,1970,"  This paper presents ConvBench, a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing benchmarks that assess individual capabilities in single-turn dialogues, ConvBench adopts a three-level multimodal capability hierarchy, mimicking human cognitive processes by stacking up perception, reasoning, and creativity. Each level focuses on a distinct capability, mirroring the cognitive progression from basic perception to logical reasoning and ultimately to advanced creativity. ConvBench comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands. Automatic evaluations quantify response performance at each turn and overall conversation level. Leveraging the capability hierarchy, ConvBench enables precise attribution of conversation mistakes to specific levels. Experimental results reveal a performance gap between multi-modal models, including GPT4-V, and human performance in multi-turn conversations. Additionally, weak fine-grained perception in multi-modal models contributes to reasoning and creation failures. ConvBench serves as a catalyst for further research aimed at enhancing visual dialogues. ",Kein DOI-Link verfügbar,2403.20194v2,Yes,"meticulous(1), meticulously(1)"
0000-0002-8947-1954,Shuo Liu,EPFL,"An Early Study on Intelligent Analysis of Speech under COVID-19:   Severity, Sleep Quality, Fatigue, and Anxiety",1970,"  The COVID-19 outbreak was announced as a global pandemic by the World Health Organisation in March 2020 and has affected a growing number of people in the past few weeks. In this context, advanced artificial intelligence techniques are brought to the fore in responding to fight against and reduce the impact of this global health crisis. In this study, we focus on developing some potential use-cases of intelligent speech analysis for COVID-19 diagnosed patients. In particular, by analysing speech recordings from these patients, we construct audio-only-based models to automatically categorise the health state of patients from four aspects, including the severity of illness, sleep quality, fatigue, and anxiety. For this purpose, two established acoustic feature sets and support vector machines are utilised. Our experiments show that an average accuracy of .69 obtained estimating the severity of illness, which is derived from the number of days in hospitalisation. We hope that this study can foster an extremely fast, low-cost, and convenient way to automatically detect the COVID-19 disease. ",Kein DOI-Link verfügbar,2005.00096v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,SOS-1K: A Fine-grained Suicide Risk Classification Dataset for Chinese   Social Media Analysis,1970,"  In the social media, users frequently express personal emotions, a subset of which may indicate potential suicidal tendencies. The implicit and varied forms of expression in internet language complicate accurate and rapid identification of suicidal intent on social media, thus creating challenges for timely intervention efforts. The development of deep learning models for suicide risk detection is a promising solution, but there is a notable lack of relevant datasets, especially in the Chinese context. To address this gap, this study presents a Chinese social media dataset designed for fine-grained suicide risk classification, focusing on indicators such as expressions of suicide intent, methods of suicide, and urgency of timing. Seven pre-trained models were evaluated in two tasks: high and low suicide risk, and fine-grained suicide risk classification on a level of 0 to 10. In our experiments, deep learning models show good performance in distinguishing between high and low suicide risk, with the best model achieving an F1 score of 88.39%. However, the results for fine-grained suicide risk classification were still unsatisfactory, with an weighted F1 score of 50.89%. To address the issues of data imbalance and limited dataset size, we investigated both traditional and advanced, large language model based data augmentation techniques, demonstrating that data augmentation can enhance model performance by up to 4.65% points in F1-score. Notably, the Chinese MentalBERT model, which was pre-trained on psychological domain data, shows superior performance in both tasks. This study provides valuable insights for automatic identification of suicidal individuals, facilitating timely psychological intervention on social media platforms. The source code and data are publicly available. ",Kein DOI-Link verfügbar,2404.12659v1,Yes,"notable(1), potent(1)"
0000-0002-8947-1954,Shuo Liu,EPFL,Exploring and Unleashing the Power of Large Language Models in Automated   Code Translation,1970,"  Code translation tools (transpilers) are developed for automatic source-to-source translation. Although learning-based transpilers have shown impressive enhancement against rule-based counterparts, owing to their task-specific pre-training on extensive monolingual corpora. Their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. LLMs pre-trained on huge amounts of human-written code/text have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific training. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs, missing clear instructions on I/O types in translation, and ignoring discrepancies between source and target programs. Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes are tested with UniTrans, and all achieve substantial improvements. ",Kein DOI-Link verfügbar,2404.14646v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,A novel EM concentrator with open-concentrator region based on   multi-folded transformation optics,1970,"  Conventional concentrators with inhomogeneous coating materials that fully enclose the destined region pose great challenges for fabrication. In this paper, we propose to design an EM concentrator with homogeneous materials. Distinguished from conventional ones, the elaborately designed EM concentrator features a concentrator region that is open to the outer-world, which is achieved with multi-folded transformation optics method by compressing and folding the coating materials to create window(s). Based on this concept, we also investigate open-rotator and open rotational-concentrator devices, which could simultaneously rotate and store the EM waves in the central destined region. Due to the open nature of our proposed designs, we believe they will find potential applications in remote controlling with impressive new functionalities. ",https://doi.org/10.1038/s41598-018-28050-4,1805.05403v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Supervised Learning and Large Language Model Benchmarks on Mental Health   Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media,1970,"  On social media, users often express their personal feelings, which may exhibit cognitive distortions or even suicidal tendencies on certain specific topics. Early recognition of these signs is critical for effective psychological intervention. In this paper, we introduce two novel datasets from Chinese social media: SOS-HL-1K for suicidal risk classification and SocialCD-3K for cognitive distortions detection. The SOS-HL-1K dataset contained 1,249 posts and SocialCD-3K dataset was a multi-label classification dataset that containing 3,407 posts. We propose a comprehensive evaluation using two supervised learning methods and eight large language models (LLMs) on the proposed datasets. From the prompt engineering perspective, we experimented with two types of prompt strategies, including four zero-shot and five few-shot strategies. We also evaluated the performance of the LLMs after fine-tuning on the proposed tasks. The experimental results show that there is still a huge gap between LLMs relying only on prompt engineering and supervised learning. In the suicide classification task, this gap is 6.95% points in F1-score, while in the cognitive distortion task, the gap is even more pronounced, reaching 31.53% points in F1-score. However, after fine-tuning, this difference is significantly reduced. In the suicide and cognitive distortion classification tasks, the gap decreases to 4.31% and 3.14%, respectively. This research highlights the potential of LLMs in psychological contexts, but supervised learning remains necessary for more challenging tasks. All datasets and code are made available. ",Kein DOI-Link verfügbar,2309.03564v3,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Polarization-controlled anisotropic coding metamaterials at terahertz   frequencies,1970,"  Metamaterials based on effective media have achieved a lot of unusual physics (e.g. negative refraction and invisibility cloaking) owing to their abilities to tailor the effective medium parameters that do not exist in nature. Recently, coding metamaterials have been suggested to control electromagnetic waves by designing the coding sequences of digital elements '0' and '1', which possess opposite phase responses. Here, we propose the concept of anisotropic coding metamaterial at terahertz frequencies, in which coding behaviors in different directions are dependent on the polarization status of terahertz waves. We experimentally demonstrate an ultrathin and flexible polarization-controlled anisotropic coding metasurface functioning in the terahertz regime using specially- designed coding elements. By encoding the elements with elaborately-designed digital sequences (in both 1 bit and 2 bits), the x- and y-polarized reflected waves can be deflected or diffused independently in three dimensions. The simulated far-field scattering patterns as well as near-electric-field distributions are given to illustrate the bifunctional performance of the encoded metasurface, which show good agreement to the measurement results. We further demonstrate the abilities of anisotropic coding metasurface to generate beam splitter and realize anomalous reflection and polarization conversion simultaneously, providing powerful controls of differently-polarized terahertz waves. The proposed method enables versatile beam behaviors under orthogonal polarizations using a single metasurface, and hence will promise interesting terahertz devices. ",Kein DOI-Link verfügbar,1509.03692v1,Yes,versatile(1)
0000-0002-8947-1954,Shuo Liu,EPFL,HEAR4Health: A blueprint for making computer audition a staple of modern   healthcare,1970,"  Recent years have seen a rapid increase in digital medicine research in an attempt to transform traditional healthcare systems to their modern, intelligent, and versatile equivalents that are adequately equipped to tackle contemporary challenges. This has led to a wave of applications that utilise AI technologies; first and foremost in the fields of medical imaging, but also in the use of wearables and other intelligent sensors. In comparison, computer audition can be seen to be lagging behind, at least in terms of commercial interest. Yet, audition has long been a staple assistant for medical practitioners, with the stethoscope being the quintessential sign of doctors around the world. Transforming this traditional technology with the use of AI entails a set of unique challenges. We categorise the advances needed in four key pillars: Hear, corresponding to the cornerstone technologies needed to analyse auditory signals in real-life conditions; Earlier, for the advances needed in computational and data efficiency; Attentively, for accounting to individual differences and handling the longitudinal nature of medical data; and, finally, Responsibly, for ensuring compliance to the ethical standards accorded to the field of medicine. ",Kein DOI-Link verfügbar,2301.10477v1,Yes,versatile(1)
0000-0002-8947-1954,Shuo Liu,EPFL,MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large   Vision-Language Models Towards Multitask AGI,1970,"  Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence. ",Kein DOI-Link verfügbar,2404.16006v1,Yes,"meticulous(1), meticulously(1)"
0000-0001-8069-2411,Nathan Ronceray,EPFL,Modeling the shape of axisymmetric skyrmions in magnetic multilayers,1970,"  We present a comprehensive micromagnetic model of isolated axisymmetric skyrmions in magnetic multilayers with perpendicular anisotropy. Most notably, the essential role of the internal dipolar field is extensively considered with a minimum amount of assumptions on the magnetization profiles. The tri-dimensional structure of the multilayered skyrmions is modeled by their radial profiles in each layer. We first compare the results of the model against a full micromagnetic description in Cartesian coordinates. Our model combines information on both layer-dependent size and chirality of the skyrmions. We also provide a convenient criterion in order to characterize the stability of skyrmions against anisotropic elongations that would break their cylindrical symmetry, which allows to confirm the stability of the determined solutions. Because this model is able to treat magnetization configurations twisted through the thickness of multilayered skyrmions, it can provide predictions on any potential hybrid chirality in skyrmions due to the interplay of Dzyaloshinskii-Moriya and dipolar interactions in multilayers. We finally apply the results of our model to the description of the current-driven dynamics of hybrid chiral skyrmions. Using the Thiele formalism, we show that we can predict the forces exerted on the multilayered skyrmions by vertical spin-polarized currents, which provides a method to conform hybrid skyrmion chiralities and spin-current injection geometries in order to optimize skyrmion motion in multilayers, to the aim of maximizing the current-induced velocity, or canceling the skyrmion Hall angle. ",https://doi.org/10.1103/PhysRevApplied.10.064042,1807.04935v1,Yes,potent(1)
0000-0001-8069-2411,Nathan Ronceray,EPFL,Monitoring electrochemical dynamics through single-molecule imaging of   hBN surface emitters in organic solvents,1970,"  Electrochemical techniques conventionally lack spatial resolution and average local information over an entire electrode. While advancements in spatial resolution have been made through scanning probe methods, monitoring dynamics over large areas is still challenging, and it would be beneficial to be able to decouple the probe from the electrode itself. In this work, we leverage single molecule microscopy to spatiotemporally monitor analyte surface concentrations over a wide area using unmodified hexagonal boron nitride (hBN) in organic solvents. Through a sensing scheme based on redox-active species interactions with fluorescent emitters at the surface of hBN, we observe a linear decrease in the number of emitters under positive voltages applied to a nearby electrode. We find consistent trends in electrode reaction kinetics vs overpotentials between potentiostat-reported currents and optically-read emitter dynamics, showing Tafel slopes greater than 290 mV per decade. Finally, we draw on the capabilities of spectral single molecule localization microscopy (SMLM) to monitor the fluorescent species identity, enabling multiplexed readout. Overall, we show dynamic measurements of analyte concentration gradients at a micrometer-length scale with nanometer-scale depth and precision. Considering the many scalable options for engineering fluorescent emitters with 2D materials, our method holds promise for optically detecting a range of interacting species with unprecedented localization precision. ",Kein DOI-Link verfügbar,2405.10686v1,Yes,potent(2)
0000-0001-6095-5173,Franz-Josef Haug,EPFL,Angular behavior of the absorption limit in thin film silicon solar   cells,1970,"  We investigate the angular behavior of the upper bound of absorption provided by the guided modes in thin film solar cells. We show that the 4n^2 limit can be potentially exceeded in a wide angular and wavelength range using two-dimensional periodic thin film structures. Two models are used to estimate the absorption enhancement; in the first one, we apply the periodicity condition along the thickness of the thin film structure but in the second one, we consider imperfect confinement of the wave to the device. To extract the guided modes, we use an automatized procedure which is established in this work. Through examples, we show that from the optical point of view, thin film structures have a high potential to be improved by changing their shape. Also, we discuss the nature of different optical resonances which can be potentially used to enhance light trapping in the solar cell. We investigate the two different polarization directions for one-dimensional gratings and we show that the transverse magnetic polarization can provide higher values of absorption enhancement. We also propose a way to reduce the angular dependence of the solar cell efficiency by the appropriate choice of periodic pattern. Finally, to get more practical values for the absorption enhancement, we consider the effect of parasitic loss which can significantly reduce the enhancement factor. ",https://doi.org/10.1002/pip.2371,1303.2835v1,Yes,potent(3)
0000-0003-1579-5558,Martin Jaggi,EPFL,Correlating Twitter Language with Community-Level Health Outcomes,1970,"  We study how language on social media is linked to diseases such as atherosclerotic heart disease (AHD), diabetes and various types of cancer. Our proposed model leverages state-of-the-art sentence embeddings, followed by a regression model and clustering, without the need of additional labelled data. It allows to predict community-level medical outcomes from language, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of medical outcomes with life-style aspects and other socioeconomic risk factors. ",Kein DOI-Link verfügbar,1906.06465v2,Yes,potent(2)
0000-0003-1579-5558,Martin Jaggi,EPFL,An Accelerated Communication-Efficient Primal-Dual Optimization   Framework for Structured Machine Learning,1970,"  Distributed optimization algorithms are essential for training machine learning models on very large-scale datasets. However, they often suffer from communication bottlenecks. Confronting this issue, a communication-efficient primal-dual coordinate ascent framework (CoCoA) and its improved variant CoCoA+ have been proposed, achieving a convergence rate of $\mathcal{O}(1/t)$ for solving empirical risk minimization problems with Lipschitz continuous losses. In this paper, an accelerated variant of CoCoA+ is proposed and shown to possess a convergence rate of $\mathcal{O}(1/t^2)$ in terms of reducing suboptimality. The analysis of this rate is also notable in that the convergence rate bounds involve constants that, except in extreme cases, are significantly reduced compared to those previously provided for CoCoA+. The results of numerical experiments are provided to show that acceleration can lead to significant performance gains. ",Kein DOI-Link verfügbar,1711.05305v1,Yes,notable(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,Spectral Preconditioning for Gradient Methods on Graded Non-convex   Functions,1970,"  The performance of optimization methods is often tied to the spectrum of the objective Hessian. Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems. Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity. This allows to partition the class of non-convex problems into a nested chain of subclasses. Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses. As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade. Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian. Our theory is validated by numerical experiments executed on multiple practical machine learning problems. ",Kein DOI-Link verfügbar,2402.04843v1,Yes,intricate(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,CoTFormer: A Chain-of-Thought Driven Architecture with Budget-Adaptive   Computation Cost at Inference,1970,"  Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same time -- regardless of the model size -- task-specific techniques continue to play a pivotal role in achieving optimal downstream performance. One of these techniques, called Chain-of-Thought (CoT), is particularly interesting since, as we point out in this work, it resembles employing a deeper transformer through re-applying the model multiple times. However, a key subtlety in computing the attention of past tokens differentiates CoT from simply applying the model several times. Based on this insight, we propose CoTFormer, a novel architecture which closely mimics CoT at the token level, allowing us to obtain significantly improved accuracies close to much larger models. While applying CoT introduces additional computation costs, we compensate for it by leveraging CoTFormer's special compatibility with token-wise variable depth. Through a compute adaptive model -- which automatically allocates the compute to tokens that need it most -- we show that it is possible to reduce the computation cost significantly without any reduction in accuracy, and with further compute cost reductions possible while maintaining a competitive accuracy. ",Kein DOI-Link verfügbar,2310.10845v2,Yes,pivotal(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,"Modular Clinical Decision Support Networks (MoDN) -- Updatable,   Interpretable, and Portable Predictions for Evolving Clinical Environments",1970,"  Data-driven Clinical Decision Support Systems (CDSS) have the potential to improve and standardise care with personalised probabilistic guidance. However, the size of data required necessitates collaborative learning from analogous CDSS's, which are often unsharable or imperfectly interoperable (IIO), meaning their feature sets are not perfectly overlapping. We propose Modular Clinical Decision Support Networks (MoDN) which allow flexible, privacy-preserving learning across IIO datasets, while providing interpretable, continuous predictive feedback to the clinician.   MoDN is a novel decision tree composed of feature-specific neural network modules. It creates dynamic personalised representations of patients, and can make multiple predictions of diagnoses, updatable at each step of a consultation. The modular design allows it to compartmentalise training updates to specific features and collaboratively learn between IIO datasets without sharing any data. ",Kein DOI-Link verfügbar,2211.06637v1,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,A Distributed Second-Order Algorithm You Can Trust,1970,"  Due to the rapid growth of data and computational resources, distributed optimization has become an active research area in recent years. While first-order methods seem to dominate the field, second-order methods are nevertheless attractive as they potentially require fewer communication rounds to converge. However, there are significant drawbacks that impede their wide adoption, such as the computation and the communication of a large Hessian matrix. In this paper we present a new algorithm for distributed training of generalized linear models that only requires the computation of diagonal blocks of the Hessian matrix on the individual workers. To deal with this approximate information we propose an adaptive approach that - akin to trust-region methods - dynamically adapts the auxiliary model to compensate for modeling errors. We provide theoretical rates of convergence for a wide class of problems including L1-regularized objectives. We also demonstrate that our approach achieves state-of-the-art results on multiple large benchmark datasets. ",Kein DOI-Link verfügbar,1806.07569v1,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,Linear Speedup in Personalized Collaborative Learning,1970,"  Collaborative training can improve the accuracy of a model for a user by trading off the model's bias (introduced by using data from other users who are potentially different) against its variance (due to the limited amount of data on any single user). In this work, we formalize the personalized collaborative learning problem as a stochastic optimization of a task 0 while giving access to N related but different tasks 1,..., N. We provide convergence guarantees for two algorithms in this setting -- a popular collaboration method known as weighted gradient averaging, and a novel bias correction method -- and explore conditions under which we can achieve linear speedup w.r.t. the number of auxiliary tasks N. Further, we also empirically study their performance confirming our theoretical insights. ",Kein DOI-Link verfügbar,2111.05968v4,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,"MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks",1970,"  Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance. ",Kein DOI-Link verfügbar,2309.14118v2,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,MEDITRON-70B: Scaling Medical Pretraining for Large Language Models,1970,"  Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs. ",Kein DOI-Link verfügbar,2311.16079v1,Yes,potent(1)
0000-0003-3438-9774,Federico Ronchetti,EPFL,Acceleration of electromagnetic shower development and enhancement of   light yield in oriented scintillating crystals,1970,  We observed a substantial increase of the scintillation light output of lead tungstate (PbWO$_4$) at a small incidence angle with respect to two main lattice axes. This reflects the acceleration of electromagnetic shower development that occurs in the crystalline Strong Field. We measured the scintillation light generated by $120$-$\mathrm{GeV}$ electrons and $10$-$100$-$\mathrm{GeV}$ $\gamma$ rays on thick samples. This result deepens the knowledge of the shower development mechanisms in crystal scintillators and could pave the way to the development of innovative accelerator- and space-borne calorimeters. ,Kein DOI-Link verfügbar,2404.12016v1,Yes,innovative(1)
0000-0002-6374-642X,Barak Gabai,EPFL,Exact quantization and analytic continuation,1970,"  In this paper we give a streamlined derivation of the exact quantization condition (EQC) on the quantum periods of the Schr\""odinger problem in one dimension with a general polynomial potential, based on Wronskian relations. We further generalize the EQC to potentials with a regular singularity, describing spherical symmetric quantum mechanical systems in a given angular momentum sector. We show that the thermodynamic Bethe ansatz (TBA) equations that govern the quantum periods undergo nontrivial monodromies as the angular momentum is analytically continued between integer values in the complex plane. The TBA equations together with the EQC are checked numerically against Hamiltonian truncation at real angular momenta and couplings, and are used to explore the analytic continuation of the spectrum on the complex angular momentum plane in examples. ",Kein DOI-Link verfügbar,2109.07516v2,Yes,potent(2)
0000-0001-7365-0651,Katherine James,The University of Edinburgh,Drug repurposing prediction for COVID-19 using probabilistic networks   and crowdsourced curation,1970,"  Severe acute respiratory syndrome coronavirus two (SARS-CoV-2), the virus responsible for the coronavirus disease 2019 (COVID-19) pandemic, represents an unprecedented global health challenge. Consequently, a large amount of research into the disease pathogenesis and potential treatments has been carried out in a short time frame. However, developing novel drugs is a costly and lengthy process, and is unlikely to deliver a timely treatment for the pandemic. Drug repurposing, by contrast, provides an attractive alternative, as existing drugs have already undergone many of the regulatory requirements. In this work we used a combination of network algorithms and human curation to search integrated knowledge graphs, identifying drug repurposing opportunities for COVID-19. We demonstrate the value of this approach, reporting on eight potential repurposing opportunities identified, and discuss how this approach could be incorporated into future studies. ",Kein DOI-Link verfügbar,2005.11088v2,Yes,potent(2)
0000-0002-8082-2818,Bernard Mulgrew,The University of Edinburgh,Latent Parameter Estimation in Fusion Networks Using Separable   Likelihoods,1970,"  Multi-sensor state space models underpin fusion applications in networks of sensors. Estimation of latent parameters in these models has the potential to provide highly desirable capabilities such as network self-calibration. Conventional solutions to the problem pose difficulties in scaling with the number of sensors due to the joint multi-sensor filtering involved when evaluating the parameter likelihood. In this article, we propose a separable pseudo-likelihood which is a more accurate approximation compared to a previously proposed alternative under typical operating conditions. In addition, we consider using separable likelihoods in the presence of many objects and ambiguity in associating measurements with objects that originated them. To this end, we use a state space model with a hypothesis based parameterisation, and, develop an empirical Bayesian perspective in order to evaluate separable likelihoods on this model using local filtering. Bayesian inference with this likelihood is carried out using belief propagation on the associated pairwise Markov random field. We specify a particle algorithm for latent parameter estimation in a linear Gaussian state space model and demonstrate its efficacy for network self-calibration using measurements from non-cooperative targets in comparison with alternatives. ",Kein DOI-Link verfügbar,1708.00842v2,Yes,potent(1)
0000-0003-3810-9856,Christoph Hennersperger,Technical University of Munich,Towards MRI-Based Autonomous Robotic US Acquisitions: A First   Feasibility Study,1970,"  Robotic ultrasound has the potential to assist and guide physicians during interventions. In this work, we present a set of methods and a workflow to enable autonomous MRI-guided ultrasound acquisitions. Our approach uses a structured-light 3D scanner for patient-to-robot and image-to-patient calibration, which in turn is used to plan 3D ultrasound trajectories. These MRI-based trajectories are followed autonomously by the robot and are further refined online using automatic MRI/US registration. Despite the low spatial resolution of structured light scanners, the initial planned acquisition path can be followed with an accuracy of 2.46 +/- 0.96 mm. This leads to a good initialization of the MRI/US registration: the 3D-scan-based alignment for planning and acquisition shows an accuracy (distance between planned ultrasound and MRI) of 4.47 mm, and 0.97 mm after an online-update of the calibration based on a closed loop registration. ",https://doi.org/10.1109/TMI.2016.2620723,1607.08371v1,Yes,potent(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Optimal Scheduling for Discounted Age Penalty Minimization in Multi-Loop   Networked Control,1970,"  Age-of-information (AoI) is a metric quantifying information freshness at the receiver. Since AoI combines packet generation frequency, packet loss, and delay into a single metric, it has received a lot of research attention as an interface between communication network and application. In this work, we apply AoI to the problem of wireless scheduling for multi-loop networked control systems (NCS), i.e., feedback control loops closed over a shared wireless network. We model the scheduling problem as a Markov decision process (MDP) with AoI as its observable states and derive a relation of control system error and AoI. We further derive a stationary scheduling policy to minimize control error over an infinite horizon. We show that our scheduler outperforms the state-of-the-art scheduling policies for NCS. To the best of our knowledge, this is the first work proposing an AoI-based wireless scheduling policy that minimizes the control error over an infinite horizon for multi-loop NCS. ",Kein DOI-Link verfügbar,1908.01503v3,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Probability Analysis of Age of Information in Multi-hop Networks,1970,"  Age-of-information (AoI) is a metric quantifying information freshness at the receiver. It captures the delay together with packet loss and packet generation rate. However, the existing literature focuses on average or peak AoI and neglects the complete distribution. In this work, we consider a N-hop network with time-invariant packet loss probabilities on each link. We derive closed form equations for the probability mass function of AoI. We verify our findings with simulations. Our results show that the performance indicators considered in the literature such as average or peak AoI may give misleading insights into the real AoI performance. ",Kein DOI-Link verfügbar,1911.09957v2,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,AoI-based Finite Horizon Scheduling for Heterogeneous Networked Control   Systems,1970,"  Age of information (AoI) measures information freshness at the receiver. AoI may provide insights into quality of service in communication systems. For this reason, it has been used as a cross-layer metric for wireless communication protocols. In this work, we employ AoI to calculate penalty functions for a centralized resource scheduling problem. We consider a single wireless link shared by multiple, heterogeneous control systems where each sub-system has a time-varying packet loss probability. Sub-systems are competing for network resources to improve the accuracy of their remote estimation process. In order to cope with the dynamically changing conditions of the wireless link, we define a finite horizon age-penalty minimization problem and propose a scheduler that takes optimal decisions by looking $H$ slots into the future. The proposed algorithm has a worst-case complexity that grows exponentially with $H$. However, by narrowing down our search space within the constrained set of actions, we are able to decrease the complexity significantly without losing optimality. On the contrary, we show by simulations that the benefit of increasing $H$ w.r.t. remote state estimation performance diminishes after a certain $H$ value. ",Kein DOI-Link verfügbar,2005.02037v1,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Task-oriented Scheduling for Networked Control Systems: An Age of   Information-Aware Implementation on Software-defined Radios,1970,"  Networked control systems (NCSs) are feedback control loops that are closed over a communication network. Emerging applications, such as telerobotics, drones and autonomous driving are the most prominent examples of such systems. Regular and timely information sharing between the components of NCSs is essential to fulfill the desired control tasks, as stale information can lead to performance degradation or even physical damage. In this work, we consider multiple heterogeneous NCSs that transmit their system state over a shared physical wireless channel towards a gateway node. We conduct a comprehensive experimental study on selected MAC protocols using software-defined radios with state-of-the-art (SotA) solutions that have been designed to increase information freshness and control performance. As a significant improvement over the SotA, we propose a novel contention-free algorithm that is able to outperform the existing solutions by combining their strengths in one protocol. In addition, we propose a new metric called normalized mean squared error that maps the age of information to a dimensionless quantity that captures the expected value of a control system's next transmission. We demonstrate its adoption and effectiveness for wireless resource scheduling in a case study involving multiple inverted pendulums. From our experimental study and results, we observe that value-aware prioritization of the sub-systems contributes to minimizing the negative effects of information staleness on control performance. In particular, as the number of devices increases, the benefit of control-awareness to the quality of control stands out when compared to protocols that focus solely on maximizing information freshness. ",Kein DOI-Link verfügbar,2202.09189v3,Yes,fresh(2)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Age-of-Information vs. Value-of-Information Scheduling for Cellular   Networked Control Systems,1970,"  Age-of-Information (AoI) is a recently introduced metric for network operation with sensor applications which quantifies the freshness of data. In the context of networked control systems (NCSs), we compare the worth of the AoI metric with the value-of-information (VoI) metric, which is related to the uncertainty reduction in stochastic processes. First, we show that the uncertainty propagates non-linearly over time depending on system dynamics. Next, we define the value of a new update of the process of interest as a function of AoI and system parameters of the NCSs. We use the aggregated update value as a utility for the centralized scheduling problem in a cellular NCS composed of multiple heterogeneous control loops. By conducting a simulative analysis, we show that prioritizing transmissions with higher VoI improves performance of the NCSs compared with providing fair data freshness to all sub-systems equally. ",Kein DOI-Link verfügbar,1903.05356v1,Yes,fresh(2)
0000-0002-9181-8435,Manuel Rieger,Technical University of Munich,Fast optoelectronic charge state conversion of silicon vacancies in   diamond,1970,"  Group IV vacancy color centers in diamond are promising spin-photon interfaces with strong potential for applications for photonic quantum technologies. Reliable methods for controlling and stabilizing their charge state are urgently needed for scaling to multi-qubit devices. Here, we manipulate the charge state of silicon vacancy (SiV) ensembles by combining luminescence and photo-current spectroscopy. We controllably convert the charge state between the optically active SiV$^-$ and dark SiV$^{2-}$ with MHz rates and 90% contrast by judiciously choosing the local potential applied to in-plane surface electrodes and the laser excitation wavelength. We observe intense SiV$^-$ photoluminescence under hole-capture, measure the intrinsic conversion time from the dark SiV$^{2-}$ to the bright SiV$^-$ to be 36.4(6.7)ms and demonstrate how it can be enhanced by a factor of $10^5$ via optical pumping. Moreover, we obtain new information on the defects that contribute to photo-conductivity, indicating the presence of substitutional nitrogen and divacancies. ",https://doi.org/10.1126/sciadv.adl4265,2310.12288v1,Yes,potent(2)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,Quantum Algorithms for Solving Ordinary Differential Equations via   Classical Integration Methods,1970,"  Identifying computational tasks suitable for (future) quantum computers is an active field of research. Here we explore utilizing quantum computers for the purpose of solving differential equations. We consider two approaches: (i) basis encoding and fixed-point arithmetic on a digital quantum computer, and (ii) representing and solving high-order Runge-Kutta methods as optimization problems on quantum annealers. As realizations applied to two-dimensional linear ordinary differential equations, we devise and simulate corresponding digital quantum circuits, and implement and run a 6$^{\mathrm{th}}$ order Gauss-Legendre collocation method on a D-Wave 2000Q system, showing good agreement with the reference solution. We find that the quantum annealing approach exhibits the largest potential for high-order implicit integration methods. As promising future scenario, the digital arithmetic method could be employed as an ""oracle"" within quantum search algorithms for inverse problems. ",https://doi.org/10.22331/q-2021-07-13-502,2012.09469v2,Yes,potent(1)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,Design Principles of Dynamic Resource Management for High-Performance   Parallel Programming Models,1970,"  With Dynamic Resource Management (DRM) the resources assigned to a job can be changed dynamically during its execution. From the system's perspective, DRM opens a new level of flexibility in resource allocation and job scheduling and therefore has the potential to improve system efficiency metrics such as the utilization rate, job throughput, energy efficiency, and responsiveness. From the application perspective, users can tailor the resources they request to their needs offering potential optimizations in queuing time or charged costs. Despite these obvious advantages and many attempts over the last decade to establish DRM in HPC, it remains a concept discussed in academia rather than being successfully deployed on production systems. This stems from the fact that support for DRM requires changes in all the layers of the HPC system software stack including applications, programming models, process managers, and resource management software, as well as an extensive and holistic co-design process to establish new techniques and policies for scheduling and resource optimization. In this work, we therefore start with the assumption that resources are accessible by processes executed either on them (e.g., on CPU) or controlling them (e.g., GPU-offloading). Then, the overall DRM problem can be decomposed into dynamic process management (DPM) and dynamic resource mapping or allocation (DRA). The former determines which processes (or which change in processes) must be managed and the latter identifies the resources where they will be executed. The interfaces for such \mbox{DPM/DPA} in these layers need to be standardized, which requires a careful design to be interoperable while providing high flexibility. Based on a survey of existing approaches we propose design principles, that form the basis of a holistic approach to DMR in HPC and provide a prototype implementation using MPI. ",Kein DOI-Link verfügbar,2403.17107v1,Yes,potent(2)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,On the Convergence of Malleability and the HPC PowerStack: Exploiting   Dynamism in Over-Provisioned and Power-Constrained HPC Systems,1970,"  Recent High-Performance Computing (HPC) systems are facing important challenges, such as massive power consumption, while at the same time significantly under-utilized system resources. Given the power consumption trends, future systems will be deployed in an over-provisioned manner where more resources are installed than they can afford to power simultaneously. In such a scenario, maximizing resource utilization and energy efficiency, while keeping a given power constraint, is pivotal. Driven by this observation, in this position paper we first highlight the recent trends of resource management techniques, with a particular focus on malleability support (i.e., dynamically scaling resource allocations/requirements for a job), co-scheduling (i.e., co-locating multiple jobs within a node), and power management. Second, we consider putting them together, assess their relationships/synergies, and discuss the functionality requirements in each software component for future over-provisioned and power-constrained HPC systems. Third, we briefly introduce our ongoing efforts on the integration of software tools, which will ultimately lead to the convergence of malleability and power management, as it is designed in the HPC PowerStack initiative. ",https://doi.org/10.1007/978-3-031-23220-6_14,2405.03847v1,Yes,pivotal(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,DPSNN: Spiking Neural Network for Low-Latency Streaming Speech   Enhancement,1970,"  Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency. ",Kein DOI-Link verfügbar,2408.07388v1,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Markov Chain Block Coordinate Descent,1970,"  The method of block coordinate gradient descent (BCD) has been a powerful method for large-scale optimization. This paper considers the BCD method that successively updates a series of blocks selected according to a Markov chain. This kind of block selection is neither i.i.d. random nor cyclic. On the other hand, it is a natural choice for some applications in distributed optimization and Markov decision process, where i.i.d. random and cyclic selections are either infeasible or very expensive. By applying mixing-time properties of a Markov chain, we prove convergence of Markov chain BCD for minimizing Lipschitz differentiable functions, which can be nonconvex. When the functions are convex and strongly convex, we establish both sublinear and linear convergence rates, respectively. We also present a method of Markov chain inertial BCD. Finally, we discuss potential applications. ",Kein DOI-Link verfügbar,1811.08990v1,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Asynchronous Coordinate Descent under More Realistic Assumptions,1970,"  Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding to these algorithms is limited because the current convergence of asynchronous (block) coordinate descent algorithms are based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update a block is assumed to be independent of the block being updated. Also, it is assumed that the updates are applied to randomly chosen blocks. In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. We construct Lyapunov functions that directly model both objective progress and delays, so delays are not treated errors or noise. A continuous-time ODE is provided to explain the construction at a high level. ",Kein DOI-Link verfügbar,1705.08494v2,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Backdoor Cleansing with Unlabeled Data,1970,"  Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal behavior. In experiments, we show that our method, trained without labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https://github.com/luluppang/BCU. ",Kein DOI-Link verfügbar,2211.12044v4,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,TraceCaps: A Capsule-based Neural Network for Semantic Segmentation,1970,"  In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure. We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network. Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. With the capability to extracted part-whole information, our traceback pipeline can potentially be utilized as the building blocks to design interpretable neural networks. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variants. ",Kein DOI-Link verfügbar,1901.02920v2,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Mapping the Depths: A Stocktake of Underground Power Distribution in   United States,1970,"  A resilient energy infrastructure is crucial for addressing increasing extreme weather and climate risks. The undergrounding of the power system is one approach to building such resiliency. In this study, we introduce Grid Underground Distribution Statistics (GUDS) for the US, the first nationwide comprehensive assessment of underground electricity distribution at a high spatial granularity. In analyzing this dataset, we find regional differences in underground distribution rates, with generally higher rates for east and west coasts and in northern states, and lower rates in the central US. We also observe relationships between underground rates and factors such as household income levels, degree of urbanization, and vulnerability to natural hazards. Notably, regions with higher electricity rates are not associated with greater proportions of underground distribution, highlighting potential equity issues in infrastructure distribution. By presenting this granular information and insights on underground distribution, our study offers valuable guidance for informing planning and decision-making by policymakers, Independent System Operators, utilities, and end-users. ",Kein DOI-Link verfügbar,2402.06668v1,Yes,potent(1)
0000-0001-6557-5604,Kathrin Dörfler,Technical University of Munich,Mobile Robotic Fabrication at 1:1 scale: the In situ Fabricator,1970,"  This paper presents the concept of an In situ Fabricator, a mobile robot intended for on-site manufacturing, assembly and digital fabrication. We present an overview of a prototype system, its capabilities, and highlight the importance of high-performance control, estimation and planning algorithms for achieving desired construction goals. Next, we detail on two architectural application scenarios: first, building a full-size undulating brick wall, which required a number of repositioning and autonomous localisation manoeuvres. Second, the Mesh Mould concrete process, which shows that an In situ Fabricator in combination with an innovative digital fabrication tool can be used to enable completely novel building technologies. Subsequently, important limitations and disadvantages of our approach are discussed. Based on that, we identify the need for a new type of robotic actuator, which facilitates the design of novel full-scale construction robots. We provide brief insight into the development of this actuator and conclude the paper with an outlook on the next-generation In situ Fabricator, which is currently under development. ",https://doi.org/10.1007/s41693-017-0003-5,1701.03573v1,Yes,innovative(1)
0000-0002-6055-6858,John Mitchell,"McGill University, McGill University Health Centre",DPAdapter: Improving Differentially Private Deep Learning through Noise   Tolerance Pre-training,1970,"  Recent developments have underscored the critical role of \textit{differential privacy} (DP) in safeguarding individual data for training machine learning models. However, integrating DP oftentimes incurs significant model performance degradation due to the perturbation introduced into the training process, presenting a formidable challenge in the {differentially private machine learning} (DPML) field. To this end, several mitigative efforts have been proposed, typically revolving around formulating new DPML algorithms or relaxing DP definitions to harmonize with distinct contexts. In spite of these initiatives, the diminishment induced by DP on models, particularly large-scale models, remains substantial and thus, necessitates an innovative solution that adeptly circumnavigates the consequential impairment of model utility.   In response, we introduce DPAdapter, a pioneering technique designed to amplify the model performance of DPML algorithms by enhancing parameter robustness. The fundamental intuition behind this strategy is that models with robust parameters are inherently more resistant to the noise introduced by DP, thereby retaining better performance despite the perturbations. DPAdapter modifies and enhances the sharpness-aware minimization (SAM) technique, utilizing a two-batch strategy to provide a more accurate perturbation estimate and an efficient gradient descent, thereby improving parameter robustness against noise. Notably, DPAdapter can act as a plug-and-play component and be combined with existing DPML algorithms to further improve their performance. Our experiments show that DPAdapter vastly enhances state-of-the-art DPML algorithms, increasing average accuracy from 72.92\% to 77.09\% with a privacy budget of $\epsilon=4$. ",Kein DOI-Link verfügbar,2403.02571v1,Yes,innovative(1)
0000-0002-8273-6681,Ruitian Lang,The Australian National University,Nonadditive quantum error correcting codes adapted to the ampltitude   damping channel,1970,"  A family of high rate quantum error correcting codes adapted to the amplitude damping channel is presented. These codes are nonadditive and exploit self-complementarity structure to correct all first-order errors. Their rates can be higher than 1/2. The recovery operations of these codes can be generated by a simple algorithm and have a projection nature, which makes them potentially easy to implement. ",Kein DOI-Link verfügbar,0712.2586v1,Yes,potent(1)
0000-0002-2196-4656,Janet Zhong,The Australian National University,Mesoscopic non-Hermitian skin effect,1970,"  We discuss a generalization of the non-Hermitian skin effect to finite-size photonic structures with   neither gain nor loss in the bulk and purely real energy spectrum under periodic boundary conditions (PBC).   We show that such systems can still have significant portions of eigenmodes concentrated at the edges and that this edge concentration can be linked to the non-trivial point-gap topology of the size-dependent regularized PBC spectrum, accounting for the radiative losses. As an example, we consider the chiral waveguide quantum electrodynamics platform with an array of atoms coupled to the waveguide. The proposed mesoscopic analogue of the non-Hermitian skin effect could be potentially applied to other seemingly lossless photonic structures, such as chiral resonant all-dielectric metamaterials. ",Kein DOI-Link verfügbar,2310.04025v1,Yes,potent(1)
0000-0002-7124-282X,Salman Durrani,The Australian National University,Device-to-Device Communication Underlaying a Finite Cellular Network   Region,1970,"  Underlay in-band device-to-device (D2D) communication can improve the spectrum efficiency of cellular networks. However, the coexistence of D2D and cellular users causes inter-cell and intra-cell interference. The former can be effectively managed through inter-cell interference coordination and, therefore, is not considered in this work. Instead, we focus on the intra-cell interference and propose a D2D mode selection scheme to manage it inside a finite cellular network region. The potential D2D users are controlled by the base station (BS) to operate in D2D mode based on the average interference generated to the BS. Using stochastic geometry, we study the outage probability experienced at the BS and a D2D receiver, and spectrum reuse ratio, which quantifies the average fraction of successfully transmitting D2D users. The analysis shows that the outage probability at the D2D receiver varies for different locations. Additionally, without impairing the performance at the BS, if the path-loss exponent on the cellular link is slightly lower than that on the D2D link, the spectrum reuse ratio can have negligible decrease while the D2D users' average number of successful transmissions increases with increasing D2D node density. This indicates that an increasing level of D2D communication can be beneficial in future networks.. ",Kein DOI-Link verfügbar,1510.03162v4,Yes,potent(1)
0000-0002-2695-5636,Alwen Tiu,The Australian National University,De Morgan Dual Nominal Quantifiers Modelling Private Names in   Non-Commutative Logic,1970,"  This paper explores the proof theory necessary for recommending an expressive but decidable first-order system, named MAV1, featuring a de Morgan dual pair of nominal quantifiers. These nominal quantifiers called `new' and `wen' are distinct from the self-dual Gabbay-Pitts and Miller-Tiu nominal quantifiers. The novelty of these nominal quantifiers is they are polarised in the sense that `new' distributes over positive operators while `wen' distributes over negative operators. This greater control of bookkeeping enables private names to be modelled in processes embedded as formulae in MAV1. The technical challenge is to establish a cut elimination result, from which essential properties including the transitivity of implication follow. Since the system is defined using the calculus of structures, a generalisation of the sequent calculus, novel techniques are employed. The proof relies on an intricately designed multiset-based measure of the size of a proof, which is used to guide a normalisation technique called splitting. The presence of equivariance, which swaps successive quantifiers, induces complex inter-dependencies between nominal quantifiers, additive conjunction and multiplicative operators in the proof of splitting. Every rule is justified by an example demonstrating why the rule is necessary for soundly embedding processes and ensuring that cut elimination holds. ",https://doi.org/10.1145/3325821,1602.06043v3,Yes,intricate(1)
0000-0002-2695-5636,Alwen Tiu,The Australian National University,A Permission-Dependent Type System for Secure Information Flow Analysis,1970,"  We introduce a novel type system for enforcing secure information flow in an imperative language. Our work is motivated by the problem of statically checking potential information leakage in Android applications. To this end, we design a lightweight type system featuring Android permission model, where the permissions are statically assigned to applications and are used to enforce access control in the applications. We take inspiration from a type system by Banerjee and Naumann (BN) to allow security types to be dependent on the permissions of the applications. A novel feature of our type system is a typing rule for conditional branching induced by permission testing, which introduces a merging operator on security types, allowing more precise security policies to be enforced. The soundness of our type system is proved with respect to a notion of noninterference. In addition, a type inference algorithm is presented for the underlying security type system, by reducing the inference problem to a constraint solving problem in the lattice of security types. ",Kein DOI-Link verfügbar,1709.09623v1,Yes,potent(1)
0000-0003-4489-4926,James Sullivan,The Australian National University,"Student Cluster Competition 2017, Team University ofTexas at   Austin/Texas State University: Reproducing Vectorization of the Tersoff   Multi-Body Potential on the Intel Skylake and NVIDIA V100 Architectures",1970,"  This paper satisfies the reproducibility challenge of the Student Cluster Competition at Supercomputing 2017. We attempted to reproduce the results of H\""{o}hnerbach et al. (2016) for an implementation of a vectorized code for the Tersoff multi-body potential kernel of the molecular dynamics code Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS). We investigated accuracy, optimization performance, and scaling with our Intel CPU and NVIDIA GPU based cluster. ",https://doi.org/10.1016/j.parco.2018.08.003,1808.07027v1,Yes,potent(1)
0000-0003-4489-4926,James Sullivan,The Australian National University,It's more than just money: The real-world harms from ransomware attacks,1970,"  As cyber-attacks continue to increase in frequency and sophistication, organisations must be better prepared to face the reality of an incident. Any organisational plan that intends to be successful at managing security risks must clearly understand the harm (i.e., negative impact) and the various parties affected in the aftermath of an attack. To this end, this article conducts a novel exploration into the multitude of real-world harms that can arise from cyber-attacks, with a particular focus on ransomware incidents given their current prominence. This exploration also leads to the proposal of a new, robust methodology for modelling harms from such incidents. We draw on publicly-available case data on high-profile ransomware incidents to examine the types of harm that emerge at various stages after a ransomware attack and how harms (e.g., an offline enterprise server) may trigger other negative, potentially more substantial impacts for stakeholders (e.g., the inability for a customer to access their social welfare benefits or bank account). Prominent findings from our analysis include the identification of a notable set of social/human harms beyond the business itself (and beyond the financial payment of a ransom) and a complex web of harms that emerge after attacks regardless of the industry sector. We also observed that deciphering the full extent and sequence of harms can be a challenging undertaking because of the lack of complete data available. This paper consequently argues for more transparency on ransomware harms, as it would lead to a better understanding of the realities of these incidents to the benefit of organisations and society more generally. ",Kein DOI-Link verfügbar,2307.02855v1,Yes,"notable(1), potent(1)"
0000-0001-6063-2622,Dawei Chen,The Australian National University,Confidence-based federated distillation for vision-based lane-centering,1970,"  A fundamental challenge of autonomous driving is maintaining the vehicle in the center of the lane by adjusting the steering angle. Recent advances leverage deep neural networks to predict steering decisions directly from images captured by the car cameras. Machine learning-based steering angle prediction needs to consider the vehicle's limitation in uploading large amounts of potentially private data for model training. Federated learning can address these constraints by enabling multiple vehicles to collaboratively train a global model without sharing their private data, but it is difficult to achieve good accuracy as the data distribution is often non-i.i.d. across the vehicles. This paper presents a new confidence-based federated distillation method to improve the performance of federated learning for steering angle prediction. Specifically, it proposes the novel use of entropy to determine the predictive confidence of each local model, and then selects the most confident local model as the teacher to guide the learning of the global model. A comprehensive evaluation of vision-based lane centering shows that the proposed approach can outperform FedAvg and FedDF by 11.3% and 9%, respectively. ",Kein DOI-Link verfügbar,2306.03222v1,Yes,potent(1)
0000-0001-6063-2622,Dawei Chen,The Australian National University,Kinetics of Rayleigh-Taylor instability in van der Waals fluid: the   influence of compressibility,1970,"  Early studies on Rayleigh-Taylor instability (RTI) primarily relied on the Navier-Stokes (NS) model. As research progresses, it becomes increasingly evident that the kinetic information that the NS model failed to capture is of great value for identifying and even controlling the RTI process; simultaneously, the lack of analysis techniques for complex physical fields results in a significant waste of data information. In addition, early RTI studies mainly focused on the incompressible case and the weakly compressible case. In the case of strong compressibility, the density of the fluid from the upper layer (originally heavy fluid) may become smaller than that of the surrounding (originally light) fluid, thus invalidating the early method of distinguishing light and heavy fluids based on density. In this paper, tracer particles are incorporated into a single-fluid discrete Boltzmann method (DBM) model that considers the van der Waals potential. By using tracer particles to label the matter-particle sources, a careful study of the matter-mixing and energy-mixing processes of the RTI evolution is realized in the single-fluid framework. The effects of compressibility on the evolution of RTI are examined mainly through the analysis of bubble and spike velocities, the ratio of area occupied by heavy fluid, and various entropy generation rates of the system. It is demonstrated that: (1) compressibility has a suppressive effect on the spike velocity, and this suppressive impact diminishes as the Atwood number ($At$) increases. The influence of compressibility on bubble velocity shows a staged behavior with increasing $At$. (2) The impact of compressibility on the entropy production rate associated with the heat flow (${{\dot{S}}_{NOEF}}$) is related to the stages of RTI evolution. ",Kein DOI-Link verfügbar,2407.02139v2,Yes,potent(1)
0000-0001-6063-2622,Dawei Chen,The Australian National University,"Integrated Sensing and Communication based Outdoor Multi-Target   Detection, Tracking and Localization in Practical 5G Networks",1970,"  The 6th generation (6G) wireless networks will likely to support a variety of capabilities beyond communication, such as sensing and localization, through the use of communication networks empowered by advanced technologies. Integrated sensing and communication (ISAC) has been recognized as a critical technology as well as an usage scenario for 6G, as widely agreed by leading global standardization bodies. ISAC utilizes communication infrastructure and devices to provide the capability of sensing the environment with high resolution, as well as tracking and localizing moving objects nearby. Meeting both the requirements for communication and sensing simultaneously, ISAC based approaches celebrate the advantages of higher spectral and energy efficiency compared to two separate systems to serve two purposes, and potentially lower costs and easy deployment. A key step towards the standardization and commercialization of ISAC is to carry out comprehensive field trials in practical networks, such as the 5th generation (5G) network, to demonstrate its true capacities in practical scenarios. In this paper, an ISAC based outdoor multi-target detection, tracking and localization approach is proposed and validated in 5G networks. The proposed system comprises of 5G base stations (BSs) which serve nearby mobile users normally, while accomplishing the task of detecting, tracking and localizing drones, vehicles and pedestrians simultaneously. Comprehensive trial results demonstrate the relatively high accuracy of the proposed method in practical outdoor environment when tracking and localizing single targets and multiple targets. ",https://doi.org/10.23919/ICN.2023.0021,2305.13924v3,Yes,potent(1)
0000-0001-6063-2622,Dawei Chen,The Australian National University,Unleashing the True Power of Age-of-Information: Service Aggregation in   Connected and Autonomous Vehicles,1970,"  Connected and autonomous vehicles (CAVs) rely heavily upon time-sensitive information update services to ensure the safety of people and assets, and satisfactory entertainment applications. Therefore, the freshness of information is a crucial performance metric for CAV services. However, information from roadside sensors and nearby vehicles can get delayed in transmission due to the high mobility of vehicles. Our research shows that a CAV's relative distance and speed play an essential role in determining the Age-of-Information (AoI). With an increase in AoI, incremental service aggregation issues are observed with out-of-sequence information updates, which hampers the performance of low-latency applications in CAVs. In this paper, we propose a novel AoI-based service aggregation method for CAVs, which can process the information updates according to their update cycles. First, the AoI for sensors and vehicles is modeled, and a predictive AoI system is designed. Then, to reduce the overall service aggregation time and computational load, intervals are used for periodic AoI prediction, and information sources are clustered based on the AoI value. Finally, the system aggregates services for CAV applications using the predicted AoI. We evaluate the system performance based on data sequencing success rate (DSSR) and overall system latency. Lastly, we compare the performance of our proposed system with three other state-of-the-art methods. The evaluation and comparison results show that our proposed predictive AoI-based service aggregation system maintains satisfactory latency and DSSR for CAV applications and outperforms other existing methods. ",Kein DOI-Link verfügbar,2403.08931v1,Yes,fresh(1)
0000-0001-6063-2622,Dawei Chen,The Australian National University,PathRec: Visual Analysis of Travel Route Recommendations,1970,"  We present an interactive visualisation tool for recommending travel trajectories. This system is based on new machine learning formulations and algorithms for the sequence recommendation problem. The system starts from a map-based overview, taking an interactive query as starting point. It then breaks down contributions from different geographical and user behavior features, and those from individual points-of-interest versus pairs of consecutive points on a route. The system also supports detailed quantitative interrogation by comparing a large number of features for multiple points. Effective trajectory visualisations can potentially benefit a large cohort of online map users and assist their decision-making. More broadly, the design of this system can inform visualisations of other structured prediction tasks, such as for sequences or trees. ",Kein DOI-Link verfügbar,1707.01627v2,Yes,potent(1)
0000-0002-6159-1233,Quanling Deng,The Australian National University,Physics-Informed Neural Networks for Discovering Localised Eigenstates   in Disordered Media,1970,"  The Schr\""{o}dinger equation with random potentials is a fundamental model for understanding the behaviour of particles in disordered systems. Disordered media are characterised by complex potentials that lead to the localisation of wavefunctions, also called Anderson localisation. These wavefunctions may have similar scales of eigenenergies which poses difficulty in their discovery. It has been a longstanding challenge due to the high computational cost and complexity of solving the Schr\""{o}dinger equation. Recently, machine-learning tools have been adopted to tackle these challenges. In this paper, based upon recent advances in machine learning, we present a novel approach for discovering localised eigenstates in disordered media using physics-informed neural networks (PINNs). We focus on the spectral approximation of Hamiltonians in one dimension with potentials that are randomly generated according to the Bernoulli, normal, and uniform distributions. We introduce a novel feature to the loss function that exploits known physical phenomena occurring in these regions to scan across the domain and successfully discover these eigenstates, regardless of the similarity of their eigenenergies. We present various examples to demonstrate the performance of the proposed approach and compare it with isogeometric analysis. ",Kein DOI-Link verfügbar,2305.06802v2,Yes,potent(3)
0000-0002-6159-1233,Quanling Deng,The Australian National University,Generalised Soft Finite Element Method for Elliptic Eigenvalue Problems,1970,"  The recently proposed soft finite element method (SoftFEM) reduces the stiffness (condition numbers), consequently improving the overall approximation accuracy. The method subtracts a least-square term that penalizes the gradient jumps across mesh interfaces from the FEM stiffness bilinear form while maintaining the system's coercivity. Herein, we present two generalizations for SoftFEM that aim to improve the approximation accuracy and further reduce the discrete systems' stiffness. Firstly and most naturally, we generalize SoftFEM by adding a least-square term to the mass bilinear form. Superconvergent results of rates $h^6$ and $h^8$ for eigenvalues are established for linear uniform elements; $h^8$ is the highest order of convergence known in the literature. Secondly, we generalize SoftFEM by applying the blended Gaussian-type quadratures. We demonstrate further reductions in stiffness compared to traditional FEM and SoftFEM. The coercivity and analysis of the optimal error convergences follow the work of SoftFEM. Thus, this paper focuses on the numerical study of these generalizations. For linear and uniform elements, analytical eigenpairs, exact eigenvalue errors, and superconvergent error analysis are established. Various numerical examples demonstrate the potential of generalized SoftFEMs for spectral approximation, particularly in high-frequency regimes. ",Kein DOI-Link verfügbar,2402.16080v1,Yes,potent(1)
0000-0001-9304-6718,Tony Travouillon,The Australian National University,Submillimetre/TeraHertz Astronomy at Dome C with CEA filled bolometer   array,1970,"  Submillimetre/TeraHertz (e.g. 200, 350, 450 microns) astronomy is the prime technique to unveil the birth and early evolution of a broad range of astrophysical objects. A major obstacle to carry out submm observations from ground is the atmosphere. Preliminary site testing and atmospheric transmission models tend to demonstrate that Dome C could offer the best conditions on Earth for submm/THz astronomy. The CAMISTIC project aims to install a filled bolometer-array camera with 16x16 pixels on IRAIT at Dome C and explore the 200-$\mu$m windows for potential ground-based observations. ",https://doi.org/10.1051/eas:2007114,astro-ph/0702480v1,Yes,potent(1)
0000-0001-9304-6718,Tony Travouillon,The Australian National University,A massive AGB donor in Scutum X-1: Identification of the first Mira   variable in an X-ray binary,1970,"  The symbiotic X-ray binary Sct X-1 was suggested as the first known neutron star accreting from a red supergiant companion. Although known for nearly 50 years, detailed characterization of the donor remains lacking, particularly due to the extremely high reddening towards the source ($A_V\gtrsim25$ mag). Here, we present i) improved localization of the counterpart using Gaia and Chandra observations, ii) the first broadband infrared spectrum ($\approx1-5\,\mu$m; $R\approx 2000$) obtained with SpeX on the NASA Infrared Telescope Facility and iii) $J$-band light curve from the Palomar Gattini-IR survey. The infrared spectrum is characterized by i) deep water absorption features (H$_2$O index $\approx 40$%), ii) strong TiO, VO and CO features, and iii) weak/absent CN lines. We show that these features are inconsistent with known red supergiants, but suggest a M8-9 III type O-rich Mira donor star. We report the discovery of large amplitude ($\Delta J\approx3.5$ mag) periodic photometric variability suggesting a pulsation period of $621\pm36\,{\rm(systematic)}\pm8\,{\rm(statistical)}$ days, which we use to constrain the donor to be a relatively luminous Mira ($M_K=-8.6\pm0.3$ mag) at a distance of $3.6^{+0.8}_{-0.7}$ kpc. Comparing these characteristics to recent models, we find the donor to be consistent with a $\approx 3-5$ M$_\odot$ star at an age of $\approx 0.1-0.3$ Gyr. Together, we show that Sct X-1 was previously mis-classified as an evolved High Mass X-ray Binary; instead it is an intermediate mass system with the first confirmed Mira donor in an X-ray binary. We discuss the implications of Mira donors in symbiotic X-ray binaries, and highlight the potential of wide-field infrared time domain surveys and broadband infrared spectroscopy to unveil their demographics. ",https://doi.org/10.3847/2041-8213/ac5b11,2201.09906v2,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Don't do it: Safer Reinforcement Learning With Rule-based Guidance,1970,"  During training, reinforcement learning systems interact with the world without considering the safety of their actions. When deployed into the real world, such systems can be dangerous and cause harm to their surroundings. Often, dangerous situations can be mitigated by defining a set of rules that the system should not violate under any conditions. For example, in robot navigation, one safety rule would be to avoid colliding with surrounding objects and people. In this work, we define safety rules in terms of the relationships between the agent and objects and use them to prevent reinforcement learning systems from performing potentially harmful actions. We propose a new safe epsilon-greedy algorithm that uses safety rules to override agents' actions if they are considered to be unsafe. In our experiments, we show that a safe epsilon-greedy policy significantly increases the safety of the agent during training, improves the learning efficiency resulting in much faster convergence, and achieves better performance than the base model. ",Kein DOI-Link verfügbar,2212.13819v1,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Efficient Open-world Reinforcement Learning via Knowledge Distillation   and Autonomous Rule Discovery,1970,"  Deep reinforcement learning suffers from catastrophic forgetting and sample inefficiency making it less applicable to the ever-changing real world. However, the ability to use previously learned knowledge is essential for AI agents to quickly adapt to novelties. Often, certain spatial information observed by the agent in the previous interactions can be leveraged to infer task-specific rules. Inferred rules can then help the agent to avoid potentially dangerous situations in the previously unseen states and guide the learning process increasing agent's novelty adaptation speed. In this work, we propose a general framework that is applicable to deep reinforcement learning agents. Our framework provides the agent with an autonomous way to discover the task-specific rules in the novel environments and self-supervise it's learning. We provide a rule-driven deep Q-learning agent (RDQ) as one possible implementation of that framework. We show that RDQ successfully extracts task-specific rules as it interacts with the world and uses them to drastically increase its learning efficiency. In our experiments, we show that the RDQ agent is significantly more resilient to the novelties than the baseline agents, and is able to detect and adapt to novel situations faster. ",Kein DOI-Link verfügbar,2311.14270v1,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Can Variational Quantum Algorithms Demonstrate Quantum Advantages? Time   Really Matters,1970,"  Applying low-depth quantum neural networks (QNNs), variational quantum algorithms (VQAs) are both promising and challenging in the noisy intermediate-scale quantum (NISQ) era: Despite its remarkable progress, criticisms on the efficiency and feasibility issues never stopped. However, whether VQAs can demonstrate quantum advantages is still undetermined till now, which will be investigated in this paper. First, we will prove that there exists a dependency between the parameter number and the gradient-evaluation cost when training QNNs. Noticing there is no such direct dependency when training classical neural networks with the backpropagation algorithm, we argue that such a dependency limits the scalability of VQAs. Second, we estimate the time for running VQAs in ideal cases, i.e., without considering realistic limitations like noise and reachability. We will show that the ideal time cost easily reaches the order of a 1-year wall time. Third, by comparing with the time cost using classical simulation of quantum circuits, we will show that VQAs can only outperform the classical simulation case when the time cost reaches the scaling of $10^0$-$10^2$ years. Finally, based on the above results, we argue that it would be difficult for VQAs to outperform classical cases in view of time scaling, and therefore, demonstrate quantum advantages, with the current workflow. Since VQAs as well as quantum computing are developing rapidly, this work does not aim to deny the potential of VQAs. The analysis in this paper provides directions for optimizing VQAs, and in the long run, seeking more natural hybrid quantum-classical algorithms would be meaningful. ",Kein DOI-Link verfügbar,2307.04089v1,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,End-to-End Quantum Vision Transformer: Towards Practical Quantum Speedup   in Large-Scale Models,1970,"  The field of quantum deep learning presents significant opportunities for advancing computational capabilities, yet it faces a major obstacle in the form of the ""information loss problem"" due to the inherent limitations of the necessary quantum tomography in scaling quantum deep neural networks. This paper introduces an end-to-end Quantum Vision Transformer (QViT), which incorporates an innovative quantum residual connection technique, to overcome these challenges and therefore optimize quantum computing processes in deep learning. Our thorough complexity analysis of the QViT reveals a theoretically exponential and empirically polynomial speedup, showcasing the model's efficiency and potential in quantum computing applications. We conducted extensive numerical tests on modern, large-scale transformers and datasets, establishing the QViT as a pioneering advancement in applying quantum deep neural networks in practical scenarios. Our work provides a comprehensive quantum deep learning paradigm, which not only demonstrates the versatility of current quantum linear algebra algorithms but also promises to enhance future research and development in quantum deep learning. ",Kein DOI-Link verfügbar,2402.18940v2,Yes,"innovative(1), potent(1)"
0000-0001-9667-623X,Cheng Xue,The Australian National University,Scalable Program Implementation and Simulation of the Large-Scale   Quantum Algorithm: $1024\times 1024$ Quantum Linear Solver and Beyond,1970,"  Program implementation and simulation are essential for research in the field of quantum algorithms. However, complex and large-scale quantum algorithms can pose challenges for existing quantum programming languages and simulators. Here, we present a scalable program implementation of the quantum walk on a sparse matrix and the quantum linear solver based on the quantum walk. Our implementation is based on a practical scenario in which the sparse matrix is stored in the compressed-sparse-column format in quantum random access memory. All necessary modules are implemented unitarily and are ensured to be decomposed at the quantum gate level, including implementing a quantum binary search and a modification of the original algorithm. The program is validated using a highly efficient quantum circuit simulator which is based on the register level and sparse state representation. With only a single core, we simulate the quantum walk on a 16384-dimensional matrix with 582 qubits in 1.1 minutes per step, as well as a quantum linear solver up to 1024 dimensions and 212245 steps in 70 hours. Our work narrows the gap between the simulation of a quantum algorithm and its classical counterparts, where the asymptotic complexity of our quantum linear solver simulation approximates a classical linear solver. These program implementation and simulation techniques have the potential to expand the boundary of numerical research for large-scale quantum algorithms, with implications for the development of error-correction-era quantum computing solutions. ",Kein DOI-Link verfügbar,2303.06890v1,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Classical-Assisted Quantum Ground State Preparation with Tensor Network   States and Monte Carlo Sampling,1970,"  Quantum computing offers potential solutions for finding ground states in condensed-matter physics and chemistry. However, achieving effective ground state preparation is also computationally hard for arbitrary Hamiltonians. It is necessary to propose certain assumptions to make this problem efficiently solvable, including preparing a trial state of a non-trivial overlap with the genuine ground state. Here, we propose a classical-assisted quantum ground state preparation method for quantum many-body systems, combining Tensor Network States (TNS) and Monte Carlo (MC) sampling as a heuristic method to prepare a trial state with a non-trivial overlap with the genuine ground state. We extract a sparse trial state by sampling from TNS, which can be efficiently prepared by a quantum algorithm on early fault-tolerant quantum computers. Our method demonstrates a polynomial improvement in scaling of overlap between the trial state and genuine ground state compared to random trial states, as evidenced by numerical tests on the spin-$1/2$ $J_1$-$J_2$ Heisenberg model. Furthermore, our method is a novel approach to hybridize a classical numerical method and a quantum algorithm and brings inspiration to ground state preparation in other fields. ",Kein DOI-Link verfügbar,2306.16831v1,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Data-driven Quantum Dynamical Embedding Method for Long-term Prediction   on Near-term Quantum Computers,1970,"  The increasing focus on long-term time series prediction across various fields has been significantly strengthened by advancements in quantum computation. In this paper, we introduce a data-driven method designed for long-term time series prediction with quantum dynamical embedding (QDE). This approach enables a trainable embedding of the data space into an extended state space, allowing for the recursive retrieval of time series information. Based on its independence of time series length, this method achieves depth-efficient quantum circuits that are crucial for near-term quantum computers. Numerical simulations demonstrate the model's improved performance in prediction accuracy and resource efficiency over existing methods, as well as its effective denoising capabilities. We implement this model on the Origin ''Wukong'' superconducting quantum processor with a learnable error-cancellation layer (LECL) for error mitigation, further validating the practical applicability of our approach on near-term quantum devices. Furthermore, the theoretical analysis of the QDE's dynamical properties and its universality enhances its potential for time series prediction. This study establishes a significant step towards the processing of long-term time series on near-term quantum computers, integrating data-driven learning with discrete dynamical embedding for enhanced forecasting capabilities. ",Kein DOI-Link verfügbar,2305.15976v3,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,Simulation of open quantum systems on universal quantum computers,1970,"  The rapid development of quantum computers has enabled demonstrations of quantum advantages on various tasks. However, real quantum systems are always dissipative due to their inevitable interaction with the environment, and the resulting non-unitary dynamics make quantum simulation challenging with only unitary quantum gates. In this work, we present an innovative and scalable method to simulate open quantum systems using quantum computers. We define an adjoint density matrix as a counterpart of the true density matrix, which reduces to a mixed-unitary quantum channel and thus can be effectively sampled using quantum computers. This method has several benefits, including no need for auxiliary qubits and noteworthy scalability. Moreover, accurate long-time simulation can also be achieved as the adjoint density matrix and the true dissipated one converge to the same state. Finally, we present deployments of this theory in the dissipative quantum $XY$ model for the evolution of correlation and entropy with short-time dynamics and the disordered Heisenberg model for many-body localization with long-time dynamics. This work promotes the study of real-world many-body dynamics with quantum computers, highlighting the potential to demonstrate practical quantum advantages. ",Kein DOI-Link verfügbar,2405.20712v1,Yes,"innovative(1), noteworthy(1), potent(1)"
0000-0001-9667-623X,Cheng Xue,The Australian National University,Statistics-Informed Parameterized Quantum Circuit via Maximum Entropy   Principle for Data Science and Finance,1970,"  Quantum machine learning has demonstrated significant potential in solving practical problems, particularly in statistics-focused areas such as data science and finance. However, challenges remain in preparing and learning statistical models on a quantum processor due to issues with trainability and interpretability. In this letter, we utilize the maximum entropy principle to design a statistics-informed parameterized quantum circuit (SI-PQC) for efficiently preparing and training of quantum computational statistical models, including arbitrary distributions and their weighted mixtures. The SI-PQC features a static structure with trainable parameters, enabling in-depth optimized circuit compilation, exponential reductions in resource and time consumption, and improved trainability and interpretability for learning quantum states and classical model parameters simultaneously. As an efficient subroutine for preparing and learning in various quantum algorithms, the SI-PQC addresses the input bottleneck and facilitates the injection of prior knowledge. ",Kein DOI-Link verfügbar,2406.01335v2,Yes,potent(1)
0000-0001-9667-623X,Cheng Xue,The Australian National University,HiMA: Hierarchical Quantum Microarchitecture for Qubit-Scaling and   Quantum Process-Level Parallelism,1970,"  Quantum computing holds immense potential for addressing a myriad of intricate challenges, which is significantly amplified when scaled to thousands of qubits. However, a major challenge lies in developing an efficient and scalable quantum control system. To address this, we propose a novel Hierarchical MicroArchitecture (HiMA) designed to facilitate qubit scaling and exploit quantum process-level parallelism. This microarchitecture is based on three core elements: (i) discrete qubit-level drive and readout, (ii) a process-based hierarchical trigger mechanism, and (iii) multiprocessing with a staggered triggering technique to enable efficient quantum process-level parallelism. We implement HiMA as a control system for a 72-qubit tunable superconducting quantum processing unit, serving a public quantum cloud computing platform, which is capable of expanding to 6144 qubits through three-layer cascading. In our benchmarking tests, HiMA achieves up to a 4.89x speedup under a 5-process parallel configuration. Consequently, to the best of our knowledge, we have achieved the highest CLOPS (Circuit Layer Operations Per Second), reaching up to 43,680, across all publicly available platforms. ",Kein DOI-Link verfügbar,2408.11311v1,Yes,"intricate(1), potent(1)"
0000-0002-8694-8446,Kevin Lu,The Australian National University,Rapid Mobile App Development for Generative AI Agents on MIT App   Inventor,1970,"  The evolution of Artificial Intelligence (AI) stands as a pivotal force shaping our society, finding applications across diverse domains such as education, sustainability, and safety. Leveraging AI within mobile applications makes it easily accessible to the public, catalyzing its transformative potential. In this paper, we present a methodology for the rapid development of AI agent applications using the development platform provided by MIT App Inventor. To demonstrate its efficacy, we share the development journey of three distinct mobile applications: SynchroNet for fostering sustainable communities; ProductiviTeams for addressing procrastination; and iHELP for enhancing community safety. All three applications seamlessly integrate a spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we offer insights gleaned from overcoming challenges in integrating diverse tools and AI functionalities, aiming to inspire young developers to join our efforts in building practical AI agent applications. ",https://doi.org/10.5281/zenodo.10899798,2405.01561v1,Yes,"pivotal(1), potent(1)"
0000-0003-4279-5967,Qing Zhang,The Australian National University,On the dependence of the local Rankin-Selberg gamma factors of   $\textrm{Sp}_{2n}\times \textrm{GL}_m$ on $ψ$,1970,"  Let $F$ be a $p$-adic field and $\pi$ be an irreducible smooth representation of $\textrm{Sp}_{2n}(F)$. In this paper, we show that if $\pi$ and $\pi^\kappa$ are both generic for a common generic character of the maximal unipotent of a fixed Borel, then $\pi\cong \pi^\kappa$, where $\pi^\kappa$ is the representation induced by the conjugation action of an element $\kappa\in \textrm{GSp}_{2n}(F)$. This result is a consequence of the standard local Langlands conjecture and local Gan-Gross-Prasad conjecture. As a consequence, we extend the dependence relation of the local Rankin-Selberg gamma factors for $\textrm{Sp}_{2n}\times \textrm{GL}_m$ on $\psi$ to the general case. ",Kein DOI-Link verfügbar,1601.07618v2,Yes,potent(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Stable-Hair: Real-World Hair Transfer via Diffusion Model,1970,"  Current hair transfer methods struggle to handle diverse and intricate hairstyles, thus limiting their applicability in real-world scenarios. In this paper, we propose a novel diffusion-based hair transfer framework, named \textit{Stable-Hair}, which robustly transfers a wide range of real-world hairstyles onto user-provided faces for virtual hair try-on. To achieve this goal, our Stable-Hair framework is designed as a two-stage pipeline. In the first stage, we train a Bald Converter alongside stable diffusion to remove hair from the user-provided face images, resulting in bald images. In the second stage, we specifically designed three modules: a Hair Extractor, a Latent IdentityNet, and Hair Cross-Attention Layers to transfer the target hairstyle with highly detailed and high-fidelity to the bald image. Specifically, the Hair Extractor is trained to encode reference images with the desired hairstyles. To preserve the consistency of identity content and background between the source images and the transfer results, we employ a Latent IdentityNet to encode the source images. With the assistance of our Hair Cross-Attention Layers in the U-Net, we can accurately and precisely transfer the highly detailed and high-fidelity hairstyle to the bald image. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing hair transfer methods. Project page: \textcolor{red}{\url{https://xiaojiu-z.github.io/Stable-Hair.github.io/}} ",Kein DOI-Link verfügbar,2407.14078v1,Yes,intricate(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Arthur packets for $G_2$ and perverse sheaves on cubics,1970,"  This paper begins the project of defining Arthur packets of all unipotent representations for the $p$-adic exceptional group $G_2$. Here we treat the most interesting case by defining and computing Arthur packets with component group $S_3$. We also show that the distributions attached to these packets are stable, subject to a hypothesis. This is done using a self-contained microlocal analysis of simple equivariant perverse sheaves on the moduli space of homogeneous cubics in two variables. In forthcoming work we will treat the remaining unipotent representations and their endoscopic classification and strengthen our result on stability. ",https://doi.org/10.1016/j.aim.2021.108074,2005.02438v2,Yes,potent(2)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Toward the endoscopic classification of unipotent representations of   $p$-adic $G_2$,1970,"  We begin this paper by reviewing the Langlands correspondence for unipotent representations of the exceptional group of type $G_2$ over a $p$-adic field $F$ and present it in an explicit form. Then we compute all ABV-packets, as defined in [CFM+21] following ideas from Vogan's 1993 paper The local Langlands Conjecture, and prove that these packets satisfy properties derived from the expectation that they are generalized A-packets. We attach distributions to ABV-packets for $G_2$ and its endoscopic groups and study a geometric endoscopic transfer of these distributions. This paper builds on earlier work by the same authors. ",Kein DOI-Link verfügbar,2101.04578v2,Yes,potent(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Dynamical Evolution of the Mass Function of Globular Star Clusters,1970,"  We present a series of simple, largely analytical models to compute the effects of disruption on the mass function of star clusters. Our calculations include evaporation by two-body relaxation and gravitational shocks and mass loss by stellar evolution. We find that, for a wide variety of initial conditions, the mass function develops a turnover or peak and that, after 12 Gyr, this is remarkably close to the observed peak for globular clusters, at M_p = 2 10^5 solar masses. Below the peak, the evolution is dominated by two-body relaxation, and the mass function always develops a tail of the form psi(M) = const, reflecting that the masses of tidally limited clusters decrease linearly with time just before they are destroyed. This also agrees well with the empirical mass function of globular clusters in the Milky Way. Above the peak, the evolution is dominated by stellar evolution at early times and gravitational shocks at late times. These processes shift the mass function to lower masses while nearly preserving its shape. The radial variation of the mass function within a galaxy depends on the initial position-velocity distribution of the clusters. We find that some radial anisotropy in the initial velocity distribution, especially when this increases outward, is needed to account for the observed near-uniformity of the mass functions of globular clusters. This may be consistent with the observed near-isotropy of the present velocity distributions because clusters on elongated orbits are preferentially destroyed. These results are based on models with static, spherical galactic potentials. We point out that there would be even more radial mixing of the orbits and hence more uniformity of the mass function if the galactic potentials were time-dependent and/or non-spherical. ",https://doi.org/10.1086/323358,astro-ph/0107298v3,Yes,potent(2)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Pyramid Embedded Generative Adversarial Network for Automated Font   Generation,1970,"  In this paper, we investigate the Chinese font synthesis problem and propose a Pyramid Embedded Generative Adversarial Network (PEGAN) to automatically generate Chinese character images. The PEGAN consists of one generator and one discriminator. The generator is built using one encoder-decoder structure with cascaded refinement connections and mirror skip connections. The cascaded refinement connections embed a multiscale pyramid of downsampled original input into the encoder feature maps of different layers, and multi-scale feature maps from the encoder are connected to the corresponding feature maps in the decoder to make the mirror skip connections. Through combining the generative adversarial loss, pixel-wise loss, category loss and perceptual loss, the generator and discriminator can be trained alternately to synthesize character images. In order to verify the effectiveness of our proposed PEGAN, we first build one evaluation set, in which the characters are selected according to their stroke number and frequency of use, and then use both qualitative and quantitative metrics to measure the performance of our model comparing with the baseline method. The experimental results demonstrate the effectiveness of our proposed model, it shows the potential to automatically extend small font banks into complete ones. ",Kein DOI-Link verfügbar,1811.08106v1,Yes,potent(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,NECA: Neural Customizable Human Avatar,1970,"  Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at https://github.com/iSEE-Laboratory/NECA. ",Kein DOI-Link verfügbar,2403.10335v1,Yes,versatile(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,ApproxPilot: A GNN-based Accelerator Approximation Framework,1970,"  A typical optimization of customized accelerators for error-tolerant applications such as multimedia, recognition, and classification is to replace traditional arithmetic units like multipliers and adders with the approximate ones to enhance energy efficiency while adhering to accuracy requirements. However, the plethora of arithmetic units and diverse approximate unit options result in an exceedingly large design space. Therefore, there is a pressing need for an end-to-end design framework capable of navigating this intricate design space for approximation optimization. Traditional methods relying on simulation-based or blackbox model evaluations suffer from either high computational costs or limitations in accuracy and scalability, posing significant challenges to the optimization process. In this paper, we propose a Graph Neural Network (GNN) model that leverages the physical connections of arithmetic units to capture their influence on the performance, power, area (PPA), and accuracy of the accelerator. Particularly, we notice that critical path plays a key role in node feature of the GNN model and having it embedded in the feature vector greatly enhances the prediction quality of the models. On top of the models that allow rapid and efficient PPA and accuracy prediction of various approximate accelerator configurations, we can further explore the large design space effectively and build an end-to-end accelerator approximation framework named ApproxPilot to optimize the accelerator approximation. Our experimental results demonstrate that ApproxPilot outperforms state-of-the-art approximation optimization frameworks in both performance and hardware overhead with the same accuracy constraints. ",Kein DOI-Link verfügbar,2407.11324v1,Yes,intricate(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Room-temperature continuous-wave pumped exciton polariton condensation   in a perovskite microcavity,1970,"  Microcavity exciton polaritons (polaritons) as part-light part-matter quasiparticles, garner significant attention for non-equilibrium Bose-Einstein condensation at elevated temperatures. Recently, halide perovskites have emerged as promising room-temperature polaritonic platforms thanks to their large exciton binding energies and superior optical properties. However, currently, inducing room-temperature non-equilibrium polariton condensation in perovskite microcavities requires optical pulsed excitations with high excitation densities. Herein, we demonstrate continuous-wave optically pumped polariton condensation with an exceptionally low threshold of ~0.6 W cm-2 and a narrow linewidth of ~1 meV. Polariton condensation is unambiguously demonstrated by characterizing the nonlinear behavior and coherence properties. We also identify a microscopic mechanism involving the potential landscape in the perovskite microcavity, where numerous discretized energy levels arising from the hybridization of adjacent potential minima enhance the polariton relaxation, facilitating polariton condensate formation. Our findings lay the foundation for the next-generation energy-efficient polaritonic devices operating at room temperature. ",Kein DOI-Link verfügbar,2311.12381v2,Yes,potent(2)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Surface Plasmon Enhanced Strong Exciton-Photon Coupling in Hybrid   Inorganic-Organic Perovskites Nanowires,1970,"  Manipulating strong light-matter interaction in semiconductor microcavities is crucial for developing high-performance exciton polariton devices with great potentials in next-generation all-solid state quantum technologies. In this work, we report surface plasmon enhanced strong exciton-photon interaction in CH3NH3PbBr3 perovskite nanowires. Characteristic anti-crossing behaviors, indicating Rabi splitting energy up to ~ 560 meV, are observed near exciton resonance in hybrid semiconductor-insulator-metallic waveguide cavity at room temperature. An exciton-photon coupling strength enhancement factor of ~ 1.4 times is evaluated, which is mainly attributed to surface plasmon induced localized excitation field redistribution. Further, systematic studies on nanowires and insulator dimension dependence of exciton-photon interaction are presented. These results provide new avenues to achieve extremely high coupling strengths and push forward the development of electrically pumped and ultra-low threshold small lasers. ",https://doi.org/10.1021/acs.nanolett.7b04847,1711.05061v1,Yes,potent(1)
0000-0003-4279-5967,Qing Zhang,The Australian National University,Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model,1970,"  Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields. ",Kein DOI-Link verfügbar,2403.07764v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,NGC 6522: A typical globular cluster in the Galactic bulge without   signatures of rapidly rotating Population III stars,1970,"  We present an abundance analysis of eight potential member stars of the old Galactic bulge globular cluster NGC6522. The same stars have previously been studied by Chiappini et al. (2011), who found very high abundances of the slow neutron capture elements compared with other clusters and field stars of similar metallicity, which they interpreted as reflecting nucleosynthesis in rapidly rotating, massive Population III stars. In contrast to their analysis, we do not find any unusual enhancements of the neutron capture elements Sr, Y, Ba and Eu and conclude that previous claims result mainly from not properly accounting for blending lines. Instead we find NGC6522 to be an unremarkable globular cluster with comparable abundance trends to other Galactic globular clusters at the same metallicity ([Fe/H] = -1.15 +/- 0.16). The stars are also chemically similar to halo and bulge field stars at the same metallicity, spanning a small range in [Y/Ba] and with normal {\alpha}-element abundances. We thus find no observational evidence for any chemical signatures of rapidly rotating Population III stars in NGC 6522. ",https://doi.org/10.1093/mnras/stu2144,1408.0290v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,The Circular Velocity Curve of the Milky Way from $5$ to $25$ kpc,1970,"  We measure the circular velocity curve $v_{\rm c}(R)$ of the Milky Way with the highest precision to date across Galactocentric distances of $5\leq R \leq 25$ kpc. Our analysis draws on the $6$-dimensional phase-space coordinates of $\gtrsim 23,000$ luminous red-giant stars, for which we previously determined precise parallaxes using a data-driven model that combines spectral data from APOGEE with photometric information from WISE, 2MASS, and Gaia. We derive the circular velocity curve with the Jeans equation assuming an axisymmetric gravitational potential. At the location of the Sun we determine the circular velocity with its formal uncertainty to be $v_{\rm c}(R_{\odot}) = (229.0\pm0.2)\rm\,km\,s^{-1}$ with systematic uncertainties at the $\sim 2-5\%$ level. We find that the velocity curve is gently but significantly declining at $(-1.7\pm0.1)\rm\,km\,s^{-1}\,kpc^{-1}$, with a systematic uncertainty of $0.46\rm\,km\,s^{-1}\,kpc^{-1}$, beyond the inner $5$ kpc. We exclude the inner $5$ kpc from our analysis due to the presence of the Galactic bar, which strongly influences the kinematic structure and requires modeling in a non-axisymmetric potential. Combining our results with external measurements of the mass distribution for the baryonic components of the Milky Way from other studies, we estimate the Galaxy's dark halo mass within the virial radius to be $M_{\rm vir} = (7.25\pm0.26)\cdot 10^{11}M_{\odot}$ and a local dark matter density of $\rho_{\rm dm}(R_{\odot}) = 0.30\pm0.03\,\rm GeV\,cm^{-3}$. ",https://doi.org/10.3847/1538-4357/aaf648,1810.09466v2,Yes,potent(2)
0000-0001-5082-6693,Melissa Ness,The Australian National University,Data-driven derivation of stellar properties from photometric time   series data using convolutional neural networks,1970,"  Stellar variability is driven by a multitude of internal physical processes that depend on fundamental stellar properties. These properties are our bridge to reconciling stellar observations with stellar physics, and for understanding the distribution of stellar populations within the context of galaxy formation. Numerous ongoing and upcoming missions are charting brightness fluctuations of stars over time, which encode information about physical processes such as rotation period, evolutionary state (such as effective temperature and surface gravity), and mass (via asteroseismic parameters). Here, we explore how well we can predict these stellar properties, across different evolutionary states, using only photometric time series data. To do this, we implement a convolutional neural network, and with data-driven modeling we predict stellar properties from light curves of various baselines and cadences. Based on a single quarter of \textit{Kepler} data, we recover stellar properties, including surface gravity for red giant stars (with an uncertainty of $\lesssim$ 0.06 dex), and rotation period for main sequence stars (with an uncertainty of $\lesssim$ 5.2 days, and unbiased from $\approx$5 to 40 days). Shortening the \textit{Kepler} data to a 27-day TESS-like baseline, we recover stellar properties with a small decrease in precision, $\sim$0.07 dex for log $g$ and $\sim$5.5 days for $P_{\rm rot}$, unbiased from $\approx$5 to 35 days. Our flexible data-driven approach leverages the full information content of the data, requires minimal feature engineering, and can be generalized to other surveys and datasets. This has the potential to provide stellar property estimates for many millions of stars in current and future surveys. ",Kein DOI-Link verfügbar,2005.09682v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,The Cannon: A data-driven approach to stellar label determination,1970,"  New spectroscopic surveys offer the promise of consistent stellar parameters and abundances ('stellar labels') for hundreds of thousands of stars in the Milky Way: this poses a formidable spectral modeling challenge. In many cases, there is a sub-set of reference objects for which the stellar labels are known with high(er) fidelity. We take advantage of this with The Cannon, a new data-driven approach for determining stellar labels from spectroscopic data. The Cannon learns from the 'known' labels of reference stars how the continuum-normalized spectra depend on these labels by fitting a flexible model at each wavelength; then, The Cannon uses this model to derive labels for the remaining survey stars. We illustrate The Cannon by training the model on only 542 stars in 19 clusters as reference objects, with Teff, log g and [Fe/H] as the labels, and then applying it to the spectra of 56,000 stars from APOGEE DR10. The Cannon is very accurate. Its stellar labels compare well to the stars for which APOGEE pipeline (ASPCAP) labels are provided in DR10, with rms differences that are basically identical to the stated ASPCAP uncertainties. Beyond the reference labels, The Cannon makes no use of stellar models nor any line-list, but needs a set of reference objects that span label-space. The Cannon performs well at lower signal-to-noise, as it delivers comparably good labels even at one ninth the APOGEE observing time. We discuss the limitations of The Cannon and its future potential, particularly, to bring different spectroscopic surveys onto a consistent scale of stellar labels. ",https://doi.org/10.1088/0004-637X/808/1/16,1501.07604v2,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,"Dynamical modelling of the Galactic bulge and bar: the Milky Way's bar   pattern speed, stellar, and dark matter mass distribution",1970,"  We construct a large set of dynamical models of the galactic bulge, bar and inner disk using the Made-to-Measure method. Our models are constrained to match the red clump giant density from a combination of the VVV, UKIDSS and 2MASS infrared surveys together with stellar kinematics in the bulge from the BRAVA and OGLE surveys, and in the entire bar region from the ARGOS survey. We are able to recover the bar pattern speed and the stellar and dark matter mass distributions in the bar region, thus recovering the entire galactic effective potential. We find a bar pattern speed of $39.0 \pm 3.5 \,\rm{km\,s^{-1}\,kpc^{-1}}$, placing the bar corotation radius at $6.1 \pm 0.5 \rm{kpc}$ and making the Milky Way bar a typical fast rotator. We evaluate the stellar mass of the long bar and bulge structure to be $M_{\rm{bar/bulge}} = 1.88 \pm 0.12 \times 10^{10} \, \rm{M}_{\odot}$, larger than the mass of disk in the bar region, $M_{\rm{inner\ disk}} = 1.29\pm0.12 \times 10^{10} \, \rm{M}_{\odot}$. The total dynamical mass in the bulge volume is $1.85\pm0.05\times 10^{10} \, \rm{M}_{\odot}$. Thanks to more extended kinematic data sets and recent measurement of the bulge IMF our models have a low dark matter fraction in the bulge of $17\%\pm2\%$. We find a dark matter density profile which flattens to a shallow cusp or core in the bulge region. Finally, we find dynamical evidence for an extra central mass of $\sim0.2\times10^{10} \,\rm{M}_{\odot}$, probably in a nuclear disk or disky pseudobulge. ",https://doi.org/10.1093/mnras/stw2819,1608.07954v3,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,The challenge of simultaneously matching the observed diversity of   chemical abundance patterns in cosmological hydrodynamical simulations,1970,"  With the advent of large spectroscopic surveys the amount of high quality chemo-dynamical data in the Milky Way (MW) increased tremendously. Accurately and correctly capturing and explaining the detailed features in the high-quality observational data is notoriously difficult for state-of-the-art numerical models. In order to keep up with the quantity and quality of observational datasets, improved prescriptions for galactic chemical evolution need to be incorporated into the simulations. Here we present a new, flexible, time resolved chemical enrichment model for cosmological simulations. Our model allows to easily change a number of stellar physics parameters such as the shape of the initial mass function (IMF), stellar lifetimes, chemical yields or SN Ia delay times. We implement our model into the Gasoline2 code and perform a series of cosmological simulations varying a number of key parameters, foremost evaluating different stellar yield sets for massive stars from the literature. We find that total metallicity, total iron abundance and gas phase oxygen abundance are robust predictions from different yield sets and in agreement with observational relations. On the other hand, individual element abundances, especially $\alpha$-elements show significant differences across different yield sets and none of our models can simultaneously match constraints on the dwarf and MW mass scale. This offers a unique way of observationally constraining model parameters. For MW mass galaxies we find for most yield tables tested in this work a bimodality in the $[\alpha$/Fe] vs. [Fe/H] plane of rather low intrinsic scatter potentially in tension with the observed abundance scatter. ",https://doi.org/10.1093/mnras/stab2736,2103.03884v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,Dynamically constraining the length of the Milky Way bar,1970,"  We present a novel method for constraining the length of the Galactic bar using 6D phase space information to directly integrate orbits. We define a pseudo-length for the Galactic bar, named $R_{Freq}$, based on the maximal extent of trapped bar orbits. We find the $R_{Freq}$ measured from orbits is consistent with the $R_{Freq}$ of the assumed potential only when the length of the bar and pattern speed of said potential is similar to the model from which the initial phase-space coordinates of the orbits are derived. Therefore, one can measure the model's or the Milky Way's bar length from 6D phase-space coordinates by determining which assumed potential leads to a self-consistent measured $R_{Freq}$. When we apply this method to $\approx$210,000 stars in APOGEE DR17 and $Gaia$ eDR3 data, we find a consistent result only for potential models with a dynamical bar length of $\approx$3.5 kpc. We find the Milky Way's trapped bar orbits extend out to only $\approx$3.5 kpc, but there is also an overdensity of stars at the end of the bar out to 4.8 kpc which could be related to an attached spiral arm. We also find that the measured orbital structure of the bar is strongly dependent on the properties of the assumed potential. ",https://doi.org/10.1093/mnras/stad406,2206.01798v2,Yes,potent(5)
0000-0001-5082-6693,Melissa Ness,The Australian National University,3D NLTE Lithium abundances for late-type stars in GALAH DR3,1970,"  Lithium's susceptibility to burning in stellar interiors makes it an invaluable tracer for delineating the evolutionary pathways of stars, offering insights into the processes governing their development. Observationally, the complex Li production and depletion mechanisms in stars manifest themselves as Li plateaus, and as Li-enhanced and Li-depleted regions of the HR diagram. The Li-dip represents a narrow range in effective temperature close to the main-sequence turn-off, where stars have slightly super-solar masses and strongly depleted Li. To study the modification of Li through stellar evolution, we measure 3D non-local thermodynamic equilibrium (NLTE) Li abundance for 581 149 stars released in GALAH DR3. We describe a novel method that fits the observed spectra using a combination of 3D NLTE Li line profiles with blending metal line strength that are optimized on a star-by-star basis. Furthermore, realistic errors are determined by a Monte Carlo nested sampling algorithm which samples the posterior distribution of the fitted spectral parameters. The method is validated by recovering parameters from a synthetic spectrum and comparing to 26 stars in the Hypatia catalogue. We find 228 613 Li detections, and 352 536 Li upper limits. Our abundance measurements are generally lower than GALAH DR3, with a mean difference of 0.23 dex. For the first time, we trace the evolution of Li-dip stars beyond the main sequence turn-off and up the subgiant branch. This is the first 3D NLTE analysis of Li applied to a large spectroscopic survey, and opens up a new era of precision analysis of abundances for large surveys. ",Kein DOI-Link verfügbar,2402.02669v1,Yes,invaluable(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,The Extremely Metal Rich Knot of Stars at the Heart of the Galaxy,1970,"  We show with Gaia XP spectroscopy that extremely metal-rich stars in the Milky Way (EMR; $[M/H]_{XP} > 0.5$) - but only those - are largely confined to a tight ""knot"" at the center of the Galaxy. This EMR knot is round in projection, has a fairly abrupt edge near $\sim 1.5$kpc, and is a dynamically hot system. This central knot also contains very metal-rich (VMR; $+0.2\le [M/H]_{XP} \le +0.4$) stars. However, in contrast to EMR stars, the bulk of VMR stars form an extended, highly flattened distribution in the inner Galaxy ($R_{\mathrm{GC}}\lesssim 5$ kpc). We draw on TNG50 simulations of Milky Way analogs for context and find that compact, metal-rich knots confined to $<1.5$kpc are a universal feature. In typical simulated analogs, the top 5-10% most metal-rich stars are confined to a central knot; however, in our Milky Way data this fraction is only 0.1%. Dust-penetrating wide-area near-infrared spectroscopy, such as SDSS-V, will be needed for a rigorous estimate of the fraction of stars in the Galactic EMR knot. Why in our Milky Way only EMR giants are confined to such a central knot remains to be explained. Remarkably, the central few kiloparsecs of the Milky Way harbor both the highest concentration of metal-poor stars (the `poor old heart') and almost all EMR stars. This highlights the stellar population diversity at the bottom of galactic potential wells. ",Kein DOI-Link verfügbar,2406.01706v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,The RAVE-on catalog of stellar atmospheric parameters and chemical   abundances for chemo-dynamic studies in the Gaia era,1970,"  The orbits, atmospheric parameters, chemical abundances, and ages of individual stars in the Milky Way provide the most comprehensive illustration of galaxy formation available. The Tycho-Gaia Astrometric Solution (TGAS) will deliver astrometric parameters for the largest ever sample of Milky Way stars, though its full potential cannot be realized without the addition of complementary spectroscopy. Among existing spectroscopic surveys, the RAdial Velocity Experiment (RAVE) has the largest overlap with TGAS ($\gtrsim$200,000 stars). We present a data-driven re-analysis of 520,781 RAVE spectra using The Cannon. For red giants, we build our model using high-fidelity APOGEE stellar parameters and abundances for stars that overlap with RAVE. For main-sequence and sub-giant stars, our model uses stellar parameters from the K2/EPIC. We derive and validate effective temperature $T_{\rm eff}$, surface gravity $\log{g}$, and chemical abundances of up to seven elements (O, Mg, Al, Si, Ca, Fe, Ni). We report a total of 1,685,851 elemental abundances with a typical precision of 0.07 dex, a substantial improvement over previous RAVE data releases. The synthesis of RAVE-on and TGAS is the most powerful data set for chemo-dynamic analyses of the Milky Way ever produced. ",https://doi.org/10.3847/1538-4357/aa69c2,1609.02914v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,In Pursuit of Galactic Archaeology: Astro2020 Science White Paper,1970,"  The next decade affords tremendous opportunity to achieve the goals of Galactic archaeology. That is, to reconstruct the evolutionary narrative of the Milky Way, based on the empirical data that describes its current morphological, dynamical, temporal and chemical structures. Here, we describe a path to achieving this goal. The critical observational objective is a Galaxy-scale, contiguous, comprehensive mapping of the disk's phase space, tracing where the majority of the stellar mass resides. An ensemble of recent, ongoing, and imminent surveys are working to deliver such a transformative stellar map. Once this empirical description of the dust-obscured disk is assembled, we will no longer be operationally limited by the observational data. The primary and significant challenge within stellar astronomy and Galactic archaeology will then be in fully utilizing these data. We outline the next-decade framework for obtaining and then realizing the potential of the data to chart the Galactic disk via its stars. One way to support the investment in the massive data assemblage will be to establish a Galactic Archaeology Consortium across the ensemble of stellar missions. This would reflect a long-term commitment to build and support a network of personnel in a dedicated effort to aggregate, engineer, and transform stellar measurements into a comprehensive perspective of our Galaxy. ",Kein DOI-Link verfügbar,1907.05422v1,Yes,potent(1)
0000-0001-5082-6693,Melissa Ness,The Australian National University,NANCY: Next-generation All-sky Near-infrared Community surveY,1970,"  The Nancy Grace Roman Space Telescope is capable of delivering an unprecedented all-sky, high-spatial resolution, multi-epoch infrared map to the astronomical community. This opportunity arises in the midst of numerous ground- and space-based surveys that will provide extensive spectroscopy and imaging together covering the entire sky (such as Rubin/LSST, Euclid, UNIONS, SPHEREx, DESI, SDSS-V, GALAH, 4MOST, WEAVE, MOONS, PFS, UVEX, NEO Surveyor, etc.). Roman can uniquely provide uniform high-spatial-resolution (~0.1 arcsec) imaging over the entire sky, vastly expanding the science reach and precision of all of these near-term and future surveys. This imaging will not only enhance other surveys, but also facilitate completely new science. By imaging the full sky over two epochs, Roman can measure the proper motions for stars across the entire Milky Way, probing 100 times fainter than Gaia out to the very edge of the Galaxy. Here, we propose NANCY: a completely public, all-sky survey that will create a high-value legacy dataset benefiting innumerable ongoing and forthcoming studies of the universe. NANCY is a pure expression of Roman's potential: it images the entire sky, at high spatial resolution, in a broad infrared bandpass that collects as many photons as possible. The majority of all ongoing astronomical surveys would benefit from incorporating observations of NANCY into their analyses, whether these surveys focus on nearby stars, the Milky Way, near-field cosmology, or the broader universe. ",Kein DOI-Link verfügbar,2306.11784v1,Yes,potent(1)
0000-0001-5095-9832,Arif Ullah,The Australian National University,Molecular Quantum Chemical Data Sets and Databases for Machine Learning   Potentials,1970,"  The field of computational chemistry is increasingly leveraging machine learning (ML) potentials to predict molecular properties with high accuracy and efficiency, providing a viable alternative to traditional quantum mechanical (QM) methods, which are often computationally intensive. Central to the success of ML models is the quality and comprehensiveness of the data sets on which they are trained. Quantum chemistry data sets and databases, comprising extensive information on molecular structures, energies, forces, and other properties derived from QM calculations, are crucial for developing robust and generalizable ML potentials. In this review, we provide an overview of the current landscape of quantum chemical data sets and databases. We examine key characteristics and functionalities of prominent resources, including the types of information they store, the level of electronic structure theory employed, the diversity of chemical space covered, and the methodologies used for data creation. Additionally, an updatable resource is provided to track new data sets and databases at https://github.com/Arif-PhyChem/datasets_and_databases_4_MLPs. Looking forward, we discuss the challenges associated with the rapid growth of quantum chemical data sets and databases, emphasizing the need for updatable and accessible resources to ensure the long-term utility of them. We also address the importance of data format standardization and the ongoing efforts to align with the FAIR principles to enhance data interoperability and reusability. Drawing inspiration from established materials databases, we advocate for the development of user-friendly and sustainable platforms for these data sets and databases. ",Kein DOI-Link verfügbar,2408.12058v1,Yes,potent(2)
0000-0001-5095-9832,Arif Ullah,The Australian National University,Stochastic Equation of Motion Approach to Fermionic Dissipative   Dynamics. I. Formalism,1970,"  In this work, we establish formally exact stochastic equations of motion (SEOM) theory to describe the dissipative dynamics of fermionic open systems. The construction of the SEOM is based on a stochastic decoupling of the dissipative interaction between the system and fermionic environment, and the influence of environmental fluctuations on the reduced system dynamics is characterized by stochastic Grassmann fields. Meanwhile, numerical realization of the time-dependent Grassmann fields has remained a long-standing challenge. To solve this problem, we propose a minimal auxiliary space (MAS) mapping scheme, with which the stochastic Grassmann fields are represented by conventional c-number fields along with a set of pseudo-levels. This eventually leads to a numerically feasible MAS-SEOM method. The important properties of the MAS-SEOM are analyzed by making connection to the well-established time-dependent perturbation theory and the hierarchical equations of motion (HEOM) theory. The MAS-SEOM method provides a potentially promising approach for accurate and efficient simulation of fermionic open systems at ultra-low temperatures. ",https://doi.org/10.1063/1.5142164,1912.05272v1,Yes,potent(1)
0000-0001-5095-9832,Arif Ullah,The Australian National University,"Probing multipartite entanglement, coherence and quantum information   preservation under classical Ornstein-Uhlenbeck noise",1970,"  We address entanglement, coherence, and information protection in a system of four non-interacting qubits coupled with different classical environments, namely: common, bipartite, tripartite, and independent environments described by Ornstein-Uhlenbeck (ORU) noise. We show that quantum information preserved by the four qubit state is more dependent on the coherence than the entanglement using time-dependent entanglement witness, purity, and Shannon entropy. We find these two quantum phenomena directly interrelated and highly vulnerable in environments with ORU noise, resulting in the pure exponential decay of a considerable amount. The current Markovian dynamical map, as well as suppression of the fluctuating character of the environments are observed to be entirely attributable to the Gaussian nature of the noise. Furthermore, the increasing number of environments are witnessed to accelerate the amount of decay. Unlike other noises, the current noise parameter's flexible range is readily exploitable, ensuring long enough preserved memory properties. The four-qubit GHZ state, besides having a large information storage potential, stands partially entangled and coherent in common environments for an indefinite duration. Thus, it appeared to be a more promising resource for functional quantum computing than bipartite and tripartite quantum systems. In addition, we derive computational values for each system-environment interaction, which will help quantum practitioners to optimize the related kind of classical environments. ",https://doi.org/10.1088/1751-8121/ac3a32,2107.11251v1,Yes,potent(1)
0000-0001-5095-9832,Arif Ullah,The Australian National University,Effects of classical fluctuating environments on decoherence and   bipartite quantum correlations dynamics,1970,"  We address the time evolution of the quantum correlations ($QCs$) such as entanglement, purity, and coherence for a model of two non-interacting qubits initially prepared as a maximally entangled bipartite state. We contrast the comparative potential of the classical fields to preserve these $QCs$ in the noisy and noiseless realms. We also disclose the characteristic dynamical behavior of the $QCs$ of the two-qubit state under the static noisy effects originating from the common and different configuration models. We show that there is a direct connection between the fluctuations allowed by an environment and the $QCs$ preservation. Due to the static noisy dephasing effects, the $QCs$ are suppressed, resulting in the separability of the two-qubit entangled state after a finite duration. Here, the $QCs$ decay effects are found much smaller in the common configuration model than that of the opponent. Furthermore, this protection of the $QCs$ under static noise for large intervals is entirely attributable to the existence of the entanglement sudden death and birth phenomenon. Most importantly, we found the bipartite $QCs$ less fragile than the tripartite ones in comparison under the static noise. In the case of the measures, the concurrence is found to be sharper for showing the entanglement sudden death and birth revivals in comparison to the purity and decoherence. ",https://doi.org/10.1088/1555-6611/ac2ccf,2107.11241v1,Yes,potent(1)
0000-0001-7956-9758,Stuart Wyithe,The Australian National University,Cosmic Hydrogen Was Significantly Neutral a Billion Years After the Big   Bang,1970,"  The ionization fraction of cosmic hydrogen, left over from the big bang, provides crucial fossil evidence for when the first stars and quasar black holes formed in the infant universe. Spectra of the two most distant quasars known show nearly complete absorption of photons with wavelengths shorter than the Ly-alpha transition of neutral hydrogen, indicating that hydrogen in the intergalactic medium (IGM) had not been completely ionized at a redshift z~6.3, about a billion years after the big bang. Here we show that the radii of influence of ionizing radiation from these quasars imply that the surrounding IGM had a neutral hydrogen fraction of tens of percent prior to the quasar activity, much higher than previous lower limits of ~0.1%. When combined with the recent inference of a large cumulative optical depth to electron scattering after cosmological recombination from the WMAP data, our result suggests the existence of a second peak in the mean ionization history, potentially due to an early formation episode of the first stars. ",https://doi.org/10.1038/nature02336,astro-ph/0401188v1,Yes,potent(1)
0000-0001-7956-9758,Stuart Wyithe,The Australian National University,A Physical Model for the Luminosity Function of High-Redshift Quasars,1970,"  We provide a simple theoretical model for the quasar luminosity function at high redshifts that naturally reproduces the statistical properties of the luminous SDSS quasar sample at redshifts z~4.3 and z>5.7. Our model is based on the assumptions that quasar emission is triggered by galaxy mergers, and that the black hole mass is proportional to a power-law in the circular velocity v of the host galactic halo. We assume that quasars shine at their Eddington luminosity over a time proportional to the mass ratio between the small and final galaxies in the merger. This simple model fits the quasar luminosity function at z~2-3, reproduces the normalization and logarithmic slope (beta -2.58) at z~4.3, explains the space density of bright SDSS quasars at z~6.0, reproduces the black hole - halo mass relation for dormant black holes in the local universe, and matches the estimated duty cycle of quasar activity (~10^7 years) in Lyman-break galaxies at z~3. Based on the derived luminosity function we predict the resulting gravitational lensing rates for high redshift quasars. The lens fractions in the SDSS samples are predicted to be ~2% at z~4.3 and ~10% at z>5.7. Interestingly, the limiting quasar luminosity in our best-fit relation of L proportional to v^5/G, scales as the binding energy of the host galaxy divided by its dynamical time, implying that feedback is the mechanism that regulates black hole growth in galactic potential wells. ",https://doi.org/10.1086/344249,astro-ph/0206154v1,Yes,potent(1)
0000-0001-7956-9758,Stuart Wyithe,The Australian National University,A gravitational lensing explanation for the excess of strong Mg-II   absorbers in GRB afterglow spectra,1970,"  GRB afterglows offer a probe of the intergalactic medium out to high redshift which complements observations along more abundant quasar lines-of-sight. Although both quasars and GRB afterglows should provide a-priori random sight-lines through the intervening IGM, it has been observed that strong Mg-II absorbers are twice as likely to be found along sight-lines toward GRBs. Several proposals to reconcile this discrepancy have been put forward, but none has been found sufficient to explain the magnitude of the effect. In this paper we estimate the effect of gravitational lensing by galaxies and their surrounding mass distributions on the statistics of Mg-II absorption. We find that the multi-band magnification bias could be very strong in the spectroscopic GRB afterglow population and that gravitational lensing can explain the discrepancy in density of absorbers, for plausibly steep luminosity functions. The model makes the prediction that approximately 20%-60% of the spectroscopic afterglow sample (i.e. ~ 5-15 of 26 sources) would have been multiply imaged, and hence result in repeating bursts. We show that despite this large lensing fraction it is likely that none would yet have been identified by chance owing to the finite sky coverage of GRB searches. We predict that continued optical monitoring of the bright GRB afterglow locations in the months and years following the initial decay would lead to identification of lensed GRB afterglows. A confirmation of the lensing hypothesis would allow us to constrain the GRB luminosity function down to otherwise inaccessibly faint levels, with potential consequences for GRB models. ",https://doi.org/10.1111/j.1365-2966.2011.18374.x,1004.2081v1,Yes,potent(1)
0000-0001-7956-9758,Stuart Wyithe,The Australian National University,Cosmological constraints from 21cm surveys after reionization,1970,"  21cm emission from residual neutral hydrogen after the epoch of reionization can be used to trace the cosmological power spectrum of density fluctuations. Using a Fisher matrix formulation, we provide a detailed forecast of the constraints on cosmological parameters that are achievable with this probe. We consider two designs: a scaled-up version of the MWA observatory as well as a Fast Fourier Transform Telescope. We find that 21cm observations dedicated to post-reionization redshifts may yield significantly better constraints than next generation Cosmic Microwave Background (CMB) experiments. We find the constraints on $\Omega_\Lambda$, $\Omega_{\rm m}h^2$, and $\Omega_\nu h^2$ to be the strongest, each improved by at least an order of magnitude over the Planck CMB satellite alone for both designs. Our results do not depend as strongly on uncertainties in the astrophysics associated with the ionization of hydrogen as similar 21cm surveys during the epoch of reionization. However, we find that modulation of the 21cm power spectrum from the ionizing background could potentially degrade constraints on the spectral index of the primordial power spectrum and its running by more than an order of magnitude. Our results also depend strongly on the maximum wavenumber of the power spectrum which can be used due to non-linearities. ",https://doi.org/10.1088/1475-7516/2009/10/030,0812.0419v3,Yes,potent(1)
0000-0001-7956-9758,Stuart Wyithe,The Australian National University,The Impact of The IGM on High-Redshift Lyman Alpha Emission Lines,1970,"  We calculate the impact of the intergalactic medium (IGM) on the observed Lyman alpha lines (hereafter Lya) emitted by galaxies in an ionised IGM at z>4. Our model accounts for gas clumping in the IGM and for the fact that high-redshift galaxies reside in overdense regions, which causes the velocity field of the IGM to depart from the Hubble flow. The observed shape of the Lya line varies widely, with dependence on the intrinsic width and systemic velocity of the line, a galaxies star formation rate and the local extra-galactic UV-background. For large star formation rates and levels of the UV-background, absorption in the IGM does not result in a Lya line that is asymmetric as is common among known high-redshift Lya emitters. For models in which the lines do show the observed strong asymmetry, the IGM typically transmits only 10-30% of the Lya flux. The increase in the ionising background that accompanied the completion of reionisation barely increased the IGM transmission, which suggests that LAEs of comparable luminosity should not appear to be significantly dimmer prior to overlap. In this light, we briefly discuss the potential of Lya emitters as a probe into the epoch of reionisation. ",https://doi.org/10.1111/j.1365-2966.2007.11666.x,astro-ph/0701667v2,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,On the energy-critical quadratic nonlinear Schrödinger system with   three waves,1970,"  In this article, we consider the dynamics of the energy-critical quadratic nonlinear Schr\""odinger system $\[ \left\{ \begin{aligned} & i u^1_t + \kappa_1 \Delta u^1 = -\overline{u^2}u^3, \\ & i u^2_t + \kappa_2 \Delta u^2 = -\overline{u^1}u^3, \\ & i u^3_t + \kappa_3 \Delta u^3 = -u^1u^2, \\ \end{aligned} \right. \qquad (t, x) \in \R \times \R^6 \] in energy-space $ {\dot H}^1 \times {\dot H}^1\times{\dot H}^1 $, where the sign of potential energy can not be determined. We prove the scattering theory with mass-resonance (or with radial initial data) below ground state via concentration compactness method. We discover a family of new physically conserved quantities with mass-resonance which play an important role in the proof of scattering. ",Kein DOI-Link verfügbar,2110.05277v3,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Public Transport Planning: When Transit Network Connectivity Meets   Commuting Demand,1970,"  In this paper, we make a first attempt to incorporate both commuting demand and transit network connectivity in bus route planning (CT-Bus), and formulate it as a constrained optimization problem: planning a new bus route with k edges over an existing transit network without building new bus stops to maximize a linear aggregation of commuting demand and connectivity of the transit network. We prove the NP-hardness of CT-Bus and propose an expansion-based greedy algorithm that iteratively scans potential candidate paths in the network. To boost the efficiency of computing the connectivity of new networks with candidate paths, we convert it to a matrix trace estimation problem and employ a Lanczos method to estimate the natural connectivity of the transit network with a guaranteed error bound. Furthermore, we derive upper bounds on the objective values and use them to greedily select candidates for expansion. Our experiments conducted on real-world transit networks in New York City and Chicago verify the efficiency, effectiveness, and scalability of our algorithms. ",https://doi.org/10.1145/3448016.3457247,2103.16084v2,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Long time behaviors for the inhomogeneous NLS with a potential in   $\mathbb{R}^3$,1970,"  In this article, we aim to study the scattering of the solution to the focusing inhomogeneous nonlinear Schr\""odinger equation with a potential of form \begin{align*}   i\partial_t u+\Delta u- Vu=-|x|^{-b}|u|^{p-1}u \end{align*} in the energy space $H^1(\R^3)$. We prove a scattering criterion, and then we use it together with Morawetz estimate to show the scattering theory, which generalizes the results of Dinh \cite{DD} to the non-radial symmetric case. ",Kein DOI-Link verfügbar,2107.06094v5,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis,1970,"  Obtaining large-scale radiology reports can be difficult for medical images due to various reasons, limiting the effectiveness of contrastive pre-training in the medical image domain and underscoring the need for alternative methods. In this paper, we propose eye-tracking as an alternative to text reports, as it allows for the passive collection of gaze signals without disturbing radiologist's routine diagnosis process. By tracking the gaze of radiologists as they read and diagnose medical images, we can understand their visual attention and clinical reasoning. When a radiologist has similar gazes for two medical images, it may indicate semantic similarity for diagnosis, and these images should be treated as positive pairs when pre-training a computer-assisted diagnosis (CAD) network through contrastive learning. Accordingly, we introduce the Medical contrastive Gaze Image Pre-training (McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP uses radiologist's gaze to guide contrastive pre-training. We evaluate our method using two representative types of medical images and two common types of gaze data. The experimental results demonstrate the practicality of McGIP, indicating its high potential for various clinical scenarios and applications. ",Kein DOI-Link verfügbar,2312.06069v2,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Intra-protein binding peptide fragments have specific and intrinsic   sequence patterns,1970,"  The key finding in the DNA double helix model is the specific pairing or binding between nucleotides A-T and C-G, and the pairing rules are the molecule basis of genetic code. Unfortunately, no such rules have been discovered for proteins. Here we show that similar rules and intrinsic sequence patterns between intra-protein binding peptide fragments do exist, and they can be extracted using a deep learning algorithm. Multi-millions of binding and non-binding peptide fragments from currently available protein X-ray structures are classified with an accuracy of up to 93%. This discovery has the potential in helping solve protein folding and protein-protein interaction problems, two open and fundamental problems in molecular biology. ",Kein DOI-Link verfügbar,1702.08078v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using   Large Language Models,1970,"  Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models. ",Kein DOI-Link verfügbar,2302.07257v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,"Panacea: A foundation model for clinical trial search, summarization,   design, and recruitment",1970,"  Clinical trials are fundamental in developing new drugs, medical devices, and treatments. However, they are often time-consuming and have low success rates. Although there have been initial attempts to create large language models (LLMs) for clinical trial design and patient-trial matching, these models remain task-specific and not adaptable to diverse clinical trial tasks. To address this challenge, we propose a clinical trial foundation model named Panacea, designed to handle multiple tasks, including trial search, trial summarization, trial design, and patient-trial matching. We also assemble a large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207 trial-related scientific papers, to infuse clinical knowledge into the model by pre-training. We further curate TrialInstruct, which has 200,866 of instruction data for fine-tuning. These resources enable Panacea to be widely applicable for a range of clinical trial tasks based on user requirements.   We evaluated Panacea on a new benchmark, named TrialPanorama, which covers eight clinical trial tasks. Our method performed the best on seven of the eight tasks compared to six cutting-edge generic or medicine-specific LLMs. Specifically, Panacea showed great potential to collaborate with human experts in crafting the design of eligibility criteria, study arms, and outcome measures, in multi-round conversations. In addition, Panacea achieved 14.42% improvement in patient-trial matching, 41.78% to 52.02% improvement in trial search, and consistently ranked at the top for five aspects of trial summarization. Our approach demonstrates the effectiveness of Panacea in clinical trials and establishes a comprehensive resource, including training data, model, and benchmark, for developing clinical trial foundation models, paving the path for AI-based clinical trial development. ",Kein DOI-Link verfügbar,2407.11007v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Inter-slice Super-resolution of Magnetic Resonance Images by   Pre-training and Self-supervised Fine-tuning,1970,"  In clinical practice, 2D magnetic resonance (MR) sequences are widely adopted. While individual 2D slices can be stacked to form a 3D volume, the relatively large slice spacing can pose challenges for both image visualization and subsequent analysis tasks, which often require isotropic voxel spacing. To reduce slice spacing, deep-learning-based super-resolution techniques are widely investigated. However, most current solutions require a substantial number of paired high-resolution and low-resolution images for supervised training, which are typically unavailable in real-world scenarios. In this work, we propose a self-supervised super-resolution framework for inter-slice super-resolution of MR images. Our framework is first featured by pre-training on video dataset, as temporal correlation of videos is found beneficial for modeling the spatial relation among MR slices. Then, we use public high-quality MR dataset to fine-tune our pre-trained model, for enhancing awareness of our model to medical data. Finally, given a target dataset at hand, we utilize self-supervised fine-tuning to further ensure our model works well with user-specific super-resolution tasks. The proposed method demonstrates superior performance compared to other self-supervised methods and also holds the potential to benefit various downstream applications. ",Kein DOI-Link verfügbar,2406.05974v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Theoretical study of the dual harmonic system and its application on the   CSNS/RCS,1970,"  The dual harmonic system has been widely used in high intensity proton synchrotrons to suppress the space charge effect, as well as reduce the beam loss. To investigate the longitudinal beam dynamics in the dual rf system, the potential well, the sub-buckets in the bunch and the multi-solutions of the phase equation have been studied theoretically. Based on these theoretical studis, the optimization of bunching factor and rf voltage waveform are made for the dual harmonic rf system in the upgrade phase of the CSNS/RCS. In the optimization process, the simulation with space charge effect is done by using a newly developed code C-SCSIM. ",https://doi.org/10.1088/1674-1137/39/12/127002,1503.04789v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA,1970,"  With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA retains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA. ",Kein DOI-Link verfügbar,2402.16902v2,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised   Medical Image Segmentation,1970,"  Medical image segmentation methods are generally designed as fully-supervised to guarantee model performance, which require a significant amount of expert annotated samples that are high-cost and laborious. Semi-supervised image segmentation can alleviate the problem by utilizing a large number of unlabeled images along with limited labeled images. However, learning a robust representation from numerous unlabeled images remains challenging due to potential noise in pseudo labels and insufficient class separability in feature space, which undermines the performance of current semi-supervised segmentation approaches. To address the issues above, we propose a novel semi-supervised segmentation method named as Rectified Contrastive Pseudo Supervision (RCPS), which combines a rectified pseudo supervision and voxel-level contrastive learning to improve the effectiveness of semi-supervised segmentation. Particularly, we design a novel rectification strategy for the pseudo supervision method based on uncertainty estimation and consistency regularization to reduce the noise influence in pseudo labels. Furthermore, we introduce a bidirectional voxel contrastive loss to the network to ensure intra-class consistency and inter-class contrast in feature space, which increases class separability in the segmentation. The proposed RCPS segmentation method has been validated on two public datasets and an in-house clinical dataset. Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art methods in semi-supervised medical image segmentation. The source code is available at https://github.com/hsiangyuzhao/RCPS. ",https://doi.org/10.1109/JBHI.2023.3322590,2301.05500v2,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Label-Driven Reconstruction for Domain Adaptation in Semantic   Segmentation,1970,"  Unsupervised domain adaptation enables to alleviate the need for pixel-wise annotation in the semantic segmentation. One of the most common strategies is to translate images from the source domain to the target domain and then align their marginal distributions in the feature space using adversarial learning. However, source-to-target translation enlarges the bias in translated images and introduces extra computations, owing to the dominant data size of the source domain. Furthermore, consistency of the joint distribution in source and target domains cannot be guaranteed through global feature alignment. Here, we present an innovative framework, designed to mitigate the image translation bias and align cross-domain features with the same category. This is achieved by 1) performing the target-to-source translation and 2) reconstructing both source and target images from their predicted labels. Extensive experiments on adapting from synthetic to real urban scene understanding demonstrate that our framework competes favorably against existing state-of-the-art methods. ",Kein DOI-Link verfügbar,2003.04614v3,Yes,innovative(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,A method for reversing the laser modulation in a Storage ring,1970,"  The pursuit of coherent radiation generation remains a central focus in advancing storage ring light sources. Despite the promise of laser modulation in achieving this goal, it brings about a noticeable decline in beam quality. Efforts to mitigate this decline have resulted in the proposal of demodulation schemes. However, implementing modulation and demodulation within the storage ring presents significant challenges due to dynamical and spatial constraints within straight sections. In this study, we propose a straightforward and easily implementable method for achieving reversible laser modulation in a storage ring. Notably, our approach circumvents the need for special storage ring requirements, such as lengthy straight sections or bypass section. Simulation results demonstrate a substantial restoration of beam quality following demodulation. This innovative scheme holds great promise for the realization of high repetition rate coherent storage ring light sources. ",Kein DOI-Link verfügbar,2405.10573v1,Yes,innovative(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,tFold-TR: Combining Deep Learning Enhanced Hybrid Potential Energy for   Template-Based Modeling Structure Refinement,1970,"  Protein structure prediction has been a grand challenge for over 50 years, owing to its broad scientific and application interests. There are two primary types of modeling algorithms, template-free modeling and template-based modeling. The latter one is suitable for easy prediction tasks and is widely adopted in computer-aided drug discoveries for drug design and screening. Although it has been several decades since its first edition, the current template-based modeling approach suffers from two critical problems: 1) there are many missing regions in the template-query sequence alignment, and 2) the accuracy of the distance pairs from different regions of the template varies, and this information is not well introduced into the modeling. To solve these two problems, we propose a structural optimization process based on template modeling, introducing two neural network models to predict the distance information of the missing regions and the accuracy of the distance pairs of different regions in the template modeling structure. The predicted distances and residue pairwise-specific deviations are incorporated into the potential energy function for structural optimization, which significantly improves the qualities of the original template modeling decoys. ",Kein DOI-Link verfügbar,2105.04350v4,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Data Augmentation of Multi-turn Psychological Dialogue via   Knowledge-driven Progressive Thought Prompting,1970,"  Existing dialogue data augmentation (DA) techniques predominantly focus on augmenting utterance-level dialogues, which makes it difficult to take dialogue contextual information into account. The advent of large language models (LLMs) has simplified the implementation of multi-turn dialogues. Due to absence of professional understanding and knowledge, it remains challenging to deliver satisfactory performance in low-resource domain, like psychological dialogue dialogue. DA involves creating new training or prompting data based on the existing data, which help the model better understand and generate psychology-related responses. In this paper, we aim to address the issue of multi-turn dialogue data augmentation for boosted performance in the psychology domain. We propose a knowledge-driven progressive thought prompting method to guide LLM to generate multi-turn psychology-related dialogue. This method integrates a progressive thought generator, a psychology knowledge generator, and a multi-turn dialogue generator. The thought generated by the progressive thought generator serves as a prompt to prevent the generated dialogue from having significant semantic deviations, while the psychology knowledge generator produces psychological knowledge to serve as the dialogue history for the LLM, guiding the dialogue generator to create multi-turn psychological dialogue. To ensure the precision of multi-turn psychological dialogue generation by LLM, a meticulous professional evaluation is required. Extensive experiments conducted on three datasets related to psychological dialogue verify the effectiveness of the proposed method. ",Kein DOI-Link verfügbar,2406.16567v1,Yes,meticulous(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,MUC: Mixture of Uncalibrated Cameras for Robust 3D Human Body   Reconstruction,1970,"  Multiple cameras can provide comprehensive multi-view video coverage of a person. Fusing this multi-view data is crucial for tasks like behavioral analysis, although it traditionally requires camera calibration, a process that is often complex. Moreover, previous studies have overlooked the challenges posed by self-occlusion under multiple views and the continuity of human body shape estimation. In this study, we introduce a method to reconstruct the 3D human body from multiple uncalibrated camera views. Initially, we utilize a pre-trained human body encoder to process each camera view individually, enabling the reconstruction of human body models and parameters for each view along with predicted camera positions. Rather than merely averaging the models across views, we develop a neural network trained to assign weights to individual views for all human body joints, based on the estimated distribution of joint distances from each camera. Additionally, we focus on the mesh surface of the human body for dynamic fusion, allowing for the seamless integration of facial expressions and body shape into a unified human body model. Our method has shown excellent performance in reconstructing the human body on two public datasets, advancing beyond previous work from the SMPL model to the SMPL-X model. This extension incorporates more complex hand poses and facial expressions, enhancing the detail and accuracy of the reconstructions. Crucially, it supports the flexible ad-hoc deployment of any number of cameras, offering significant potential for various applications. Our code is available at https://github.com/AbsterZhu/MUC. ",Kein DOI-Link verfügbar,2403.05055v3,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,mr2NST: Multi-Resolution and Multi-Reference Neural Style Transfer for   Mammography,1970,"  Computer-aided diagnosis with deep learning techniques has been shown to be helpful for the diagnosis of the mammography in many clinical studies. However, the image styles of different vendors are very distinctive, and there may exist domain gap among different vendors that could potentially compromise the universal applicability of one deep learning model. In this study, we explicitly address style variety issue with the proposed multi-resolution and multi-reference neural style transfer (mr2NST) network. The mr2NST can normalize the styles from different vendors to the same style baseline with very high resolution. We illustrate that the image quality of the transferred images is comparable to the quality of original images of the target domain (vendor) in terms of NIMA scores. Meanwhile, the mr2NST results are also shown to be helpful for the lesion detection in mammograms. ",Kein DOI-Link verfügbar,2005.11926v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,A fully differentiable ligand pose optimization framework guided by deep   learning and traditional scoring functions,1970,"  The machine learning (ML) and deep learning (DL) techniques are widely recognized to be powerful tools for virtual drug screening. The recently reported ML- or DL-based scoring functions have shown exciting performance in predicting protein-ligand binding affinities with fruitful application prospects. However, the differentiation between highly similar ligand conformations, including the native binding pose (the global energy minimum state), remains challenging which could greatly enhance the docking. In this work, we propose a fully differentiable framework for ligand pose optimization based on a hybrid scoring function (SF) combined with a multi-layer perceptron (DeepRMSD) and the traditional AutoDock Vina SF. The DeepRMSD+Vina, which combines (1) the root mean square deviation (RMSD) of the docking pose with respect to the native pose and (2) the AutoDock Vina score, is fully differentiable thus is capable of optimizing the ligand binding pose to the energy-lowest conformation. Evaluated by the CASF-2016 docking power dataset, the DeepRMSD+Vina reaches a success rate of 95.4%, which is by far the best reported SF to date. Based on this SF, an end-to-end ligand pose optimization framework was implemented to improve the docking pose quality. We demonstrated that this method significantly improves the docking success rate (by 15%) in redocking and crossdocking tasks, revealing the high potentialities of this framework in drug design and discovery. ",https://doi.org/10.1093/bib/bbac520,2206.13345v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,CopilotCAD: Empowering Radiologists with Report Completion Models and   Quantitative Evidence from Medical Image Foundation Models,1970,"  Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency. However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists' expertise. This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging Large Language Models (LLMs) and medical image analysis tools. Specifically, we develop a collaborative framework to integrate LLMs and quantitative medical image analysis results generated by foundation models with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals. This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians. Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology. ",Kein DOI-Link verfügbar,2404.07424v1,Yes,"innovative(1), potent(1)"
0000-0003-2613-2442,Sheng Wang,The Australian National University,Joint Progressive and Coarse-to-fine Registration of Brain MRI via   Deformation Field Integration and Non-Rigid Feature Fusion,1970,"  Registration of brain MRI images requires to solve a deformation field, which is extremely difficult in aligning intricate brain tissues, e.g., subcortical nuclei, etc. Existing efforts resort to decomposing the target deformation field into intermediate sub-fields with either tiny motions, i.e., progressive registration stage by stage, or lower resolutions, i.e., coarse-to-fine estimation of the full-size deformation field. In this paper, we argue that those efforts are not mutually exclusive, and propose a unified framework for robust brain MRI registration in both progressive and coarse-to-fine manners simultaneously. Specifically, building on a dual-encoder U-Net, the fixed-moving MRI pair is encoded and decoded into multi-scale deformation sub-fields from coarse to fine. Each decoding block contains two proposed novel modules: i) in Deformation Field Integration (DFI), a single integrated sub-field is calculated, warping by which is equivalent to warping progressively by sub-fields from all previous decoding blocks, and ii) in Non-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned by DFI-integrated sub-field, and then fused to predict a finer sub-field. Leveraging both DFI and NFF, the target deformation field is factorized into multi-scale sub-fields, where the coarser fields alleviate the estimate of a finer one and the finer field learns to make up those misalignments insolvable by previous coarser ones. The extensive and comprehensive experimental results on both private and public datasets demonstrate a superior registration performance of brain MRI images over progressive registration only and coarse-to-fine estimation only, with an increase by at most 8% in the average Dice. ",Kein DOI-Link verfügbar,2109.12384v3,Yes,intricate(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs,1970,"  The integration of Computer-Aided Diagnosis (CAD) with Large Language Models (LLMs) presents a promising frontier in clinical applications, notably in automating diagnostic processes akin to those performed by radiologists and providing consultations similar to a virtual family doctor. Despite the promising potential of this integration, current works face at least two limitations: (1) From the perspective of a radiologist, existing studies typically have a restricted scope of applicable imaging domains, failing to meet the diagnostic needs of different patients. Also, the insufficient diagnostic capability of LLMs further undermine the quality and reliability of the generated medical reports. (2) Current LLMs lack the requisite depth in medical expertise, rendering them less effective as virtual family doctors due to the potential unreliability of the advice provided during patient consultations. To address these limitations, we introduce ChatCAD+, to be universal and reliable. Specifically, it is featured by two main modules: (1) Reliable Report Generation and (2) Reliable Interaction. The Reliable Report Generation module is capable of interpreting medical images from diverse domains and generate high-quality medical reports via our proposed hierarchical in-context learning. Concurrently, the interaction module leverages up-to-date information from reputable medical websites to provide reliable medical advice. Together, these designed modules synergize to closely align with the expertise of human medical professionals, offering enhanced consistency and reliability for interpretation and advice. The source code is available at https://github.com/zhaozh10/ChatCAD. ",https://doi.org/10.1109/TMI.2024.3398350,2305.15964v5,Yes,potent(2)
0000-0003-2613-2442,Sheng Wang,The Australian National University,AdLER: Adversarial Training with Label Error Rectification for One-Shot   Medical Image Segmentation,1970,"  Accurate automatic segmentation of medical images typically requires large datasets with high-quality annotations, making it less applicable in clinical settings due to limited training data. One-shot segmentation based on learned transformations (OSSLT) has shown promise when labeled data is extremely limited, typically including unsupervised deformable registration, data augmentation with learned registration, and segmentation learned from augmented data. However, current one-shot segmentation methods are challenged by limited data diversity during augmentation, and potential label errors caused by imperfect registration. To address these issues, we propose a novel one-shot medical image segmentation method with adversarial training and label error rectification (AdLER), with the aim of improving the diversity of generated data and correcting label errors to enhance segmentation performance. Specifically, we implement a novel dual consistency constraint to ensure anatomy-aligned registration that lessens registration errors. Furthermore, we develop an adversarial training strategy to augment the atlas image, which ensures both generation diversity and segmentation robustness. We also propose to rectify potential label errors in the augmented atlas images by estimating segmentation uncertainty, which can compensate for the imperfect nature of deformable registration and improve segmentation authenticity. Experiments on the CANDI and ABIDE datasets demonstrate that the proposed AdLER outperforms previous state-of-the-art methods by 0.7% (CANDI), 3.6% (ABIDE ""seen""), and 4.9% (ABIDE ""unseen"") in segmentation based on Dice scores, respectively. The source code will be available at https://github.com/hsiangyuzhao/AdLER. ",Kein DOI-Link verfügbar,2309.00971v1,Yes,potent(2)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Collective electronic excitation in a trapped ensemble of photogenerated   dipolar excitons and free holes revealed by inelastic light scattering,1970,"  Photogenerated excitonic ensembles confined in coupled GaAs quantum wells are probed by a complementary approach of emission spectroscopy and resonant inelastic light scattering. Lateral electrostatic trap geometries are used to create dense systems of spatially indirect excitons and excess holes with similar densities in the order of 10$^{11}$ cm$^{-2}$. Inelastic light scattering spectra reveal a very sharp low-lying collective mode that is identified at an energy of 0.44 meV and a FWHM of only ~50 $\mu$eV. This mode is interpreted as a plasmon excitation of the excess hole system coupled to the photogenerated indirect excitons. The emission energy of the indirect excitons shifts under the application of a perpendicular applied electric field with the quantum-confined Stark effect unperturbed from the presence of free charge carriers. Our results illustrate the potential of studying low-lying collective excitations in photogenerated exciton systems to explore the many-body phase diagram, related phase transitions, and interaction physics. ",https://doi.org/10.1103/PhysRevB.95.085312,1612.04072v1,Yes,potent(1)
0000-0003-2613-2442,Sheng Wang,The Australian National University,Visualization of skyrmion-superconducting vortex pairs in a chiral   magnet-superconductor heterostructure,1970,"  Magnetic skyrmions, the topological states possessing chiral magnetic structure with non-trivial topology, have been widely investigated as a promising candidate for spintronic devices. They can also couple with superconducting vortices to form skyrmion-vortex pairs, hosting Majorana zero mode which is a potential candidate for topological quantum computering. A lot of theoretical proposals have been put forward on constructing skyrmion-vortex pairs in heterostructures of chiral magnet and superconductor. Nevertheless, how to generate skyrmion-vortex pairs in a controllable way experimentally remains a significant challenge. We have designed a heterostructure of chiral magnet and superconductor [CoFeB/Ir/Ta]7/Nb in which zero field N\'eel-type skyrmions can be stabilized and the superconducting vortices can couple with the skyrmions when Nb is in the superconducting state. We have directly observed the formation of skyrmion-superconducting vortex pairs which is dependent on the direction of the applied magnetic field. Our results provide an effective method to manipulate the quantum states of skyrmions with the help of superconducting vortices, which can be used to explore the possible existence of Majorana zero mode for future quantum computation. ",Kein DOI-Link verfügbar,2310.13363v1,Yes,potent(1)
0000-0002-6507-0347,Ben Andrews,The Australian National University,Proof of the fundamental gap conjecture,1970,"  We prove the Fundamental Gap Conjecture, which states that the difference between the first two Dirichlet eigenvalues (the spectral gap) of a Schr\""odinger operator with convex potential and Dirichlet boundary data on a convex domain is bounded below by the spectral gap on an interval of the same diameter with zero potential. More generally, for an arbitrary smooth potential in higher dimensions, our proof gives both a sharp lower bound for the spectral gap and a sharp modulus of concavity for the logarithm of the first eigenfunction, in terms of the diameter of the domain and a modulus of convexity for the potential. ",Kein DOI-Link verfügbar,1006.1686v2,Yes,potent(4)
0000-0002-6507-0347,Ben Andrews,The Australian National University,The fundamental gap for a one-dimensional Schrödinger operator with   Robin boundary conditions,1970,"  For Schr\""odinger operators on an interval with either convex or symmetric single-well potentials, and Robin or Neumann boundary conditions, the gap between the two lowest eigenvalues is minimised when the potential is constant. We also have results for the $p$-Laplacian. ",Kein DOI-Link verfügbar,2002.06900v1,Yes,potent(2)
0000-0003-4982-5516,William Scott,The Australian National University,Talek: Private Group Messaging with Hidden Access Patterns,1970,"  Talek is a private group messaging system that sends messages through potentially untrustworthy servers, while hiding both data content and the communication patterns among its users. Talek explores a new point in the design space of private messaging; it guarantees access sequence indistinguishability, which is among the strongest guarantees in the space, while assuming an anytrust threat model, which is only slightly weaker than the strongest threat model currently found in related work. Our results suggest that this is a pragmatic point in the design space, since it supports strong privacy and good performance: we demonstrate a 3-server Talek cluster that achieves throughput of 9,433 messages/second for 32,000 active users with 1.7-second end-to-end latency. To achieve its security goals without coordination between clients, Talek relies on information-theoretic private information retrieval. To achieve good performance and minimize server-side storage, Talek introduces new techniques and optimizations that may be of independent interest, e.g., a novel use of blocked cuckoo hashing and support for private notifications. The latter provide a private, efficient mechanism for users to learn, without polling, which logs have new messages. ",https://doi.org/10.1145/3427228.3427231,2001.08250v3,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Graph-theoretical optimization of fusion-based graph state generation,1970,"  Graph states are versatile resources for various quantum information processing tasks, including measurement-based quantum computing and quantum repeaters. Although the type-II fusion gate enables all-optical generation of graph states by combining small graph states, its non-deterministic nature hinders the efficient generation of large graph states. In this work, we present a graph-theoretical strategy to effectively optimize fusion-based generation of any given graph state, along with a Python package OptGraphState. Our strategy comprises three stages: simplifying the target graph state, building a fusion network, and determining the order of fusions. Utilizing this proposed method, we evaluate the resource overheads of random graphs and various well-known graphs. Additionally, we investigate the success probability of graph state generation given a restricted number of available resource states. We expect that our strategy and software will assist researchers in developing and assessing experimentally viable schemes that use photonic graph states. ",https://doi.org/10.22331/q-2023-12-20-1212,2304.11988v4,Yes,versatile(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",On computational complexity and average-case hardness of shallow-depth   boson sampling,1970,"  Boson sampling, a computational task believed to be classically hard to simulate, is expected to hold promise for demonstrating quantum computational advantage using near-term quantum devices. However, noise in experimental implementations poses a significant challenge, potentially rendering boson sampling classically simulable and compromising its classical intractability. Numerous studies have proposed classical algorithms under various noise models that can efficiently simulate boson sampling as noise rates increase with circuit depth. To address this issue particularly related to circuit depth, we explore the viability of achieving quantum computational advantage through boson sampling with shallow-depth linear optical circuits. Specifically, as the average-case hardness of estimating output probabilities of boson sampling is a crucial ingredient in demonstrating its classical intractability, we make progress on establishing the average-case hardness confined to logarithmic-depth regimes. We also obtain the average-case hardness for logarithmic-depth Fock-state boson sampling subject to lossy environments and for the logarithmic-depth Gaussian boson sampling. By providing complexity-theoretical backgrounds for the classical simulation hardness of logarithmic-depth boson sampling, we expect that our findings will mark a crucial step towards a more noise-tolerant demonstration of quantum advantage with shallow-depth boson sampling. ",Kein DOI-Link verfügbar,2405.01786v1,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Dequantizing quantum machine learning models using tensor networks,1970,"  Ascertaining whether a classical model can efficiently replace a given quantum model -- dequantization -- is crucial in assessing the true potential of quantum algorithms. In this work, we introduced the dequantizability of the function class of variational quantum-machine-learning~(VQML) models by employing the tensor network formalism, effectively identifying every VQML model as a subclass of matrix product state (MPS) model characterized by constrained coefficient MPS and tensor product-based feature maps. From this formalism, we identify the conditions for which a VQML model's function class is dequantizable or not. Furthermore, we introduce an efficient quantum kernel-induced classical kernel which is as expressive as given any quantum kernel, hinting at a possible way to dequantize quantum kernel methods. This presents a thorough analysis of VQML models and demonstrates the versatility of our tensor-network formalism to properly distinguish VQML models according to their genuine quantum characteristics, thereby unifying classical and quantum machine-learning models within a single framework. ",https://doi.org/10.1103/PhysRevResearch.6.023218,2307.06937v2,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Exploring Shallow-Depth Boson Sampling: Towards Scalable Quantum   Supremacy,1970,"  Boson sampling is a sampling task proven to be hard to simulate efficiently using classical computers under plausible assumptions, which makes it an appealing candidate for quantum supremacy. However, due to a large noise rate for near-term quantum devices, it is still unclear whether those noisy devices maintain the quantum advantage for much larger quantum systems. Since the noise rate typically grows with the circuit depth, an alternative is to find evidence of simulation hardness at the shallow-depth quantum circuit. To find the evidence, one way is to identify the minimum depth required for the average-case hardness of approximating output probabilities, which is considered a necessary condition for the state-of-the-art technique to prove the simulation hardness of boson sampling. In this work, we analyze the output probability distribution of shallow-depth boson sampling for Fock-states and Gaussian states, and examine the limitation of the average-case hardness argument at this shallow-depth regime for geometrically local architectures. We propose a shallow-depth linear optical circuit architecture that can overcome the problems associated with geometrically local architectures. Our numerical results suggest that this architecture demonstrates possibilities of average-case hardness properties in a shallow-depth regime, through its resemblance to the global Haar-random boson sampling circuit. This result implies that the corresponding architecture has the potential to be utilized for scalable quantum supremacy with its shallow-depth boson sampling. ",https://doi.org/10.1103/PhysRevA.109.052613,2306.10671v2,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",On the theoretical prospects of multiport devices for   photon-number-resolving detections,1970,"  Ideal photon-number-resolving detectors form a class of important optical components in quantum optics and quantum information theory. In this article, we theoretically investigate the potential of multiport devices having reconstruction performances approaching that of the Fock-state measurement. By recognizing that all multiport devices are minimally complete, we first provide a general analytical framework to describe the tomographic accuracy (or quality) of these devices. Next, we show that a perfect multiport device with an infinite number of output ports functions as either the Fock-state measurement when photon losses are absent or binomial mixtures of Fock-state measurements when photon losses are present, and derive their respective expressions for the tomographic transfer function. This function is the scaled asymptotic mean squared-error of the reconstructed photon-number distributions uniformly averaged over all distributions in the probability simplex. We then supply more general analytical formulas for the transfer function for finite numbers of output ports in both the absence and presence of photon losses. The effects of photon losses on the photon-number resolving power of both infinite- and finite-size multiport devices are also investigated. ",Kein DOI-Link verfügbar,1909.04527v1,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Fault-tolerant quantum computation by hybrid qubits with bosonic   cat-code and single photons,1970,"  Hybridizing different degrees of freedom or physical platforms potentially offers various advantages in building scalable quantum architectures. We here introduce a fault-tolerant hybrid quantum computation by taking the advantages of both discrete variable (DV) and continuous variable (CV) systems. Particularly, we define a CV-DV hybrid qubit with bosonic cat-code and single photon, which is implementable in current photonic platforms. By the cat-code encoded in the CV part, the dominant loss errors are readily correctable without multi-qubit encoding, while the logical basis is inherently orthogonal due to the DV part. We design fault-tolerant architectures by concatenating hybrid qubits and an outer DV quantum error correction code such as topological codes, exploring their potential merits in developing scalable quantum computation. We demonstrate by numerical simulations that our scheme is at least an order of magnitude more resource-efficient over all previous proposals in photonic platforms, allowing to achieve a record-high loss threshold among existing CV and hybrid approaches. We discuss its realization not only in all-photonic platforms but also in other hybrid platforms including superconduting and trapped-ion systems, which allows us to find various efficient routes towards fault-tolerant quantum computing. ",Kein DOI-Link verfügbar,2401.00450v1,Yes,potent(2)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Entangling quantum and classical states of light,1970,"  Entanglement between quantum and classical objects is of special interest in the context of fundamental studies of quantum mechanics and potential applications to quantum information processing. In quantum optics, single photons are treated as light quanta while coherent states are considered the most classical among all pure states. Recently, entanglement between a single photon and a coherent state in a free-traveling field was identified to be a useful resource for optical quantum information processing. However, it was pointed out to be extremely difficult to generate such states since it requires a clean cross-Kerr nonlinear interaction. Here, we devise and experimentally demonstrate a scheme to generate such hybrid entanglement by implementing a coherent superposition of two distinct quantum operations. The generated states clearly show entanglement between the two different types of states. Our work opens a way to generate hybrid entanglement of a larger size and to develop efficient quantum information processing using such a new type of qubits. ",https://doi.org/10.1038/nphoton.2014.136,1309.6192v1,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Universal compressive characterization of quantum dynamics,1970,"  Recent quantum technologies utilize complex multidimensional processes that govern the dynamics of quantum systems. We develop an adaptive diagonal-element-probing compression technique that feasibly characterizes any unknown quantum processes using much fewer measurements compared to conventional methods. This technique utilizes compressive projective measurements that are generalizable to arbitrary number of subsystems. Both numerical analysis and experimental results with unitary gates demonstrate low measurement costs, of order $O(d^2)$ for $d$-dimensional systems, and robustness against statistical noise. Our work potentially paves the way for a reliable and highly compressive characterization of general quantum devices. ",https://doi.org/10.1103/PhysRevLett.124.210401,2005.13776v1,Yes,potent(1)
0000-0003-4740-6975,Hyunseok Jeong,"Seoul National University, Seoul National University Bundang Hospital, Seoul National University Hospital",Benchmarking quantum tomography completeness and fidelity with machine   learning,1970,"  We train convolutional neural networks to predict whether or not a set of measurements is informationally complete to uniquely reconstruct any given quantum state with no prior information. In addition, we perform fidelity benchmarking based on this measurement set without explicitly carrying out state tomography. The networks are trained to recognize the fidelity and a reliable measure for informational completeness. By gradually accumulating measurements and data, these trained convolutional networks can efficiently establish a compressive quantum-state characterization scheme by accelerating runtime computation and greatly reducing systematic drifts in experiments. We confirm the potential of this machine-learning approach by presenting experimental results for both spatial-mode and multiphoton systems of large dimensions. These predictions are further shown to improve when the networks are trained with additional bootstrapped training sets from real experimental data. Using a realistic beam-profile displacement error model for Hermite-Gaussian sources, we further demonstrate numerically that the orders-of-magnitude reduction in certification time with trained networks greatly increases the computation yield of a large-scale quantum processor using these sources, before state fidelity deteriorates significantly. ",https://doi.org/10.1088/1367-2630/ac1fcb,2103.01535v4,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Fully Geometric Panoramic Localization,1970,"  We introduce a lightweight and accurate localization method that only utilizes the geometry of 2D-3D lines. Given a pre-captured 3D map, our approach localizes a panorama image, taking advantage of the holistic 360 view. The system mitigates potential privacy breaches or domain discrepancies by avoiding trained or hand-crafted visual descriptors. However, as lines alone can be ambiguous, we express distinctive yet compact spatial contexts from relationships between lines, namely the dominant directions of parallel lines and the intersection between non-parallel lines. The resulting representations are efficient in processing time and memory compared to conventional visual descriptor-based methods. Given the groups of dominant line directions and their intersections, we accelerate the search process to test thousands of pose candidates in less than a millisecond without sacrificing accuracy. We empirically show that the proposed 2D-3D matching can localize panoramas for challenging scenes with similar structures, dramatic domain shifts or illumination changes. Our fully geometric approach does not involve extensive parameter tuning or neural network training, making it a practical algorithm that can be readily deployed in the real world. Project page including the code is available through this link: https://82magnolia.github.io/fgpl/. ",Kein DOI-Link verfügbar,2403.19904v1,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",LDL: Line Distance Functions for Panoramic Localization,1970,"  We introduce LDL, a fast and robust algorithm that localizes a panorama to a 3D map using line segments. LDL focuses on the sparse structural information of lines in the scene, which is robust to illumination changes and can potentially enable efficient computation. While previous line-based localization approaches tend to sacrifice accuracy or computation time, our method effectively observes the holistic distribution of lines within panoramic images and 3D maps. Specifically, LDL matches the distribution of lines with 2D and 3D line distance functions, which are further decomposed along principal directions of lines to increase the expressiveness. The distance functions provide coarse pose estimates by comparing the distributional information, where the poses are further optimized using conventional local feature matching. As our pipeline solely leverages line geometry and local features, it does not require costly additional training of line-specific features or correspondence matching. Nevertheless, our method demonstrates robust performance on challenging scenarios including object layout changes, illumination shifts, and large-scale scenes, while exhibiting fast pose search terminating within a matter of milliseconds. We thus expect our method to serve as a practical solution for line-based localization, and complement the well-established point-based paradigm. The code for LDL is available through the following link: https://github.com/82magnolia/panoramic-localization. ",Kein DOI-Link verfügbar,2308.13989v1,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Masking Adversarial Damage: Finding Adversarial Saliency for Robust and   Sparse Network,1970,"  Adversarial examples provoke weak reliability and potential security issues in deep neural networks. Although adversarial training has been widely studied to improve adversarial robustness, it works in an over-parameterized regime and requires high computations and large memory budgets. To bridge adversarial robustness and model compression, we propose a novel adversarial pruning method, Masking Adversarial Damage (MAD) that employs second-order information of adversarial loss. By using it, we can accurately estimate adversarial saliency for model parameters and determine which parameters can be pruned without weakening adversarial robustness. Furthermore, we reveal that model parameters of initial layer are highly sensitive to the adversarial examples and show that compressed feature representation retains semantic information for the target objects. Through extensive experiments on three public datasets, we demonstrate that MAD effectively prunes adversarially trained networks without loosing adversarial robustness and shows better performance than previous adversarial pruning methods. ",Kein DOI-Link verfügbar,2204.02738v1,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Mitigating Adversarial Vulnerability through Causal Parameter Estimation   by Adversarial Double Machine Learning,1970,"  Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridging a causal perspective into the adversarial vulnerability. Through extensive experiments on various CNN and Transformer architectures, we corroborate that ADML improves adversarial robustness with large margins and relieve the empirical observation. ",Kein DOI-Link verfügbar,2307.07250v2,Yes,potent(2)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Self-Supervised Domain Adaptation for Visual Navigation with Global Map   Consistency,1970,"  We propose a light-weight, self-supervised adaptation for a visual navigation agent to generalize to unseen environment. Given an embodied agent trained in a noiseless environment, our objective is to transfer the agent to a noisy environment where actuation and odometry sensor noise is present. Our method encourages the agent to maximize the consistency between the global maps generated at different time steps in a round-trip trajectory. The proposed task is completely self-supervised, not requiring any supervision from ground-truth pose data or explicit noise model. In addition, optimization of the task objective is extremely light-weight, as training terminates within a few minutes on a commodity GPU. Our experiments show that the proposed task helps the agent to successfully transfer to new, noisy environments. The transferred agent exhibits improved localization and mapping accuracy, further leading to enhanced performance in downstream visual navigation tasks. Moreover, we demonstrate test-time adaptation with our self-supervised task to show its potential applicability in real-world deployment. ",Kein DOI-Link verfügbar,2110.07184v1,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Privacy-Preserving Visual Localization with Event Cameras,1970,"  We present a robust, privacy-preserving visual localization algorithm using event cameras. While event cameras can potentially make robust localization due to high dynamic range and small motion blur, the sensors exhibit large domain gaps making it difficult to directly apply conventional image-based localization algorithms. To mitigate the gap, we propose applying event-to-image conversion prior to localization which leads to stable localization. In the privacy perspective, event cameras capture only a fraction of visual information compared to normal cameras, and thus can naturally hide sensitive visual details. To further enhance the privacy protection in our event-based pipeline, we introduce privacy protection at two levels, namely sensor and network level. Sensor level protection aims at hiding facial details with lightweight filtering while network level protection targets hiding the entire user's view in private scene applications using a novel neural network inference pipeline. Both levels of protection involve light-weight computation and incur only a small performance loss. We thus project our method to serve as a building block for practical location-based services using event cameras. The code and dataset will be made public through the following link: https://github.com/82magnolia/event_localization. ",Kein DOI-Link verfügbar,2212.03177v2,Yes,potent(1)
0000-0002-5989-7655,Junho Kim,"Seoul National University, Seoul National University Hospital",Direct Unlearning Optimization for Robust and Safe Text-to-Image Models,1970,"  Recent advancements in text-to-image (T2I) models have greatly benefited from large-scale datasets, but they also pose significant risks due to the potential generation of unsafe content. To mitigate this issue, researchers have developed unlearning techniques to remove the model's ability to generate potentially harmful content. However, these methods are easily bypassed by adversarial attacks, making them unreliable for ensuring the safety of generated images. In this paper, we propose Direct Unlearning Optimization (DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I models while preserving their performance on unrelated topics. DUO employs a preference optimization approach using curated paired image data, ensuring that the model learns to remove unsafe visual concepts while retaining unrelated features. Furthermore, we introduce an output-preserving regularization term to maintain the model's generative capabilities on safe content. Extensive experiments demonstrate that DUO can robustly defend against various state-of-the-art red teaming methods without significant performance degradation on unrelated topics, as measured by FID and CLIP scores. Our work contributes to the development of safer and more reliable T2I models, paving the way for their responsible deployment in both closed-source and open-source scenarios. ",Kein DOI-Link verfügbar,2407.21035v1,Yes,potent(2)
0000-0001-5760-2545,Jun Yeon Won,"Seoul National University, Seoul National University Hospital",Drift with Devil: Security of Multi-Sensor Fusion based Localization in   High-Level Autonomous Driving under GPS Spoofing (Extended Version),1970,"  For high-level Autonomous Vehicles (AV), localization is highly security and safety critical. One direct threat to it is GPS spoofing, but fortunately, AV systems today predominantly use Multi-Sensor Fusion (MSF) algorithms that are generally believed to have the potential to practically defeat GPS spoofing. However, no prior work has studied whether today's MSF algorithms are indeed sufficiently secure under GPS spoofing, especially in AV settings. In this work, we perform the first study to fill this critical gap. As the first study, we focus on a production-grade MSF with both design and implementation level representativeness, and identify two AV-specific attack goals, off-road and wrong-way attacks.   To systematically understand the security property, we first analyze the upper-bound attack effectiveness, and discover a take-over effect that can fundamentally defeat the MSF design principle. We perform a cause analysis and find that such vulnerability only appears dynamically and non-deterministically. Leveraging this insight, we design FusionRipper, a novel and general attack that opportunistically captures and exploits take-over vulnerabilities. We evaluate it on 6 real-world sensor traces, and find that FusionRipper can achieve at least 97% and 91.3% success rates in all traces for off-road and wrong-way attacks respectively. We also find that it is highly robust to practical factors such as spoofing inaccuracies. To improve the practicality, we further design an offline method that can effectively identify attack parameters with over 80% average success rates for both attack goals, with the cost of at most half a day. We also discuss promising defense directions. ",Kein DOI-Link verfügbar,2006.10318v3,Yes,potent(1)
0000-0002-5869-9649,Jiyeon Kim,"Seoul National University, Seoul National University Hospital",Machine Learning Prediction Models for Solid Electrolytes based on   Lattice Dynamics Properties,1970,"  Recently, machine-learning approaches have accelerated computational materials design and the search for advanced solid electrolytes. However, the predictors are currently limited to static structural parameters, which may not fully account for the dynamic nature of ionic transport. In this study, we meticulously curated features considering dynamic properties and developed machine-learning models to predict the ionic conductivity of solid electrolytes. We compiled 14 phonon-related descriptors from first-principles phonon calculations along with 16 descriptors related to structure and electronic properties. Our logistic regression classifiers exhibit an accuracy of 93 %, while the random forest regression model yields a root mean square error of 1.179 S/cm and $R^2$ of 0.710. Notably, phonon-related features are essential for estimating the ionic conductivity in both models. Furthermore, we applied our prediction model to screen 264 Li-containing materials and identified 11 promising candidates as potential superionic conductors. ",Kein DOI-Link verfügbar,2404.13858v1,Yes,"meticulous(1), potent(1), meticulously(1)"
0000-0002-5869-9649,Jiyeon Kim,"Seoul National University, Seoul National University Hospital",ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot   Retrieval,1970,"  We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \url{https://github.com/soyoung97/ListT5}. ",Kein DOI-Link verfügbar,2402.15838v3,Yes,notable(1)
0000-0002-5869-9649,Jiyeon Kim,"Seoul National University, Seoul National University Hospital","Physical properties of transparent perovskite oxides (Ba,La)SnO3 with   high electrical mobility at room temperature",1970,"  Transparent electronic materials are increasingly in demand for a variety of optoelectronic applications. BaSnO3 is a semiconducting oxide with a large band gap of more than 3.1 eV. Recently, we discovered that La doped BaSnO3 exhibits unusually high electrical mobility of 320 cm^2(Vs)^-1 at room temperature and superior thermal stability at high temperatures [H. J. Kim et al. Appl. Phys. Express. 5, 061102 (2012)]. Following that work, we report various physical properties of (Ba,La)SnO3 single crystals and films including temperature-dependent transport and phonon properties, optical properties and first-principles calculations. We find that almost doping-independent mobility of 200-300 cm^2(Vs)^-1 is realized in the single crystals in a broad doping range from 1.0x10^19 to 4.0x10^20 cm^-3. Moreover, the conductivity of ~10^4 ohm^-1cm^-1 reached at the latter carrier density is comparable to the highest value. We attribute the high mobility to several physical properties of (Ba,La)SnO3: a small effective mass coming from the ideal Sn-O-Sn bonding, small disorder effects due to the doping away from the SnO2 conduction channel, and reduced carrier scattering due to the high dielectric constant. The observation of a reduced mobility of ~70 cm^2(Vs)^-1 in the film is mainly attributed to additional carrier-scatterings which are presumably created by the lattice mismatch between the substrate SrTiO3 and (Ba,La)SnO3. The main optical gap of (Ba,La)SnO3 single crystals remained at about 3.33 eV and the in-gap states only slightly increased, thus maintaining optical transparency in the visible region. Based on these, we suggest that the doped BaSnO3 system holds great potential for realizing all perovskite-based, transparent high-frequency high-power functional devices as well as highly mobile two-dimensional electron gas via interface control of heterostructured films. ",https://doi.org/10.1103/PhysRevB.86.165205,1207.0764v2,Yes,potent(1)
0000-0002-6755-9728,Chan-Young Ock,"Seoul National University, Seoul National University Hospital",Generalizing AI-driven Assessment of Immunohistochemistry across   Immunostains and Cancer Types: A Universal Immunohistochemistry Analyzer,1970,"  Despite advancements in methodologies, immunohistochemistry (IHC) remains the most utilized ancillary test for histopathologic and companion diagnostics in targeted therapies. However, objective IHC assessment poses challenges. Artificial intelligence (AI) has emerged as a potential solution, yet its development requires extensive training for each cancer and IHC type, limiting versatility. We developed a Universal IHC (UIHC) analyzer, an AI model for interpreting IHC images regardless of tumor or IHC types, using training datasets from various cancers stained for PD-L1 and/or HER2. This multi-cohort trained model outperforms conventional single-cohort models in interpreting unseen IHCs (Kappa score 0.578 vs. up to 0.509) and consistently shows superior performance across different positive staining cutoff values. Qualitative analysis reveals that UIHC effectively clusters patches based on expression levels. The UIHC model also quantitatively assesses c-MET expression with MET mutations, representing a significant advancement in AI application in the era of personalized medicine and accumulating novel biomarkers. ",Kein DOI-Link verfügbar,2407.20643v1,Yes,potent(1)
0000-0002-0918-4227,Xirui Hou,"Johns Hopkins University, School of Medicine, Johns Hopkins University",Deep-learning-enabled Brain Hemodynamic Mapping Using Resting-state fMRI,1970,"  Cerebrovascular disease is a leading cause of death globally. Prevention and early intervention are known to be the most effective forms of its management. Non-invasive imaging methods hold great promises for early stratification, but at present lack the sensitivity for personalized prognosis. Resting-state functional magnetic resonance imaging (rs-fMRI), a powerful tool previously used for mapping neural activity, is available in most hospitals. Here we show that rs-fMRI can be used to map cerebral hemodynamic function and delineate impairment. By exploiting time variations in breathing pattern during rs-fMRI, deep learning enables reproducible mapping of cerebrovascular reactivity (CVR) and bolus arrive time (BAT) of the human brain using resting-state CO2 fluctuations as a natural 'contrast media'. The deep-learning network was trained with CVR and BAT maps obtained with a reference method of CO2-inhalation MRI, which included data from young and older healthy subjects and patients with Moyamoya disease and brain tumors. We demonstrate the performance of deep-learning cerebrovascular mapping in the detection of vascular abnormalities, evaluation of revascularization effects, and vascular alterations in normal aging. In addition, cerebrovascular maps obtained with the proposed method exhibited excellent reproducibility in both healthy volunteers and stroke patients. Deep-learning resting-state vascular imaging has the potential to become a useful tool in clinical cerebrovascular imaging. ",https://doi.org/10.1038/s41746-023-00859-y,2204.11669v1,Yes,potent(1)
0000-0002-3428-2953,Kenichi Iwatsuki,The University of Tokyo,Extraction and Evaluation of Formulaic Expressions Used in Scholarly   Papers,1970,"  Formulaic expressions, such as 'in this paper we propose', are helpful for authors of scholarly papers because they convey communicative functions; in the above, it is showing the aim of this paper'. Thus, resources of formulaic expressions, such as a dictionary, that could be looked up easily would be useful. However, forms of formulaic expressions can often vary to a great extent. For example, 'in this paper we propose', 'in this study we propose' and 'in this paper we propose a new method to' are all regarded as formulaic expressions. Such a diversity of spans and forms causes problems in both extraction and evaluation of formulaic expressions. In this paper, we propose a new approach that is robust to variation of spans and forms of formulaic expressions. Our approach regards a sentence as consisting of a formulaic part and non-formulaic part. Then, instead of trying to extract formulaic expressions from a whole corpus, by extracting them from each sentence, different forms can be dealt with at once. Based on this formulation, to avoid the diversity problem, we propose evaluating extraction methods by how much they convey specific communicative functions rather than by comparing extracted expressions to an existing lexicon. We also propose a new extraction method that utilises named entities and dependency structures to remove the non-formulaic part from a sentence. Experimental results show that the proposed extraction method achieved the best performance compared to other existing methods. ",Kein DOI-Link verfügbar,2006.10334v1,Yes,scholarly(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Inducing Schemes with Finite Weighted Complexity,1970,"  In this paper, we consider a Borel measurable map of a compact metric space which admits an inducing scheme. Under the finite weighted complexity condition, we establish a thermodynamic formalism for a parameter family of potentials $\varphi+t\psi$ in an interval containing $t=0$. Furthermore, if there is a generating partition compatible to the inducing scheme, we show that all ergodic invariant measures with sufficiently large pressure are liftable. ",Kein DOI-Link verfügbar,2302.12561v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Markov partition and Thermodynamic Formalism for Hyperbolic Systems with   Singularities,1970,"  For 2-d hyperbolic systems with singularities, statistical properties are rather difficult to establish because of the fragmentation of the phase space by singular curves. In this paper, we construct a Markov partition of the phase space with countable states for a general class of hyperbolic systems with singularities. Stochastic properties with respect to the SRB measure immediately follow from our construction of the Markov partition, including the decay rates of correlations and the central limit theorem. We further establish the thermodynamic formalism for the family of geometric potentials, by using the inducing scheme of hyperbolic type. All the results apply to Sinai dispersing billiards, and their small perturbations due to external forces and nonelastic reflections with kicks and slips. ",Kein DOI-Link verfügbar,1709.00527v2,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot   Manipulation,1970,"  Reinforcement learning shows great potential to solve complex contact-rich robot manipulation tasks. However, the safety of using RL in the real world is a crucial problem, since unexpected dangerous collisions might happen when the RL policy is imperfect during training or in unseen scenarios. In this paper, we propose a contact-safe reinforcement learning framework for contact-rich robot manipulation, which maintains safety in both the task space and joint space. When the RL policy causes unexpected collisions between the robot arm and the environment, our framework is able to immediately detect the collision and ensure the contact force to be small. Furthermore, the end-effector is enforced to perform contact-rich tasks compliantly, while keeping robust to external disturbances. We train the RL policy in simulation and transfer it to the real robot. Real world experiments on robot wiping tasks show that our method is able to keep the contact force small both in task space and joint space even when the policy is under unseen scenario with unexpected collision, while rejecting the disturbances on the main task. ",Kein DOI-Link verfügbar,2207.13438v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Flow-based Recurrent Belief State Learning for POMDPs,1970,"  Partially Observable Markov Decision Process (POMDP) provides a principled and generic framework to model real world sequential decision making processes but yet remains unsolved, especially for high dimensional continuous space and unknown models. The main challenge lies in how to accurately obtain the belief state, which is the probability distribution over the unobservable environment states given historical information. Accurately calculating this belief state is a precondition for obtaining an optimal policy of POMDPs. Recent advances in deep learning techniques show great potential to learn good belief states. However, existing methods can only learn approximated distribution with limited flexibility. In this paper, we introduce the \textbf{F}l\textbf{O}w-based \textbf{R}ecurrent \textbf{BE}lief \textbf{S}tate model (FORBES), which incorporates normalizing flows into the variational inference to learn general continuous belief states for POMDPs. Furthermore, we show that the learned belief states can be plugged into downstream RL algorithms to improve performance. In experiments, we show that our methods successfully capture the complex belief states that enable multi-modal predictions as well as high quality reconstructions, and results on challenging visual-motor control tasks show that our method achieves superior performance and sample efficiency. ",Kein DOI-Link verfügbar,2205.11051v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Asking Before Acting: Gather Information in Embodied Decision Making   with Language Models,1970,"  With strong capabilities of reasoning and a broad understanding of the world, Large Language Models (LLMs) have demonstrated immense potential in building versatile embodied decision-making agents capable of executing a wide array of tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM agents encounter challenges in efficiently gathering essential information, leading to suboptimal performance. Conversely, human individuals often seek additional information from their peers prior to taking action, harnessing external knowledge to avoid unnecessary trial and error. Drawing inspiration from this behavior, we propose \textit{Asking Before Acting} (ABA), a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment. In this way, the agent is able to enhance its efficiency and performance by circumventing potentially laborious steps and combating the difficulties associated with exploration in unfamiliar environments and vagueness of the instructions. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. The experiments involve various models from Vicuna to GPT-4. The results demonstrate that, even with modest prompts modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve. ",Kein DOI-Link verfügbar,2305.15695v2,Yes,"versatile(1), potent(2)"
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Performance-Driven Controller Tuning via Derivative-Free Reinforcement   Learning,1970,"  Choosing an appropriate parameter set for the designed controller is critical for the final performance but usually requires a tedious and careful tuning process, which implies a strong need for automatic tuning methods. However, among existing methods, derivative-free ones suffer from poor scalability or low efficiency, while gradient-based ones are often unavailable due to possibly non-differentiable controller structure. To resolve the issues, we tackle the controller tuning problem using a novel derivative-free reinforcement learning (RL) framework, which performs timestep-wise perturbation in parameter space during experience collection and integrates derivative-free policy updates into the advanced actor-critic RL architecture to achieve high versatility and efficiency. To demonstrate the framework's efficacy, we conduct numerical experiments on two concrete examples from autonomous driving, namely, adaptive cruise control with PID controller and trajectory tracking with MPC controller. Experimental results show that the proposed method outperforms popular baselines and highlight its strong potential for controller tuning. ",Kein DOI-Link verfügbar,2209.04854v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",From Artificially Real to Real: Leveraging Pseudo Data from Large   Language Models for Low-Resource Molecule Discovery,1970,"  Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery. Our code and data are available at https://github.com/SCIR-HI/ArtificiallyR2R. ",Kein DOI-Link verfügbar,2309.05203v3,Yes,"innovative(1), potent(1)"
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Mixed Policy Gradient: off-policy reinforcement learning driven jointly   by data and model,1970,"  Reinforcement learning (RL) shows great potential in sequential decision-making. At present, mainstream RL algorithms are data-driven, which usually yield better asymptotic performance but much slower convergence compared with model-driven methods. This paper proposes mixed policy gradient (MPG) algorithm, which fuses the empirical data and the transition model in policy gradient (PG) to accelerate convergence without performance degradation. Formally, MPG is constructed as a weighted average of the data-driven and model-driven PGs, where the former is the derivative of the learned Q-value function, and the latter is that of the model-predictive return. To guide the weight design, we analyze and compare the upper bound of each PG error. Relying on that, a rule-based method is employed to heuristically adjust the weights. In particular, to get a better PG, the weight of the data-driven PG is designed to grow along the learning process while the other to decrease. Simulation results show that the MPG method achieves the best asymptotic performance and convergence speed compared with other baseline algorithms. ",Kein DOI-Link verfügbar,2102.11513v2,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Constrained Iterative LQG for Real-Time Chance-Constrained Gaussian   Belief Space Planning,1970,"  Motion planning under uncertainty is of significant importance for safety-critical systems such as autonomous vehicles. Such systems have to satisfy necessary constraints (e.g., collision avoidance) with potential uncertainties coming from either disturbed system dynamics or noisy sensor measurements. However, existing motion planning methods cannot efficiently find the robust optimal solutions under general nonlinear and non-convex settings. In this paper, we formulate such problem as chance-constrained Gaussian belief space planning and propose the constrained iterative Linear Quadratic Gaussian (CILQG) algorithm as a real-time solution. In this algorithm, we iteratively calculate a Gaussian approximation of the belief and transform the chance-constraints. We evaluate the effectiveness of our method in simulations of autonomous driving planning tasks with static and dynamic obstacles. Results show that CILQG can handle uncertainties more appropriately and has faster computation time than baseline methods. ",Kein DOI-Link verfügbar,2108.06533v2,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Scale-Equivalent Distillation for Semi-Supervised Object Detection,1970,"  Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on self-training, i.e., generating hard pseudo-labels by a teacher model on unlabeled data as supervisory signals. Although they achieved certain success, the limited labeled data in semi-supervised learning scales up the challenges of object detection. We analyze the challenges these methods meet with the empirical experiment results. We find that the massive False Negative samples and inferior localization precision lack consideration. Besides, the large variance of object sizes and class imbalance (i.e., the extreme ratio between background and object) hinder the performance of prior arts. Further, we overcome these challenges by introducing a novel approach, Scale-Equivalent Distillation (SED), which is a simple yet effective end-to-end knowledge distillation framework robust to large object size variance and class imbalance. SED has several appealing benefits compared to the previous works. (1) SED imposes a consistency regularization to handle the large scale variance problem. (2) SED alleviates the noise problem from the False Negative samples and inferior localization precision. (3) A re-weighting strategy can implicitly screen the potential foreground regions of the unlabeled data to reduce the effect of class imbalance. Extensive experiments show that SED consistently outperforms the recent state-of-the-art methods on different datasets with significant margins. For example, it surpasses the supervised counterpart by more than 10 mAP when using 5% and 10% labeled data on MS-COCO. ",Kein DOI-Link verfügbar,2203.12244v2,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Learn Zero-Constraint-Violation Policy in Model-Free Constrained   Reinforcement Learning,1970,"  In the trial-and-error mechanism of reinforcement learning (RL), a notorious contradiction arises when we expect to learn a safe policy: how to learn a safe policy without enough data and prior model about the dangerous region? Existing methods mostly use the posterior penalty for dangerous actions, which means that the agent is not penalized until experiencing danger. This fact causes that the agent cannot learn a zero-violation policy even after convergence. Otherwise, it would not receive any penalty and lose the knowledge about danger. In this paper, we propose the safe set actor-critic (SSAC) algorithm, which confines the policy update using safety-oriented energy functions, or the safety indexes. The safety index is designed to increase rapidly for potentially dangerous actions, which allows us to locate the safe set on the action space, or the control safe set. Therefore, we can identify the dangerous actions prior to taking them, and further obtain a zero constraint-violation policy after convergence.We claim that we can learn the energy function in a model-free manner similar to learning a value function. By using the energy function transition as the constraint objective, we formulate a constrained RL problem. We prove that our Lagrangian-based solutions make sure that the learned policy will converge to the constrained optimum under some assumptions. The proposed algorithm is evaluated on both the complex simulation environments and a hardware-in-loop (HIL) experiment with a real controller from the autonomous vehicle. Experimental results suggest that the converged policy in all environments achieves zero constraint violation and comparable performance with model-based baselines. ",Kein DOI-Link verfügbar,2111.12953v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Steadily Learn to Drive with Virtual Memory,1970,"  Reinforcement learning has shown great potential in developing high-level autonomous driving. However, for high-dimensional tasks, current RL methods suffer from low data efficiency and oscillation in the training process. This paper proposes an algorithm called Learn to drive with Virtual Memory (LVM) to overcome these problems. LVM compresses the high-dimensional information into compact latent states and learns a latent dynamic model to summarize the agent's experience. Various imagined latent trajectories are generated as virtual memory by the latent dynamic model. The policy is learned by propagating gradient through the learned latent model with the imagined latent trajectories and thus leads to high data efficiency. Furthermore, a double critic structure is designed to reduce the oscillation during the training process. The effectiveness of LVM is demonstrated by an image-input autonomous driving task, in which LVM outperforms the existing method in terms of data efficiency, learning stability, and control performance. ",Kein DOI-Link verfügbar,2102.08072v1,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",Model-based Constrained Reinforcement Learning using Generalized Control   Barrier Function,1970,"  Model information can be used to predict future trajectories, so it has huge potential to avoid dangerous region when implementing reinforcement learning (RL) on real-world tasks, like autonomous driving. However, existing studies mostly use model-free constrained RL, which causes inevitable constraint violations. This paper proposes a model-based feasibility enhancement technique of constrained RL, which enhances the feasibility of policy using generalized control barrier function (GCBF) defined on the distance to constraint boundary. By using the model information, the policy can be optimized safely without violating actual safety constraints, and the sample efficiency is increased. The major difficulty of infeasibility in solving the constrained policy gradient is handled by an adaptive coefficient mechanism. We evaluate the proposed method in both simulations and real vehicle experiments in a complex autonomous driving collision avoidance task. The proposed method achieves up to four times fewer constraint violations and converges 3.36 times faster than baseline constrained RL approaches. ",Kein DOI-Link verfügbar,2103.01556v2,Yes,potent(1)
0000-0002-0908-0022,Jianyu Chen,"The University of Tokyo, the University of Tokyo",RaffeSDG: Random Frequency Filtering enabled Single-source Domain   Generalization for Medical Image Segmentation,1970,"  Deep learning models often encounter challenges in making accurate inferences when there are domain shifts between the source and target data. This issue is particularly pronounced in clinical settings due to the scarcity of annotated data resulting from the professional and private nature of medical data. Despite the existence of decent solutions, many of them are hindered in clinical settings due to limitations in data collection and computational complexity. To tackle domain shifts in data-scarce medical scenarios, we propose a Random frequency filtering enabled Single-source Domain Generalization algorithm (RaffeSDG), which promises robust out-of-domain inference with segmentation models trained on a single-source domain. A filter-based data augmentation strategy is first proposed to promote domain variability within a single-source domain by introducing variations in frequency space and blending homologous samples. Then Gaussian filter-based structural saliency is also leveraged to learn robust representations across augmented samples, further facilitating the training of generalizable segmentation models. To validate the effectiveness of RaffeSDG, we conducted extensive experiments involving out-of-domain inference on segmentation tasks for three human tissues imaged by four diverse modalities. Through thorough investigations and comparisons, compelling evidence was observed in these experiments, demonstrating the potential and generalizability of RaffeSDG. The code is available at https://github.com/liamheng/Non-IID_Medical_Image_Segmentation. ",Kein DOI-Link verfügbar,2405.01228v2,Yes,potent(1)
0000-0003-4515-2115,Lento Nagano,"ICEPP, the University of Tokyo, The University of Tokyo",Classically emulated digital quantum simulation for screening and   confinement in the Schwinger model with a topological term,1970,"  We perform digital quantum simulation, using a classical simulator, to study screening and confinement in a gauge theory with a topological term, focusing on ($1+1$)-dimensional quantum electrodynamics (Schwinger model) with a theta term. We compute the ground state energy in the presence of probe charges to estimate the potential between them, via adiabatic state preparation. We compare our simulation results and analytical predictions for a finite volume, finding good agreements. In particular our result in the massive case shows a linear behavior for non-integer charges and a non-linear behavior for integer charges, consistently with the expected confinement (screening) behavior for non-integer (integer) charges. ",https://doi.org/10.1103/PhysRevD.105.014504,2105.03276v2,Yes,potent(1)
0000-0003-4515-2115,Lento Nagano,"ICEPP, the University of Tokyo, The University of Tokyo",Quantum convolutional neural networks for jet images classification,1970,"  Recently, interest in quantum computing has significantly increased, driven by its potential advantages over classical techniques. Quantum machine learning (QML) exemplifies one of the important quantum computing applications that are expected to surpass classical machine learning in a wide range of instances. This paper addresses the performance of QML in the context of high-energy physics (HEP). As an example, we focus on the top-quark tagging, for which classical convolutional neural networks (CNNs) have been effective but fall short in accuracy when dealing with highly energetic jet images. In this paper, we use a quantum convolutional neural network (QCNN) for this task and compare its performance with CNN using a classical noiseless simulator. We compare various setups for the QCNN, varying the convolutional circuit, type of encoding, loss function, and batch sizes. For every quantum setup, we design a similar setup to the corresponding classical model for a fair comparison. Our results indicate that QCNN with proper setups tend to perform better than their CNN counterparts, particularly when the convolution block has a lower number of parameters. This suggests that quantum models, especially with appropriate encodings, can hold potential promise for enhancing performance in HEP tasks such as top quark jet tagging. ",Kein DOI-Link verfügbar,2408.08701v1,Yes,potent(2)
0000-0003-4515-2115,Lento Nagano,"ICEPP, the University of Tokyo, The University of Tokyo",Quantum data learning for quantum simulations in high-energy physics,1970,"  Quantum machine learning with parametrised quantum circuits has attracted significant attention over the past years as an early application for the era of noisy quantum processors. However, the possibility of achieving concrete advantages over classical counterparts in practical learning tasks is yet to be demonstrated. A promising avenue to explore potential advantages is the learning of data generated by quantum mechanical systems and presented in an inherently quantum mechanical form. In this article, we explore the applicability of quantum-data learning to practical problems in high-energy physics, aiming to identify domain specific use-cases where quantum models can be employed. We consider quantum states governed by one-dimensional lattice gauge theories and a phenomenological quantum field theory in particle physics, generated by digital quantum simulations or variational methods to approximate target states. We make use of an ansatz based on quantum convolutional neural networks and numerically show that it is capable of recognizing quantum phases of ground states in the Schwinger model, (de)confinement phases from time-evolved states in the $\mathbb{Z}_2$ gauge theory, and that it can extract fermion flavor/coupling constants in a quantum simulation of parton shower. The observation of non-trivial learning properties demonstrated in these benchmarks will motivate further exploration of the quantum-data learning architecture in high-energy physics. ",Kein DOI-Link verfügbar,2306.17214v1,Yes,potent(1)
0000-0003-4515-2115,Lento Nagano,"ICEPP, the University of Tokyo, The University of Tokyo",Quantum Computing for High-Energy Physics: State of the Art and   Challenges. Summary of the QC4HEP Working Group,1970,"  Quantum computers offer an intriguing path for a paradigmatic change of computing in the natural sciences and beyond, with the potential for achieving a so-called quantum advantage, namely a significant (in some cases exponential) speed-up of numerical simulations. The rapid development of hardware devices with various realizations of qubits enables the execution of small scale but representative applications on quantum computers. In particular, the high-energy physics community plays a pivotal role in accessing the power of quantum computing, since the field is a driving source for challenging computational problems. This concerns, on the theoretical side, the exploration of models which are very hard or even impossible to address with classical techniques and, on the experimental side, the enormous data challenge of newly emerging experiments, such as the upgrade of the Large Hadron Collider. In this roadmap paper, led by CERN, DESY and IBM, we provide the status of high-energy physics quantum computations and give examples for theoretical and experimental target benchmark applications, which can be addressed in the near future. Having the IBM 100 x 100 challenge in mind, where possible, we also provide resource estimates for the examples given using error mitigated quantum computing. ",Kein DOI-Link verfügbar,2307.03236v1,Yes,"pivotal(1), potent(1)"
0000-0003-3703-4995,Kang Gao,The University of Tokyo,Beamforming with Multiple One-Bit Wireless Transceivers,1970,"  Classical beamforming techniques rely on highly linear transmitters and receivers to allow phase-coherent combining at the transmitter and receiver. The transmitter uses beamforming to steer signal power towards the receiver, and the receiver uses beamforming to gather and coherently combine the signals from multiple receiver antennas. When the transmitters and receivers are instead constrained for power and cost reasons to be non-linear one-bit devices, the potential advantages and performance metrics associated with beamforming are not as well understood. We define beamforming at the transmitter as a codebook design problem to maximize the minimum distance between codewords. We define beamforming at the receiver as the maximum likelihood detector of the transmitted codeword. We show that beamforming with one-bit transceivers is a constellation design problem, and that we can come within a few dB SNR of the capacity attained by linear transceivers. ",Kein DOI-Link verfügbar,1802.04923v1,Yes,potent(1)
0000-0003-3703-4995,Kang Gao,The University of Tokyo,High-frequency financial market simulation and flash crash scenarios   analysis: an agent-based modelling approach,1970,"  This paper describes simulations and analysis of flash crash scenarios in an agent-based modelling framework. We design, implement, and assess a novel high-frequency agent-based financial market simulator that generates realistic millisecond-level financial price time series for the E-Mini S&P 500 futures market. Specifically, a microstructure model of a single security traded on a central limit order book is provided, where different types of traders follow different behavioural rules. The model is calibrated using the machine learning surrogate modelling approach. Statistical test and moment coverage ratio results show that the model has excellent capability of reproducing realistic stylised facts in financial markets. By introducing an institutional trader that mimics the real-world Sell Algorithm on May 6th, 2010, the proposed high-frequency agent-based financial market simulator is used to simulate the Flash Crash that took place that day. We scrutinise the market dynamics during the simulated flash crash and show that the simulated dynamics are consistent with what happened in historical flash crash scenarios. With the help of Monte Carlo simulations, we discover functional relationships between the amplitude of the simulated 2010 Flash Crash and three conditions: the percentage of volume of the Sell Algorithm, the market maker inventory limit, and the trading frequency of fundamental traders. Similar analyses are carried out for mini flash crash events. An innovative ""Spiking Trader"" is introduced to the model, aiming at precipitating mini flash crash events. We analyse the market dynamics during the course of a typical simulated mini flash crash event and study the conditions affecting its characteristics. The proposed model can be used for testing resiliency and robustness of trading algorithms and providing advice for policymakers. ",https://doi.org/10.18564/jasss.5403,2208.13654v1,Yes,innovative(1)
0000-0003-3703-4995,Kang Gao,The University of Tokyo,Deeper Hedging: A New Agent-based Model for Effective Deep Hedging,1970,"  We propose the Chiarella-Heston model, a new agent-based model for improving the effectiveness of deep hedging strategies. This model includes momentum traders, fundamental traders, and volatility traders. The volatility traders participate in the market by innovatively following a Heston-style volatility signal. The proposed model generalises both the extended Chiarella model and the Heston stochastic volatility model, and is calibrated to reproduce as many empirical stylized facts as possible. According to the stylised facts distance metric, the proposed model is able to reproduce more realistic financial time series than three baseline models: the extended Chiarella model, the Heston model, and the Geometric Brownian Motion. The proposed model is further validated by the Generalized Subtracted L-divergence metric. With the proposed Chiarella-Heston model, we generate a training dataset to train a deep hedging agent for optimal hedging strategies under various transaction cost levels. The deep hedging agent employs the Deep Deterministic Policy Gradient algorithm and is trained to maximize profits and minimize risks. Our testing results reveal that the deep hedging agent, trained with data generated by our proposed model, outperforms the baseline in most transaction cost levels. Furthermore, the testing process, which is conducted using empirical data, demonstrates the effective performance of the trained deep hedging agent in a realistic trading environment. ",https://doi.org/10.1145/3604237.3626913,2310.18755v1,Yes,"innovative(1), innovatively(1)"
0000-0001-7853-9370,Kensaku Mori,The University of Tokyo,Employing Weak Annotations for Medical Image Analysis Problems,1970,"  To efficiently establish training databases for machine learning methods, collaborative and crowdsourcing platforms have been investigated to collectively tackle the annotation effort. However, when this concept is ported to the medical imaging domain, reading expertise will have a direct impact on the annotation accuracy. In this study, we examine the impact of expertise and the amount of available annotations on the accuracy outcome of a liver segmentation problem in an abdominal computed tomography (CT) image database. In controlled experiments, we study this impact for different types of weak annotations. To address the decrease in accuracy associated with lower expertise, we propose a method for outlier correction making use of a weakly labelled atlas. Using this approach, we demonstrate that weak annotations subject to high error rates can achieve a similarly high accuracy as state-of-the-art multi-atlas segmentation approaches relying on a large amount of expert manual segmentations. Annotations of this nature can realistically be obtained from a non-expert crowd and can potentially enable crowdsourcing of weak annotation tasks for medical image analysis. ",Kein DOI-Link verfügbar,1708.06297v1,Yes,potent(1)
0000-0001-7853-9370,Kensaku Mori,The University of Tokyo,Unsupervised Segmentation of 3D Medical Images Based on Clustering and   Deep Representation Learning,1970,"  This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation. ",https://doi.org/10.1117/12.2293414,1804.03830v1,Yes,potent(1)
0000-0001-7853-9370,Kensaku Mori,The University of Tokyo,Identifying Suspicious Regions of Covid-19 by Abnormality-Sensitive   Activation Mapping,1970,"  This paper presents a fully-automated method for the identification of suspicious regions of a coronavirus disease (COVID-19) on chest CT volumes. One major role of chest CT scanning in COVID-19 diagnoses is identification of an inflammation particular to the disease. This task is generally performed by radiologists through an interpretation of the CT volumes, however, because of the heavy workload, an automatic analysis method using a computer is desired. Most computer-aided diagnosis studies have addressed only a portion of the elements necessary for the identification. In this work, we realize the identification method through a classification task by using a 2.5-dimensional CNN with three-dimensional attention mechanisms. We visualize the suspicious regions by applying a backpropagation based on positive gradients to attention-weighted features. We perform experiments on an in-house dataset and two public datasets to reveal the generalization ability of the proposed method. The proposed architecture achieved AUCs of over 0.900 for all the datasets, and mean sensitivity $0.853 \pm 0.036$ and specificity $0.870 \pm 0.040$. The method can also identify notable lesions pointed out in the radiology report as suspicious regions. ",Kein DOI-Link verfügbar,2303.14901v1,Yes,notable(1)
0000-0001-7853-9370,Kensaku Mori,The University of Tokyo,"Precise Estimation of Renal Vascular Dominant Regions Using Spatially   Aware Fully Convolutional Networks, Tensor-Cut and Voronoi Diagrams",1970,"  This paper presents a new approach for precisely estimating the renal vascular dominant region using a Voronoi diagram. To provide computer-assisted diagnostics for the pre-surgical simulation of partial nephrectomy surgery, we must obtain information on the renal arteries and the renal vascular dominant regions. We propose a fully automatic segmentation method that combines a neural network and tensor-based graph-cut methods to precisely extract the kidney and renal arteries. First, we use a convolutional neural network to localize the kidney regions and extract tiny renal arteries with a tensor-based graph-cut method. Then we generate a Voronoi diagram to estimate the renal vascular dominant regions based on the segmented kidney and renal arteries. The accuracy of kidney segmentation in 27 cases with 8-fold cross validation reached a Dice score of 95%. The accuracy of renal artery segmentation in 8 cases obtained a centerline overlap ratio of 80%. Each partition region corresponds to a renal vascular dominant region. The final dominant-region estimation accuracy achieved a Dice coefficient of 80%. A clinical application showed the potential of our proposed estimation approach in a real clinical surgical environment. Further validation using large-scale database is our future work. ",https://doi.org/10.1016/j.compmedimag.2019.101642,1908.01543v1,Yes,potent(1)
0000-0002-6060-758X,Hiraku Nakajima,The University of Tokyo,Cluster algebras and singular supports of perverse sheaves,1970,"  We propose an approach to Geiss-Leclerc-Schroer's conjecture on the cluster algebra structure on the coordinate ring of a unipotent subgroup and the dual canonical base. It is based on singular supports of perverse sheaves on the space of representations of a quiver, which give the canonical base. ",Kein DOI-Link verfügbar,1301.5079v2,Yes,potent(1)
0000-0002-6060-758X,Hiraku Nakajima,The University of Tokyo,Affine cellularity of quantum affine algebras,1970,"  Cui (arXiv:1405.6441) has shown that the modified quantum affine algebra $\widetilde{\mathbf U}$ (more precisely its quotients, BLN algebras) is affine cellular in the sense of Koenig and Xi. The proof is based on the structure of cells of $\widetilde{\mathbf U}$, studied previously in [http://arxiv.org/abs/math/0212253], the author's joint work with Beck. We here give more direct proof based on Lemma 6.17 in [http://arxiv.org/abs/math/0212253], together with a property of the bilinear form introduced in [http://arxiv.org/abs/math/0204183]. We also prove that cell ideals are idempotent, and hence Theorem 4.4 in [Koenig-Xi] is applicable. ",Kein DOI-Link verfügbar,1406.1298v3,Yes,potent(1)
0000-0002-6060-758X,Hiraku Nakajima,The University of Tokyo,Instanton counting on blowup. I. 4-dimensional pure gauge theory,1970,"  We give a mathematically rigorous proof of Nekrasov's conjecture: the integration in the equivariant cohomology over the moduli spaces of instantons on $\mathbb R^4$ gives a deformation of the Seiberg-Witten prepotential for N=2 SUSY Yang-Mills theory. Through a study of moduli spaces on the blowup of $\mathbb R^4$, we derive a differential equation for the Nekrasov's partition function. It is a deformation of the equation for the Seiberg-Witten prepotential, found by Losev et al., and further studied by Gorsky et al. ",https://doi.org/10.1007/s00222-005-0444-1,math/0306198v2,Yes,potent(2)
0000-0002-6060-758X,Hiraku Nakajima,The University of Tokyo,Lectures on Instanton Counting,1970,"  These notes have two parts. The first is a study of Nekrasov's deformed partition functions $Z(\ve_1,\ve_2,\vec{a};\q,\vec{\tau})$ of N=2 SUSY Yang-Mills theories, which are generating functions of the integration in the equivariant cohomology over the moduli spaces of instantons on $\mathbb R^4$. The second is review of geometry of the Seiberg-Witten curves and the geometric engineering of the gauge theory, which are physical backgrounds of Nekrasov's partition functions.   The first part is continuation of math.AG/0306198, where we identified the Seiberg-Witten prepotential with $Z(0,0,\vec{a};\q,0)$.   We put higher Casimir operators to the partition function and clarify their relation to the Seiberg-Witten $u$-plane. We also determine the coefficients of $\ve_1\ve_2$ and $(\ve_1^2+\ve_2^2)/3$ (the genus 1 part) of the partition function, which coincide with two measure factors $A$, $B$ appeared in the $u$-plane integral.   The proof is based on the blowup equation which we derived in the previous paper. ",Kein DOI-Link verfügbar,math/0311058v1,Yes,potent(1)
0000-0002-9787-3857,Junichiro Mori,The University of Tokyo,Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias   Mitigation,1970,"  Large language models (LLMs) often inherit biases from vast amounts of training corpora. Traditional debiasing methods, while effective to some extent, do not completely eliminate memorized biases and toxicity in LLMs. In this paper, we study an unlearning-based approach to debiasing in LLMs by performing gradient ascent on hate speech against minority groups, i.e., minimizing the likelihood of biased or toxic content. Specifically, we propose a mask language modeling unlearning technique, which unlearns the harmful part of the text. This method enables LLMs to selectively forget and disassociate from biased and harmful content. Experimental results demonstrate the effectiveness of our approach in diminishing bias while maintaining the language modeling abilities. Surprisingly, the results also unveil an unexpected potential for cross-domain transfer unlearning: debiasing in one bias form (e.g. gender) may contribute to mitigating others (e.g. race and religion). ",Kein DOI-Link verfügbar,2407.16951v1,Yes,potent(1)
0000-0002-4235-8989,Kosuke Yoshioka,The University of Tokyo,Observation of ultra-high mobility excitons in a strain field by space-   and time-resolved spectroscopy at sub-Kelvin temperatures,1970,"  We measured basic parameters such as the lifetime, mobility, and diffusion constant of trapped paraexcitons in cuprous oxide at very low temperatures (below 1 K) using a dilution refrigerator. To obtain these parameters, we observed the space- and time-resolved luminescence spectrum of paraexcitons in strain-induced trap potential. We extracted the lifetime of 410 ns from the measurements of the decay of the luminescence intensity. By comparing the experimental results and numerical calculations, we found that the mobility and the diffusion constant increase as the temperature of the paraexcitons decreases below 1 K. In particular, we obtained a mobility of 5.1e7 cm^2/V*s at the corresponding temperature of 280 mK. To the best of our knowledge, this value is the highest exciton mobility that has been measured. These results show that the mean free path of the paraexcitons reaches a size (~300 um) comparable to that of the cloud of trapped paraexcitons (~100 um). From our analyses, we found that the spatial distribution of the paraexcitons can reach a distribution that is defined by the statistical distribution function and the shape of the three-dimensional trap potential at ultra-low temperatures (well below 1 K). Our survey shows that the ultra-low temperature ensures that the Bose--Einstein condensation transition in a trap potential can be investigated by examining the spatial distribution of the density of 1s paraexcitons. ",https://doi.org/10.1103/PhysRevB.100.035206,1904.00418v1,Yes,potent(3)
0000-0002-4235-8989,Kosuke Yoshioka,The University of Tokyo,The OVAL experiment: A new experiment to measure vacuum magnetic   birefringence using high repetition pulsed magnets,1970,"  A new experiment to measure vacuum magnetic birefringence (VMB), the OVAL experiment, is reported. We developed an original pulsed magnet that has a high repetition rate and applies the strongest magnetic field among VMB experiments. The vibration isolation design and feedback system enable the direct combination of the magnet with a Fabry-P\'erot cavity. To ensure the searching potential, a calibration measurement with dilute nitrogen gas and a prototype search for vacuum magnetic birefringence are performed. Based on the results, a strategy to observe vacuum magnetic birefringence is reported. ",https://doi.org/10.1140/epjd/e2017-80290-7,1705.00495v2,Yes,potent(1)
0000-0002-8399-3831,Takeshi Tsuchiya,The University of Tokyo,State Drift and Gait Plan in Feedback Linearization Control of A Tilt   Vehicle,1970,"  To stabilize a conventional quadrotor, simplified equivalent vehicles, e.g., autonomous car, are developed to test the designed controller. Based on that, various controllers based on feedback linearization have been developed. With the recently developed concept of tilt-rotor, there lacks the simplified or equivalent model, however. Indeed, the tilt structure is relatively unusual in vehicles. In this research, we put forward a unique fictional vehicle with tilt structure, which is to help evaluate the property of the tilt-structure-aimed controllers. One phenomenon, state drift, in controlling an over-actuated tilt structure by feedback linearization is presented subsequently. State drift can be easily neglected and is not paid attention to in the current researches in tilt-rotor controller design so far. We report this phenomenon and provide a potential approach to avoid this behavior. ",https://doi.org/10.5121/csit.2022.120501,2111.04307v2,Yes,potent(1)
0000-0002-8528-9418,Yuta Suzuki,The University of Tokyo,Antiparallel spin polarization and spin current induced by thermal   current and locally-broken inversion symmetry in a double-quantum-well   structure,1970,  Generating a nonequilibrium spin polarization with a driving force has been first realized by the electric current in a system with broken inversion symmetry and extended to that induced by the thermal current and that appearing in an inversion-symmetric system with locally-broken inversion symmetry. This paper theoretically explores the spin polarization generated by the thermal current and the locally-broken inversion symmetry in a symmetric double-quantum-well structure (DQWS). This thermally-induced spin polarization (TISP) appears in the antiparallel configuration with the TISP of two wells in opposite directions. The calculation using the Boltzmann equation in the relaxation-time approximation under the condition of zero charge current shows that the local TISP exhibits the maximum at a finite Rashba spin-orbit interaction when the electron density is fixed. This is because the local TISP in the DQWS is enhanced at the chemical potential near the bottom of the first-excited subband. This enhancement also occurs in a single quantum well with globally-broken inversion symmetry. Another finding is that the maximum of the local TISP appears at a nonzero interwell coupling. The spin current by the diffusion of the local TISP into an adjacent electrode is also calculated. ,Kein DOI-Link verfügbar,2312.07944v1,Yes,potent(1)
0000-0002-8528-9418,Yuta Suzuki,The University of Tokyo,Crystalformer: Infinitely Connected Attention for Periodic Structure   Encoding,1970,"  Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, with minimal modifications to the original Transformer architecture. Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets. ",Kein DOI-Link verfügbar,2403.11686v1,Yes,potent(2)
0000-0003-3383-2279,Jiro Shimoda,The University of Tokyo,The Dzhanibekov Effect as a Possible Source of Magnetar Activity,1970,"  Magnetars, which are neutron stars with strong magnetic fields, exhibit occasional bursting activities. The shape of a magnetar is not perfectly spherical due to the Lorentz force exerted by its strong magnetic fields and is described as a triaxial body. We study the unstable free precession in a triaxial magnetar; one of the principal axes undergoes an upside-down flip. This flip is known as the Dzhanibekov effect. We find that during the flip, the Euler force can suddenly disturb the force balance on the surface layer of the magnetar, potentially leading to plastic flow of the layer. This, in turn, may trigger different forms of magnetar activity, such as the emission of the bursts and/or of gravitational waves. ",Kein DOI-Link verfügbar,2401.16783v2,Yes,potent(1)
0000-0002-1994-1068,Kazuyuki Kuroyama,The University of Tokyo,Full Counting Statistics of Spin-Flip/Conserving Charge Transitions in   Pauli-Spin Blockade,1970,"  We investigate the full counting statistics (FCS) of spin-conserving and spin-flip charge transitions in Pauli-spin blockade regime of a GaAs double quantum dot. A theoretical model is proposed to evaluate all spin-conserving and spin-flip tunnel rates, and to demonstrate the fundamental relation between FCS and waiting time distribution. We observe the remarkable features of parity effect and a tail structure in the constructed FCS, which do not appear in the Poisson distribution, and are originated from spin degeneracy and coexistence of slow and fast transitions, respectively. This study is potentially useful for elucidating the spin-related and other complex transition dynamics in quantum systems. ",https://doi.org/10.1103/PhysRevResearch.2.033120,1909.12027v1,Yes,potent(1)
0000-0002-6406-7074,Sean Lim,"Columbia University, Columbia University Irving Medical Center",Direct Hydrogen Production from Water/Seawater by   Irradiation/Vibration-Activated Using Defective Ferroelectric BaTiO3-x   Nanoparticles,1970,"  Hydrogen is a promising fossil-fuel alternative fuel owing to its environmentally neutral emissions and high energy density. However, the need for purified water and external power are critical hindrances to implementation of hydrogen production. The present work reveals the potential to overcome these shortcomings through piezo-photocatalysis of seawater using BaTiO3-x (BTO) nanoparticles. This material was made piezoelectrically active by annealing under different atmospheres, including O2, N2, Ar, and H2, the latter of which caused Ti4+ to Ti(4-x)+ multiple reductions and structural expansions that stabilized piezoelectric tetragonal BTO domains. The resultant defect equilibria combine ionic and electron effects, including Ti redox reactions, charge-compensating surface oxygen vacancy formation, and color centre alterations. Further, variety of experimental techniques revealed the effects of reduction on the energy band structure. A strong piezoelectric effect and the presence of self-polarization were confirmed by piezoresponse force microscopy, while simulation work clarified the role of vibration on band bending deriving from the former. The performance data contrasted H2 evolution using deionized (DI) water, simulated seawater, and natural seawater subjected to photocatalysis, piezocatalysis, and piezo-photocatalysis. An efficient H2 evolution rate of 132.4 micromol/g/h was achieved from DI water using piezo-photocatalysis for 5 h. In contrast, piezocatalysis for 2 h followed by piezo-photocatalysis for 3 h resulted in H2 evolution rates of 100.7 micromol/g/h for DI water, 63.4 micromol/g/h for simulated seawater, and 48.7 micromol/g/h for natural seawater. This work provides potential new strategies for large-scale green H2 production using abundant natural resources with conventional piezoelectric material while leveraging the effects of ions dissolved in seawater. ",Kein DOI-Link verfügbar,2206.14961v1,Yes,potent(2)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",Tropical and Extratropical Cyclone Detection Using Deep Learning,1970,"  Extracting valuable information from large sets of diverse meteorological data is a time-intensive process. Machine learning methods can help improve both speed and accuracy of this process. Specifically, deep learning image segmentation models using the U-Net structure perform faster and can identify areas missed by more restrictive approaches, such as expert hand-labeling and a priori heuristic methods. This paper discusses four different state-of-the-art U-Net models designed for detection of tropical and extratropical cyclone Regions Of Interest (ROI) from two separate input sources: total precipitable water output from the Global Forecasting System (GFS) model and water vapor radiance images from the Geostationary Operational Environmental Satellite (GOES). These models are referred to as IBTrACS-GFS, Heuristic-GFS, IBTrACS-GOES, and Heuristic-GOES. All four U-Nets are fast information extraction tools and perform with a ROI detection accuracy ranging from 80% to 99%. These are additionally evaluated with the Dice and Tversky Intersection over Union (IoU) metrics, having Dice coefficient scores ranging from 0.51 to 0.76 and Tversky coefficients ranging from 0.56 to 0.74. The extratropical cyclone U-Net model performed 3 times faster than the comparable heuristic model used to detect the same ROI. The U-Nets were specifically selected for their capabilities in detecting cyclone ROI beyond the scope of the training labels. These machine learning models identified more ambiguous and active ROI missed by the heuristic model and hand-labeling methods commonly used in generating real-time weather alerts, having a potentially direct impact on public safety. ",https://doi.org/10.1175/JAMC-D-20-0117.1,2005.09056v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",Comparing simulations and test data of a radiation damaged charge-couple   device for the Euclid mission,1970,"  The VIS instrument on board the Euclid mission is a weak-lensing experiment that depends on very precise shape measurements of distant galaxies obtained by a large CCD array. Due to the harsh radiative environment outside the Earth's atmosphere, it is anticipated that the CCDs over the mission lifetime will be degraded to an extent that these measurements will only be possible through the correction of radiation damage effects. We have therefore created a Monte Carlo model that simulates the physical processes taking place when transferring signal through a radiation-damaged CCD. The software is based on Shockley-Read-Hall theory, and is made to mimic the physical properties in the CCD as closely as possible. The code runs on a single electrode level and takes three dimensional trap position, potential structure of the pixel, and multi-level clocking into account. A key element of the model is that it also takes device specific simulations of electron density as a direct input, thereby avoiding to make any analytical assumptions about the size and density of the charge cloud. This paper illustrates how test data and simulated data can be compared in order to further our understanding of the positions and properties of the individual radiation-induced traps. ",https://doi.org/10.1117/1.JATIS.3.2.028001,1710.10958v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",An energy consistent discretization of the nonhydrostatic equations in   primitive variables,1970,"  We derive a formulation of the nonhydrostatic equations in spherical geometry with a Lorenz staggered vertical discretization. The combination conserves a discrete energy in exact time integration when coupled with a mimetic horizontal discretization. The formulation is a version of Dubos and Tort (2014) rewritten in terms of primitive variables. It is valid for terrain following mass or height coordinates and for both Eulerian or vertically Lagrangian discretizations. The discretization relies on an extension to Simmons and Burridge (1981) vertical differencing which we show obeys a discrete derivative product rule. This product rule allows us to simplify the treatment of the vertical transport terms. Energy conservation is obtained via a term-by-term balance in the kinetic, internal and potential energy budgets, ensuring an energy-consistent discretization with no spurious sources of energy. We demonstrate convergence with respect to time truncation error in a spectral element code with a HEVI IMEX timestepping algorithm ",https://doi.org/10.1029/2019MS001783,1908.04430v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",What can robotics research learn from computer vision research?,1970,"  The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos. ",Kein DOI-Link verfügbar,2001.02366v2,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Brain Structure-Function Fusing Representation Learning using   Adversarial Decomposed-VAE for Analyzing MCI,1970,"  Integrating the brain structural and functional connectivity features is of great significance in both exploring brain science and analyzing cognitive impairment clinically. However, it remains a challenge to effectively fuse structural and functional features in exploring the brain network. In this paper, a novel brain structure-function fusing-representation learning (BSFL) model is proposed to effectively learn fused representation from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (fMRI) for mild cognitive impairment (MCI) analysis. Specifically, the decomposition-fusion framework is developed to first decompose the feature space into the union of the uniform and the unique spaces for each modality, and then adaptively fuse the decomposed features to learn MCI-related representation. Moreover, a knowledge-aware transformer module is designed to automatically capture local and global connectivity features throughout the brain. Also, a uniform-unique contrastive loss is further devised to make the decomposition more effective and enhance the complementarity of structural and functional features. The extensive experiments demonstrate that the proposed model achieves better performance than other competitive methods in predicting and analyzing MCI. More importantly, the proposed model could be a potential tool for reconstructing unified brain networks and predicting abnormal connections during the degenerative processes in MCI. ",Kein DOI-Link verfügbar,2305.14404v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Generative Adversarial Networks: A Survey Towards Private and Secure   Applications,1970,"  Generative Adversarial Networks (GAN) have promoted a variety of applications in computer vision, natural language processing, etc. due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey paper to summarize those state-of-the-art works systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey paper conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this paper also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions. ",Kein DOI-Link verfügbar,2106.03785v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Graph Convolution Networks Using Message Passing and Multi-Source   Similarity Features for Predicting circRNA-Disease Association,1970,"  Graphs can be used to effectively represent complex data structures. Learning these irregular data in graphs is challenging and still suffers from shallow learning. Applying deep learning on graphs has recently showed good performance in many applications in social analysis, bioinformatics etc. A message passing graph convolution network is such a powerful method which has expressive power to learn graph structures. Meanwhile, circRNA is a type of non-coding RNA which plays a critical role in human diseases. Identifying the associations between circRNAs and diseases is important to diagnosis and treatment of complex diseases. However, there are limited number of known associations between them and conducting biological experiments to identify new associations is time consuming and expensive. As a result, there is a need of building efficient and feasible computation methods to predict potential circRNA-disease associations. In this paper, we propose a novel graph convolution network framework to learn features from a graph built with multi-source similarity information to predict circRNA-disease associations. First we use multi-source information of circRNA similarity, disease and circRNA Gaussian Interaction Profile (GIP) kernel similarity to extract the features using first graph convolution. Then we predict disease associations for each circRNA with second graph convolution. Proposed framework with five-fold cross validation on various experiments shows promising results in predicting circRNA-disease association and outperforms other existing methods. ",Kein DOI-Link verfügbar,2009.07173v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Autism Spectrum Disorder Classification in Children based on Structural   MRI Features Extracted using Contrastive Variational Autoencoder,1970,"  Autism spectrum disorder (ASD) is a highly disabling mental disease that brings significant impairments of social interaction ability to the patients, making early screening and intervention of ASD critical. With the development of the machine learning and neuroimaging technology, extensive research has been conducted on machine classification of ASD based on structural MRI (s-MRI). However, most studies involve with datasets where participants' age are above 5. Few studies conduct machine classification of ASD for participants below 5-year-old, but, with mediocre predictive accuracy. In this paper, we push the boundary of predictive accuracy (above 0.97) of machine classification of ASD in children (age range: 0.92-4.83 years), based on s-MRI features extracted using contrastive variational autoencoder (CVAE). 78 s-MRI, collected from Shenzhen Children's Hospital, are used for training CVAE, which consists of both ASD-specific feature channel and common shared feature channel. The ASD participants represented by ASD-specific features can be easily discriminated from TC participants represented by the common shared features, leading to high classification accuracy. In case of degraded predictive accuracy when data size is extremely small, a transfer learning strategy is proposed here as a potential solution. Finally, we conduct neuroanatomical interpretation based on the correlation between s-MRI features extracted from CVAE and surface area of different cortical regions, which discloses potential biomarkers that could help target treatments of ASD in the future. ",Kein DOI-Link verfügbar,2307.00976v1,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Reconstruction of Hidden Representation for Robust Feature Extraction,1970,"  This paper aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretical summarize the general properties of all algorithms that are based on traditional Auto-Encoders: 1) The reconstruction error of the input can not be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also can not be lower than a lower bound. 2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. 3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pre-training methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models. ",https://doi.org/10.1145/3284174,1710.02844v2,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Topological States in Dimerized Quantum-Dot Chains Created by Atom   Manipulation,1970,"  Topological electronic phases exist in a variety of naturally occurring materials but can also be created artificially. We used a cryogenic scanning tunneling microscope to create dimerized chains of identical quantum dots on a semiconductor surface and to demonstrate that these chains give rise to one-dimensional topological phases. The dots were assembled from charged adatoms, creating a confining potential with single-atom precision acting on electrons in surface states of the semiconductor. Quantum coupling between the dots leads to electronic states localized at the ends of the chains, as well as at deliberately created internal domain walls, in agreement with the predictions of the Su-Schrieffer-Heeger model. Scanning tunneling spectroscopy also reveals deviations from this well-established model manifested in an asymmetric level spectrum and energy shifts of the boundary states. The deviations arise because the dots are charged and hence lead to an onsite potential that varies along the chain. We show that this variation can be mitigated by electrostatic gating using auxiliary charged adatoms, enabling fine-tuning of the boundary states and control of their quantum superposition. The experimental data, which are complemented by theoretical modeling of the potential and the resulting eigenstates, reveal the important role of electrostatics in these engineered quantum structures. ",https://doi.org/10.1103/PhysRevB.105.125418,2112.00801v1,Yes,potent(3)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Topological boundary states in engineered quantum-dot molecules on the   InAs(111)A surface,1970,"  Atom manipulation by scanning tunneling microscopy was used to construct quantum dots on the InAs(111)A surface. Each dot comprised six ionized indium adatoms. The positively charged adatoms create a confining potential acting on surface-state electrons, leading to the emergence of a bound state associated with the dot. By lining up the dots into N-dot chains with alternating tunnel coupling between them, quantum-dot molecules were constructed that revealed electronic boundary states as predicted by the Su-Schrieffer-Heeger (SSH) model of one-dimensional topological phases. Dot chains with odd N were constructed such that they host a single end or domain-wall state, allowing one to probe the localization of the boundary state on a given sublattice by scanning tunneling spectroscopy. We found probability density also on the forbidden sublattice together with an asymmetric energy spectrum of the chain-confined states. This deviation from the SSH model arises because the dots are charged and create a variation in onsite potential along the chain - which does not remove the boundary states but shifts their energy away from the midgap position. Our results demonstrate that topological boundary states can be created in quantum-dot arrays engineered with atomic-scale precision. ",Kein DOI-Link verfügbar,2406.13347v1,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Ophtha-LLaMA2: A Large Language Model for Ophthalmology,1970,"  In recent years, pre-trained large language models (LLMs) have achieved tremendous success in the field of Natural Language Processing (NLP). Prior studies have primarily focused on general and generic domains, with relatively less research on specialized LLMs in the medical field. The specialization and high accuracy requirements for diagnosis in the medical field, as well as the challenges in collecting large-scale data, have constrained the application and development of LLMs in medical scenarios. In the field of ophthalmology, clinical diagnosis mainly relies on doctors' interpretation of reports and making diagnostic decisions. In order to take advantage of LLMs to provide decision support for doctors, we collected three modalities of ophthalmic report data and fine-tuned the LLaMA2 model, successfully constructing an LLM termed the ""Ophtha-LLaMA2"" specifically tailored for ophthalmic disease diagnosis. Inference test results show that even with a smaller fine-tuning dataset, Ophtha-LLaMA2 performs significantly better in ophthalmic diagnosis compared to other LLMs. It demonstrates that the Ophtha-LLaMA2 exhibits satisfying accuracy and efficiency in ophthalmic disease diagnosis, making it a valuable tool for ophthalmologists to provide improved diagnostic support for patients. This research provides a useful reference for the application of LLMs in the field of ophthalmology, while showcasing the immense potential and prospects in this domain. ",Kein DOI-Link verfügbar,2312.04906v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,High-accuracy numerical simulation of black-hole binaries: Computation   of the gravitational-wave energy flux and comparisons with post-Newtonian   approximants,1970,"  Expressions for the gravitational wave (GW) energy flux and center-of-mass energy of a compact binary are integral building blocks of post-Newtonian (PN) waveforms. In this paper, we compute the GW energy flux and GW frequency derivative from a highly accurate numerical simulation of an equal-mass, non-spinning black hole binary. We also estimate the (derivative of the) center-of-mass energy from the simulation by assuming energy balance. We compare these quantities with the predictions of various PN approximants (adiabatic Taylor and Pade models; non-adiabatic effective-one-body (EOB) models). We find that Pade summation of the energy flux does not accelerate the convergence of the flux series; nevertheless, the Pade flux is markedly closer to the numerical result for the whole range of the simulation (about 30 GW cycles). Taylor and Pade models overestimate the increase in flux and frequency derivative close to merger, whereas EOB models reproduce more faithfully the shape of and are closer to the numerical flux, frequency derivative and derivative of energy. We also compare the GW phase of the numerical simulation with Pade and EOB models. Matching numerical and untuned 3.5 PN order waveforms, we find that the phase difference accumulated until $M \omega = 0.1$ is -0.12 radians for Pade approximants, and 0.50 (0.45) radians for an EOB approximant with Keplerian (non-Keplerian) flux. We fit free parameters within the EOB models to minimize the phase difference, and confirm degeneracies among these parameters. By tuning pseudo 4PN order coefficients in the radial potential or in the flux, or, if present, the location of the pole in the flux, we find that the accumulated phase difference can be reduced - if desired - to much less than the estimated numerical phase error (0.02 radians). ",https://doi.org/10.1103/PhysRevD.78.104020,0804.4184v2,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Toward faithful templates for non-spinning binary black holes using the   effective-one-body approach,1970,"  We present an accurate approximation of the full gravitational radiation waveforms generated in the merger of non-eccentric systems of two non-spinning black holes. Utilizing information from recent numerical relativity simulations and the natural flexibility of the effective-one-body (EOB) model, we extend the latter so that it can successfully match the numerical relativity waveforms during the last stages of inspiral, merger and ringdown. By ``successfully'' here, we mean with phase differences < 8% of a gravitational-wave cycle accumulated by the end of the ringdown phase, maximizing only over time of arrival and initial phase. We obtain this result by simply adding a 4-post-Newtonian order correction in the EOB radial potential and determining the (constant) coefficient by imposing high-matching performances with numerical waveforms of mass ratios m1/m2 = 1, 3/2, 2 and 4, m1 and m2 being the individual black-hole masses. The final black-hole mass and spin predicted by the numerical simulations are used to determine the ringdown frequency and decay time of three quasi-normal-mode damped sinusoids that are attached to the EOB inspiral-(plunge) waveform at the EOB light-ring. The EOB waveforms might be tested and further improved in the future by comparison with extremely long and accurate inspiral numerical-relativity waveforms. They may already be employed for coherent searches and parameter estimation of gravitational waves emitted by non-spinning coalescing binary black holes with ground-based laser-interferometer detectors. ",https://doi.org/10.1103/PhysRevD.76.104049,0706.3732v3,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,"Silicon Layer Intercalation of Centimeter-Scale, Epitaxially-Grown   Monolayer Graphene on Ru(0001)",1970,"  We develop a strategy for graphene growth on Ru(0001) followed by silicon-layer intercalation that not only weakens the interaction of graphene with the metal substrate but also retains its superlative properties. This G/Si/Ru architecture, produced by silicon-layer intercalation approach (SIA), was characterized by scanning tunneling microscopy/spectroscopy and angle resolved electron photoemission spectroscopy. These experiments show high structural and electronic qualities of this new composite. The SIA allows for an atomic control of the distance between the graphene and the metal substrate that can be used as a top gate. Our results show potential for the next generation of graphene-based materials with tailored properties. ",https://doi.org/10.1063/1.3687190,1112.4228v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,1970,"  Prompts have been proven to play a crucial role in large language models, and in recent years, vision models have also been using prompts to improve scalability for multiple downstream tasks. In this paper, we focus on adapting prompt design based on instruction tuning into a visual transformer model for image classification which we called Instruction-ViT. The key idea is to implement multi-modal prompts (text or image prompt) related to category information to guide the fine-tuning of the model. Based on the experiments of several image captionining tasks, the performance and domain adaptability were improved. Our work provided an innovative strategy to fuse multi-modal prompts with better performance and faster adaptability for visual classification models. ",Kein DOI-Link verfügbar,2305.00201v1,Yes,innovative(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Potential of Multimodal Large Language Models for Data Mining of Medical   Images and Free-text Reports,1970,"  Medical images and radiology reports are crucial for diagnosing medical conditions, highlighting the importance of quantitative analysis for clinical decision-making. However, the diversity and cross-source heterogeneity of these data challenge the generalizability of current data-mining methods. Multimodal large language models (MLLMs) have recently transformed many domains, significantly affecting the medical field. Notably, Gemini-Vision-series (Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in Artificial General Intelligence (AGI) for computer vision, showcasing their potential in the biomedical domain. In this study, we evaluated the performance of the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation across 14 medical imaging datasets, including 5 medical imaging categories (dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3 radiology report datasets. The investigated tasks encompass disease classification, lesion segmentation, anatomical localization, disease diagnosis, report generation, and lesion detection. Our experimental results demonstrated that Gemini-series models excelled in report generation and lesion detection but faces challenges in disease classification and anatomical localization. Conversely, GPT-series models exhibited proficiency in lesion segmentation and anatomical localization but encountered difficulties in disease diagnosis and lesion detection. Additionally, both the Gemini series and GPT series contain models that have demonstrated commendable generation efficiency. While both models hold promise in reducing physician workload, alleviating pressure on limited healthcare resources, and fostering collaboration between clinical practitioners and artificial intelligence technologies, substantial enhancements and comprehensive validations remain imperative before clinical deployment. ",Kein DOI-Link verfügbar,2407.05758v1,Yes,"commendable(1), potent(1)"
0000-0002-6203-7867,Qian Yang,The University of Manchester,Exploring the Path of Transformation and Development for Study Abroad   Consultancy Firms in China,1970,"  In recent years, with the changing landscape of international education and the growing demand from Chinese students, study abroad consultancy firms in China need to adopt transformational development strategies to address challenges and maintain competitiveness. This study investigated the relationships between key performance indicators and several factors through a questionnaire survey of 158 consultancy firms. The factors examined included service diversification, technology adoption, talent management, and regulatory compliance. Descriptive statistical analysis was employed to analyze the data. The results showed that service scope diversification was positively correlated with firm performance. Technology adoption was positively correlated with operational efficiency. Talent management was positively correlated with service quality. Regulatory compliance was positively correlated with firm reputation. Consultancy firms that took progressive approaches in diversifying services, adopting new technologies, cultivating talent, and ensuring compliance demonstrated superior performance, efficiency, quality, and reputation compared to their less innovative counterparts. This research provides empirical evidence to support the transformation of Chinese study abroad consultancy firms. It also highlights the need for future studies to consider causality and contextual variations to gain deeper insights into this issue. ",Kein DOI-Link verfügbar,2404.11034v1,Yes,innovative(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,"Towards Prototyping Driverless Vehicle Behaviors, City Design, and   Policies Simultaneously",1970,"  Autonomous Vehicles (AVs) can potentially improve urban living by reducing accidents, increasing transportation accessibility and equity, and decreasing emissions. Realizing these promises requires the innovations of AV driving behaviors, city plans and infrastructure, and traffic and transportation policies to join forces. However, the complex interdependencies among AV, city, and policy design issues can hinder their innovation. We argue the path towards better AV cities is not a process of matching city designs and policies with AVs' technological innovations, but a process of iterative prototyping of all three simultaneously: Innovations can happen step-wise as the knot of AV, city, and policy design loosens and tightens, unwinds and reties. In this paper, we ask: How can innovators innovate AVs, city environments, and policies simultaneously and productively toward better AV cities? The paper has two parts. First, we map out the interconnections among the many AV, city, and policy design decisions, based on a literature review spanning HCI/HRI, transportation science, urban studies, law and policy, operations research, economy, and philosophy. This map can help innovators identify design constraints and opportunities across the traditional AV/city/policy design disciplinary bounds. Second, we review the respective methods for AV, city, and policy design, and identify key barriers in combining them: (1) Organizational barriers to AV-city-policy design collaboration, (2) computational barriers to multi-granularity AV-city-policy simulation, and (3) different assumptions and goals in joint AV-city-policy optimization. We discuss two broad approaches that can potentially address these challenges, namely, ""low-fidelity integrative City-AV-Policy Simulation (iCAPS)"" and ""participatory design optimization"". ",Kein DOI-Link verfügbar,2304.06639v1,Yes,potent(2)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Exploring the Best Practices of Query Expansion with Large Language   Models,1970,"  Large Language Models (LLMs) are foundational in language technologies, particularly in information retrieval (IR). Previous studies have utilized LLMs for query expansion, achieving notable improvements in IR. In this paper, we thoroughly explore the best practice of leveraging LLMs for query expansion. To this end, we introduce a training-free, straightforward yet effective framework called Multi-Text Generation Integration (\textsc{MuGI}). It leverages LLMs to generate multiple pseudo-references, integrating them with queries to enhance both sparse and dense retrievers. Our empirical findings reveal that: (1) Increasing the number of samples from LLMs benefits IR systems; (2) A balance between the query and pseudo-documents, and an effective integration strategy, is critical for high performance; (3) Contextual information from LLMs is essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo relevance feedback can further calibrate queries for improved performance; and (5) Query expansion is widely applicable and versatile, consistently enhancing models ranging from 23M to 7B parameters. Our code and all generated references are made available at \url{https://github.com/lezhang7/Retrieval_MuGI} ",Kein DOI-Link verfügbar,2401.06311v3,Yes,"notable(1), versatile(1)"
0000-0002-6203-7867,Qian Yang,The University of Manchester,Discovery of two broad absorption line quasars at redshift about 4.75   using the Lijiang 2.4m telescope,1970,"  The ultraviolet broad absorption lines have been seen in the spectra of quasars at high redshift, and are generally considered to be caused by outflows with velocities from thousands kilometers per second to one tenth of the speed of light. They provide crucial implications for the cosmological structures and physical evolutions related to the feedback of active galactic nuclei (AGNs). Recently, through a dedicated program of optically spectroscopic identifications of selected quasar candidates at redshift 5 by using the Lijiang 2.4 m telescope, we discovered two luminous broad absorption line quasars (BALQSOs) at redshift about 4.75. One of them may even have the potentially highest absorption Balnicity Index (BI) ever found to date, which is remarkably characterized by its deep, broad absorption lines and sub-relativistic outflows. Further physical properties, including the metal abundances, variabilities, evolutions of the supermassive black holes (SMBH) and accretion disks associated with the feedback process, can be investigated with multi-wavelength follow-up observations in the future. ",https://doi.org/10.1007/s11433-015-5685-4,1511.08278v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Faster On-Device Training Using New Federated Momentum Algorithm,1970,"  Mobile crowdsensing has gained significant attention in recent years and has become a critical paradigm for emerging Internet of Things applications. The sensing devices continuously generate a significant quantity of data, which provide tremendous opportunities to develop innovative intelligent applications. To utilize these data to train machine learning models while not compromising user privacy, federated learning has become a promising solution. However, there is little understanding of whether federated learning algorithms are guaranteed to converge. We reconsider model averaging in federated learning and formulate it as a gradient-based method with biased gradients. This novel perspective assists analysis of its convergence rate and provides a new direction for more acceleration. We prove for the first time that the federated averaging algorithm is guaranteed to converge for non-convex problems, without imposing additional assumptions. We further propose a novel accelerated federated learning algorithm and provide a convergence guarantee. Simulated federated learning experiments are conducted to train deep neural networks on benchmark datasets, and experimental results show that our proposed method converges faster than previous approaches. ",Kein DOI-Link verfügbar,2002.02090v1,Yes,innovative(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Attacking Recommender Systems with Augmented User Profiles,1970,"  Recommendation Systems (RS) have become an essential part of many online services. Due to its pivotal role in guiding customers towards purchasing, there is a natural motivation for unscrupulous parties to spoof RS for profits. In this paper, we study the shilling attack: a subsistent and profitable attack where an adversarial party injects a number of user profiles to promote or demote a target item. Conventional shilling attack models are based on simple heuristics that can be easily detected, or directly adopt adversarial attack methods without a special design for RS. Moreover, the study on the attack impact on deep learning based RS is missing in the literature, making the effects of shilling attack against real RS doubtful. We present a novel Augmented Shilling Attack framework (AUSH) and implement it with the idea of Generative Adversarial Network. AUSH is capable of tailoring attacks against RS according to budget and complex attack goals, such as targeting a specific user group. We experimentally show that the attack impact of AUSH is noticeable on a wide range of RS including both classic and modern deep learning based RS, while it is virtually undetectable by the state-of-the-art attack detection model. ",https://doi.org/10.1145/3340531.3411884,2005.08164v2,Yes,pivotal(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Sentence-level Online Handwritten Chinese Character Recognition,1970,"  Single online handwritten Chinese character recognition~(single OLHCCR) has achieved prominent performance. However, in real application scenarios, users always write multiple Chinese characters to form one complete sentence and the contextual information within these characters holds the significant potential to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In this work, we first propose a simple and straightforward end-to-end network, namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR. It couples convolutional neural network with sequence modeling architecture to exploit the handwritten character's previous contextual information. Although VCN performs much better than the state-of-the-art single OLHCCR model, it exposes high fragility when confronting with not well written characters such as sloppy writing, missing or broken strokes. To improve the robustness of sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the backbone component, which projects each Chinese character into word embeddings, and integrates the spatial glyph features of handwritten characters and their contextual information multiple times at multi-layer fusion module. We also construct a large-scale sentence-level handwriting dataset, named as CSOHD to evaluate models. Extensive experiment results demonstrate that DSTFN achieves the state-of-the-art performance, which presents strong robustness compared with VCN and exiting single OLHCCR models. The in-depth empirical analysis and case studies indicate that DSTFN can significantly improve the efficiency of handwriting input, with the handwritten Chinese character with incomplete strokes being recognized precisely. ",Kein DOI-Link verfügbar,2108.02561v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Clinical Evidence Engine: Proof-of-Concept For A   Clinical-Domain-Agnostic Decision Support Infrastructure,1970,"  Abstruse learning algorithms and complex datasets increasingly characterize modern clinical decision support systems (CDSS). As a result, clinicians cannot easily or rapidly scrutinize the CDSS recommendation when facing a difficult diagnosis or treatment decision in practice. Over-trust or under-trust are frequent. Prior research has explored supporting such assessments by explaining DST data inputs and algorithmic mechanisms. This paper explores a different approach: Providing precisely relevant, scientific evidence from biomedical literature. We present a proof-of-concept system, Clinical Evidence Engine, to demonstrate the technical and design feasibility of this approach across three domains (cardiovascular diseases, autism, cancer). Leveraging Clinical BioBERT, the system can effectively identify clinical trial reports based on lengthy clinical questions (e.g., ""risks of catheter infection among adult patients in intensive care unit who require arterial catheters, if treated with povidone iodine-alcohol""). This capability enables the system to identify clinical trials relevant to diagnostic/treatment hypotheses -- a clinician's or a CDSS's. Further, Clinical Evidence Engine can identify key parts of a clinical trial abstract, including patient population (e.g., adult patients in intensive care unit who require arterial catheters), intervention (povidone iodine-alcohol), and outcome (risks of catheter infection). This capability opens up the possibility of enabling clinicians to 1) rapidly determine the match between a clinical trial and a clinical question, and 2) understand the result and contexts of the trial without extensive reading. We demonstrate this potential by illustrating two example use scenarios of the system. We discuss the idea of designing DST explanations not as specific to a DST or an algorithm, but as a domain-agnostic decision support infrastructure. ",Kein DOI-Link verfügbar,2111.00621v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Optical Variability of the Dwarf AGN NGC 4395 from the Transiting   Exoplanet Survey Satellite,1970,"  We present optical light curves from the Transiting Exoplanet Survey Satellite (TESS) for the archetypical dwarf active galactic nucleus (AGN) in the nearby galaxy NGC 4395 hosting a $\sim 10^5\,M_\odot$ supermassive black hole (SMBH). Significant variability is detected on timescales from weeks to hours before reaching the background noise level. The $\sim$month-long, 30 minute-cadence, high-precision TESS light curve can be well fit by a simple damped random walk (DRW) model, with the damping timescale $\tau_{\rm DRW}$ constrained to be $2.3_{-0.7}^{+1.8}$~days ($1\sigma$). NGC 4395 lies almost exactly on the extrapolation of the $\tau_{\rm DRW}-M_{\rm BH}$ relation measured for AGNs with BH masses that are more than three orders of magnitude larger. The optical variability periodogram can be well fit by a broken power law with the high-frequency slope ($-1.88\pm0.15$) and the characteristic timescale ($\tau_{\rm br}\equiv 1/(2\pi f_{\rm br})=1.4_{-0.5}^{+1.9}\,$days) consistent with the DRW model within 1$\sigma$. This work demonstrates the power of TESS light curves in identifying low-mass accreting SMBHs with optical variability, and a potential global $\tau_{\rm DRW}-M_{\rm BH}$ relation that can be used to estimate SMBH masses with optical variability measurements. ",https://doi.org/10.3847/1538-4357/aba3ce,2005.04491v2,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Diffusion-controlled Alloying of Single-phase Multi-principal Covalent   Transition Metal Carbides with Enhanced Damage tolerance and Exceptional   Thermal Properties,1970,"  Multicomponent alloying has displayed extraordinary potential for producing exceptional structural and functional materials. However, the synthesis of single-phase, multi-principal covalent compounds remains a challenge. Here we present a diffusion-controlled alloying strategy for the successful realization of covalent multi-principal transition metal carbides (MPTMCs) with a single face-centered cubic (FCC) phase. The increased interfacial diffusion promoted by the addition of a nonstoichiometric compound leads to rapid formation of the new single phase at much lower sintering temperature. Direct atomic-level observations via scanning transmission electron microscopy demonstrate that MPTMCs are composed of a single phase with a random distribution of all cations, which holds the key to the unique combinations of improved fracture toughness, superior Vickers hardness, and extremely lower thermal diffusivity achieved in MPTMCs. The present discovery provides a promising approach toward the design and synthesis of next-generation high-performance materials. ",Kein DOI-Link verfügbar,1810.01944v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Quasar Photometric Redshifts and Candidate Selection: A New Algorithm   Based on Optical and Mid-Infrared Photometric Data,1970,"  We present a new algorithm to estimate quasar photometric redshifts (photo-$z$s), by considering the asymmetries in the relative flux distributions of quasars. The relative flux models are built with multivariate Skew-t distributions in the multi-dimensional space of relative fluxes as a function of redshift and magnitude. For 151,392 quasars in the SDSS, we achieve a photo-$z$ accuracy, defined as the fraction of quasars with the difference between the photo-$z$ $z_p$ and the spectroscopic redshift $z_s$, $|\Delta z| = |z_s-z_p|/(1+z_s)$ within 0.1, of 74%. Combining the WISE W1 and W2 infrared data with the SDSS data, the photo-$z$ accuracy is enhanced to 87%. Using the Pan-STARRS1 or DECaLS photometry with WISE W1 and W2 data, the photo-$z$ accuracies are 79% and 72%, respectively. The prior probabilities as a function of magnitude for quasars, stars and galaxies are calculated respectively based on (1) the quasar luminosity function; (2) the Milky Way synthetic simulation with the Besan\c{c}on model; (3) the Bayesian Galaxy Photometric Redshift estimation. The relative fluxes of stars are obtained with the Padova isochrones, and the relative fluxes of galaxies are modeled through galaxy templates. We test our classification method to select quasars using the DECaLS $g$, $r$, $z$, and WISE W1 and W2 photometry. The quasar selection completeness is higher than 70% for a wide redshift range $0.5<z<4.5$, and a wide magnitude range $18<r<21.5$ mag. Our photo-$z$ regression and classification method has the potential to extend to future surveys. The photo-$z$ code will be publicly available. ",https://doi.org/10.3847/1538-3881/aa943c,1710.09155v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Membership Inference Attacks and Defenses in Neural Network Pruning,1970,"  Neural network pruning has been an essential technique to reduce the computation and memory requirements for using deep neural networks for resource-constrained devices. Most existing research focuses primarily on balancing the sparsity and accuracy of a pruned neural network by strategically removing insignificant parameters and retraining the pruned model. Such efforts on reusing training samples pose serious privacy risks due to increased memorization, which, however, has not been investigated yet.   In this paper, we conduct the first analysis of privacy risks in neural network pruning. Specifically, we investigate the impacts of neural network pruning on training data privacy, i.e., membership inference attacks. We first explore the impact of neural network pruning on prediction divergence, where the pruning process disproportionately affects the pruned model's behavior for members and non-members. Meanwhile, the influence of divergence even varies among different classes in a fine-grained manner. Enlighten by such divergence, we proposed a self-attention membership inference attack against the pruned neural networks. Extensive experiments are conducted to rigorously evaluate the privacy impacts of different pruning approaches, sparsity levels, and adversary knowledge. The proposed attack shows the higher attack performance on the pruned models when compared with eight existing membership inference attacks. In addition, we propose a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance, whose effectiveness has been experimentally demonstrated to effectively mitigate the privacy risks while maintaining the sparsity and accuracy of the pruned models. ",Kein DOI-Link verfügbar,2202.03335v2,Yes,strategically(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,A Comparison Study of Coupled and Decoupled Uplink Heterogeneous   Cellular Networks,1970,"  The evolution of mobile cellular networks has brought great changes of network architecture. For example, heterogeneous cellular network (HetNet) and Ultra dense network (UDN) have been proposed as promising techniques for 5G systems. Dense deployment of base stations (BSs) allows a mobile user to be able to access multiple BSs. Meanwhile the unbalance between UL and DL in HetNets, such as different received SINR threshold and traffic load, etc., becomes increasingly obvious. All these factors naturally inspire us to consider decoupling of uplink and downlink in radio access network. An interesting question is that whether the decoupled uplink (UL) /downlink (DL) access (DUDA) mode outperforms traditional coupled uplink (UL)/downlink (DL) access (CUDA) mode or not, and how big is the performance difference in terms of system rate, spectrum efficiency (SE) and energy efficiency (EE), etc. in HetNets. In this paper, we aim at thoroughly comparing the performance of the two modes based on stochastic geometry theory. In our analytical model, we take into account dynamic transmit power control in UL communication. Specifically, we employ fractional power control (FPC) to model a location-dependent channel state. Numerical results reveals that DUDA mode significantly outperforms CUDA mode in system rate, SE and EE in HetNets. In addition, DUDA mode improves load balance and potential fairness for both different type BSs and associated UEs. ",Kein DOI-Link verfügbar,1502.01887v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Federated Semi-Supervised Domain Adaptation via Knowledge Transfer,1970,"  Given the rapidly changing machine learning environments and expensive data labeling, semi-supervised domain adaptation (SSDA) is imperative when the labeled data from the source domain is statistically different from the partially labeled data from the target domain. Most prior SSDA research is centrally performed, requiring access to both source and target data. However, data in many fields nowadays is generated by distributed end devices. Due to privacy concerns, the data might be locally stored and cannot be shared, resulting in the ineffectiveness of existing SSDA research. This paper proposes an innovative approach to achieve SSDA over multiple distributed and confidential datasets, named by Federated Semi-Supervised Domain Adaptation (FSSDA). FSSDA integrates SSDA with federated learning based on strategically designed knowledge distillation techniques, whose efficiency is improved by performing source and target training in parallel. Moreover, FSSDA controls the amount of knowledge transferred across domains by properly selecting a key parameter, i.e., the imitation parameter. Further, the proposed FSSDA can be effectively generalized to multi-source domain adaptation scenarios. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of FSSDA design. ",Kein DOI-Link verfügbar,2207.10727v2,Yes,"innovative(1), strategically(1)"
0009-0007-4678-0419,Lan Zhang,The University of Manchester,FedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained   Federated Learning with Heterogeneous On-Device Models,1970,"  Federated learning enables multiple distributed devices to collaboratively learn a shared prediction model without centralizing their on-device data. Most of the current algorithms require comparable individual efforts for local training with the same structure and size of on-device models, which, however, impedes participation from resource-constrained devices. Given the widespread yet heterogeneous devices nowadays, in this paper, we propose an innovative federated learning framework with heterogeneous on-device models through Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows devices to independently determine the on-device models upon their local resources. To achieve knowledge transfer across these heterogeneous on-device models, a zero-shot distillation approach is designed without any prerequisites for private on-device data, which is contrary to certain prior research based on a public dataset or a pre-trained data generator. Moreover, this compute-intensive distillation task is assigned to the server to allow the participation of resource-constrained devices, where a generator is adversarially learned with the ensemble of collected on-device models. The distilled central knowledge is then sent back in the form of the corresponding on-device model parameters, which can be easily absorbed on the device side. Extensive experimental studies demonstrate the effectiveness and robustness of FedZKT towards on-device knowledge agnostic, on-device model heterogeneity, and other challenging federated learning scenarios, such as heterogeneous on-device data and straggler effects. ",Kein DOI-Link verfügbar,2109.03775v2,Yes,innovative(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Towards Robust On-Ramp Merging via Augmented Multimodal Reinforcement   Learning,1970,"  Despite the success of AI-enabled onboard perception, on-ramp merging has been one of the main challenges for autonomous driving. Due to limited sensing range of onboard sensors, a merging vehicle can hardly observe main road conditions and merge properly. By leveraging the wireless communications between connected and automated vehicles (CAVs), a merging CAV has potential to proactively obtain the intentions of nearby vehicles. However, CAVs can be prone to inaccurate observations, such as the noisy basic safety messages (BSM) and poor quality surveillance images. In this paper, we present a novel approach for Robust on-ramp merge of CAVs via Augmented and Multi-modal Reinforcement Learning, named by RAMRL. Specifically, we formulate the on-ramp merging problem as a Markov decision process (MDP) by taking driving safety, comfort driving behavior, and traffic efficiency into account. To provide reliable merging maneuvers, we simultaneously leverage BSM and surveillance images for multi-modal observation, which is used to learn a policy model through proximal policy optimization (PPO). Moreover, to improve data efficiency and provide better generalization performance, we train the policy model with augmented data (e.g., noisy BSM and noisy surveillance images). Extensive experiments are conducted with Simulation of Urban MObility (SUMO) platform under two typical merging scenarios. Experimental results demonstrate the effectiveness and efficiency of our robust on-ramp merging design. ",Kein DOI-Link verfügbar,2208.07307v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,CTS: A Consistency-Based Medical Image Segmentation Model,1970,"  In medical image segmentation tasks, diffusion models have shown significant potential. However, mainstream diffusion models suffer from drawbacks such as multiple sampling times and slow prediction results. Recently, consistency models, as a standalone generative network, have resolved this issue. Compared to diffusion models, consistency models can reduce the sampling times to once, not only achieving similar generative effects but also significantly speeding up training and prediction. However, they are not suitable for image segmentation tasks, and their application in the medical imaging field has not yet been explored. Therefore, this paper applies the consistency model to medical image segmentation tasks, designing multi-scale feature signal supervision modes and loss function guidance to achieve model convergence. Experiments have verified that the CTS model can obtain better medical image segmentation results with a single sampling during the test phase. ",Kein DOI-Link verfügbar,2405.09056v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Residue-based Label Protection Mechanisms in Vertical Logistic   Regression,1970,"  Federated learning (FL) enables distributed participants to collaboratively learn a global model without revealing their private data to each other. Recently, vertical FL, where the participants hold the same set of samples but with different features, has received increased attention. This paper first presents one label inference attack method to investigate the potential privacy leakages of the vertical logistic regression model. Specifically, we discover that the attacker can utilize the residue variables, which are calculated by solving the system of linear equations constructed by local dataset and the received decrypted gradients, to infer the privately owned labels. To deal with this, we then propose three protection mechanisms, e.g., additive noise mechanism, multiplicative noise mechanism, and hybrid mechanism which leverages local differential privacy and homomorphic encryption techniques, to prevent the attack and improve the robustness of the vertical logistic regression. model. Experimental results show that both the additive noise mechanism and the multiplicative noise mechanism can achieve efficient label protection with only a slight drop in model testing accuracy, furthermore, the hybrid mechanism can achieve label protection without any testing accuracy degradation, which demonstrates the effectiveness and efficiency of our protection techniques ",Kein DOI-Link verfügbar,2205.04166v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,1970,"  To support various applications, a prevalent and efficient approach for business owners is leveraging their valuable datasets to fine-tune a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ``Double-I watermark''. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both quantitative and qualitative analyses. ",Kein DOI-Link verfügbar,2402.14883v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection,1970,"  3D object detection plays an important role in autonomous driving; however, its vulnerability to backdoor attacks has become evident. By injecting ''triggers'' to poison the training dataset, backdoor attacks manipulate the detector's prediction for inputs containing these triggers. Existing backdoor attacks against 3D object detection primarily poison 3D LiDAR signals, where large-sized 3D triggers are injected to ensure their visibility within the sparse 3D space, rendering them easy to detect and impractical in real-world scenarios.   In this paper, we delve into the robustness of 3D object detection, exploring a new backdoor attack surface through 2D cameras. Given the prevalent adoption of camera and LiDAR signal fusion for high-fidelity 3D perception, we investigate the latent potential of camera signals to disrupt the process. Although the dense nature of camera signals enables the use of nearly imperceptible small-sized triggers to mislead 2D object detection, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. The primary challenge emerges from the fusion process that transforms camera signals into a 3D space, compromising the association with the 2D trigger to the target output. To tackle this issue, we propose an innovative 2D-oriented backdoor attack against LiDAR-camera fusion methods for 3D object detection, named BadFusion, for preserving trigger effectiveness throughout the entire fusion process. The evaluation demonstrates the effectiveness of BadFusion, achieving a significantly higher attack success rate compared to existing 2D-oriented attacks. ",Kein DOI-Link verfügbar,2405.03884v1,Yes,"innovative(1), potent(1)"
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Hidden Anderson Localization in Disorder-Free Ising-Kondo Lattice,1970,"  Anderson localization (AL) phenomena usually exists in systems with random potential. However, disorder-free quantum many-body systems with local conservation can also exhibit AL or even many-body localization transition. In this work, we show that the AL phase exists in a modified Kondo lattice without external random potential. The density of state, inverse participation ratio and temperature-dependent resistance are computed by classical Monte Carlo simulation, which uncovers the AL phase from previously studied Fermi liquid and Mott insulator regime. The occurrence of AL roots from quenched disorder formed by conservative localized moments. Interestingly, a many-body wavefunction is found, which captures elements in all three paramagnetic phases and is used to compute their quantum entanglement. In light of these findings, we expect the disorder-free AL phenomena can exit in generic translation-invariant quantum many-body systems. ",https://doi.org/10.1088/1674-1056/ab99b0,1907.13507v3,Yes,potent(2)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,S-RAN: Semantic-Aware Radio Access Networks,1970,"  Semantic communication (SemCom) has been a transformative paradigm, emphasizing the precise exchange of meaningful information over traditional bit-level transmissions. However, existing SemCom research, primarily centered on simplified scenarios like single-pair transmissions with direct wireless links, faces significant challenges when applied to real-world radio access networks (RANs). This article introduces a Semantic-aware Radio Access Network (S-RAN), offering a holistic systematic view of SemCom beyond single-pair transmissions. We begin by outlining the S-RAN architecture, introducing new physical components and logical functions along with key design challenges. We then present transceiver design for end-to-end transmission to overcome conventional SemCom transceiver limitations, including static channel conditions, oversimplified background knowledge models, and hardware constraints. Later, we delve into the discussion on radio resource management for multiple users, covering semantic channel modeling, performance metrics, resource management algorithms, and a case study, to elaborate distinctions from resource management for legacy RANs. Finally, we highlight open research challenges and potential solutions. The objective of this article is to serve as a basis for advancing SemCom research into practical wireless systems. ",Kein DOI-Link verfügbar,2407.11161v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Coexistence of antiferromagnetism and superconductivity of   t-t$^\prime$-J model on honeycomb lattice,1970,"  Motivated by recent experimental study of antiferromagnetic property of honeycomb compound In$_{3}$Cu$_{2}$VO$_{9}$ [Yan \textit{et al.}, PRB \textbf{85}, 085102 (2012)], we explore possible superconductivity and its coexistence with antiferromagnetism. We use the t-t$^\prime$-J model on the honeycomb lattice as our starting point and employ the slave-boson mean-field theory. In the antiferromagnetic normal state, the characteristic doping evolution of Fermi surface shows that only one effective singe band is active, which suggests that the potential pairing symmetry is the time-reversal symmetry breaking $d+id$, rather than the extended $s$-wave. It is found that this superconducting state coexists with the antiferromagnetism in a broad doping regime, which is consistent with the numerical calculations. The local density of states and its thermodynamic property of the superconducting state has been studied in detail with an effective single-band picture for understanding other physical observable such as superfluid density. The present work may be useful in experimentally exploring possible superconductivity of this kind of materials on the honeycomb lattice and contributes to the understanding of the unconventional superconductivity on general two-dimensional correlated electron systems. ",https://doi.org/10.1016/j.physb.2015.01.010,1404.1795v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Wireless Resource Optimization in Hybrid Semantic/Bit Communication   Networks,1970,"  Recently, semantic communication (SemCom) has shown great potential in significant resource savings and efficient information exchanges, thus naturally introducing a novel and practical cellular network paradigm where two modes of SemCom and conventional bit communication (BitCom) coexist. Nevertheless, the involved wireless resource management becomes rather complicated and challenging, given the unique background knowledge matching and time-consuming semantic coding requirements in SemCom. To this end, this paper jointly investigates user association (UA), mode selection (MS), and bandwidth allocation (BA) problems in a hybrid semantic/bit communication network (HSB-Net). Concretely, we first identify a unified performance metric of message throughput for both SemCom and BitCom links. Next, we specially develop a knowledge matching-aware two-stage tandem packet queuing model and theoretically derive the average packet loss ratio and queuing latency. Combined with practical constraints, we then formulate a joint optimization problem for UA, MS, and BA to maximize the overall message throughput of HSB-Net. Afterward, we propose an optimal resource management strategy by utilizing a Lagrange primal-dual transformation method and a preference list-based heuristic algorithm with polynomial-time complexity. Numerical results not only demonstrate the accuracy of our analytical queuing model, but also validate the performance superiority of our proposed strategy compared with different benchmarks. ",Kein DOI-Link verfügbar,2404.04162v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,The Gravitational Potential Near the Sun From SEGUE K-dwarf Kinematics,1970,"  To constrain the Galactic gravitational potential near the Sun ($\sim$1.5 kpc), we derive and model the spatial and velocity distribution for a sample of 9000 K-dwarfs that have spectra from SDSS/SEGUE, which yield radial velocities and abundances ([Fe/H] & [$\alpha$/Fe]). We first derive the spatial density distribution for stars of three abundance-selected sub-populations by accounting for the survey's selection function. The vertical profile of these sub-populations are simple exponentials and their vertical dispersion profile is nearly isothermal. To model these data, we apply the `vertical' Jeans Equation, which relates the observable tracer number density and vertical velocity dispersion to the gravitational potential or vertical force. We explore a number of functional forms for the vertical force law, and fit the dispersion and density profiles of all abundance selected sub-populations simultaneously in the same potential, and explore all parameter co-variances using MCMC. Our fits constrain a disk {\it mass} scale height $\lesssim$ 300 pc and the total surface mass density to be $67 \pm 6 M_{\odot} {\rm pc^{-2}}$ at $|z| = 1.0$ kpc of which the contribution from all stars is $42 \pm 5 M_{\odot} {\rm pc^{-2}}$ (presuming a contribution from cold gas of $13 M_{\odot} {\rm pc^{-2}}$). We find significant constraints on the local dark matter density of $0.0065\pm0.0023 M_{\odot} {\rm pc^{-3}}$ ($0.25\pm0.09 {\rm GeV cm^{-3}} $). Together with recent experiments this firms up the best estimate of $0.0075\pm0.0021 M_{\odot} {\rm pc^{-3}}$ ($0.28\pm0.08 {\rm GeV cm^{-3}} $), consistent with global fits of approximately round dark matter halos to kinematic data in the outskirts of the Galaxy. ",https://doi.org/10.1088/0004-637X/772/2/108,1209.0256v2,Yes,potent(3)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,InFi: End-to-End Learning to Filter Input for Resource-Efficiency in   Mobile-Centric Inference,1970,"  Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in feature embedding with robust discriminability. We design and implement InFi that supports six input modalities and multiple mobile-centric deployments. Comprehensive evaluations confirm our theoretical results and show that InFi outperforms strong baselines in applicability, accuracy, and efficiency. InFi achieve 8.5x throughput and save 95% bandwidth, while keeping over 90% accuracy, for a video analytics application on mobile platforms. ",Kein DOI-Link verfügbar,2209.13873v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,"The Origins of Young Stars in the Direction of the Leading Arm of the   Magellanic Stream: Abundances, Kinematics, and Orbits",1970,"  We explore the origins of the young B-type stars found by Casetti-Dinescu et al.(2014) at the outskirts of the Milky-Way disk in the sky region of Leading Arm of the Magellanic Stream. High-resolution spectroscopic observations made with the MIKE instrument on the Magellan Clay 6.5m telescope for nine stars are added to the previous sample analyzed by Zhang et al. (2017). We compile a sample of fifteen young stars with well-determined stellar types, ages, abundances and kinematics. With proper motions from Gaia DR2 we also derive orbits in a realistic Milky-Way potential. We find that our previous radial-velocity selected LA candidates have substantial orbital angular momentum. The substantial amount of rotational component for these stars is in contrast with the near-polar Magellanic orbit, thus rendering these stars unlikely members of the LA. There are four large orbital-energy stars in our sample. The highest orbital-energy one has an age shorter than the time to disk crossing, with a birthplace $z=2.5$~kpc and $R_{\rm GC}\sim 28$~kpc. Therefore, the origin of this star is uncertain. The remaining three stars have disk runaway origin with birthplaces between 12 and 25 kpc from the Galactic center. Also, the most energetic stars are more metal poor ([Mg/H] =$-0.50\pm0.07$) and with larger He scatter ($\sigma_{\rm [He/H]} = 0.72$) than the inner disk ones ([Mg/H] $=0.12\pm0.36$, $\sigma_{\rm [He/H]} = 0.15$). While the former group's abundance is compatible with that of the Large Magellanic Cloud, it could also reflect the metallicity gradient of the MW disk and their runaway status via different runaway mechanisms. ",https://doi.org/10.3847/1538-4357/aaf560,1812.00198v1,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,Equations in virtually class 2 nilpotent groups,1970,"  We give an algorithm that decides whether a single equation in a group that is virtually a class $2$ nilpotent group with a virtually cyclic commutator subgroup, such as the Heisenberg group, admits a solution. This generalises the work of Duchin, Liang and Shapiro to finite extensions. ",https://doi.org/10.46298/jgcc.2022.14.1.9776,2009.10651v7,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,"Quadratic Diophantine equations, the Heisenberg group and formal   languages",1970,"  We express the solutions to quadratic equations with two variables in the ring of integers using EDT0L languages. We use this to show that EDT0L languages can be used to describe the solutions to one-variable equations in the Heisenberg group. This is done by reducing the question of solving a one-variable equation in the Heisenberg group to solving an equation in the ring of integers, exploiting the strong link between the ring of integers and nilpotent groups. ",Kein DOI-Link verfügbar,2203.04849v2,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,E-disjunctive inverse semigroups,1970,  In this paper we provide an overview of the class of inverse semigroups $S$ such that every congruence on $S$ relates at least one idempotent to a non-idempotent; such inverse semigroups are called $E$-disjunctive. This overview includes the study of the inverse semigroup theoretic structure of $E$-disjunctive semigroups; a large number of natural examples; some asymptotic results establishing the rarity of such inverse semigroups; and a general structure theorem for all inverse semigroups where the building blocks are $E$-disjunctive. ,Kein DOI-Link verfügbar,2405.19825v1,Yes,potent(2)
0000-0001-9633-9313,Alex Levine,The University of Manchester,Post's correspondence problem for hyperbolic and virtually nilpotent   groups,1970,"  Post's Correspondence Problem (the PCP) is a classical decision problem in theoretical computer science that asks whether for pairs of free monoid morphisms $g, h\colon\Sigma^*\to\Delta^*$ there exists any non-trivial $x\in\Sigma^*$ such that $g(x)=h(x)$.   Post's Correspondence Problem for a group $\Gamma$ takes pairs of group homomorphisms $g, h\colon F(\Sigma)\to \Gamma$ instead, and similarly asks whether there exists an $x$ such that $g(x)=h(x)$ holds for non-elementary reasons. The restrictions imposed on $x$ in order to get non-elementary solutions lead to several interpretations of the problem; we consider the natural restriction asking that $x \notin \ker(g) \cap \ker(h)$ and prove that the resulting interpretation of the PCP is undecidable for arbitrary hyperbolic $\Gamma$, but decidable when $\Gamma$ is virtually nilpotent. We also study this problem for group constructions such as subgroups, direct products and finite extensions. This problem is equivalent to an interpretation due to Myasnikov, Nikolev and Ushakov when one map is injective. ",https://doi.org/10.1112/blms.12921,2211.12158v2,Yes,potent(1)
0000-0003-3961-2023,Richard Brown,The University of Manchester,Tracking areas with increased likelihood of surface particle aggregation   in the Gulf of Finland: A first look at persistent Lagrangian Coherent   Structures (LCS),1970,"  We explore the possibility to identify areas of intense patch formation from floating items due to systematic convergence of surface velocity fields by means of a visual comparison of Lagrangian Coherent Structures (LCS) and estimates of areas prone to patch formation using the concept of Finite-Time Compressibility (FTC, a generalisation of the notion of time series of divergence). The LCSs are evaluated using the Finite Time Lyapunov Exponent (FTLE) method. The test area is the Gulf of Finland (GoF) in the Baltic Sea. A basin-wide spatial average of backward FTLE is calculated for the GoF for the first time. This measure of the mixing strength displays a clear seasonal pattern. The evaluated backward FTLE features are linked with potential patch formation regions with high FTC levels. It is shown that areas hosting frequent upwelling or downwelling have consistently stronger than average mixing intensity. The combination of both methods, FTC and LCS, has the potential of being a powerful tool to identify the formation of patches of pollution at the sea surface. ",https://doi.org/10.1016/j.jmarsys.2021.103514,2101.09358v1,Yes,potent(2)
0000-0002-5822-5435,Kaled Alshmrany,The University of Manchester,Finding Security Vulnerabilities in Network Protocol Implementations,1970,"  Implementations of network protocols are often prone to vulnerabilities caused by developers' mistakes when accessing memory regions and dealing with arithmetic operations. Finding practical approaches for checking the security of network protocol implementations has proven to be a challenging problem. The main reason is that the protocol software state-space is too large to be explored. Here we propose a novel verification approach that combines fuzzing with symbolic execution to verify intricate properties in network protocol implementations. We use fuzzing for an initial exploration of the network protocol, while symbolic execution explores both the program paths and protocol states, which were uncovered by fuzzing. From this combination, we automatically generate high-coverage test input packets for a network protocol implementation. We surveyed various approaches based on fuzzing and symbolic execution to understand how these techniques can be effectively combined and then choose a suitable tool to develop further our model on top of it. In our preliminary evaluation, we used ESBMC, Map2Check, and KLEE as software verifiers and SPIKE as fuzzer to check their suitability to verify our network protocol implementations. Our experimental results show that ESBMC can be further developed within our verification framework called \textit{FuSeBMC}, to efficiently and effectively detect intricate security vulnerabilities in network protocol implementations. ",Kein DOI-Link verfügbar,2001.09592v1,Yes,intricate(2)
0000-0002-6282-7682,Stephen Smith,"MONASH UNIVERSITY, Monash University",Stochastic Control Barrier Functions with Bayesian Inference for Unknown   Stochastic Differential Equations,1970,"  Control barrier functions are widely used to synthesize safety-critical controls. However, the presence of Gaussian-type noise in dynamical systems can generate unbounded signals and potentially result in severe consequences. Although research has been conducted in the field of safety-critical control for stochastic systems, in many real-world scenarios, we do not have precise knowledge about the stochastic dynamics. In this paper, we delve into the safety-critical control for stochastic systems where both the drift and diffusion components are unknown. We employ Bayesian inference as a data-driven approach to approximate the system. To be more specific, we utilize Bayesian linear regression along with the central limit theorem to estimate the drift term, and employ Bayesian inference to approximate the diffusion term. Through simulations, we verify our findings by applying them to a nonlinear dynamical model and an adaptive cruise control model. ",Kein DOI-Link verfügbar,2312.12759v1,Yes,potent(1)
0000-0002-6282-7682,Stephen Smith,"MONASH UNIVERSITY, Monash University",Longitudinal Compliance Analysis of Android Applications with Privacy   Policies,1970,"  Contemporary mobile applications (apps) are designed to track, use, and share users' data, often without their consent, which results in potential privacy and transparency issues. To investigate whether mobile apps have always been (non-)transparent regarding how they collect information about users, we perform a longitudinal analysis of the historical versions of 268 Android apps. These apps comprise 5,240 app releases or versions between 2008 and 2016. We detect inconsistencies between apps' behaviors and the stated use of data collection in privacy policies to reveal compliance issues. We utilize machine learning techniques for the classification of the privacy policy text to identify the purported practices that collect and/or share users' personal information, such as phone numbers and email addresses. We then uncover the data leaks of an app through static and dynamic analysis. Over time, our results show a steady increase in the number of apps' data collection practices that are undisclosed in the privacy policies. This behavior is particularly troubling since privacy policy is the primary tool for describing the app's privacy protection practices. We find that newer versions of the apps are likely to be more non-compliant than their preceding versions. The discrepancies between the purported and the actual data practices show that privacy policies are often incoherent with the apps' behaviors, thus defying the 'notice and choice' principle when users install apps. ",Kein DOI-Link verfügbar,2106.10035v2,Yes,potent(1)
0000-0002-6282-7682,Stephen Smith,"MONASH UNIVERSITY, Monash University",Biologically-plausible backpropagation through arbitrary timespans via   local neuromodulators,1970,"  The spectacular successes of recurrent neural network models where key parameters are adjusted via backpropagation-based gradient descent have inspired much thought as to how biological neuronal networks might solve the corresponding synaptic credit assignment problem. There is so far little agreement, however, as to how biological networks could implement the necessary backpropagation through time, given widely recognized constraints of biological synaptic network signaling architectures. Here, we propose that extra-synaptic diffusion of local neuromodulators such as neuropeptides may afford an effective mode of backpropagation lying within the bounds of biological plausibility. Going beyond existing temporal truncation-based gradient approximations, our approximate gradient-based update rule, ModProp, propagates credit information through arbitrary time steps. ModProp suggests that modulatory signals can act on receiving cells by convolving their eligibility traces via causal, time-invariant and synapse-type-specific filter taps. Our mathematical analysis of ModProp learning, together with simulation results on benchmark temporal tasks, demonstrate the advantage of ModProp over existing biologically-plausible temporal credit assignment rules. These results suggest a potential neuronal mechanism for signaling credit information related to recurrent interactions over a longer time horizon. Finally, we derive an in-silico implementation of ModProp that could serve as a low-complexity and causal alternative to backpropagation through time. ",Kein DOI-Link verfügbar,2206.01338v4,Yes,potent(1)
0000-0003-4466-2447,Frits de Nijs,"MONASH UNIVERSITY, Monash University",Model-Free Approach to Fair Solar PV Curtailment Using Reinforcement   Learning,1970,"  The rapid adoption of residential solar photovoltaics (PV) has resulted in regular overvoltage events, due to correlated reverse power flows. Currently, PV inverters prevent damage to electronics by curtailing energy production in response to overvoltage. However, this disproportionately affects households at the far end of the feeder, leading to an unfair allocation of the potential value of energy produced. Globally optimizing for fair curtailment requires accurate feeder parameters, which are often unknown. This paper investigates reinforcement learning, which gradually optimizes a fair PV curtailment strategy by interacting with the system. We evaluate six fairness metrics on how well they can be learned compared to an optimal solution oracle. We show that all definitions permit efficient learning, suggesting that reinforcement learning is a promising approach to achieving both safe and fair PV coordination. ",https://doi.org/10.1145/3575813.3576871,2212.06542v1,Yes,potent(1)
0000-0003-0718-6760,Nellie Georgiou-Karistianis,"MONASH UNIVERSITY, Monash University",Large-scale comparative visualisation of sets of multidimensional data,1970,"  We present encube $-$ a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: 1) the integration of comparative visualisation and analysis into a unified system; 2) the documentation of the discovery process; and 3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios. ",https://doi.org/10.7717/peerj-cs.88,1610.00760v1,Yes,versatile(1)
0000-0001-8866-3041,David Wood,"MONASH UNIVERSITY, Monash University",Letter to the Editor: What are the legal and ethical considerations of   submitting radiology reports to ChatGPT?,1970,"  This letter critically examines the recent article by Infante et al. assessing the utility of large language models (LLMs) like GPT-4, Perplexity, and Bard in identifying urgent findings in emergency radiology reports. While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR. ",Kein DOI-Link verfügbar,2405.05647v1,Yes,potent(1)
0000-0001-8866-3041,David Wood,"MONASH UNIVERSITY, Monash University",EdgeSphere: A Three-Tier Architecture for Cognitive Edge Computing,1970,"  Computing at the edge is increasingly important as Internet of Things (IoT) devices at the edge generate massive amounts of data and pose challenges in transporting all that data to the Cloud where they can be analyzed. On the other hand, harnessing the edge data is essential for offering cognitive applications, if the challenges, such as device capabilities, connectivity, and heterogeneity can be overcome. This paper proposes a novel three-tier architecture, called EdgeSphere, which harnesses resources of the edge devices, to analyze the data in situ at the edge. In contrast to the state-of-the-art cloud and mobile applications, EdgeSphere applications span across cloud, edge gateways, and edge devices. At its core, EdgeSphere builds on Apache Mesos to optimize resources usage and scheduling. EdgeSphere has been applied to practical scenarios and this paper describes the engineering challenges faced as well as innovative solutions. ",Kein DOI-Link verfügbar,2405.16685v1,Yes,innovative(1)
0000-0001-8866-3041,David Wood,"MONASH UNIVERSITY, Monash University","Thiol-ene photo-click collagen-PEG hydrogels: impact of water-soluble   photoinitiators on cell viability, gelation kinetics and rheological   properties",1970,"  Thiol-ene photo-click hydrogels were prepared via step-growth polymerisation using thiol-functionalised type-I collagen and 8-arm poly(ethylene glycol) norbornene terminated (PEG-NB), as a potential injectable regenerative device. Type-I collagen was thiol-functionalised by a ring opening reaction with 2-iminothiolane (2IT), whereby up to 80 Abs% functionalisation and 90 RPN% triple helical preservation were recorded via 2,4,6-Trinitrobenzenesulfonic acid (TNBS) colorimetric assay and circular dichroism (CD). Type, i.e. 2-Hydroxy-1-[4-(2-hydroxyethoxy)phenyl]-2-methyl-1-propanone (I2959) or lithium phenyl-2,4,6-trimethylbenzoylphosphinate (LAP), and concentration of photoinitiator were varied to ensure minimal photoinitiator-induced cytotoxicity and to enable thiol-ene network formation of collagen-PEG mixtures. The viability of G292 cells following 24 h culture in photoinitiator-supplemented media was largely affected by the photoinitiator concentration, with I2959-supplemented media observed to induce higher toxic response compared to LAP-supplemented media. In line with the in vitro study, selected photoinitiator concentrations were used to prepare thiol-ene photo-click hydrogels. Gelation kinetics proved to be largely affected by the specific photoinitiator, with LAP-containing thiol-ene mixtures leading to significantly reduced complete gelation time with respect to I2959-containing mixtures. Other than the specific photoinitiator, the photoinitiator concentration was key to adjust hydrogel storage modulus (G'), whereby 15-fold G' increase was observed in samples prepared with 0.5% (w/v) compared to 0.1% (w/v) LAP. The photoinitiator-gelation-cytotoxicity relationships established in this study will be instrumental to the design of orthogonal collagen-based niches for regenerative medicine. ",Kein DOI-Link verfügbar,1706.03043v1,Yes,potent(1)
0000-0001-8866-3041,David Wood,"MONASH UNIVERSITY, Monash University",Influence of telopeptides on the structural and physical properties of   polymeric and monomeric acid-soluble type I collagen,1970,"  Currently two factors hinder the use of collagen as building block of regenerative devices: the potential antigenicity, and limited mechanical strength in aqueous environment. Polymeric collagen is naturally found in the cross-linked state and is mechanically tougher than the monomeric, cross-link-free (acid-soluble) collagen ex vivo. The antigenicity of collagen, on the other hand, is mainly ascribed to inter-species variations in amino acid sequences, which are primarily located in the non-helical terminal telopeptides. Although these can be removed through enzymatic treatment to produce atelocollagen, the effect of telopeptide removal on triple helix organization, amino acidic composition and thermal properties is often disregarded. Here, we compare the structural, chemical and physical properties of polymeric and monomeric type I collagen with and without telopeptides, in an effort to elucidate the influence that either covalent crosslinks or telopeptides possess. Circular dichroism (CD) was used to examine the triple helical conformation and quantify the denaturation temperature (Td) of both monomeric collagen (36.5 {\deg}C) and monomeric atelocollagen (35.5 {\deg}C). CD measurements were combined with differential scanning calorimetry (DSC) in order to gain insight into the triple helix-to-coil thermal transition and shrinkage temperature (Ts) of polymeric atelo collagen (44.8 {\deg}C), polymeric collagen (62.7 {\deg}C), monomeric atelo collagen (51.4 {\deg}C) and monomeric collagen (66.5 {\deg}C). Structural and thermal analyses were combined with high pressure liquid chromatography (HPLC) to determine the content of specific collagen amino acidic residues used as markers for the presence of telopeptides and mature crosslinks. ",https://doi.org/10.1016/j.msec.2017.03.267,1706.03080v1,Yes,potent(1)
0000-0002-5516-9984,Chakkrit Tantithamthavorn,"MONASH UNIVERSITY, Monash University","Assessing the Students' Understanding and their Mistakes in Code Review   Checklists -- An Experience Report of 1,791 Code Review Checklist Questions   from 394 Students",1970,"  Code review is a widely-used practice in software development companies to identify defects. Hence, code review has been included in many software engineering curricula at universities worldwide. However, teaching code review is still a challenging task because the code review effectiveness depends on the code reading and analytical skills of a reviewer. While several studies have investigated the code reading techniques that students should use to find defects during code review, little has focused on a learning activity that involves analytical skills. Indeed, developing a code review checklist should stimulate students to develop their analytical skills to anticipate potential issues (i.e., software defects). Yet, it is unclear whether students can anticipate potential issues given their limited experience in software development (programming, testing, etc.). We perform a qualitative analysis to investigate whether students are capable of creating code review checklists, and if the checklists can be used to guide reviewers to find defects. In addition, we identify common mistakes that students make when developing a code review checklist. Our results show that while there are some misconceptions among students about the purpose of code review, students are able to anticipate potential defects and create a relatively good code review checklist. Hence, our results lead us to conclude that developing a code review checklist can be a part of the learning activities for code review in order to scaffold students' skills. ",Kein DOI-Link verfügbar,2101.04837v1,Yes,potent(3)
0000-0002-5516-9984,Chakkrit Tantithamthavorn,"MONASH UNIVERSITY, Monash University",Learning to Quantize Vulnerability Patterns and Match to Locate   Statement-Level Vulnerabilities,1970,"  Deep learning (DL) models have become increasingly popular in identifying software vulnerabilities. Prior studies found that vulnerabilities across different vulnerable programs may exhibit similar vulnerable scopes, implicitly forming discernible vulnerability patterns that can be learned by DL models through supervised training. However, vulnerable scopes still manifest in various spatial locations and formats within a program, posing challenges for models to accurately identify vulnerable statements. Despite this challenge, state-of-the-art vulnerability detection approaches fail to exploit the vulnerability patterns that arise in vulnerable programs. To take full advantage of vulnerability patterns and unleash the ability of DL models, we propose a novel vulnerability-matching approach in this paper, drawing inspiration from program analysis tools that locate vulnerabilities based on pre-defined patterns. Specifically, a vulnerability codebook is learned, which consists of quantized vectors representing various vulnerability patterns. During inference, the codebook is iterated to match all learned patterns and predict the presence of potential vulnerabilities within a given program. Our approach was extensively evaluated on a real-world dataset comprising more than 188,000 C/C++ functions. The evaluation results show that our approach achieves an F1-score of 94% (6% higher than the previous best) and 82% (19% higher than the previous best) for function and statement-level vulnerability identification, respectively. These substantial enhancements highlight the effectiveness of our approach to identifying vulnerabilities. The training code and pre-trained models are available at https://github.com/optimatch/optimatch. ",Kein DOI-Link verfügbar,2306.06109v1,Yes,potent(1)
0000-0002-5516-9984,Chakkrit Tantithamthavorn,"MONASH UNIVERSITY, Monash University",Refining ChatGPT-Generated Code: Characterizing and Mitigating Code   Quality Issues,1970,"  We systematically study the quality of 4,066 ChatGPT-generated code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is three folds. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT's self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of AI models like ChatGPT. ",Kein DOI-Link verfügbar,2307.12596v2,Yes,potent(1)
0000-0002-5516-9984,Chakkrit Tantithamthavorn,"MONASH UNIVERSITY, Monash University",Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey,1970,"  Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, implications, current solutions, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways. ",Kein DOI-Link verfügbar,2310.17903v1,Yes,potent(2)
0000-0002-5516-9984,Chakkrit Tantithamthavorn,"MONASH UNIVERSITY, Monash University",A Systematic Literature Review of Explainable AI for Software   Engineering,1970,"  Context: In recent years, leveraging machine learning (ML) techniques has become one of the main solutions to tackle many software engineering (SE) tasks, in research studies (ML4SE). This has been achieved by utilizing state-of-the-art models that tend to be more complex and black-box, which is led to less explainable solutions that reduce trust and uptake of ML4SE solutions by professionals in the industry.   Objective: One potential remedy is to offer explainable AI (XAI) methods to provide the missing explainability. In this paper, we aim to explore to what extent XAI has been studied in the SE community (XAI4SE) and provide a comprehensive view of the current state-of-the-art as well as challenge and roadmap for future work.   Method: We conduct a systematic literature review on 24 (out of 869 primary studies that were selected by keyword search) most relevant published studies in XAI4SE. We have three research questions that were answered by meta-analysis of the collected data per paper.   Results: Our study reveals that among the identified studies, software maintenance (\%68) and particularly defect prediction has the highest share on the SE stages and tasks being studied. Additionally, we found that XAI methods were mainly applied to classic ML models rather than more complex models. We also noticed a clear lack of standard evaluation metrics for XAI methods in the literature which has caused confusion among researchers and a lack of benchmarks for comparisons.   Conclusions: XAI has been identified as a helpful tool by most studies, which we cover in the systematic review. However, XAI4SE is a relatively new domain with a lot of untouched potentials, including the SE tasks to help with, the ML4SE methods to explain, and the types of explanations to offer. This study encourages the researchers to work on the identified challenges and roadmap reported in the paper. ",Kein DOI-Link verfügbar,2302.06065v1,Yes,potent(2)
0000-0001-7001-5011,Thanh Pham,"MONASH UNIVERSITY, Monash University",AI-assisted Learning for Electronic Engineering Courses in High   Education,1970,"  This study evaluates the efficacy of ChatGPT as an AI teaching and learning support tool in an integrated circuit systems course at a higher education institution in an Asian country. Various question types were completed, and ChatGPT responses were assessed to gain valuable insights for further investigation. The objective is to assess ChatGPT's ability to provide insights, personalized support, and interactive learning experiences in engineering education. The study includes the evaluation and reflection of different stakeholders: students, lecturers, and engineers. The findings of this study shed light on the benefits and limitations of ChatGPT as an AI tool, paving the way for innovative learning approaches in technical disciplines. Furthermore, the study contributes to our understanding of how digital transformation is likely to unfold in the education sector. ",Kein DOI-Link verfügbar,2311.01048v1,Yes,innovative(1)
0000-0001-7001-5011,Thanh Pham,"MONASH UNIVERSITY, Monash University",Multi-dimensional data refining strategy for effective fine-tuning LLMs,1970,"  Data is a cornerstone for fine-tuning large language models, yet acquiring suitable data remains challenging. Challenges encompassed data scarcity, linguistic diversity, and domain-specific content. This paper presents lessons learned while crawling and refining data tailored for fine-tuning Vietnamese language models. Crafting such a dataset, while accounting for linguistic intricacies and striking a balance between inclusivity and accuracy, demands meticulous planning. Our paper presents a multidimensional strategy including leveraging existing datasets in the English language and developing customized data-crawling scripts with the assistance of generative AI tools. A fine-tuned LLM model for the Vietnamese language, which was produced using resultant datasets, demonstrated good performance while generating Vietnamese news articles from prompts. The study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese. ",Kein DOI-Link verfügbar,2311.01049v1,Yes,meticulous(1)
0000-0002-5234-461X,Hongzhi Zhang,"MONASH UNIVERSITY, Monash University",Joint Video Multi-Frame Interpolation and Deblurring under Unknown   Exposure Time,1970,"  Natural videos captured by consumer cameras often suffer from low framerate and motion blur due to the combination of dynamic scene complexity, lens and sensor imperfection, and less than ideal exposure setting. As a result, computational methods that jointly perform video frame interpolation and deblurring begin to emerge with the unrealistic assumption that the exposure time is known and fixed. In this work, we aim ambitiously for a more realistic and challenging task - joint video multi-frame interpolation and deblurring under unknown exposure time. Toward this goal, we first adopt a variant of supervised contrastive learning to construct an exposure-aware representation from input blurred frames. We then train two U-Nets for intra-motion and inter-motion analysis, respectively, adapting to the learned exposure representation via gain tuning. We finally build our video reconstruction network upon the exposure and motion representation by progressive exposure-adaptive convolution and motion refinement. Extensive experiments on both simulated and real-world datasets show that our optimized method achieves notable performance gains over the state-of-the-art on the joint video x8 interpolation and deblurring task. Moreover, on the seemingly implausible x16 interpolation task, our method outperforms existing methods by more than 1.5 dB in terms of PSNR. ",Kein DOI-Link verfügbar,2303.15043v1,Yes,notable(1)
0000-0002-5234-461X,Hongzhi Zhang,"MONASH UNIVERSITY, Monash University",FewFedPIT: Towards Privacy-preserving and Few-shot Federated Instruction   Tuning,1970,"  Instruction tuning has been identified as a crucial technique for optimizing the performance of large language models (LLMs) in generating human-aligned responses. Nonetheless, gathering diversified and superior-quality instruction data for such tuning presents notable obstacles, especially in domains with rigid privacy provisions. Federated instruction tuning (FedIT) has emerged as a promising solution, by consolidating collaborative training across multiple data owners, thereby resulting in a privacy-preserving learning model. However, FedIT encounters limitations such as scarcity of instructional data and risk of exposure to training data extraction attacks. In this paper, we propose a novel federated algorithm, FewFedPIT, designed to simultaneously enhance privacy protection and model performance of federated few-shot learning. FewFedPITcomprises three vital components on the client side: (1) synthetic data generation, which utilizes LLMs' in-context learning capacity to generate synthetic data autonomously, thus expanding the local database; (2) parameter isolation training, which individually updates the public parameters in the synthetic data and the private parameters in the local data, consequently mitigating the noise impact of the synthetic data; (3) local aggregation sharing, which mixes public and private parameters before uploading, effectively preventing data extraction attacks. Extensive experiments on three open-source datasets demonstrate the effectiveness of FewFedPITin, enhancing privacy preservation and improving federated few-shot performance. ",Kein DOI-Link verfügbar,2403.06131v2,Yes,notable(1)
0000-0003-3726-2103,Callum Atkinson,"MONASH UNIVERSITY, Monash University",Direct numerical simulation of a thermal turbulent boundary layer: an   analogy to simulate bushfires and a testbed for artificial intelligence   remote sensing of bushfire propagation,1970,"  Direct numerical simulation of a turbulent thermal boundary layer (TTBL) can perform the role of an analogy to simulate bushfires that can serve as a testbed for artificial intelligence (AI) enhanced remote sensing of bushfire propagation. By solving the Navier-Stokes equations for a turbulent flow, DNS predicts the flow field and allows for a detailed study of the interactions between the turbulent flow and thermal plumes. In addition to potentially providing insights into the complex bushfire behaviour, direct numerical simulation (DNS) can generate synthetic remote sensing data to train AI algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which can process large amounts of remotely sensed data associated with bushfire. Using the results of DNS as training data can improve the accuracy of AI remote sensing in predicting firefront propagation of bushfires. DNS can also test the accuracy of the AI remote sensing algorithms by generating synthetic remote sensing data that allows their performance assessment and uncertainty quantification in predicting the evolution of a bushfire. The combination of DNS and AI can improve our understanding of bushfire dynamics, develop more accurate prediction models, and aid in bushfire management and mitigation. ",Kein DOI-Link verfügbar,2402.08157v1,Yes,potent(1)
0000-0001-6984-2060,Hai L. Vu,"MONASH UNIVERSITY, Monash University",A novel framework for adaptive stress testing of autonomous vehicles in   highways,1970,"  Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway. The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway. In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework. Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes. The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology. ",Kein DOI-Link verfügbar,2402.11813v1,Yes,potent(1)
0000-0001-6984-2060,Hai L. Vu,"MONASH UNIVERSITY, Monash University",Testing Ocean Software with Metamorphic Testing,1970,"  Advancing ocean science has a significant impact to the development of the world, from operating a safe navigation for vessels to maintaining a healthy and diverse ocean ecosystem. Various ocean software systems have been extensively adopted for different purposes, for instance, predicting hourly sea level elevation across shorelines, simulating large-scale ocean circulations, as well as integrating into Earth system models for weather forecasts and climate projections. Regardless of their significance, guaranteeing the trustworthiness of ocean software and modelling systems is a long-standing challenge. The testing of ocean software suffers a lot from the so-called oracle problem, which refers to the absence of test oracles mainly due to the nonlinear interactions of multiple physical variables and the high complexity in computation. In the ocean, observed tidal signals are distorted by non-deterministic physical variables, hindering us from knowing the ""true"" astronomical tidal constituents existing in the timeseries. In this paper, we present how to test tidal analysis and prediction (TAP) software based on metamorphic testing (MT), a simple yet effective testing approach to the oracle problem. In particular, we construct metamorphic relations from the periodic property of astronomical tide, and then use them to successfully detect a real-life defect in an open-source TAP software. We also conduct a series of experiments to further demonstrate the applicability and effectiveness of MT in the testing of TAP software. Our study not only justifies the potential of MT in testing more complex ocean software and modelling systems, but also can be expanded to assess and improve the quality of a broader range of scientific simulation software systems. ",https://doi.org/10.1145/3524846.3527341,2206.05457v1,Yes,potent(1)
0000-0001-6984-2060,Hai L. Vu,"MONASH UNIVERSITY, Monash University",A hybrid neural network for real-time OD demand calibration under   disruptions,1970,"  Existing automated urban traffic management systems, designed to mitigate traffic congestion and reduce emissions in real time, face significant challenges in effectively adapting to rapidly evolving conditions. Predominantly reactive, these systems typically respond to incidents only after they have transpired. A promising solution lies in implementing real-time traffic simulation models capable of accurately modelling environmental changes. Central to these real-time traffic simulations are origin-destination (OD) demand matrices. However, the inherent variability, stochasticity, and unpredictability of traffic demand complicate the precise calibration of these matrices in the face of disruptions. This paper introduces a hybrid neural network (NN) architecture specifically designed for real-time OD demand calibration to enhance traffic simulations' accuracy and reliability under both recurrent and non-recurrent traffic conditions. The proposed hybrid NN predicts the OD demand to reconcile the discrepancies between actual and simulated traffic patterns. To facilitate real-time updating of the internal parameters of the NN, we develop a metamodel-based backpropagation method by integrating data from real-world traffic systems and simulated environments. This ensures precise predictions of the OD demand even in the case of abnormal or unpredictable traffic patterns. Furthermore, we incorporate offline pre-training of the NN using the metamodel to improve computational efficiency. Validation through a toy network and a Tokyo expressway corridor case study illustrates the model's ability to dynamically adjust to shifting traffic patterns across various disruption scenarios. Our findings underscore the potential of advanced machine learning techniques in developing proactive traffic management strategies, offering substantial improvements over traditional reactive systems. ",Kein DOI-Link verfügbar,2408.06659v1,Yes,potent(1)
0000-0002-9443-0671,Michael Burgess,"University of British Columbia, University of British Columbia Okanagan",Temporal Deconvolution study of Long and Short Gamma-Ray Burst Light   curves,1970,"  The light curves of Gamma-Ray Bursts (GRBs) are believed to result from internal shocks reflecting the activity of the GRB central engine. Their temporal deconvolution can reveal potential differences in the properties of the central engines in the two populations of GRBs which are believed to originate from the deaths of massive stars (long) and from mergers of compact objects (short). We present here the results of the temporal analysis of 42 GRBs detected with the Gamma-ray Burst Monitor onboard the Fermi Gamma-ray Space Telescope. We deconvolved the profiles into pulses, which we fit with lognormal functions. The distributions of the pulse shape parameters and intervals between neighboring pulses are distinct for both burst types and also fit with lognormal functions. We have studied the evolution of these parameters in different energy bands and found that they differ between long and short bursts. We discuss the implications of the differences in the temporal properties of long and short bursts within the framework of the internal shock model for GRB prompt emission. ",https://doi.org/10.1088/0004-637X/744/2/141,1109.4064v1,Yes,potent(1)
0000-0001-5252-7005,Yuheng Wang,"The University of British Columbia, University of British Columbia",AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical   Knowledge Graphs and Data-Driven Quantification,1970,"  The 7-point checklist (7PCL) is widely used in dermoscopy to identify malignant melanoma lesions needing urgent medical attention. It assigns point values to seven attributes: major attributes are worth two points each, and minor ones are worth one point each. A total score of three or higher prompts further evaluation, often including a biopsy. However, a significant limitation of current methods is the uniform weighting of attributes, which leads to imprecision and neglects their interconnections. Previous deep learning studies have treated the prediction of each attribute with the same importance as predicting melanoma, which fails to recognize the clinical significance of the attributes for melanoma. To address these limitations, we introduce a novel diagnostic method that integrates two innovative elements: a Clinical Knowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy with Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL attributes with diagnostic information, revealing both internal and external associations. By employing adaptive receptive domains and weighted edges, we establish connections among melanoma's relevant features. Concurrently, GD-DDW emulates dermatologists' diagnostic processes, who first observe the visual characteristics associated with melanoma and then make predictions. Our model uses two imaging modalities for the same lesion, ensuring comprehensive feature acquisition. Our method shows outstanding performance in predicting malignant melanoma and its features, achieving an average AUC value of 85%. This was validated on the EDRA dataset, the largest publicly available dataset for the 7-point checklist algorithm. Specifically, the integrated weighting system can provide clinicians with valuable data-driven benchmarks for their evaluations. ",Kein DOI-Link verfügbar,2407.16822v1,Yes,innovative(1)
0000-0001-5252-7005,Yuheng Wang,"The University of British Columbia, University of British Columbia",Artwork Protection Against Neural Style Transfer Using Locally Adaptive   Adversarial Color Attack,1970,"  Neural style transfer (NST) generates new images by combining the style of one image with the content of another. However, unauthorized NST can exploit artwork, raising concerns about artists' rights and motivating the development of proactive protection methods. We propose Locally Adaptive Adversarial Color Attack (LAACA), empowering artists to protect their artwork from unauthorized style transfer by processing before public release. By delving into the intricacies of human visual perception and the role of different frequency components, our method strategically introduces frequency-adaptive perturbations in the image. These perturbations significantly degrade the generation quality of NST while maintaining an acceptable level of visual change in the original image, ensuring that potential infringers are discouraged from using the protected artworks, because of its bad NST generation quality. Additionally, existing metrics often overlook the importance of color fidelity in evaluating color-mattered tasks, such as the quality of NST-generated images, which is crucial in the context of artistic works. To comprehensively assess the color-mattered tasks, we propose the Adversarial Color Distance Metric (ACDM), designed to quantify the color difference of images pre- and post-manipulations. Experimental results confirm that attacking NST using LAACA results in visually inferior style transfer, and the ACDM can efficiently measure color-mattered tasks. By providing artists with a tool to safeguard their intellectual property, our work relieves the socio-technical challenges posed by the misuse of NST in the art community. ",Kein DOI-Link verfügbar,2401.09673v3,Yes,"potent(1), strategically(1)"
0000-0003-3043-9236,Varun Nair,"Faculty of Medicine, University of British Columbia, University of British Columbia",RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms,1970,"  Semi-Supervised Learning (SSL) algorithms have shown great potential in training regimes when access to labeled data is scarce but access to unlabeled data is plentiful. However, our experiments illustrate several shortcomings that prior SSL algorithms suffer from. In particular, poor performance when unlabeled and labeled data distributions differ. To address these observations, we develop RealMix, which achieves state-of-the-art results on standard benchmark datasets across different labeled and unlabeled set sizes while overcoming the aforementioned challenges. Notably, RealMix achieves an error rate of 9.79% on CIFAR10 with 250 labels and is the only SSL method tested able to surpass baseline performance when there is significant mismatch in the labeled and unlabeled data distributions. RealMix demonstrates how SSL can be used in real world situations with limited access to both data and compute and guides further research in SSL with practical applicability in mind. ",Kein DOI-Link verfügbar,1912.08766v1,Yes,potent(1)
0000-0003-3043-9236,Varun Nair,"Faculty of Medicine, University of British Columbia, University of British Columbia",Extrinsically-Focused Evaluation of Omissions in Medical Summarization,1970,"  The goal of automated summarization techniques (Paice, 1990; Kupiec et al, 1995) is to condense text by focusing on the most critical information. Generative large language models (LLMs) have shown to be robust summarizers, yet traditional metrics struggle to capture resulting performance (Goyal et al, 2022) in more powerful LLMs. In safety-critical domains such as medicine, more rigorous evaluation is required, especially given the potential for LLMs to omit important information in the resulting summary. We propose MED-OMIT, a new omission benchmark for medical summarization. Given a doctor-patient conversation and a generated summary, MED-OMIT categorizes the chat into a set of facts and identifies which are omitted from the summary. We further propose to determine fact importance by simulating the impact of each fact on a downstream clinical task: differential diagnosis (DDx) generation. MED-OMIT leverages LLM prompt-based approaches which categorize the importance of facts and cluster them as supporting or negating evidence to the diagnosis. We evaluate MED-OMIT on a publicly-released dataset of patient-doctor conversations and find that MED-OMIT captures omissions better than alternative metrics. ",Kein DOI-Link verfügbar,2311.08303v1,Yes,potent(1)
0000-0003-3043-9236,Varun Nair,"Faculty of Medicine, University of British Columbia, University of British Columbia",Quantitative analysis of collagen remodeling in pancreatic lesions using   computationally translated collagen images derived from brightfield   microscopy images,1970,"  The changes in stromal collagen play a crucial role during the pathogenesis and progression of pancreatic intraepithelial neoplasm (PanIN) to pancreatic ductal adenocarcinoma (PDAC) while misdiagnosis of PanIN is common because of the resemblance to chronic pancreatitis (CP) in its symptoms and subsequent evaluations similarities. To visualize fibrillar collagen in tissues, second harmonic generation microscopy is now utilized as a gold standard in various stromal-based research analyses. However, a technical approach that can perform a quantitative analysis of fibrillar collagen directly on standard slides stained with H&E can (i) discard the need for specialized and costly equipment or labels, (ii) further supplement the conventional histopathological insights and, (iii) potentially be integrated within the framework of standard histopathology workflow. In this study, the whole-core brightfield H&E-stained images of pancreatic tissues were translated computationally into the new collagen images. Subsequently, collagen characteristics of PDAC, PanIN, CP, and normal pancreatic tissues (control) were extracted and compared. The highest alignment (p < 0.01, R2 = 0.2594) was observed in PDAC cores in comparison to the remaining three groups, while the lowest fiber density (p < 0.0001, R2 = 0.3569) was observed in case of normal tissue cores. Moreover, the collagen area and fiber length had shown higher area under curve (0.83 and 0.81, respectively) in discriminating neoplastic and non-neoplastic tissues based on their receiver operating characteristics. The study demonstrated that the computationally generated collagen images can provide a quantitative assessment of collagen remodeling in pancreatic lesions. The cross-modality image synthesis may further lead towards better histopathological and tissue microenvironment insights without the need of specialized imaging equipment or labels. ",Kein DOI-Link verfügbar,2304.12725v1,Yes,potent(1)
0000-0003-3655-872X,Zekuan Yu,Fudan University,Open-Access Data and Toolbox for Tracking COVID-19 Impact on Power   Systems,1970,"  Intervention policies against COVID-19 have caused large-scale disruptions globally, and led to a series of pattern changes in the power system operation. Analyzing these pandemic-induced patterns is imperative to identify the potential risks and impacts of this extreme event. With this purpose, we developed an open-access data hub (COVID-EMDA+), an open-source toolbox (CoVEMDA), and a few evaluation methods to explore what the U.S. power systems are experiencing during COVID-19. These resources could be broadly used for research, public policy, and educational purposes. Technically, our data hub harmonizes a variety of raw data such as generation mix, demand profiles, electricity price, weather observations, mobility, confirmed cases and deaths. Typical methods are reformulated and standardized in our toolbox, including baseline estimation, regression analysis, and scientific visualization. Here the fluctuation index and probabilistic baseline are proposed for the first time to consider data fluctuation and estimation uncertainty. Based on these, we conduct three empirical studies on the U.S. power systems, and share new solutions and unexpected findings to address the issues of public concerns. This conveys a more complete picture of the pandemic's impacts, and also opens up several attractive topics for future work. Python, Matlab source codes, and user manuals are all publicly shared on a Github repository. ",Kein DOI-Link verfügbar,2112.05320v2,Yes,potent(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",Transcription Factor-DNA Binding Via Machine Learning Ensembles,1970,"  We present ensemble methods in a machine learning (ML) framework combining predictions from five known motif/binding site exploration algorithms. For a given TF the ensemble starts with position weight matrices (PWM's) for the motif, collected from the component algorithms. Using dimension reduction, we identify significant PWM-based subspaces for analysis. Within each subspace a machine classifier is built for identifying the TF's gene (promoter) targets (Problem 1). These PWM-based subspaces form an ML-based sequence analysis tool. Problem 2 (finding binding motifs) is solved by agglomerating k-mer (string) feature PWM-based subspaces that stand out in identifying gene targets. We approach Problem 3 (binding sites) with a novel machine learning approach that uses promoter string features and ML importance scores in a classification algorithm locating binding sites across the genome. For target gene identification this method improves performance (measured by the F1 score) by about 10 percentage points over the (a) motif scanning method and (b) the coexpression-based association method. Top motif outperformed 5 component algorithms as well as two other common algorithms (BEST and DEME). For identifying individual binding sites on a benchmark cross species database (Tompa et al., 2005) we match the best performer without much human intervention. It also improved the performance on mammalian TFs.   The ensemble can integrate orthogonal information from different weak learners (potentially using entirely different types of features) into a machine learner that can perform consistently better for more TFs. The TF gene target identification component (problem 1 above) is useful in constructing a transcriptional regulatory network from known TF-target associations. The ensemble is easily extendable to include more tools as well as future PWM-based information. ",Kein DOI-Link verfügbar,1805.03771v1,Yes,potent(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",R2H: Building Multimodal Navigation Helpers that Respond to Help   Requests,1970,"  Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent's ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations. Project website: https://sites.google.com/view/response2helprequests/home. ",Kein DOI-Link verfügbar,2305.14260v2,Yes,potent(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",Soft mode parameter as an indicator for the activation energy spectra in   metallic glass,1970,"  The activation energy (E_A) spectra of potential energy landscape (PEL) provides a convenient perspective for interpreting complex phenomena in amorphous materials; however, the link between the E_A spectra and other physical properties in metallic glasses is still mysterious. By systematically probing the E_A spectra for numerous metallic glass samples with distinct local geometric ordering, which correspond to broad processing histories, it is found that the shear modulus of the samples are strongly correlated with the arithmetic mean of the E_A spectra rather than with the local geometrical ordering. Furthermore, we studied the correlation of the obtained E_A spectra and various well-established physical parameters. The outcome of our research clearly demonstrates that the soft mode parameter {\Psi} and the E_A spectrum are correlated; therefore, it could be a good indicator of metallic glass properties and sheds important light on the structure-property relationship in metallic glass through the medium of PEL. ",Kein DOI-Link verfügbar,2002.09630v1,Yes,potent(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian   Splatting,1970,"  Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, with wide-ranging applications in embodied agents and augmented reality systems. Existing methods adopt neurel rendering methods as 3D representations and jointly optimize color and semantic features to achieve rendering and scene understanding simultaneously. In this paper, we introduce Semantic Gaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our key idea is to distill knowledge from 2D pre-trained models to 3D Gaussians. Unlike existing methods, we design a versatile projection approach that maps various 2D semantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, which is based on spatial relationship and need no additional training. We further build a 3D semantic network that directly predicts the semantic component from raw 3D Gaussians for fast inference. The quantitative results on ScanNet segmentation and LERF object localization demonstates the superior performance of our method. Additionally, we explore several applications of Semantic Gaussians including object part segmentation, instance segmentation, scene editing, and spatiotemporal segmentation with better qualitative results over 2D and 3D baselines, highlighting its versatility and effectiveness on supporting diverse downstream tasks. ",Kein DOI-Link verfügbar,2403.15624v2,Yes,versatile(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey,1970,"  Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning. ",Kein DOI-Link verfügbar,2402.02242v2,Yes,potent(2)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",Structural origin of plasticity in strained high-entropy alloy,1970,"  High-entropy alloys (HEAs) are solid solutions of multiple elements with equal atomic ratios which present an innovative pathway for de novo alloy engineering. While there exist extensive studies to ascertain the important structural aspects governing their mechanical behaviors, elucidating the underlying deformation mechanisms still remains a challenge. Using atomistic simulations, we probe the particle rearrangements in a yielding, model HEA system to understand the structural origin of its plasticity. We find the plastic deformation is initiated by irreversible topological fluctuations which tend to spatially localize in regions termed as soft spots which consist of particles actively participating in slow vibrational motions, an observation strikingly reminiscent of nonlinear glassy rheology. Due to the varying local elastic moduli resulting from the loss of compositional periodicity, these plastic responses exhibit significant spatial heterogeneity and are found to be inversely correlated with the distribution of local electronegativity. Further mechanical loading promotes the cooperativity among these local plastic events and triggers the formation of dislocation loops. As in strained crystalline solids, different dislocation loops can further merge together and propagate as the main carrier of large-scale plastic deformation. However, the energy barriers located at the spatial regions with higher local electronegativity severely hinders the motion of dislocations. By delineating the transient mechanical response in terms of atomic configuration, our computational findings shed new light on understanding the nature of plasticity of single-phase HEA. ",Kein DOI-Link verfügbar,2005.07088v1,Yes,innovative(1)
0000-0001-8400-4399,Yue Fan,"Fudan University, Zhongshan Hospital, Fudan University",An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised   Learning,1970,"  Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respectively. The reduced imbalance results in faster convergence and better pseudo-label accuracy of SimiS. The simplicity of our method also makes it possible to be combined with other re-balancing techniques to improve the performance further. Moreover, our method shows great robustness to a wide range of data distributions, which holds enormous potential in practice. Code will be publicly available. ",Kein DOI-Link verfügbar,2211.11086v2,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",A lower bound for disconnection by simple random walk,1970,"  We consider simple random walk on Z^d, d bigger or equal to 3. Motivated by the work of A.-S. Sznitman and the author in arXiv:1304.7477 and arXiv:1310.2177, we investigate the asymptotic behaviour of the probability that a large body gets disconnected from infinity by the set of points visited by a simple random walk. We derive asymptotic lower bounds that bring into play random interlacements. Although open at the moment, some of the lower bounds that we obtain possibly match the asymptotic upper bounds obtained in a recent article of A.-S. Sznitman. This potentially yields special significance to the tilted walks that we use in this work, and to the strategy that we employ to implement disconnection. ",https://doi.org/10.1214/15-AOP1077,1412.3959v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",On Natural Measures of SLE- and CLE-Related Random Fractals,1970,"  In this paper, we construct and then prove the up-to constants uniqueness of the natural measure on several random fractals, namely the SLE cut points, SLE boundary touching points, CLE pivotal points and the CLE carpet/gasket. As an application, we also show the equivalence between our natural measures defined in this paper (i.e. CLE pivotal and gasket measures) and their discrete analogs of counting measures in critical continuum planar Bernoulli percolation in [Garban-Pete-Schramm, J. Amer. Math. Soc.,2013]. Although the existence and uniqueness for the natural measure for CLE carpet/gasket have already been proved in [Miller-Schoug, arXiv:2201.01748], in this paper we provide with a different argument via the coupling of CLE and LQG. ",Kein DOI-Link verfügbar,2205.01584v2,Yes,pivotal(2)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Natural parametrization of percolation interface and pivotal points,1970,"  We prove that the interface of critical site percolation on the triangular lattice converges to SLE$_6$ in its natural parametrization, where the discrete interface is parametrized such that each edge is crossed in one unit of time, while the limiting curve is parametrized by the $7/4$-dimensional Minkowski content. We also prove that the scaling limit of counting measure on the pivotal points, which was proved to exist by Garban, Pete, and Schramm (2013), is the $3/4$-dimensional Minkowski content up to a deterministic multiplicative constant. ",Kein DOI-Link verfügbar,1804.07286v3,Yes,pivotal(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Exploring Fine-tuning ChatGPT for News Recommendation,1970,"  News recommendation systems (RS) play a pivotal role in the current digital age, shaping how individuals access and engage with information. The fusion of natural language processing (NLP) and RS, spurred by the rise of large language models such as the GPT and T5 series, blurs the boundaries between these domains, making a tendency to treat RS as a language task. ChatGPT, renowned for its user-friendly interface and increasing popularity, has become a prominent choice for a wide range of NLP tasks. While previous studies have explored ChatGPT on recommendation tasks, this study breaks new ground by investigating its fine-tuning capability, particularly within the news domain. In this study, we design two distinct prompts: one designed to treat news RS as the ranking task and another tailored for the rating task. We evaluate ChatGPT's performance in news recommendation by eliciting direct responses through the formulation of these two tasks. More importantly, we unravel the pivotal role of fine-tuning data quality in enhancing ChatGPT's personalized recommendation capabilities, and illustrates its potential in addressing the longstanding challenge of the ""cold item"" problem in RS. Our experiments, conducted using the Microsoft News dataset (MIND), reveal significant improvements achieved by ChatGPT after fine-tuning, especially in scenarios where a user's topic interests remain consistent, treating news RS as a ranking task. This study illuminates the transformative potential of fine-tuning ChatGPT as a means to advance news RS, offering more effective news consumption experiences. ",Kein DOI-Link verfügbar,2311.05850v1,Yes,"pivotal(2), potent(2)"
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Large Language Model Agent for Fake News Detection,1970,"  In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains. ",Kein DOI-Link verfügbar,2405.01593v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",A GPU accelerated mixed-precision Smoothed Particle Hydrodynamics   framework with cell-based relative coordinates,1970,"  Smoothed Particle Hydrodynamics (SPH) is essential for modeling complex large-deformation problems across various applications, requiring significant computational power. A major portion of SPH computation time is dedicated to the Nearest Neighboring Particle Search (NNPS) process. While advanced NNPS algorithms have been developed to enhance SPH efficiency, the potential efficiency gains from modern computation hardware remain underexplored. This study investigates the impact of GPU parallel architecture, low-precision computing on GPUs, and GPU memory management on NNPS efficiency. Our approach employs a GPU-accelerated mixed-precision SPH framework, utilizing low-precision float-point 16 (FP16) for NNPS while maintaining high precision for other components. To ensure FP16 accuracy in NNPS, we introduce a Relative Coordinated-based Link List (RCLL) algorithm, storing FP16 relative coordinates of particles within background cells. Our testing results show three significant speedup rounds for CPU-based NNPS algorithms. The first comes from parallel GPU computations, with up to a 1000x efficiency gain. The second is achieved through low-precision GPU computing, where the proposed FP16-based RCLL algorithm offers a 1.5x efficiency improvement over the FP64-based approach on GPUs. By optimizing GPU memory bandwidth utilization, the efficiency of the FP16 RCLL algorithm can be further boosted by 2.7x, as demonstrated in an example with 1 million particles. Our code is released at https://github.com/pnnl/lpNNPS4SPH. ",Kein DOI-Link verfügbar,2401.08586v2,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Discussion of Multiscale Fisher's Independence Test for Multivariate   Dependence,1970,"  The multiscale Fisher's independence test (MULTIFIT hereafter) proposed by Gorsky & Ma (2022) is a novel method to test independence between two random vectors. By its design, this test is particularly useful in detecting local dependence. Moreover, by adopting a resampling-free approach, it can easily accommodate massive sample sizes. Another benefit of the proposed method is its ability to interpret the nature of dependency. We congratulate the authors, Shai Gorksy and Li Ma, for their very interesting and elegant work. In this comment, we would like to discuss a general framework unifying the MULTIFIT and other tests and compare it with the binary expansion randomized ensemble test (BERET hereafter) proposed by Lee et al. (In press). We also would like to contribute our thoughts on potential extensions of the method. ",Kein DOI-Link verfügbar,2204.12319v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",A Fully Bayesian Approach for Comprehensive Mapping of Magnitude and   Phase Brain Activation in Complex-Valued fMRI Data,1970,"  Functional magnetic resonance imaging (fMRI) plays a crucial role in neuroimaging, enabling the exploration of brain activity through complex-valued signals. These signals, composed of magnitude and phase, offer a rich source of information for understanding brain functions. Traditional fMRI analyses have largely focused on magnitude information, often overlooking the potential insights offered by phase data. In this paper, we propose a novel fully Bayesian model designed for analyzing single-subject complex-valued fMRI (cv-fMRI) data. Our model, which we refer to as the CV-M&P model, is distinctive in its comprehensive utilization of both magnitude and phase information in fMRI signals, allowing for independent prediction of different types of activation maps. We incorporate Gaussian Markov random fields (GMRFs) to capture spatial correlations within the data, and employ image partitioning and parallel computation to enhance computational efficiency. Our model is rigorously tested through simulation studies, and then applied to a real dataset from a unilateral finger-tapping experiment. The results demonstrate the model's effectiveness in accurately identifying brain regions activated in response to specific tasks, distinguishing between magnitude and phase activation. ",Kein DOI-Link verfügbar,2401.06348v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Company Competition Graph,1970,"  Financial market participants frequently rely on numerous business relationships to make investment decisions. Investors can learn about potential risks and opportunities associated with other connected entities through these corporate connections. Nonetheless, human annotation of a large corpus to extract such relationships is highly time-consuming, not to mention that it requires a considerable amount of industry expertise and professional training. Meanwhile, we have yet to observe means to generate reliable knowledge graphs of corporate relationships due to the lack of impartial and granular data sources. This study proposes a system to process financial reports and construct the public competitor graph to fill the void. Our method can retrieve more than 83\% competition relationship of the S\&P 500 index companies. Based on the output from our system, we construct a knowledge graph with more than 700 nodes and 1200 edges. A demo interactive graph interface is available. ",Kein DOI-Link verfügbar,2304.00323v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Towards Entity Alignment in the Open World: An Unsupervised Approach,1970,"  Entity alignment (EA) aims to discover the equivalent entities in different knowledge graphs (KGs). It is a pivotal step for integrating KGs to increase knowledge coverage and quality. Recent years have witnessed a rapid increase of EA frameworks. However, state-of-the-art solutions tend to rely on labeled data for model training. Additionally, they work under the closed-domain setting and cannot deal with entities that are unmatchable. To address these deficiencies, we offer an unsupervised framework that performs entity alignment in the open world. Specifically, we first mine useful features from the side information of KGs. Then, we devise an unmatchable entity prediction module to filter out unmatchable entities and produce preliminary alignment results. These preliminary results are regarded as the pseudo-labeled data and forwarded to the progressive learning framework to generate structural representations, which are integrated with the side information to provide a more comprehensive view for alignment. Finally, the progressive learning framework gradually improves the quality of structural embeddings and enhances the alignment performance by enriching the pseudo-labeled data with alignment results from the previous round. Our solution does not require labeled data and can effectively filter out unmatchable entities. Comprehensive experimental evaluations validate its superiority. ",Kein DOI-Link verfügbar,2101.10535v1,Yes,pivotal(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Continual Learning for Abdominal Multi-Organ and Tumor Segmentation,1970,"  The ability to dynamically extend a model to new data and classes is critical for multiple organ and tumor segmentation. However, due to privacy regulations, accessing previous data and annotations can be problematic in the medical domain. This poses a significant barrier to preserving the high segmentation accuracy of the old classes when learning from new classes because of the catastrophic forgetting problem. In this paper, we first empirically demonstrate that simply using high-quality pseudo labels can fairly mitigate this problem in the setting of organ segmentation. Furthermore, we put forward an innovative architecture designed specifically for continuous organ and tumor segmentation, which incurs minimal computational overhead. Our proposed design involves replacing the conventional output layer with a suite of lightweight, class-specific heads, thereby offering the flexibility to accommodate newly emerging classes. These heads enable independent predictions for newly introduced and previously learned classes, effectively minimizing the impact of new classes on old ones during the course of continual learning. We further propose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings into the organ-specific heads. These embeddings encapsulate the semantic information of each class, informed by extensive image-text co-training. The proposed method is evaluated on both in-house and public abdominal CT datasets under organ and tumor segmentation tasks. Empirical results suggest that the proposed design improves the segmentation performance of a baseline neural network on newly-introduced and previously-learned classes along the learning trajectory. ",Kein DOI-Link verfügbar,2306.00988v2,Yes,innovative(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Embracing Events and Frames with Hierarchical Feature Refinement Network   for Object Detection,1970,"  In frame-based vision, object detection faces substantial performance degradation under challenging conditions due to the limited sensing capability of conventional cameras. Event cameras output sparse and asynchronous events, providing a potential solution to solve these problems. However, effectively fusing two heterogeneous modalities remains an open issue. In this work, we propose a novel hierarchical feature refinement network for event-frame fusion. The core concept is the design of the coarse-to-fine fusion module, denoted as the cross-modality adaptive feature refinement (CAFR) module. In the initial phase, the bidirectional cross-modality interaction (BCI) part facilitates information bridging from two distinct sources. Subsequently, the features are further refined by aligning the channel-level mean and variance in the two-fold adaptive feature refinement (TAFR) part. We conducted extensive experiments on two benchmarks: the low-resolution PKU-DDD17-Car dataset and the high-resolution DSEC dataset. Experimental results show that our method surpasses the state-of-the-art by an impressive margin of $\textbf{8.0}\%$ on the DSEC dataset. Besides, our method exhibits significantly better robustness (\textbf{69.5}\% versus \textbf{38.7}\%) when introducing 15 different corruption types to the frame images. The code can be found at the link (https://github.com/HuCaoFighting/FRN). ",Kein DOI-Link verfügbar,2407.12582v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",Discovery of A candidate Hypervelocity star originated from the   Sagittarius Dwarf Spheroidal galaxy,1970,"  In this letter, we report the discovery of an intriguing HVS (J1443+1453) candidate that is probably from the Sagittarius Dwarf Spheroidal galaxy (Sgr dSph). The star is an old and very metal-poor low-mass main-sequence turn-off star (age $\sim14.0$ Gyr and [Fe/H] $= -2.23$ dex) and has a total velocity of $559.01^{+135.07}_{-87.40}$ km s$^{-1}$ in the Galactic rest-frame and a heliocentric distance of $2.90^{+0.72}_{-0.48}$ kpc. The velocity of J1443+1453 is larger than the escape speed at its position, suggesting it a promising HVS candidate. By reconstructing its trajectory in the Galactic potential, we find that the orbit of J1443+1453 intersects closely with that of the Sgr dSph $37.8^{+4.6}_{-6.0}$ Myr ago, when the latter has its latest pericentric passage through the Milky Way. The encounter occurs at a distance $2.42^{+1.80}_{-0.77}$ kpc from the centre of Sgr dSph, smaller than the size of the Sgr dSph. Chemical properties of this star are also consistent with those of one Sgr dSph associated globular cluster or of the Sgr stream member stars. Our finding suggests that J1443+1453 is an HVS either tidally stripped from the Sgr dSph or ejected from the Sgr dSph by the gravitational slingshot effect, requiring a (central) massive/intermediate-mass black hole or a (central) massive primordial black hole in the Sgr dSph. ",https://doi.org/10.3847/2041-8213/abd413,2012.09338v1,Yes,potent(1)
0000-0001-6353-4825,Xinyi Li,"Fudan University, Huashan Hospital Fudan University",An Artificial Intelligence-Driven Agent for Real-Time Head-and-Neck IMRT   Plan Generation using Conditional Generative Adversarial Network (cGAN),1970,"  Purpose: To develop an Artificial Intelligence (AI) agent for fully-automated rapid head and neck (H&N) IMRT plan generation without time-consuming inverse planning.$$$$   Methods: This AI agent was trained using a conditional Generative Adversarial Network architecture. The generator, PyraNet, is a novel Deep Learning network that implements 28 classic ResNet blocks in pyramid-like concatenations. The discriminator is a customized 4-layer DenseNet. The AI agent first generates customized 2D projections at 9 template beam angles from 3D CT volume and structures of a patient. These projections are then stacked as 4D inputs of PyraNet, from which 9 radiation fluence maps are generated simultaneously. Finally, the predicted fluence maps are imported into a commercial treatment planning system (TPS) for plan integrity checks. The AI agent was built and tested upon 231 oropharyngeal plans from a TPS plan library. Only the primary plans in the sequential boost regime were studied. A customized Harr wavelet loss was adopted for fluence map comparison. Isodose distributions in test AI plans and TPS plans were qualitatively evaluated. Key dosimetric metrics were statistically compared.$$$$   Results: All test AI plans were successfully generated. Isodose gradients outside of PTV in AI plans were comparable with TPS plans. After PTV coverage normalization, $D_{mean}$ of parotids and oral cavity in AI plans and TPS plans were comparable without statistical significance. AI plans achieved comparable $D_{max}$ at 0.01cc of brainstem and cord+5mm without clinically relevant differences, but body $D_{max}$ was higher than the TPS plan results. The AI agent needs ~3s per case to predict fluence maps.$$$$   Conclusions: The developed AI agent can generate H&N IMRT plans with satisfying dosimetry quality. With rapid and fully automated implementation, it holds great potential for clinical applications. ",https://doi.org/10.1002/mp.14770,2009.12898v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",The Sobolev Inequalities on Real Hyperbolic Spaces and Eigenvalue Bounds   for Schrödinger Operators with Complex Potentials,1970,"  In this paper, we prove the uniform estimates for the resolvent $(\Delta - \alpha)^{-1}$ as a map from $L^q$ to $L^{q'}$ on real hyperbolic space $\mathbb{H}^n$ where $\alpha \in \mathbb{C}\setminus [(n - 1)^2/4, \infty)$ and $2n/(n + 2) \leq q < 2$. In contrast with analogous results on Euclidean space $\mathbb{R}^n$, the exponent $q$ here can be arbitrarily close to $2$. This striking improvement is due to two non-Euclidean features of hyperbolic space: the Kunze-Stein phenomenon and the exponential decay of the spectral measure. In addition, we apply this result to the study of eigenvalue bounds of the Schr\""{o}dinger operator with a complex potential. The improved Sobolev inequality results in a better long range eigenvalue bound on $\mathbb{H}^n$ than that on $\mathbb{R}^n$. ",https://doi.org/10.2140/apde.2022.15.1861,1811.08874v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Scattering of relativistic electrons and analogies with optical   phenomena: A study of longitudinal and transverse shifts at step potentials,1970,"  We investigate the behavior of relativistic electrons encountering a potential step through analogies with optical phenomena. By accounting for the conservation of Dirac current, we elucidate that the Goos-H\""anchen shift can be understood as a combination of two components: one arising from the current entering the transmission region and the other originating from the interference between the incident and reflected beams. This result has been proven to be consistent with findings obtained utilizing the stationary phase method. Moreover, we explore the transverse Imbert-Fedorov shift, by applying both current conservation and total angular momentum conservation, revealing intriguing parallel to the spin Hall effect. Beyond enriching our comprehension of fundamental quantum phenomena, our findings have potential applications for designing and characterizing devices using Dirac and topological materials. ",https://doi.org/10.1103/PhysRevA.108.042218,2308.05413v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A   Transfer Learning Approach with Noise Robustness Analysis,1970,"  Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performance across different SNRs, exhibiting enhanced fitting accuracy, particularly for low SNR datasets when compared with other fitting methods. This highlights its potential for clinical diagnosis and treatment across various scenarios under different clinical setups. ",Kein DOI-Link verfügbar,2401.05580v3,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Connecting The Dots To Combat Collective Fraud,1970,"  Modern fraudsters write malicious programs to coordinate a group of accounts to commit collective fraud for illegal profits in online platforms. These programs have access to a set of finite resources - a set of IPs, devices, and accounts etc. and sometime manipulate fake accounts to collaboratively attack the target system. Inspired by these observations, we share our experience in building two real-time risk control systems to detect collective fraud. We show that with TigerGraph, a powerful graph database, and its innovative query language - GSQL, data scientists and fraud experts can conveniently implement and deploy an end-to-end risk control system as a graph database application. ",Kein DOI-Link verfügbar,2101.01898v1,Yes,innovative(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Head-tail Loss: A simple function for Oriented Object Detection and   Anchor-free models,1970,"  This paper presents a new loss function for the prediction of oriented bounding boxes, named head-tail-loss. The loss function consists in minimizing the distance between the prediction and the annotation of two key points that are representing the annotation of the object. The first point is the center point and the second is the head of the object. However, for the second point, the minimum distance between the prediction and either the head or tail of the groundtruth is used. On this way, either prediction is valid (with the head pointing to the tail or the tail pointing to the head). At the end the importance is to detect the direction of the object but not its heading. The new loss function has been evaluated on the DOTA and HRSC2016 datasets and has shown potential for elongated objects such as ships and also for other types of objects with different shapes. ",Kein DOI-Link verfügbar,2304.04503v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Negative group delay for Dirac particles traveling through a potential   well,1970,"  The properties of group delay for Dirac particles traveling through a potential well are investigated. A necessary condition is put forward for the group delay to be negative. It is shown that this negative group delay is closely related to its anomalous dependence on the width of the potential well. In order to demonstrate the validity of stationary-phase approach, numerical simulations are made for Gaussian-shaped temporal wave packets. A restriction to the potential-well's width is obtained that is necessary for the wave packet to remain distortionless in the travelling. Numerical comparison shows that the relativistic group delay is larger than its corresponding non-relativistic one. ",https://doi.org/10.1103/PhysRevA.68.052105,quant-ph/0411052v1,Yes,potent(3)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Design of electron wave filters in monolayer graphene by tunable   transmission gap,1970,"  We have investigated the transmission in monolayer graphene barrier at nonzero angle of incidence. Taking the influence of parallel wave vector into account, the transmission as the function of incidence energy has a gap due to the evanescent waves in two cases of Klein tunneling and classical motion. The modulation of the transmission gap by the incidence angle, the height, and width of potential barrier may lead to potential applications in graphene-based electronic devices. ",https://doi.org/10.1063/1.3168527,0907.0331v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Map-assisted TDOA Localization Enhancement Based On CNN,1970,"  For signal processing related to localization technologies, non line of sight (NLOS) multipaths have a significant impact on the localization error level. This study proposes a localization correction method based on convolution neural network (CNN), which extracts obstacle features from maps to predict the localization errors caused by NLOS effects. A novel compensation scheme is developed and structured around the localization error in terms of distance and azimuth angle predicted by the CNN. Four prediction tasks are executed over different building distributions within the maps for typical urban scenario, resulting in CNN models with high prediction accuracy. Finally, a thorough comparison of the accuracy performance between the time difference of arrival (TDOA) localization algorithm and the results after the error compensation reveals that, generally, the CNN prediction approach demonstrates great localization error correction performance, improving TDOA accuracy by 75%. It can be observed that the powerful feature extraction capability of CNN can be exploited by processing surrounding maps to predict the localization error distribution, showing great potential for further enhancement of TDOA performance under challenging scenarios with rich multipath propagation. ",Kein DOI-Link verfügbar,2311.01291v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility   Models,1970,"  In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand's mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments.   Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets. ",Kein DOI-Link verfügbar,2405.06866v3,Yes,innovative(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Physical mechanism of superluminal traversal time: interference between   multiple finite wave packets,1970,"  The mechanism of superluminal traversal time through a potential well or potential barrier is investigated from the viewpoint of interference between multiple finite wave packets, due to the multiple reflections inside the well or barrier. In the case of potential-well traveling that is classically allowed, each of the successively transmitted constituents is delayed by a subluminal time. When the thickness of the well is much smaller in comparision with a characteristic length of the incident wave packet, the reshaped wave packet in transmission maintains the profile of the incident wave packet. In the case of potential-barrier tunneling that is classically forbidden, though each of the successively transmitted constituents is delayed by a time that is independent of the barrier thickness, the interference between multiple transmitted constituents explains the barrier-thickness dependence of the traversal time for thin barriers and its barrier-thickness independence for thick barriers. This manifests the nature of Hartman effect. ",https://doi.org/10.1209/0295-5075/82/30009,quant-ph/0611154v1,Yes,potent(4)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Class I methanol masers: Masers with EGOs,1970,"  We have compared the results of a number of published class I methanol maser surveys with the catalogue of high-mass outflow candidates identified from the GLIMPSE survey (known as extended green objects or EGOs). We find class I methanol masers associated with approximately two-thirds of EGOs. Although the association between outflows and class I methanol masers has long been postulated on the basis of detailed studies of a small number of sources, this result demonstrates the relationship for the first time on a statistical basis. Despite the publication of a number of searches for class I methanol masers, a close physical association with another astrophysical object which could be targeted for the search is still lacking. The close association between class I methanol masers and EGOs therefore provides a large catalogue of candidate sources, most of which have not previously been searched for class I methanol masers. Interstellar masers and outflows have both been proposed to trace an evolutionary sequence for high-mass star formation, therefore a better understanding of the relationship between class I methanol masers and outflow offers the potential for comparison and amalgamation of these two evolutionary sequences. ",Kein DOI-Link verfügbar,0903.4223v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Voltage-tunable lateral shifts of ballistic electrons in semiconductor   quantum slabs,1970,"  It is investigated that the lateral shifts of the ballistic electrons transmitted through a semiconductor quantum slabs can be negative as well as positive, which are analogous to the anomalous lateral shifts of the transmitted light beam through a dielectric slab. The necessary condition for the shift to be negative is advanced. It is shown that the lateral shifts depend not only on the structure parameters of semiconductor quantum slab, but also on the incidence angle and the incident energy. Numerical calculations further indicate that the lateral shifts can be tuned from negative to positive by the external applied electric field. The voltage-tunable lateral shifts may lead to potential applications in quantum electronic devices. ",https://doi.org/10.1063/1.3124450,0903.5187v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Goos-Hänchen-like shifts for Dirac fermions in monolayer graphene   barrier,1970,"  We investigate the Goos-H\""{a}nchen-like shifts for Dirac fermions in transmission through a monolayer graphene barrier. The lateral shifts, as the functions of the barrier's width and the incidence angle, can be negative and positive in Klein tunneling and classical motion, respectively. Due to their relations to the transmission gap, the lateral shifts can be enhanced by the transmission resonances when the incidence angle is less than the critical angle for total reflection, while their magnitudes become only the order of Fermi wavelength when the incidence angle is larger than the critical angle. These tunable beam shifts can also be modulated by the height of potential barrier and the induced gap, which gives rise to the applications in graphene-based devices. ",https://doi.org/10.1140/epjb/e2010-10553-6,1004.0350v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Lewis-Riesenfeld invariants and transitionless tracking algorithm,1970,"  Different methods have been recently put forward and implemented experimentally to inverse engineer the time dependent Hamiltonian of a quantum system and accelerate slow adiabatic processes via non-adiabatic shortcuts. In the ""transitionless tracking algorithm"" proposed by Berry, shortcut Hamiltonians are designed so that the system follows exactly, in an arbitrarily short time, the approximate adiabatic path defined by a reference Hamiltonian. A different approach is based on designing first a Lewis-Riesenfeld invariant to carry the eigenstates of a Hamiltonian from specified initial to final configurations, again in an arbitrary time, and then constructing from the invariant the transient Hamiltonian connecting these boundary configurations. We show that the two approaches, apparently quite different in form and so far in results, are in fact strongly related and potentially equivalent, so that the inverse-engineering operations in one of them can be reinterpreted and understood in terms of the concepts and operations of the other one. We study as explicit examples the expansions of time-dependent harmonic traps and state preparation of two level systems. ",https://doi.org/10.1103/PhysRevA.83.062116,1102.3449v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Pan-Cancer Epigenetic Biomarker Selection from Blood Samples Using SAS,1970,"  A key focus in current cancer research is the discovery of cancer biomarkers that allow earlier detection with high accuracy and lower costs for both patients and hospitals. Blood samples have long been used as a health status indicator, but DNA methylation signatures in blood have not been fully appreciated in cancer research. Historically, analysis of cancer has been conducted directly with the patient's tumor or related tissues. Such analyses allow physicians to diagnose a patient's health and cancer status; however, physicians must observe certain symptoms that prompt them to use biopsies or imaging to verify the diagnosis. This is a post-hoc approach. Our study will focus on epigenetic information for cancer detection, specifically information about DNA methylation in human peripheral blood samples in cancer discordant monozygotic twin-pairs. This information might be able to help us detect cancer much earlier, before the first symptom appears. Several other types of epigenetic data can also be used, but here we demonstrate the potential of blood DNA methylation data as a biomarker for pan-cancer using SAS 9.3 and SAS EM. We report that 55 methylation CpG sites measurable in blood samples can be used as biomarkers for early cancer detection and classification. ",Kein DOI-Link verfügbar,1812.09203v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",CONA: A novel CONtext-Aware instruction paradigm for communication using   large language model,1970,"  We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge, Wisdom) hierarchy to automatically instruct and optimise presentation content, anticipate potential audience inquiries, and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents, ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era, offering effective support for everyday knowledge sharing scenarios. We conduct experiments on a range of audience roles, along with materials from various disciplines using GPT4. Both quantitative and qualitative results demonstrated that the proposed CONA paradigm achieved remarkable performance compared to the outputs guided by conventional prompt engineering. ",Kein DOI-Link verfügbar,2305.18620v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Fast transport and splitting of spin-orbit-coupled spin-1 Bose-Einstein   Condensates,1970,"  In this study, we investigate the dynamics of tunable spin-orbit-coupled spin-1 Bose-Einstein condensates confined within a harmonic trap, focusing on rapid transport, spin manipulation, and splitting dynamics. Using shortcuts to adiabaticity, we design time-dependent trap trajectories and spin-orbit-coupling strength to facilitate fast transport with simultaneous spin flip. Additionally, we showcase the creation of spin-dependent coherent states via engineering the spin-orbit-coupling strength. To deepen our understanding, we elucidate non-adiabatic transport and associated spin dynamics, contrasting them with simple scenarios characterized by constant spin-orbit coupling and trap velocity. Furthermore, we discuss the transverse Zeeman potential and nonlinear effect induced by interatomic interactions using the Gross-Pitaevskii equation, highlighting the stability and feasibility of the proposed protocols for the state-of-the-art experiments with cold atoms. ",https://doi.org/10.1103/PhysRevA.109.063310,2405.10727v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Conformal Trajectory Prediction with Multi-View Data Integration in   Cooperative Driving,1970,"  Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory prediction framework designed to model multi-view data by extending existing single-view models. Unlike previous approaches where the multi-view data is manually fused or formulated as a separate training stage, our model supports end-to-end training, enhancing both flexibility and performance. Moreover, the predicted multimodal trajectories are calibrated by a post-hoc conformal prediction module to get valid and efficient confidence regions. We evaluated the entire framework using the real-world V2I dataset V2X-Seq. Our results demonstrate superior performance in terms of Final Displacement Error (FDE) and Miss Rate (MR) using a single GPU. The code is publicly available at: \url{https://github.com/xichennn/V2I_trajectory_prediction}. ",Kein DOI-Link verfügbar,2408.00374v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Robust zero-averaged wave-number gap inside gapped graphene   superlattices,1970,"  In this paper, the electronic band structures and its transport properties in the gapped graphene superlattices, with one-dimensional (1D) periodic potentials of square barriers, are systematically investigated. It is found that a zero averaged wave-number (zero-$\overline{k}$ ) gap is formed inside the gapped graphene-based superlattices, and the condition for obtaining such a zero-$\overline{k}$ gap is analytically presented. The properties of this zero-$\overline{k}$ gap including its transmission, conductance and Fano factor are studied in detail. Finally it is revealed that the properties of the electronic transmission, conductance and Fano factor near the zero-$\overline{k}$ gap are very insensitive to the structural disorder for the finite graphene-based periodic-barrier systems. ",https://doi.org/10.1063/1.3525270,1008.0504v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Fast transport of Bose-Einstein condensates in anharmonic traps,1970,"  We present a method to transport Bose-Einstein condensates (BECs) in anharmonic traps and in the presence of atom-atom interactions in short times without residual excitation. Using a combination of a variational approach and inverse engineering methods, we derive a set of Ermakov-like equations that take into account the coupling between the center of mass motion and the breathing mode. By an appropriate inverse engineering strategy of those equations, we then design the trap trajectory to achieve the desired boundary conditions. Numerical examples for cubic or quartic anharmonicities are provided for fast and high-fidelity transport of BECs. Potential applications are atom interferometry and quantum information processing. ",https://doi.org/10.1098/rsta.2021.0280,2210.03788v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",The fundamental drivers of electrochemical barriers,1970,"  We find that ion creation/destruction dominates the behavior of electrochemical reaction barriers, through grand-canonical electronic structure calculations of proton-deposition on transition metal surfaces. We show that barriers respond to potential in a nonlinear manner and trace this to the continuous degree of electron transfer as an ion is created or destroyed. This explains both Marcus-like curvature and Hammond-like shifts. Across materials, we find the barrier energy to be driven primarily by the charge presented on the surface, which in turn is dictated by the native work function, a fundamentally different driving force than non-electrochemical systems. ",Kein DOI-Link verfügbar,2306.05521v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Quantifying wall turbulence via a symmetry approach. Part I. A Lie group   theory,1970,"  First principle based prediction of mean flow quantities of wall-bounded turbulent flows (channel, pipe, and turbulent boundary layer - TBL) is of great importance from both physics and engineering standpoints. Here (Part I), we present a symmetry-based approach which derives analytic expressions governing the mean velocity profile (MVP) from an innovative Lie-group analysis. The new approach begins by identifying a set of order functions (e.g. stress length, shear-induced eddy length), in analogy with the order parameter in Landau's mean-field theory, which aims at capturing symmetry aspects of the fluctuations (e.g. Reynolds stress, dissipation). The order functions are assumed to satisfy a dilation group invariance - representing the effects of the wall on fluctuations - which allows us to postulate three new kinds of invariant solutions of the mean momentum equation (MME), focusing on group invariants of the order functions (rather than those of the mean velocity as done in previous studies). The first - a power law solution - gives functional forms for the viscous sublayer, the buffer layer, the log-layer, and a newly identified central `core' (for channel and pipe, but non-existent for TBL). The second - a defect power law of form $1-r^{m}$ ($r$ being the distance from the center line) - describes the `bulk zone' (the region of balance between production and dissipation). The third - a relation between the group invariants of the stress length function and its first derivative - describes scaling transition between adjacent layers. A combination of these three expressions yields a multi-layer formula covering the entire flow domain, identifying three important parameters: scaling exponent, layer thickness, and transition sharpness. All three kinds of invariant solutions are validated, individually and in combination, by data from direct numerical simulations (DNS). ",Kein DOI-Link verfügbar,1112.6312v5,Yes,innovative(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Self-assembled Nanocapsules in Water: A Molecular Mechanism Study,1970,"  The self-assembly mechanism of one-end-open carbon nanotubes (CNTs) suspended in an aqueous solution was studied by molecular dynamics simulations. It was shown that two one-end-open CNTs with different diameters can coaxially self-assemble into a nanocapsule. The nanocapsules formed were stable in aqueous solution under ambient conditions, and the pressure inside the nanocapsule was much higher than the ambient pressure due to the van der Waals interactions between two parts of the nanocapsule. The effect of the normalized radius difference, normalized inter-tube distance and aspect ratio of the CNT pairs were systematically explored. The electric field response of nanocapsules was studied with ab initio molecular dynamics simulations, which shows that nanocapsules can be opened by applying an external electric field, due to the polarization of carbon atoms. This discovery not only sheds light on a simple yet robust nanocapsule self-assembly mechanism, but also underpins potential innovations in drug delivery, nano-reactors, etc. ",https://doi.org/10.1039/C7CP02631E,1704.02529v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",A unified framework of continuous and discontinuous Galerkin methods for   solving the incompressible Navier--Stokes equation,1970,"  In this paper, we propose a unified numerical framework for the time-dependent incompressible Navier--Stokes equation which yields the $H^1$-, $H(\text{div})$-conforming, and discontinuous Galerkin methods with the use of different viscous stress tensors and penalty terms for pressure robustness. Under minimum assumption on Galerkin spaces, the semi- and fully-discrete stability is proved when a family of implicit Runge--Kutta methods are used for time discretization. Furthermore, we present a unified discussion on the penalty term. Numerical experiments are presented to compare our schemes with classical schemes in the literature in both unsteady and steady situations. It turns out that our scheme is competitive when applied to well-known benchmark problems such as Taylor--Green vortex, Kovasznay flow, potential flow, lid driven cavity flow, and the flow around a cylinder. ",https://doi.org/10.1016/j.jcp.2020.109799,2008.09485v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Single molecule capturer by doped monatomic carbon chain,1970,"  The B-doped monatomic carbon chain has fine molecular capture ability to H2O and especially to NO2, better than other doped monatomic carbon chains.At 300 K and 1 atm, the capture probability of the B-doped monatomic carbon chain is appreciable even in a NO2 concentration of 1 p.p.m., and the adsorbates' influence on the quantum transport is notable for the detection. In contrast, the pure monatomic carbon chain shows its invulnerability to N2, O2, H2O, NO2, CO and CO2, and it is incapable for molecule capturing due to too low adsorption ability and weak response on the quantum conductance. In the investigation of these issues, a statistic mechanical model [EPL 94, 40002 (2011); Chin. Phys. Lett. 29, 2012 (080501)] was extended to predict the adsorption and desorption rates of molecules on nanodevices. The theoretical foundation of this model was further discussed and its accuracy was verified by molecular dynamics simulations. ",https://doi.org/10.1088/0953-8984/25/20/205302,1311.2144v1,Yes,notable(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Query Tracking for E-commerce Conversational Search: A Machine   Comprehension Perspective,1970,"  With the development of dialog techniques, conversational search has attracted more and more attention as it enables users to interact with the search engine in a natural and efficient manner. However, comparing with the natural language understanding in traditional task-oriented dialog which focuses on slot filling and tracking, the query understanding in E-commerce conversational search is quite different and more challenging due to more diverse user expressions and complex intentions. In this work, we define the real-world problem of query tracking in E-commerce conversational search, in which the goal is to update the internal query after each round of interaction. We also propose a self attention based neural network to handle the task in a machine comprehension perspective. Further more we build a novel E-commerce query tracking dataset from an operational E-commerce Search Engine, and experimental results on this dataset suggest that our proposed model outperforms several baseline methods by a substantial gain for Exact Match accuracy and F1 score, showing the potential of machine comprehension like model for this task. ",Kein DOI-Link verfügbar,1810.03274v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Input Modeling and Uncertainty Quantification for Improving Volatile   Residential Load Forecasting,1970,"  Load forecasting has long been recognized as an important building block for all utility operational planning efforts. Over the recent years, it has become ever more challenging to make accurate forecasts due to the proliferation of distributed energy resources, despite the abundance of existing load forecasting methods. In this paper, we identify one drawback suffered by most load forecasting methods: neglect to thoroughly address the impact of input errors on load forecasts. As a potential solution, we propose to incorporate input modeling and uncertainty quantification to improve load forecasting performance via a two-stage approach. The proposed two-stage approach has the following merits. (1) It provides input modeling and quantifies the impact of input errors, rather than neglecting or mitigating the impact, a prevalent practice of existing methods. (2) It propagates the impact of input errors into the ultimate point and interval predictions for the target customer's load to improve predictive performance. (3) A variance-based global sensitivity analysis method is further proposed for input-space dimensionality reduction in both stages to enhance the computational efficiency. Numerical experiments show that the proposed two-stage approach outperforms competing load forecasting methods in terms of both point predictive accuracy and coverage ability of the predictive intervals. ",Kein DOI-Link verfügbar,1905.06773v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",The Complexity of Pacing for Second-Price Auctions,1970,"  Budget constraints are ubiquitous in online advertisement auctions. To manage these constraints and smooth out the expenditure across auctions, the bidders (or the platform on behalf of them) often employ pacing: each bidder is assigned a pacing multiplier between zero and one, and her bid on each item is multiplicatively scaled down by the pacing multiplier. This naturally gives rise to a game in which each bidder strategically selects a multiplier. The appropriate notion of equilibrium in this game is known as a pacing equilibrium. In this work, we show that the problem of finding an approximate pacing equilibrium is PPAD-complete for second-price auctions. This resolves an open question of Conitzer et al. [2021]. As a consequence of our hardness result, we show that the tatonnement-style budget-management dynamics introduced by Borgs et al. [2007] are unlikely to converge efficiently for repeated second-price auctions. This disproves a conjecture by Borgs et al. [2007], under the assumption that the complexity class PPAD is not equal to P. Our hardness result also implies the existence of a refinement of supply-aware market equilibria which is hard to compute with simple linear utilities. ",Kein DOI-Link verfügbar,2103.13969v2,Yes,strategically(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Weak-valued correlation functions: Insights and precise readout   strategies,1970,"  The correlation function in quantum systems plays a vital role in decoding their properties and gaining insights into physical phenomena. Its interpretation corresponds to the propagation of particle excitations between space-time, similar in spirit to the idea of quantum weak measurement in terms of recording the system information by interaction. By defining weak-valued correlation function, we propose the basic insights and the universal methods for recording them on the apparatus through weak measurement. To demonstrate the feasibility of our approach, we perform numerical experiments of perturbed quantum harmonic oscillators, addressing the intricate interplay between the coupling strength and the number of ensemble copies. Additionally, we extend our protocol to the domain of quantum field theory, where joint weak values encode crucial information about the correlation function. Hopefully, this comprehensive investigation can advance our understanding of the fundamental nature of the correlation function and weak measurement in quantum theories. ",https://doi.org/10.1103/PhysRevA.109.052210,2306.04398v3,Yes,intricate(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Online Estimation and Inference for Robust Policy Evaluation in   Reinforcement Learning,1970,"  Recently, reinforcement learning has gained prominence in modern statistics, with policy evaluation being a key component. Unlike traditional machine learning literature on this topic, our work places emphasis on statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop an online robust policy evaluation procedure, and establish the limiting distribution of our estimator, based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in real-world reinforcement learning experiments. ",Kein DOI-Link verfügbar,2310.02581v1,Yes,versatile(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Rethinking Generative Large Language Model Evaluation for Semantic   Comprehension,1970,"  Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards. ",Kein DOI-Link verfügbar,2403.07872v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",It Takes Two: A Peer-Prediction Solution for Blockchain Verifier's   Dilemma,1970,"  The security of blockchain systems is fundamentally based on the decentralized consensus in which the majority of parties behave honestly, and the process of content verification is essential to keep the robustness of blockchain systems. However, the phenomenon that a secure blockchain system with few or no cheaters could not provide sufficient incentive for verifiers to honestly perform the costly verification, referred to as the Verifier's Dilemma, could severely undermine the fundamental security of blockchain systems. While existing works have attempted to insert deliberate errors to disincentivize lazy verification, the decentralized environment makes it impossible to judge the correctness of verification or detect malicious verifiers directly.   In this paper, we initiate the research that leverages the peer prediction approach towards the design of Bayesian truthful mechanisms for the decentralized verification game among multiple verifiers, incentivizing all verifiers to perform honest verification without access to the ground truth even in the presence of noisy observations in the verification process. With theoretically guaranteed truthfulness of our mechanism for the verification game, our work provides a framework of verification mechanisms that enhances the security and robustness of the blockchain and potentially other decentralized systems. ",Kein DOI-Link verfügbar,2406.01794v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Shortcuts for Adiabatic and Variational Algorithms in Molecular   Simulation,1970,"  Quantum algorithms are prominent in the pursuit of achieving quantum advantage in various computational tasks. However, addressing challenges, such as limited qubit coherence and high error rate in near-term devices, requires extensive efforts. In this paper, we present a substantial stride in quantum chemistry by integrating shortcuts-to-adiabaticity techniques into adiabatic and variational algorithms for calculating the molecular ground state. Our approach includes the counter-diabatic driving that accelerates adiabatic evolution by mitigating adiabatic errors. Additionally, we introduce the counter-diabatic terms as the adiabatic gauge ansatz for the variational quantum eigensolver, which exhibits favorable convergence properties with a fewer number of parameters, thereby reducing the circuit depth. Our approach achieves comparable accuracy to other established ansatzes, while enhancing the potential for applications in material science, drug discovery, and molecular simulations. ",Kein DOI-Link verfügbar,2407.20957v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",HERMES: A Hierarchical Broadcast-Based Silicon Photonic Interconnect for   Scalable Many-Core Systems,1970,"  Optical interconnection networks, as enabled by recent advances in silicon photonic device and fabrication technology, have the potential to address on-chip and off-chip communication bottlenecks in many-core systems. Although several designs have shown superior power efficiency and performance compared to electrical alternatives, these networks will not scale to the thousands of cores required in the future.   In this paper, we introduce Hermes, a hybrid network composed of an optimized broadcast for power-efficient low-latency global-scale coordination and circuit-switch sub-networks for high-throughput data delivery. This network will scale for use in thousand core chip systems. At the physical level, SoI-based adiabatic coupler has been designed to provide low-loss and compact optical power splitting. Based on the adiabatic coupler, a topology based on 2-ary folded butterfly is designed to provide linear power division in a thousand core layout with minimal cross-overs. To address the network agility and provide for efficient use of optical bandwidth, a flow control and routing mechanism is introduced to dynamically allocate bandwidth and provide fairness usage of network resources. At the system level, bloom filter-based filtering for localization of communication are designed for reducing global traffic. In addition, a novel greedy-based data and workload migration are leveraged to increase the locality of communication in a NUCA (non-uniform cache access) architecture. First order analytic evaluation results have indicated that Hermes is scalable to at least 1024 cores and offers significant performance improvement and power savings over prior silicon photonic designs. ",Kein DOI-Link verfügbar,1401.4629v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",An Optimal Policy for Dynamic Assortment Planning Under Uncapacitated   Multinomial Logit Models,1970,"  We study the dynamic assortment planning problem, where for each arriving customer, the seller offers an assortment of substitutable products and customer makes the purchase among offered products according to an uncapacitated multinomial logit (MNL) model. Since all the utility parameters of MNL are unknown, the seller needs to simultaneously learn customers' choice behavior and make dynamic decisions on assortments based on the current knowledge. The goal of the seller is to maximize the expected revenue, or equivalently, to minimize the expected regret. Although dynamic assortment planning problem has received an increasing attention in revenue management, most existing policies require the estimation of mean utility for each product and the final regret usually involves the number of products $N$. The optimal regret of the dynamic assortment planning problem under the most basic and popular choice model---MNL model is still open. By carefully analyzing a revenue potential function, we develop a trisection based policy combined with adaptive confidence bound construction, which achieves an {item-independent} regret bound of $O(\sqrt{T})$, where $T$ is the length of selling horizon. We further establish the matching lower bound result to show the optimality of our policy. There are two major advantages of the proposed policy. First, the regret of all our policies has no dependence on $N$. Second, our policies are almost assumption free: there is no assumption on mean utility nor any ""separability"" condition on the expected revenues for different assortments. Our result also extends the unimodal bandit literature. ",Kein DOI-Link verfügbar,1805.04785v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Versatile Mixed Methods for the Incompressible Navier-Stokes Equations,1970,"  In the spirit of the ""Principle of Equipresence"" introduced by Truesdell & Toupin, The Classical Field Theories (1960), we use the full version of the viscous stress tensor which was originally derived for compressible flows, instead of the classical incompressible stress tensor. In our approach, the divergence-free constraint for the viscous stress term is not enforced ahead of discretization. Instead, our formulation allows the scheme itself to ""choose"" a consistent way to interpret the divergence-free constraint: i.e., the divergence-free constraint is interpreted (or enforced) in a consistent fashion in both the mass conservation equation and the stress tensor term (in the momentum equation). Furthermore, our approach preserves the original symmetrical properties of the stress tensor, e.g. its rotational invariance, and it remains physically correct in the context of compressible flows. As a result, our approach facilitates versatility and code reuse. In this paper, we introduce our approach and establish some important mathematical properties for the resulting class of finite element schemes. More precisely, for general mixed methods, which are not necessarily pointwise divergence-free, we establish the existence of a new norm induced by the full, viscous bilinear form. Thereafter, we prove the coercivity of the viscous bilinear form and the semi-coercivity of a convective trilinear form. In addition, we demonstrate L2-stability of the discrete velocity fields for the general class of methods and (by deduction) the H(div)-conforming methods. Finally, we run some numerical experiments to illustrate the behavior of the versatile mixed methods, and we make careful comparisons with a conventional H(div)-conforming scheme. ",Kein DOI-Link verfügbar,2007.08015v1,Yes,versatile(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Privacy-Preserving Dynamic Personalized Pricing with Demand Learning,1970,"  The prevalence of e-commerce has made detailed customers' personal information readily accessible to retailers, and this information has been widely used in pricing decisions. When involving personalized information, how to protect the privacy of such information becomes a critical issue in practice. In this paper, we consider a dynamic pricing problem over $T$ time periods with an \emph{unknown} demand function of posted price and personalized information. At each time $t$, the retailer observes an arriving customer's personal information and offers a price. The customer then makes the purchase decision, which will be utilized by the retailer to learn the underlying demand function. There is potentially a serious privacy concern during this process: a third party agent might infer the personalized information and purchase decisions from price changes from the pricing system. Using the fundamental framework of differential privacy from computer science, we develop a privacy-preserving dynamic pricing policy, which tries to maximize the retailer revenue while avoiding information leakage of individual customer's information and purchasing decisions. To this end, we first introduce a notion of \emph{anticipating} $(\varepsilon, \delta)$-differential privacy that is tailored to dynamic pricing problem. Our policy achieves both the privacy guarantee and the performance guarantee in terms of regret. Roughly speaking, for $d$-dimensional personalized information, our algorithm achieves the expected regret at the order of $\tilde{O}(\varepsilon^{-1} \sqrt{d^3 T})$, when the customers' information is adversarially chosen. For stochastic personalized information, the regret bound can be further improved to $\tilde{O}(\sqrt{d^2T} + \varepsilon^{-2} d^2)$ ",Kein DOI-Link verfügbar,2009.12920v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Differential Privacy in Personalized Pricing with Nonparametric Demand   Models,1970,"  In the recent decades, the advance of information technology and abundant personal data facilitate the application of algorithmic personalized pricing. However, this leads to the growing concern of potential violation of privacy due to adversarial attack. To address the privacy issue, this paper studies a dynamic personalized pricing problem with \textit{unknown} nonparametric demand models under data privacy protection. Two concepts of data privacy, which have been widely applied in practices, are introduced: \textit{central differential privacy (CDP)} and \textit{local differential privacy (LDP)}, which is proved to be stronger than CDP in many cases. We develop two algorithms which make pricing decisions and learn the unknown demand on the fly, while satisfying the CDP and LDP gurantees respectively. In particular, for the algorithm with CDP guarantee, the regret is proved to be at most $\tilde O(T^{(d+2)/(d+4)}+\varepsilon^{-1}T^{d/(d+4)})$. Here, the parameter $T$ denotes the length of the time horizon, $d$ is the dimension of the personalized information vector, and the key parameter $\varepsilon>0$ measures the strength of privacy (smaller $\varepsilon$ indicates a stronger privacy protection). On the other hand, for the algorithm with LDP guarantee, its regret is proved to be at most $\tilde O(\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$, which is near-optimal as we prove a lower bound of $\Omega(\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$ for any algorithm with LDP guarantee. ",Kein DOI-Link verfügbar,2109.04615v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Network Revenue Management with Demand Learning and Fair   Resource-Consumption Balancing,1970,"  In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a healthy channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innovative techniques to make our algorithm a unified and computationally efficient framework for the continuous price set and a wide class of balancing regularizers. Our algorithm achieves a worst-case regret of $\widetilde O(N^{5/2}\sqrt{T})$, where $N$ denotes the number of products and $T$ denotes the number of time periods. Numerical experiments in a few NRM examples demonstrate the effectiveness of our algorithm in simultaneously achieving revenue maximization and fair resource-consumption balancing ",Kein DOI-Link verfügbar,2207.11159v3,Yes,innovative(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",High-Dimensional Dynamic Pricing under Non-Stationarity: Learning and   Earning with Change-Point Detection,1970,"  We consider a high-dimensional dynamic pricing problem under non-stationarity, where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model with potential changes at unknown times. The demand model is assumed to be a high-dimensional generalized linear model (GLM), allowing for a feature vector in $\mathbb R^d$ that encodes products and consumer information. To achieve optimal revenue (i.e., least regret), the firm needs to learn and exploit the unknown GLMs while monitoring for potential change-points. To tackle such a problem, we first design a novel penalized likelihood-based online change-point detection algorithm for high-dimensional GLMs, which is the first algorithm in the change-point literature that achieves optimal minimax localization error rate for high-dimensional GLMs. A change-point detection assisted dynamic pricing (CPDP) policy is further proposed and achieves a near-optimal regret of order $O(s\sqrt{\Upsilon_T T}\log(Td))$, where $s$ is the sparsity level and $\Upsilon_T$ is the number of change-points. This regret is accompanied with a minimax lower bound, demonstrating the optimality of CPDP (up to logarithmic factors). In particular, the optimality with respect to $\Upsilon_T$ is seen for the first time in the dynamic pricing literature, and is achieved via a novel accelerated exploration mechanism. Extensive simulation experiments and a real data application on online lending illustrate the efficiency of the proposed policy and the importance and practical value of handling non-stationarity in dynamic pricing. ",Kein DOI-Link verfügbar,2303.07570v2,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Algorithm-Oriented Qubit Mapping for Variational Quantum Algorithms,1970,"  Quantum algorithms implemented on near-term devices require qubit mapping due to noise and limited qubit connectivity. In this paper we propose a strategy called algorithm-oriented qubit mapping (AOQMAP) that aims to bridge the gap between exact and scalable mapping methods by utilizing the inherent structure of algorithms. While exact methods provide optimal solutions, they become intractable for large circuits. Scalable methods, like SWAP networks, offer fast solutions but lack optimality. AOQMAP bridges this gap by leveraging algorithmic features and their association with specific device substructures to achieve optimal and scalable solutions. The proposed strategy follows a two stage approach. First, it maps circuits to subtopologies to meet connectivity constraints. Second, it identifies the optimal qubits for execution using a cost function. Notably, AOQMAP provides both scalable and optimal solutions for variational quantum algorithms with fully connected two qubit interactions on common subtopologies including linear, T-, and H-shaped, minimizing circuit depth. Benchmarking experiments conducted on IBM quantum devices demonstrate significant reductions in gate count and circuit depth compared to Qiskit, Tket, and SWAP network. Specifically, AOQMAP achieves up to an 82% reduction in circuit depth and an average 138% increase in success probability. This scalable and algorithm-specific approach holds the potential to optimize a wider range of quantum algorithms. ",Kein DOI-Link verfügbar,2310.09826v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Untrained Neural Nets for Snapshot Compressive Imaging: Theory and   Algorithms,1970,"  Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency. In this paper, we focus on SCI recovery algorithms that employ untrained neural networks (UNNs), such as deep image prior (DIP), to model source structure. Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios. We first develop a theoretical framework for characterizing the performance of such UNN-based methods. The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN. We also employ the recently proposed bagged-deep-image-prior (bagged-DIP) idea to develop SCI Bagged Deep Video Prior (SCI-BDVP) algorithms that address the common challenges faced by standard UNN solutions. Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions. ",Kein DOI-Link verfügbar,2406.03694v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Transient particle energies in shortcuts to adiabatic expansions of   harmonic traps,1970,"  The expansion of a harmonic potential that holds a quantum particle may be realized without any final particle excitation but much faster than adiabatically via ""shortcuts to adiabaticity"" (STA). While ideally the process time can be reduced to zero, practical limitations and constraints impose minimal finite times for the externally controlled time-dependent frequency protocols. We examine the role of different time-averaged energies (total, kinetic, potential, non-adiabatic) and of the instantaneous power in characterizing or selecting different protocols.Specifically, we prove a virial theorem for STA processes, set minimal energies for specific times or viceversa, and discuss their realizability by means of Dirac impulses or otherwise. ",https://doi.org/10.1021/acs.jpca.5b06090,1505.03051v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Retrieving Yang--Mills--Higgs fields in Minkowski space from active   local measurements,1970,  We show that we can retrieve a Yang--Mills potential and a Higgs field (up to gauge) from source-to-solution type data associated with the classical Yang--Mills--Higgs equations in Minkowski space $\mathbb{R}^{1+3}$. We impose natural non-degeneracy conditions on the representation for the Higgs field and on the Lie algebra of the structure group which are satisfied for the case of the Standard Model. Our approach exploits the non-linear interaction of waves generated by sources with values in the centre of the Lie algebra showing that abelian components can be used effectively to recover the Higgs field. ,Kein DOI-Link verfügbar,2204.12776v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University","Transmission gap, Bragg-like reflection, and Goos-Hänchen shifts   near the Dirac point inside a negative-zero-positive index metamaterial slab",1970,"  Motivated by the realization of the Dirac point (DP) with a double-cone structure for optical field in the negative-zero-positive index metamaterial (NZPIM), the reflection, transmission, and Goos-H\""{a}nchen (GH) shifts inside the NZPIM slab are investigated. Due to the linear Dirac dispersion, the transmission as the function of the frequency has a gap, thus the correspond reflection has a frequency or wavelength window for the perfect reflection, which is similar to the Bragg reflection in the one-dimensional photonic crystals. Near the DP, the associated GH shifts in the transmission and reflection can be changed from positive to negative with increasing the wavelength. These negative and positive shifts can also be enhanced by transmission resonances, when the frequency is far from that at the DP. All these phenomena will lead to some potential applications in the integrated optics and optical devices. ",https://doi.org/10.1103/PhysRevA.80.043839,0908.3792v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Shortcuts to adiabaticity for an interacting Bose-Einstein condensate   via exact solutions of the generalized Ermakov equation,1970,"  Shortcuts to adiabatic expansion of the effectively one-dimensional Bose-Einstein condensate (BEC) loaded in the harmonic-oscillator (HO) trap is investigated by combining techniques of the variational approximation and inverse engineering. Piecewise-constant (discontinuous) intermediate trap frequencies, similar to the known bang-bang forms in the optimal-control theory, are derived from an exact solution of a generalized Ermakov equation. Control schemes considered in the paper include imaginary trap frequencies at short time scales, i.e., the HO potential replaced by the quadratic repulsive one. Taking into regard the BEC's intrinsic nonlinearity, results are reported for the minimal transfer time, excitation energy (which measures deviation from the effective adiabaticity), and stability for the shortcut-to-adiabaticity protocols. These results are not only useful for the realization of fast frictionless cooling, but also help to address fundamental problems of the quantum speed limit and thermodynamics. ",https://doi.org/10.1063/5.0004309,2002.03632v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Versatile Mixed Methods for Non-Isothermal Incompressible Flows,1970,"  The purpose of this paper is to extend the versatile mixed methods originally developed by Chen and Williams for isothermal flows in ""Versatile Mixed Methods for the Incompressible Navier-Stokes Equations,"" Computers & Mathematics with Applications, 2020, (under review), to simulate non-isothermal incompressible flows. These new mixed methods are particularly interesting, as with only minor modifications they can be applied to a much broader range of flows, including non-isothermal weakly-compressible flows, and fully-compressible flows. In the main body of this paper, we carefully develop these mixed methods for solving the Boussinesq model equations. Thereafter, we prove the L2-stability of the discrete temperature field, and assess the practical behavior of the methods by applying them to a set of well-known convection problems. ",Kein DOI-Link verfügbar,2007.08679v1,Yes,versatile(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Multi-Scale Semantic Segmentation with Modified MBConv Blocks,1970,"  Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks. Despite their success, their application in semantic segmentation has remained relatively unexplored. This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation. Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification. We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities. By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance. ",Kein DOI-Link verfügbar,2402.04618v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Benchmarking Deep Reinforcement Learning for Continuous Control,1970,"  Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers. ",Kein DOI-Link verfügbar,1604.06778v3,Yes,notable(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Architectures of Soft Robotic Locomotion Enabled by Simple Mechanical   Principles,1970,"  In nature, a variety of limbless locomotion patterns flourish from the small or basic life form (Escherichia coli, the amoeba, etc.) to the large or intelligent creatures (e.g., slugs, starfishes, earthworms, octopuses, jellyfishes, and snakes). Many bioinspired soft robots based on locomotion have been developed in the past decades. In this work, based on the kinematics and dynamics of two representative locomotion modes (i.e., worm-like crawling and snake-like slithering), we propose a broad set of innovative designs for soft mobile robots through simple mechanical principles. Inspired by and go beyond existing biological systems, these designs include 1-D (dimensional), 2-D, and 3-D robotic locomotion patterns enabled by simple actuation of continuous beams. We report herein over 20 locomotion modes achieving various locomotion functions, including crawling, rising, running, creeping, squirming, slithering, swimming, jumping, turning, turning over, helix rolling, wheeling, etc. Some of them are able to reach high speed, high efficiency, and overcome obstacles. All these locomotion strategies and functions can be integrated into a simple beam model. The proposed simple and robust models are adaptive for severe and complex environments. These elegant designs for diverse robotic locomotion patterns are expected to underpin future deployments of soft robots and to inspire series of advanced designs. ",Kein DOI-Link verfügbar,1611.08963v1,Yes,innovative(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University","Active Learning in Physics: From 101, to Progress, and Perspective",1970,"  Active Learning (AL) is a family of machine learning (ML) algorithms that predates the current era of artificial intelligence. Unlike traditional approaches that require labeled samples for training, AL iteratively selects unlabeled samples to be annotated by an expert. This protocol aims to prioritize the most informative samples, leading to improved model performance compared to training with all labeled samples. In recent years, AL has gained increasing attention, particularly in the field of physics. This paper presents a comprehensive and accessible introduction to the theory of AL reviewing the latest advancements across various domains. Additionally, we explore the potential integration of AL with quantum ML, envisioning a synergistic fusion of these two fields rather than viewing AL as a mere extension of classical ML into the quantum realm. ",https://doi.org/10.1002/qute.202300208,2307.03899v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs,1970,"  Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities. ",Kein DOI-Link verfügbar,2403.04114v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Phy-Diff: Physics-guided Hourglass Diffusion Model for Diffusion MRI   Synthesis,1970,"  Diffusion MRI (dMRI) is an important neuroimaging technique with high acquisition costs. Deep learning approaches have been used to enhance dMRI and predict diffusion biomarkers through undersampled dMRI. To generate more comprehensive raw dMRI, generative adversarial network based methods are proposed to include b-values and b-vectors as conditions, but they are limited by unstable training and less desirable diversity. The emerging diffusion model (DM) promises to improve generative performance. However, it remains challenging to include essential information in conditioning DM for more relevant generation, i.e., the physical principles of dMRI and white matter tract structures. In this study, we propose a physics-guided diffusion model to generate high-quality dMRI. Our model introduces the physical principles of dMRI in the noise evolution in the diffusion process and introduce a query-based conditional mapping within the difussion model. In addition, to enhance the anatomical fine detials of the generation, we introduce the XTRACT atlas as prior of white matter tracts by adopting an adapter technique. Our experiment results show that our method outperforms other state-of-the-art methods and has the potential to advance dMRI enhancement. ",Kein DOI-Link verfügbar,2406.03002v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",LogoSticker: Inserting Logos into Diffusion Models for Customized   Generation,1970,"  Recent advances in text-to-image model customization have underscored the importance of integrating new concepts with a few examples. Yet, these progresses are largely confined to widely recognized subjects, which can be learned with relative ease through models' adequate shared prior knowledge. In contrast, logos, characterized by unique patterns and textual elements, are hard to establish shared knowledge within diffusion models, thus presenting a unique challenge. To bridge this gap, we introduce the task of logo insertion. Our goal is to insert logo identities into diffusion models and enable their seamless synthesis in varied contexts. We present a novel two-phase pipeline LogoSticker to tackle this task. First, we propose the actor-critic relation pre-training algorithm, which addresses the nontrivial gaps in models' understanding of the potential spatial positioning of logos and interactions with other objects. Second, we propose a decoupled identity learning algorithm, which enables precise localization and identity extraction of logos. LogoSticker can generate logos accurately and harmoniously in diverse contexts. We comprehensively validate the effectiveness of LogoSticker over customization methods and large models such as DALLE~3. \href{https://mingkangz.github.io/logosticker}{Project page}. ",Kein DOI-Link verfügbar,2407.13752v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image   Enhancement,1970,"  Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications. ",Kein DOI-Link verfügbar,2201.10294v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Uni3DETR: Unified 3D Detection Transformer,1970,"  Existing point cloud based 3D detectors are designed for the particular scene, either indoor or outdoor ones. Because of the substantial differences in object distribution and point density within point clouds collected from various environments, coupled with the intricate nature of 3D metrics, there is still a lack of a unified network architecture that can accommodate diverse scenes. In this paper, we propose Uni3DETR, a unified 3D detector that addresses indoor and outdoor 3D detection within the same framework. Specifically, we employ the detection transformer with point-voxel interaction for object prediction, which leverages voxel features and points for cross-attention and behaves resistant to the discrepancies from data. We then propose the mixture of query points, which sufficiently exploits global information for dense small-range indoor scenes and local information for large-range sparse outdoor ones. Furthermore, our proposed decoupled IoU provides an easy-to-optimize training target for localization by disentangling the xy and z space. Extensive experiments validate that Uni3DETR exhibits excellent performance consistently on both indoor and outdoor 3D detection. In contrast to previous specialized detectors, which may perform well on some particular datasets but suffer a substantial degradation on different scenes, Uni3DETR demonstrates the strong generalization ability under heterogeneous conditions (Fig. 1).   Codes are available at \href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}. ",Kein DOI-Link verfügbar,2310.05699v1,Yes,intricate(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Optimizing edge state transfer in a Su-Schrieffer-Heeger chain via   hybrid analog-digital strategies,1970,"  The Su-Schrieffer-Heeger (SSH) chain, which serves as a paradigmatic model for comprehending topological phases and their associated edge states, plays an essential role in advancing our understanding of quantum materials and quantum information processing and technology. In this paper, we introduce a hybrid analog-digital protocol designed for the nonadiabatic yet high-fidelity transfer of edge states in an SSH chain, featuring two sublattices, A and B. The core of our approach lies in harnessing the approximate time-dependent counterdiabatic (CD) interaction, derived from adiabatic gauge potentials. However, to enhance transfer fidelity, particularly in long-distance chains, higher-order nested commutators become crucial. To simplify the experimental implementation and navigate computational complexities, we identify the next-to-nearest-neighbor hopping terms between sublattice A sites as dominant CD driving and further optimize them by using variational quantum circuits. Through digital quantum simulation, our protocol showcases the capability to achieve rapid and robust solutions, even in the presence of disorder. This analog-digital transfer protocol, an extension of quantum control methodology, establishes a robust framework for edge-state transfer. Importantly, the optimal CD driving identified can be seamlessly implemented across various quantum registers, highlighting the versatility of our approach. ",https://doi.org/10.1103/PhysRevApplied.21.034033,2310.12179v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Self-Reasoning Assistant Learning for non-Abelian Gauge Fields Design,1970,"  Non-Abelian braiding has attracted substantial attention because of its pivotal role in describing the exchange behaviour of anyons, in which the input and outcome of non-Abelian braiding are connected by a unitary matrix. Implementing braiding in a classical system can assist the experimental investigation of non-Abelian physics. However, the design of non-Abelian gauge fields faces numerous challenges stemmed from the intricate interplay of group structures, Lie algebra properties, representation theory, topology, and symmetry breaking. The extreme diversity makes it a powerful tool for the study of condensed matter physics. Whereas the widely used artificial intelligence with data-driven approaches has greatly promoted the development of physics, most works are limited on the data-to-data design. Here we propose a self-reasoning assistant learning framework capable of directly generating non-Abelian gauge fields. This framework utilizes the forward diffusion process to capture and reproduce the complex patterns and details inherent in the target distribution through continuous transformation. Then the reverse diffusion process is used to make the generated data closer to the distribution of the original situation. Thus, it owns strong self-reasoning capabilities, allowing to automatically discover the feature representation and capture more subtle relationships from the dataset. Moreover, the self-reasoning eliminates the need for manual feature engineering and simplifies the process of model building. Our framework offers a disruptive paradigm shift to parse complex physical processes, automatically uncovering patterns from massive datasets. ",Kein DOI-Link verfügbar,2407.16255v1,Yes,"intricate(1), pivotal(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities   and Risks,1970,"  In the Proof of Stake (PoS) Ethereum ecosystem, users can stake ETH on Lido to receive stETH, a Liquid Staking Derivative (LSD) that represents staked ETH and accrues staking rewards. LSDs improve the liquidity of staked assets by facilitating their use in secondary markets, such as for collateralized borrowing on Aave or asset exchanges on Curve. The composability of Lido, Aave, and Curve enables an emerging strategy known as leverage staking, where users supply stETH as collateral on Aave to borrow ETH and then acquire more stETH. This can be done directly by initially staking ETH on Lido or indirectly by swapping ETH for stETH on Curve. While this iterative process enhances financial returns, it also introduces potential risks.   This paper explores the opportunities and risks of leverage staking. We establish a formal framework for leverage staking with stETH and identify 442 such positions on Ethereum over 963 days. These positions represent a total volume of 537,123 ETH (877m USD). Our data reveal that the majority (81.7%) of leverage staking positions achieved an Annual Percentage Rate (APR) higher than that of conventional staking on Lido. Despite the high returns, we also recognize the risks of leverage staking. From the Terra crash incident, we understand that token devaluation can greatly impact the market. Therefore, we conduct stress tests under extreme conditions, particularly during stETH devaluations, to thoroughly evaluate the associated risks. Our simulations indicate that leverage staking can exacerbate the risk of cascading liquidations by introducing additional selling pressures from liquidation and deleveraging activities. Moreover, this strategy poses broader systemic risks as it undermines the stability of ordinary positions by intensifying their liquidations. ",Kein DOI-Link verfügbar,2401.08610v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Electronic analogy of Goos-Hänchen effect: a review,1970,"  The analogies between optical and electronic Goos-H\""{a}nchen effects are established based on electron wave optics in semiconductor or graphene-based nanostructures. In this paper, we give a brief overview of the progress achieved so far in the field of electronic Goos-H\""{a}nchen shifts, and show the relevant optical analogies. In particular, we present several theoretical results on the giant positive and negative Goos-H\""{a}nchen shifts in various semiconductor or graphen-based nanostructures, their controllability, and potential applications in electronic devices, e.g. spin (or valley) beam splitters. ",https://doi.org/10.1088/2040-8978/15/3/033001,1301.3549v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Towards Realistic Low-resource Relation Extraction: A Benchmark with   Empirical Baseline Study,1970,"  This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distribution; (iii) Data augmentation complements existing baselines and can bring much performance gain, while self-training may not consistently achieve advancement to low-resource RE. Code and datasets are in https://github.com/zjunlp/LREBench. ",Kein DOI-Link verfügbar,2210.10678v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph   Construction,1970,"  With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supervised data as a prompt for each sample, which is model-agnostic and can be plugged into widespread existing approaches. Experimental results demonstrate that previous methods integrated with RAP can achieve impressive performance gains in low-resource settings on five datasets of relational triple extraction and event extraction for knowledge graph construction. Code is available in https://github.com/zjunlp/RAP. ",https://doi.org/10.1145/3539618.3591763,2210.10709v5,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Tunable Goos-Hänchen shift and polarization beam splitter in   electro-optic crystals,1970,"  We have investigated the tunable lateral shift and polarization beam splitting of the transmitted light beam through electro-optic crystals, based on the Pockels effect. The positive and negative lateral shifts could be easily controlled by adjusting the permittivity tensor, which is modulated by the external applied electric field. An alternative way to realize the polarization beam splitter was also proposed by the polarization-dependent lateral shifts. Numerical simulations for Gaussian-shaped incident beam have demonstrated the above theoretical results obtained by stationary phase method. All these phenomena have potential applications in optical devices. ",https://doi.org/10.1063/1.3041423,0809.0774v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Fast atomic transport without vibrational heating,1970,"  We use the dynamical invariants associated with the Hamiltonian of an atom in a one dimensional moving trap to inverse engineer the trap motion and perform fast atomic transport without final vibrational heating. The atom is driven non-adiabatically through a shortcut to the result of adiabatic, slow trap motion. For harmonic potentials this only requires designing appropriate trap trajectories, whereas perfect transport in anharmonic traps may be achieved by applying an extra field to compensate the forces in the rest frame of the trap. The results can be extended to atom stopping or launching. The limitations due to geometrical constraints, energies and accelerations involved are analyzed, as well as the relation to previous approaches (based on classical trajectories or ""fast-forward"" and ""bang-bang"" methods) which can be integrated in the invariant-based framework. ",https://doi.org/10.1103/PhysRevA.83.013415,1010.3271v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Controllable Goos-Hänchen shifts and spin beam splitter for   ballistic electrons in a parabolic quantum well under a uniform magnetic   field,1970,"  The quantum Goos-H\""{a}nchen shift for ballistic electrons is investigated in a parabolic potential well under a uniform vertical magnetic field. It is found that the Goos-H\""{a}nchen shift can be negative as well as positive, and becomes zero at transmission resonances. The beam shift depends not only on the incident energy and incidence angle, but also on the magnetic field and Landau quantum number. Based on these phenomena, we propose an alternative way to realize the spin beam splitter in the proposed spintronic device, which can completely separate spin-up and spin-down electron beams by negative and positive Goos-H\""{a}nchen shifts. ",https://doi.org/10.1103/PhysRevB.83.195409,1103.4995v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Electronic beam shifts in monolayer graphene superlattice,1970,"  Electronic analogue of generalized Goos-H\""{a}nchen shifts is investigated in the monolayer graphene superlattice with one-dimensional periodic potentials of square barriers. It is found that the lateral shifts for the electron beam transmitted through the monolayer graphene superlattice can be negative as well as positive near the band edges of zero-$\bar{k}$ gap, which are different from those near the band edges of Bragg gap. These negative and positive beam shifts have close relation to the Dirac point. When the condition $q_A d_A= -q_B d_B= m \pi$ ($m=1,2,3...$) is satisfied, the beam shifts can be controlled from negative to positive when the incident energy is above the Dirac point, and vice versa. In addition, the beam shifts can be greatly enhanced by the defect mode inside the zero-$\bar{k}$ gap. These intriguing phenomena can be verified in a relatively simple optical setup, and have potential applications in the graphene-based electron wave devices. ",https://doi.org/10.1140/epjb/e2013-40092-5,1111.1753v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Open-vocabulary Panoptic Segmentation with Embedding Modulation,1970,"  Open-vocabulary image segmentation is attracting increasing attention due to its critical applications in the real world. Traditional closed-vocabulary segmentation methods are not able to characterize novel objects, whereas several recent open-vocabulary attempts obtain unsatisfactory results, i.e., notable performance reduction on the closed vocabulary and massive demand for extra data. To this end, we propose OPSNet, an omnipotent and data-efficient framework for Open-vocabulary Panoptic Segmentation. Specifically, the exquisitely designed Embedding Modulation module, together with several meticulous components, enables adequate embedding enhancement and information exchange between the segmentation model and the visual-linguistic well-aligned CLIP encoder, resulting in superior segmentation performance under both open- and closed-vocabulary settings with much fewer need of additional data. Extensive experimental evaluations are conducted across multiple datasets (e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various circumstances, where the proposed OPSNet achieves state-of-the-art results, which demonstrates the effectiveness and generality of the proposed approach. The code and trained models will be made publicly available. ",Kein DOI-Link verfügbar,2303.11324v2,Yes,"meticulous(1), notable(1), potent(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",AnyDoor: Zero-shot Object-level Image Customization,1970,"  This work presents AnyDoor, a diffusion-based image generator with the power to teleport target objects to new scenes at user-specified locations in a harmonious way. Instead of tuning parameters for each object, our model is trained only once and effortlessly generalizes to diverse object-scene combinations at the inference stage. Such a challenging zero-shot setting requires an adequate characterization of a certain object. To this end, we complement the commonly used identity feature with detail features, which are carefully designed to maintain texture details yet allow versatile local variations (e.g., lighting, orientation, posture, etc.), supporting the object in favorably blending with different surroundings. We further propose to borrow knowledge from video datasets, where we can observe various forms (i.e., along the time axis) of a single object, leading to stronger model generalizability and robustness. Extensive experiments demonstrate the superiority of our approach over existing alternatives as well as its great potential in real-world applications, such as virtual try-on and object moving. Project page is https://damo-vilab.github.io/AnyDoor-Page/. ",Kein DOI-Link verfügbar,2307.09481v2,Yes,"versatile(1), potent(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Can We Edit Multimodal Large Language Models?,1970,"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit. ",Kein DOI-Link verfügbar,2310.08475v5,Yes,"innovative(1), potent(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Dropout is all you need: robust two-qubit gate with reinforcement   learning,1970,"  In the realm of quantum control, reinforcement learning, a prominent branch of machine learning, emerges as a competitive candidate for computer-assisted optimal design for experiments. This study investigates the extent to which guidance from human experts is necessary for the effective implementation of reinforcement learning in designing quantum control protocols. Specifically, we focus on the engineering of a robust two-qubit gate within a nuclear magnetic resonance system, utilizing a combination of analytical solutions as prior knowledge and techniques from the field of computer science. Through extensive benchmarking of different models, we identify dropout, a widely-used method for mitigating overfitting in machine learning, as an especially robust approach. Our findings demonstrate the potential of incorporating computer science concepts to propel the development of advanced quantum technologies. ",Kein DOI-Link verfügbar,2312.06335v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",An Active Search Strategy with Multiple Unmanned Aerial Systems for   Multiple Targets,1970,"  The challenge of efficient target searching in vast natural environments has driven the need for advanced multi-UAV active search strategies. This paper introduces a novel method in which global and local information is adeptly merged to avoid issues such as myopia and redundant back-and-forth movements. In addition, a trajectory generation method is used to ensure the search pattern within continuous space. To further optimize multi-agent cooperation, the Voronoi partition technique is employed, ensuring a reduction in repetitive flight patterns and making the control of multiple agents in a decentralized way. Through a series of experiments, the evaluation and comparison results demonstrate the efficiency of our approach in various environments. The primary application of this innovative approach is demonstrated in the search for horseshoe crabs within their wild habitats, showcasing its potential to revolutionize ecological survey and conservation efforts. ",Kein DOI-Link verfügbar,2406.16370v1,Yes,"innovative(1), potent(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Fast optimal frictionless atom cooling in harmonic traps,1970,"  A method is proposed to cool down atoms in a harmonic trap without phase-space compression as in a perfectly slow adiabatic expansion, i.e., keeping the populations of the instantaneous initial and final levels invariant, but in a much shorter time. This may require that the harmonic trap becomes an expulsive parabolic potential in some time interval. The cooling times achieved are also shorter than previous minimal times using optimal-control bang-bang methods and real frequencies. ",https://doi.org/10.1103/PhysRevLett.104.063002,0910.0709v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Fast transport of Bose-Einstein condensates,1970,"  We propose an inverse method to accelerate without final excitation the adiabatic transport of a Bose Einstein condensate. The method, applicable to arbitrary potential traps, is based on a partial extension of the Lewis-Riesenfeld invariants, and provides transport protocols that satisfy exactly the no-excitation conditions without constraints or approximations. This inverse method is complemented by optimizing the trap trajectory with respect to different physical criteria and by studying the effect of noise. ",https://doi.org/10.1088/1367-2630/14/1/013031,1103.2532v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Infrared absorption by graphene-hBN heterostructures,1970,"  We propose a theory of optical absorption in monolayer graphene-hexagonal boron nitride (hBN) heterostructures. In highly oriented heterostructures, the hBN underlay produces a long-range moir\'e superlattice potential for the graphene electrons which modifies the selection rules for absorption of incoming photons in the infrared to visible frequency range. The details of the absorption spectrum modification depend on the relative strength of the various symmetry-allowed couplings between the graphene electrons and the hBN, and the resulting nature of the reconstructed band structure. ",https://doi.org/10.1088/1367-2630/15/12/123009,1309.2292v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Qubit gates with simultaneous transport in double quantum dots,1970,"  A single electron spin in a double quantum dot in a magnetic field is considered in terms of a four-level system. By describing the electron motion between the potential minima by spin-conserving tunneling and spin flip caused by a spin-orbit coupling, we inversely engineer faster-than-adiabatic state manipulation operations based on the geometry of four-dimensional (4D) rotations. In particular, we show how to transport a qubit among the quantum dots performing simultaneously a required spin rotation. ",https://doi.org/10.1088/1367-2630/aaedd9,1809.08952v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",The Future of ChatGPT-enabled Labor Market: A Preliminary Study in China,1970,"  As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill relations in the labor market. As a result, we find that additional 45% occupations in the future will require ChatGPT-related skills. In particular, industries related to technology, products, and operations are expected to have higher proficiency requirements for ChatGPT-related skills, while the manufacturing, services, education, and health science related industries will have lower requirements for ChatGPT-related skills. ",Kein DOI-Link verfügbar,2304.09823v4,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",SPEED: Streaming Partition and Parallel Acceleration for Temporal   Interaction Graph Embedding,1970,"  Temporal Interaction Graphs (TIGs) are widely employed to model intricate real-world systems such as financial systems and social networks. To capture the dynamism and interdependencies of nodes, existing TIG embedding models need to process edges sequentially and chronologically. However, this requirement prevents it from being processed in parallel and struggle to accommodate burgeoning data volumes to GPU. Consequently, many large-scale temporal interaction graphs are confined to CPU processing. Furthermore, a generalized GPU scaling and acceleration approach remains unavailable. To facilitate large-scale TIGs' implementation on GPUs for acceleration, we introduce a novel training approach namely Streaming Edge Partitioning and Parallel Acceleration for Temporal Interaction Graph Embedding (SPEED). The SPEED is comprised of a Streaming Edge Partitioning Component (SEP) which addresses space overhead issue by assigning fewer nodes to each GPU, and a Parallel Acceleration Component (PAC) which enables simultaneous training of different sub-graphs, addressing time overhead issue. Our method can achieve a good balance in computing resources, computing time, and downstream task performance. Empirical validation across 7 real-world datasets demonstrates the potential to expedite training speeds by a factor of up to 19.29x. Simultaneously, resource consumption of a single-GPU can be diminished by up to 69%, thus enabling the multiple GPU-based training and acceleration encompassing millions of nodes and billions of edges. Furthermore, our approach also maintains its competitiveness in downstream tasks. ",Kein DOI-Link verfügbar,2308.14129v2,Yes,"intricate(1), potent(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",MEV Makes Everyone Happy under Greedy Sequencing Rule,1970,"  Trading through decentralized exchanges (DEXs) has become crucial in today's blockchain ecosystem, enabling users to swap tokens efficiently and automatically. However, the capacity of miners to strategically order transactions has led to exploitative practices (e.g., front-running attacks, sandwich attacks) and gain substantial Maximal Extractable Value (MEV) for their own advantage. To mitigate such manipulation, Ferreira and Parkes recently proposed a greedy sequencing rule such that the execution price of transactions in a block moves back and forth around the starting price. Utilizing this sequencing rule makes it impossible for miners to conduct sandwich attacks, consequently mitigating the MEV problem.   However, no sequencing rule can prevent miners from obtaining risk-free profits. This paper systemically studies the computation of a miner's optimal strategy for maximizing MEV under the greedy sequencing rule, where the utility of miners is measured by the overall value of their token holdings. Our results unveil a dichotomy between the no trading fee scenario, which can be optimally strategized in polynomial time, and the scenario with a constant fraction of trading fee, where finding the optimal strategy is proven NP-hard. The latter represents a significant challenge for miners seeking optimal MEV.   Following the computation results, we further show a remarkable phenomenon: Miner's optimal MEV also benefits users. Precisely, in the scenarios without trading fees, when miners adopt the optimal strategy given by our algorithm, all users' transactions will be executed, and each user will receive equivalent or surpass profits compared to their expectations. This outcome provides further support for the study and design of sequencing rules in decentralized exchanges. ",Kein DOI-Link verfügbar,2309.12640v1,Yes,strategically(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Unveiling the Pitfalls of Knowledge Editing for Large Language Models,1970,"  As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing. ",Kein DOI-Link verfügbar,2310.02129v5,Yes,"innovative(1), pivotal(1), potent(2)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",A Multilingual Similarity Dataset for News Article Frame,1970,"  Understanding the writing frame of news articles is vital for addressing social issues, and thus has attracted notable attention in the fields of communication studies. Yet, assessing such news article frames remains a challenge due to the absence of a concrete and unified standard dataset that considers the comprehensive nuances within news content.   To address this gap, we introduce an extended version of a large labeled news article dataset with 16,687 new labeled pairs. Leveraging the pairwise comparison of news articles, our method frees the work of manual identification of frame classes in traditional news frame analysis studies. Overall we introduce the most extensive cross-lingual news article similarity dataset available to date with 26,555 labeled news article pairs across 10 languages. Each data point has been meticulously annotated according to a codebook detailing eight critical aspects of news content, under a human-in-the-loop framework. Application examples demonstrate its potential in unearthing country communities within global news coverage, exposing media bias among news outlets, and quantifying the factors related to news creation. We envision that this news similarity dataset will broaden our understanding of the media ecosystem in terms of news coverage of events and perspectives across countries, locations, languages, and other social constructs. By doing so, it can catalyze advancements in social science research and applied methodologies, thereby exerting a profound impact on our society. ",Kein DOI-Link verfügbar,2405.13272v1,Yes,"meticulous(1), notable(1), potent(1), meticulously(1)"
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Evaluation of Bias Towards Medical Professionals in Large Language   Models,1970,"  This study evaluates whether large language models (LLMs) exhibit biases towards medical professionals. Fictitious candidate resumes were created to control for identity factors while maintaining consistent qualifications. Three LLMs (GPT-4, Claude-3-haiku, and Mistral-Large) were tested using a standardized prompt to evaluate resumes for specific residency programs. Explicit bias was tested by changing gender and race information, while implicit bias was tested by changing names while hiding race and gender. Physician data from the Association of American Medical Colleges was used to compare with real-world demographics. 900,000 resumes were evaluated. All LLMs exhibited significant gender and racial biases across medical specialties. Gender preferences varied, favoring male candidates in surgery and orthopedics, while preferring females in dermatology, family medicine, obstetrics and gynecology, pediatrics, and psychiatry. Claude-3 and Mistral-Large generally favored Asian candidates, while GPT-4 preferred Black and Hispanic candidates in several specialties. Tests revealed strong preferences towards Hispanic females and Asian males in various specialties. Compared to real-world data, LLMs consistently chose higher proportions of female and underrepresented racial candidates than their actual representation in the medical workforce. GPT-4, Claude-3, and Mistral-Large showed significant gender and racial biases when evaluating medical professionals for residency selection. These findings highlight the potential for LLMs to perpetuate biases and compromise healthcare workforce diversity if used without proper bias mitigation strategies. ",Kein DOI-Link verfügbar,2407.12031v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",A Low-dose CT Reconstruction Network Based on TV-regularized OSEM   Algorithm,1970,"  Low-dose computed tomography (LDCT) offers significant advantages in reducing the potential harm to human bodies. However, reducing the X-ray dose in CT scanning often leads to severe noise and artifacts in the reconstructed images, which might adversely affect diagnosis. By utilizing the expectation maximization (EM) algorithm, statistical priors could be combined with artificial priors to improve LDCT reconstruction quality. However, conventional EM-based regularization methods adopt an alternating solving strategy, i.e. full reconstruction followed by image-regularization, resulting in over-smoothing and slow convergence. In this paper, we propose to integrate TV regularization into the ``M''-step of the EM algorithm, thus achieving effective and efficient regularization. Besides, by employing the Chambolle-Pock (CP) algorithm and the ordered subset (OS) strategy, we propose the OSEM-CP algorithm for LDCT reconstruction, in which both reconstruction and regularization are conducted view-by-view. Furthermore, by unrolling OSEM-CP, we propose an end-to-end reconstruction neural network (NN), named OSEM-CPNN, with remarkable performance and efficiency that achieves high-quality reconstructions in just one full-view iteration. Experiments on different models and datasets demonstrate our methods' outstanding performance compared to traditional and state-of-the-art deep-learning methods. ",Kein DOI-Link verfügbar,2408.13832v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Zero-energy modes and valley asymmetry in the Hofstadter spectrum of   bilayer graphene van der Waals heterostructures with hBN,1970,"  We investigate the magnetic minibands of a heterostructure consisting of bilayer graphene (BLG) and hexagonal boron nitride (hBN) by numerically diagonalizing a two-band Hamiltonian that describes electrons in BLG in the presence of a moire potential. Due to inversion-symmetry breaking characteristic for the moire potential, the valley symmetry of the spectrum is broken, but despite this, the zero-energy Landau level in BLG survives, albeit with reduced degeneracy. In addition, we derive effective models for the low-energy features in the magnetic minibands and demonstrate the appearance of secondary Dirac points in the valence band, which we confirm by numerical simulations. Then, we analyze how single-particle gaps in the fractal energy spectrum produce a sequence of incompressible states observable under a variation of carrier density and magnetic field. ",https://doi.org/10.1103/PhysRevB.94.045442,1603.02035v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",The Role of Context Selection in Object Detection,1970,"  We investigate the reasons why context in object detection has limited utility by isolating and evaluating the predictive power of different context cues under ideal conditions in which context provided by an oracle. Based on this study, we propose a region-based context re-scoring method with dynamic context selection to remove noise and emphasize informative context. We introduce latent indicator variables to select (or ignore) potential contextual regions, and learn the selection strategy with latent-SVM. We conduct experiments to evaluate the performance of the proposed context selection method on the SUN RGB-D dataset. The method achieves a significant improvement in terms of mean average precision (mAP), compared with both appearance based detectors and a conventional context model without the selection scheme. ",Kein DOI-Link verfügbar,1609.02948v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Predicting a Two-dimensional P2S3 Monolayer: A Global Minimum Structure,1970,"  Based on extensive evolutionary algorithm driven structural search, we propose a new diphosphorus trisulfide (P2S3) 2D crystal, which is dynamically, thermally and chemically stable as confirmed by the computed phonon spectrum and ab initio molecular dynamics simulations. This 2D crystalline phase of P2S3 corresponds to the global minimum in the Born-Oppenheimer surface of the phosphorus sulfide monolayers with 2:3 stoichiometries. It is a wide band gap (4.55 eV) semiconductor with P-S {\sigma} bonds. The electronic properties of P2S3 structure can be modulated by stacking into multilayer P2S3 structures, forming P2S3 nanoribbons or rolling into P2S3 nanotubes, expanding its potential applications for the emerging field of 2D electronics. ",Kein DOI-Link verfügbar,1703.00605v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Detection of Hermitian connections in wave equations with cubic   non-linearity,1970,"  We consider the geometric non-linear inverse problem of recovering a Hermitian connection $A$ from the source-to-solution map of the cubic wave equation $\Box_{A}\phi+\kappa |\phi|^{2}\phi=f$, where $\kappa\neq 0$ and $\Box_{A}$ is the connection wave operator in the Minkowski space $\mathbb{R}^{1+3}$. The equation arises naturally when considering the Yang-Mills-Higgs equations with Mexican hat type potentials. Our proof exploits the microlocal analysis of nonlinear wave interactions, but instead of employing information contained in the geometry of the wave front sets as in previous literature, we study the principal symbols of waves generated by suitable interactions. Moreover, our approach relies on inversion of a novel non-abelian broken light ray transform. ",Kein DOI-Link verfügbar,1902.05711v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Investigation of Singing Voice Separation for Singing Voice Detection in   Polyphonic Music,1970,"  Singing voice detection (SVD), to recognize vocal parts in the song, is an essential task in music information retrieval (MIR). The task remains challenging since singing voice varies and intertwines with the accompaniment music, especially for some complicated polyphonic music such as choral music recordings. To address this problem, we investigate singing voice detection while discarding the interference from the accompaniment. The proposed SVD has two steps: i. The singing voice separation (SVS) technique is first utilized to filter out the singing voice's potential part coarsely. ii. Upon the continuity of vocal in the time domain, Long-term Recurrent Convolutional Networks (LRCN) is used to learn compositional features. Moreover, to eliminate the outliers, we choose to use a median filter for time-domain smoothing. Experimental results show that the proposed method outperforms the existing state-of-the-art works on two public datasets, the Jamendo Corpus and the RWC pop dataset. ",Kein DOI-Link verfügbar,2004.04040v3,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Sequence Modeling of Temporal Credit Assignment for Episodic   Reinforcement Learning,1970,"  Recent advances in deep reinforcement learning algorithms have shown great potential and success for solving many challenging real-world problems, including Go game and robotic applications. Usually, these algorithms need a carefully designed reward function to guide training in each time step. However, in real world, it is non-trivial to design such a reward function, and the only signal available is usually obtained at the end of a trajectory, also known as the episodic reward or return. In this work, we introduce a new algorithm for temporal credit assignment, which learns to decompose the episodic return back to each time-step in the trajectory using deep neural networks. With this learned reward signal, the learning efficiency can be substantially improved for episodic reinforcement learning. In particular, we find that expressive language models such as the Transformer can be adopted for learning the importance and the dependency of states in the trajectory, therefore providing high-quality and interpretable learned reward signals. We have performed extensive experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns and demonstrated the effectiveness of our algorithm. ",Kein DOI-Link verfügbar,1905.13420v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Effects of coherence on quantum speed limits and shortcuts to   adiabaticity in many-particle systems,1970,"  We discuss the effects of many-body coherence on the speed of evolution of ultracold atomic gases and the relation to quantum speed limits. Our approach is focused on two related systems, spinless fermions and the bosonic Tonks-Girardeau gas, which possess equivalent density dynamics but very different coherence properties. To illustrate the effect of the coherence on the dynamics we consider squeezing an anharmonic potential which confines the particles and find that the speed of the evolution exhibits subtle, but fundamental differences between the two systems. Furthermore, we explore the difference in the driven dynamics by implementing a shortcut to adiabaticity designed to reduce spurious excitations. We show that collisions between the strongly interacting bosons can lead to changes in the coherence which results in different evolution speeds and therefore different fidelities of the final states. ",https://doi.org/10.1103/PhysRevResearch.2.023125,1910.06581v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Molding Free-Space Light with Guided-Wave-Driven Metasurfaces,1970,"  Metasurfaces with unparalleled controllability of light have shown great potential to revolutionize conventional optics. However, they mainly work with free-space light input, which makes it difficult for full on-chip integration. On the other hand, integrated photonics enables densely packed devices but has limited free-space light controllability. Here, we show that judiciously designed guided-wave-driven metasurfaces can mold guided waves into arbitrary free-space modes to achieve complex free-space functions, such as beam steering and focusing, with ultrasmall footprints and potentially no diffraction loss. Based on the same concept together with broken inversion symmetry induced by metasurfaces, we also realized direct orbital angular momentum (OAM) lasing from a micro-ring resonator. Our study works towards complete control of light across integrated photonics and free-space platforms, and paves new exciting ways for creating multifunctional photonic integrated devices with agile access to free space which could enable a plethora of applications in communications, remote sensing, displays, and etc. ",https://doi.org/10.1126/sciadv.abb4142,2001.03001v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Electrostatics in a crooked nanochannel in a newly developed curvilinear   coordinate system,1970,"  Both biological and artificial nanochannels in crooked shape exhibit unusual transportational characteristics, bringing about a challenge to the traditional theoretical analysis of nanofluidics, partly due to their complicated boundary description. In this paper, by developing a curvilinear coordinate system for crooked nanochannels, we successfully solve the electrostatic Poisson-Boltzmann equation analytically for a two-dimensional nanochannel, with its effectiveness confirmed through numerical calculation. The influences of the geometric profile of the nanochannel on the distribution of electric potential, ionic concentration, and surface charge on channel walls can be quantitatively evaluated in a facilitated way in terms of these curvilinear coordinates. Such a technique can be widely applied to many nanofluidic systems. ",Kein DOI-Link verfügbar,2012.07482v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Screening and understanding Li adsorption on 2-dimensional metallic   materials by learning physics,1970,"  Two-dimensional (2D) materials have received considerable attention as possible electrodes in Li-ion batteries (LIBs), although a deeper understanding of the Li adsorption behavior as well as broad screening of the materials space is still needed. In this work, we build a high-throughput screening scheme that incorporates a learned interaction. First, density functional theory and graph convolution networks are utilized to calculate minimum Li adsorption energies for a small set of 2D metallic materials. The data is then used to find a dependence of the minimum Li adsorption energies on the sum of ionization potential, work function of the 2D metal, and coupling energy between Li+ and substrate. Our results show that variances of elemental properties and density are the most correlated features with coupling. To illustrate the applicability of this approach, the model is employed to show that some fluorides and chromium oxides are potential high-voltage materials with adsorption energies < -7 eV, and the found physics is used as the design principle to enhance the Li adsorption ability of graphene. This physics-driven approach shows higher accuracy and transferability compared with purely data-driven models. ",https://doi.org/10.1021/jacsau.1c00260,2101.00067v1,Yes,potent(2)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Digitized Adiabatic Quantum Factorization,1970,"  Quantum integer factorization is a potential quantum computing solution that may revolutionize cryptography. Nevertheless, a scalable and efficient quantum algorithm for noisy intermediate-scale quantum computers looks far-fetched. We propose an alternative factorization method, within the digitized-adiabatic quantum computing paradigm, by digitizing an adiabatic quantum factorization algorithm enhanced by shortcuts to adiabaticity techniques. We find that this fast factorization algorithm is suitable for available gate-based quantum computers. We test our quantum algorithm in an IBM quantum computer with up to six qubits, surpassing the performance of the more commonly used factorization algorithms on the long way towards quantum advantage. ",https://doi.org/10.1103/PhysRevA.104.L050403,2105.09480v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Machine-learning assisted quantum control in random environment,1970,"  Disorder in condensed matter and atomic physics is responsible for a great variety of fascinating quantum phenomena, which are still challenging for understanding, not to mention the relevant dynamical control. Here we introduce proof of the concept and analyze neural network-based machine learning algorithm for achieving feasible high-fidelity quantum control of a particle in random environment. To explicitly demonstrate its capabilities, we show that convolutional neural networks are able to solve this problem as they can recognize the disorder and, by supervised learning, further produce the policy for the efficient low-energy cost control of a quantum particle in a time-dependent random potential. We have shown that the accuracy of the proposed algorithm is enhanced by a higher-dimensional mapping of the disorder pattern and using two neural networks, each properly trained for the given task. The designed method, being computationally more efficient than the gradient-descent optimization, can be applicable to identify and control various noisy quantum systems on a heuristic basis. ",https://doi.org/10.1103/PhysRevApplied.17.024040,2202.10291v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Molecular conformer search with low-energy latent space,1970,"  Identifying low-energy conformers with quantum mechanical accuracy for molecules with many degrees of freedom is challenging. In this work, we use the molecular dihedral angles as features and explore the possibility of performing molecular conformer search in a latent space with a generative model named variational auto-encoder (VAE). We bias the VAE towards low-energy molecular configurations to generate more informative data. In this way, we can effectively build a reliable energy model for the low-energy potential energy surface. After the energy model has been built, we extract local-minimum conformations and refine them with structure optimization. We have tested and benchmarked our low-energy latent-space (LOLS) structure search method on organic molecules with $5-9$ searching dimensions. Our results agree with previous studies. ",Kein DOI-Link verfügbar,2203.14012v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Photonic counterdiabatic quantum optimization algorithm,1970,"  We propose a hybrid quantum-classical approximate optimization algorithm for photonic quantum computing, specifically tailored for addressing continuous-variable optimization problems. Inspired by counterdiabatic protocols, our algorithm significantly reduces the required quantum operations for optimization as compared to adiabatic protocols. This reduction enables us to tackle non-convex continuous optimization and countably infinite integer programming within the near-term era of quantum computing. Through comprehensive benchmarking, we demonstrate that our approach outperforms existing state-of-the-art hybrid adiabatic quantum algorithms in terms of convergence and implementability. Remarkably, our algorithm offers a practical and accessible experimental realization, bypassing the need for high-order operations and overcoming experimental constraints. We conduct proof-of-principle experiments on an eight-mode nanophotonic quantum chip, successfully showcasing the feasibility and potential impact of the algorithm. ",Kein DOI-Link verfügbar,2307.14853v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Does Combining Parameter-efficient Modules Improve Few-shot Transfer   Accuracy?,1970,"  Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full fine-tuning and training a LoRA from scratch. Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters. Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters. ",Kein DOI-Link verfügbar,2402.15414v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Unconventional Localization Prior to Wrinkles and Controllable Surface   Patterns of Film Substrate Bilayers Through Patterned Defects in Substrate,1970,"  A novel bilayer is introduced, consisting of a stiff film adhered to a soft substrate with patterned holes beneath the film and substrate interface. To uncover the transition of surface patterns, two dimensional plane strain simulations are performed on the defected bilayer subjected to uniaxial compression. Although the substrate is considered as the linear elastic material, the presence of defects can directly trigger the formation of locally ridged and then folding configurations from flat surface with a relatively small compressive strain. It is followed by the coexisting phases of folds and wrinkles under further overall compression. This phase transition reverses the traditional transition of wrinkle to ridge or fold for defect free substrates. It is also found that the onset of initial bifurcation is highly dependent on the spatial configuration and geometries of holes, since the interaction of defects allows more strain relief mechanisms beyond wrinkling. Furthermore, a rich diversity of periodic surface topologies, including overall waves, localizations, saw like and coexisting features of folds and wrinkles can be obtained by varying the diameter, depth and spacing of holes as well as compressive strain, which provides a potential approach to engineer various surface patterns for applications. ",Kein DOI-Link verfügbar,1703.10078v1,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Quantum Advantage in Cryptography with a Low-Connectivity Quantum   Annealer,1970,"  The application in cryptography of quantum algorithms for prime factorization fostered the interest in quantum computing. However, quantum computers, and particularly quantum annealers, can also be helpful to construct secure cryptographic keys. Indeed, finding robust Boolean functions for cryptography is an important problem in sequence ciphers, block ciphers, and hash functions, among others. Due to the super-exponential size $\mathcal{O}(2^{2^n})$ of the associated space, finding $n$-variable Boolean functions with global cryptographic constraints is computationally hard. This problem has already been addressed employing generic low-connected incoherent D-Wave quantum annealers. However, the limited connectivity of the Chimera graph, together with the exponential growth in the complexity of the Boolean function design problem, limit the problem scalability. Here, we propose a special-purpose coherent quantum annealing architecture with three couplers per qubit, designed to optimally encode the bent function design problem. A coherent quantum annealer with this tree-type architecture has the potential to solve the $8$-variable bent function design problem, which is classically unsolved, with only $127$ physical qubits and $126$ couplers. This paves the way to reach useful quantum supremacy within the framework of quantum annealing for cryptographic purposes. ",https://doi.org/10.1103/PhysRevApplied.13.054062,1906.08140v2,Yes,potent(1)
0000-0002-3935-6480,Xi Chen,"Fudan University, Huashan Hospital Fudan University",Autoregressive-Model-Based Methods for Online Time Series Prediction   with Missing Values: an Experimental Evaluation,1970,"  Time series prediction with missing values is an important problem of time series analysis since complete data is usually hard to obtain in many real-world applications. To model the generation of time series, autoregressive (AR) model is a basic and widely used one, which assumes that each observation in the time series is a noisy linear combination of some previous observations along with a constant shift. To tackle the problem of prediction with missing values, a number of methods were proposed based on various data models. For real application scenarios, how do these methods perform over different types of time series with different levels of data missing remains to be investigated. In this paper, we focus on online methods for AR-model-based time series prediction with missing values. We adapted five mainstream methods to fit in such a scenario. We make detailed discussion on each of them by introducing their core ideas about how to estimate the AR coefficients and their different strategies to deal with missing values. We also present algorithmic implementations for better understanding. In order to comprehensively evaluate these methods and do the comparison, we conduct experiments with various configurations of relative parameters over both synthetic and real data. From the experimental results, we derived several noteworthy conclusions and shows that imputation is a simple but reliable strategy to handle missing values in online prediction tasks. ",Kein DOI-Link verfügbar,1908.06729v2,Yes,noteworthy(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",UniCL: A Universal Contrastive Learning Framework for Large Time Series   Models,1970,"  Time-series analysis plays a pivotal role across a range of critical applications, from finance to healthcare, which involves various tasks, such as forecasting and classification. To handle the inherent complexities of time-series data, such as high dimensionality and noise, traditional supervised learning methods first annotate extensive labels for time-series data in each task, which is very costly and impractical in real-world applications. In contrast, pre-trained foundation models offer a promising alternative by leveraging unlabeled data to capture general time series patterns, which can then be fine-tuned for specific tasks. However, existing approaches to pre-training such models typically suffer from high-bias and low-generality issues due to the use of predefined and rigid augmentation operations and domain-specific data training. To overcome these limitations, this paper introduces UniCL, a universal and scalable contrastive learning framework designed for pretraining time-series foundation models across cross-domain datasets. Specifically, we propose a unified and trainable time-series augmentation operation to generate pattern-preserved, diverse, and low-bias time-series data by leveraging spectral information. Besides, we introduce a scalable augmentation algorithm capable of handling datasets with varying lengths, facilitating cross-domain pretraining. Extensive experiments on two benchmark datasets across eleven domains validate the effectiveness of UniCL, demonstrating its high generalization on time-series analysis across various fields. ",Kein DOI-Link verfügbar,2405.10597v1,Yes,pivotal(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",New horizon in the statistical physics of earthquakes: Dragon-king   theory and dragon-king earthquakes,1970,"  A systematic quantitative investigation into whether the mechanisms of large earthquakes are unique could significantly deepen our understanding of fault rupture and seismicity patterns. This research holds the potential to advance our ability to predict large earthquakes and enhance the effectiveness of disaster prevention and mitigation strategies. In 2009, one of us introduced the dragon-king theory, offering a quantitative framework for identifying and testing extreme outliers-referred to as dragon-king events-that are endogenously generated. This theory provides valuable tools for explaining, predicting, and managing the risks associated with these rare but highly impactful events. The present paper discusses the feasibility of applying this theory to seismology, proposing that dragon-king earthquake events can be identified as outliers to the Gutenberg-Richter law. It also examines several seismological mechanisms that may contribute to the occurrence of these extraordinary events. Although applying the dragon-king theory to seismology presents practical challenges, it offers the potential to significantly enrich statistical seismology. By reexamining the classification of earthquake rupture types through a statistical testing lens and integrating these insights with underlying physical mechanisms, this approach can greatly enhance the analytical tools and depth of research in the field of statistical seismology. ",Kein DOI-Link verfügbar,2408.10857v1,Yes,potent(2)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Ask to Understand: Question Generation for Multi-hop Question Answering,1970,"  Multi-hop Question Answering (QA) requires the machine to answer complex questions by finding scattering clues and reasoning from multiple documents. Graph Network (GN) and Question Decomposition (QD) are two common approaches at present. The former uses the ""black-box"" reasoning process to capture the potential relationship between entities and sentences, thus achieving good performance. At the same time, the latter provides a clear reasoning logical route by decomposing multi-hop questions into simple single-hop sub-questions. In this paper, we propose a novel method to complete multi-hop QA from the perspective of Question Generation (QG). Specifically, we carefully design an end-to-end QG module on the basis of a classical QA module, which could help the model understand the context by asking inherently logical sub-questions, thus inheriting interpretability from the QD-based method and showing superior performance. Experiments on the HotpotQA dataset demonstrate that the effectiveness of our proposed QG module, human evaluation further clarifies its interpretability quantitatively, and thorough analysis shows that the QG module could generate better sub-questions than QD methods in terms of fluency, consistency, and diversity. ",Kein DOI-Link verfügbar,2203.09073v1,Yes,potent(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",An Adaptive Dimension Reduction Estimation Method for High-dimensional   Bayesian Optimization,1970,"  Bayesian optimization (BO) has shown impressive results in a variety of applications within low-to-moderate dimensional Euclidean spaces. However, extending BO to high-dimensional settings remains a significant challenge. We address this challenge by proposing a two-step optimization framework. Initially, we identify the effective dimension reduction (EDR) subspace for the objective function using the minimum average variance estimation (MAVE) method. Subsequently, we construct a Gaussian process model within this EDR subspace and optimize it using the expected improvement criterion. Our algorithm offers the flexibility to operate these steps either concurrently or in sequence. In the sequential approach, we meticulously balance the exploration-exploitation trade-off by distributing the sampling budget between subspace estimation and function optimization, and the convergence rate of our algorithm in high-dimensional contexts has been established. Numerical experiments validate the efficacy of our method in challenging scenarios. ",Kein DOI-Link verfügbar,2403.05425v1,Yes,"meticulous(1), meticulously(1)"
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Revisiting Seismicity Criticality: A New Framework for Bias Correction   of Statistical Seismology Model Calibrations,1970,"  The Epidemic-Type Aftershock Sequences (ETAS) model and its variants effectively capture the space-time clustering of seismicity, setting the standard for earthquake forecasting. Accurate unbiased ETAS calibration is thus crucial. But we identify three sources of bias, (i) boundary effects, (ii) finite-size effects, and (iii) censorship, which are often overlooked or misinterpreted, causing errors in seismic analysis and predictions. By employing an ETAS model variant with variable spatial background rates, we propose a method to correct for these biases, focusing on the branching ratio n, a key indicator of earthquake triggering potential. Our approach quantifies the variation in the apparent branching ratio (napp) with increased cut-off magnitude (Mco) above the optimal cut-off (Mcobest). The napp(Mco) function yields insights superior to traditional point estimates. We validate our method using synthetic earthquake catalogs, accurately recovering the true branching ratio (ntrue) after correcting biases with napp(Mco). Additionally, our method introduces a refined estimation of the minimum triggering magnitude (m0), a crucial parameter in the ETAS model. Applying our framework to the earthquake catalogs of California, New Zealand, and the China Seismic Experimental Site (CSES) in Sichuan and Yunnan provinces, we find that seismicity hovers away from the critical point, nc = 1, remaining distinctly subcritical, however with values tending to be larger than recent reports that do not consider the above biases. It is interesting that, m0 is found around 4 for California, 3 for New Zealand and 2 for CSES, suggesting that many small triggered earthquakes may not be fertile. Understanding seismicity's critical state significantly enhances our comprehension of seismic patterns, aftershock predictability, and informs earthquake risk mitigation and management strategies. ",Kein DOI-Link verfügbar,2404.16374v1,Yes,potent(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Complexity Matters: Rethinking the Latent Space for Generative Modeling,1970,"  In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel ""distance"" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity. ",Kein DOI-Link verfügbar,2307.08283v2,Yes,pivotal(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Giant Anomalous Hall and Nernst Effects in a Heavy Fermion Ferromagnet,1970,"  The anomalous Hall and Nernst effects describe the voltage drop perpendicular to an applied current and temperature gradient due to the magnetization of a magnetic material. These effects can be utilized to measure the Berry curvature at the Fermi energy, and have potential applications in future electronic devices and thermoelectric energy conversion. In this paper, we report giant anomalous Hall conductivity and anomalous Nernst coefficient, as high as about 1000 $\Omega^{-1}$ cm$^{-1}$ and 10 $\mu$V K$^{-1}$, respectively, in a heavy fermion ferromagnet, CeCrGe$_3$. This compound uniquely manifests strong hybridization between the 4$f$ and conduction electrons, leading to a Kondo lattice state in the presence of ferromagnetic order. Unlike conventional topological semimetals in which the electron correlation is weak, CeCrGe$_3$ manifests a strong Berry curvature field of the heavy fermion with an extremely low Fermi energy. Our findings pave the way for exploring correlation-driven topological responses in a ferromagnetic Kondo lattice environment. ",Kein DOI-Link verfügbar,2401.17624v1,Yes,potent(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University","MindLLM: Pre-training Lightweight Large Language Model from Scratch,   Evaluations and Domain Applications",1970,"  Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models. ",Kein DOI-Link verfügbar,2310.15777v2,Yes,innovative(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Methods for Acquiring and Incorporating Knowledge into Stock Price   Prediction: A Survey,1970,"  Predicting stock prices presents a challenging research problem due to the inherent volatility and non-linear nature of the stock market. In recent years, knowledge-enhanced stock price prediction methods have shown groundbreaking results by utilizing external knowledge to understand the stock market. Despite the importance of these methods, there is a scarcity of scholarly works that systematically synthesize previous studies from the perspective of external knowledge types. Specifically, the external knowledge can be modeled in different data structures, which we group into non-graph-based formats and graph-based formats: 1) non-graph-based knowledge captures contextual information and multimedia descriptions specifically associated with an individual stock; 2) graph-based knowledge captures interconnected and interdependent information in the stock market. This survey paper aims to provide a systematic and comprehensive description of methods for acquiring external knowledge from various unstructured data sources and then incorporating it into stock price prediction models. We also explore fusion methods for combining external knowledge with historical price features. Moreover, this paper includes a compilation of relevant datasets and delves into potential future research directions in this domain. ",Kein DOI-Link verfügbar,2308.04947v1,Yes,"potent(1), scholarly(1)"
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",ALIP: Adaptive Language-Image Pre-training with Synthetic Caption,1970,"  Contrastive Language-Image Pre-training (CLIP) has significantly boosted the performance of various vision-language tasks by scaling up the dataset with image-text pairs collected from the web. However, the presence of intrinsic noise and unmatched image-text pairs in web data can potentially affect the performance of representation learning. To address this issue, we first utilize the OFA model to generate synthetic captions that focus on the image content. The generated captions contain complementary information that is beneficial for pre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP), a bi-path model that integrates supervision from both raw text and synthetic caption. As the core components of ALIP, the Language Consistency Gate (LCG) and Description Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during the training process. Meanwhile, the adaptive contrastive loss can effectively reduce the impact of noise data and enhances the efficiency of pre-training data. We validate ALIP with experiments on different scales of models and pre-training datasets. Experiments results show that ALIP achieves state-of-the-art performance on multiple downstream tasks including zero-shot image-text retrieval and linear probe. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/ALIP. ",Kein DOI-Link verfügbar,2308.08428v2,Yes,potent(1)
0000-0001-9957-8192,Jiawei Li,"Fudan University, Zhongshan Hospital Fudan University",Code Smells in Machine Learning Systems,1970,"  As Deep learning (DL) systems continuously evolve and grow, assuring their quality becomes an important yet challenging task. Compared to non-DL systems, DL systems have more complex team compositions and heavier data dependency. These inherent characteristics would potentially cause DL systems to be more vulnerable to bugs and, in the long run, to maintenance issues. Code smells are empirically tested as efficient indicators of non-DL systems. Therefore, we took a step forward into identifying code smells, and understanding their impact on maintenance in this comprehensive study. This is the first study on investigating code smells in the context of DL software systems, which helps researchers and practitioners to get a first look at what kind of maintenance modification made and what code smells developers have been dealing with. Our paper has three major contributions. First, we comprehensively investigated the maintenance modifications that have been made by DL developers via studying the evolution of DL systems, and we identified nine frequently occurred maintenance-related modification categories in DL systems. Second, we summarized five code smells in DL systems. Third, we validated the prevalence, and the impact of our newly identified code smells through a mixture of qualitative and quantitative analysis. We found that our newly identified code smells are prevalent and impactful on the maintenance of DL systems from the developer's perspective. ",Kein DOI-Link verfügbar,2203.00803v1,Yes,potent(1)
0000-0002-8792-1704,Hongzhi Xu,"Fudan University, Huashan Hospital Fudan University",Real-time landmark detection for precise endoscopic submucosal   dissection via shape-aware relation network,1970,"  We propose a novel shape-aware relation network for accurate and real-time landmark detection in endoscopic submucosal dissection (ESD) surgery. This task is of great clinical significance but extremely challenging due to bleeding, lighting reflection, and motion blur in the complicated surgical environment. Compared with existing solutions, which either neglect geometric relationships among targeting objects or capture the relationships by using complicated aggregation schemes, the proposed network is capable of achieving satisfactory accuracy while maintaining real-time performance by taking full advantage of the spatial relations among landmarks. We first devise an algorithm to automatically generate relation keypoint heatmaps, which are able to intuitively represent the prior knowledge of spatial relations among landmarks without using any extra manual annotation efforts. We then develop two complementary regularization schemes to progressively incorporate the prior knowledge into the training process. While one scheme introduces pixel-level regularization by multi-task learning, the other integrates global-level regularization by harnessing a newly designed grouped consistency evaluator, which adds relation constraints to the proposed network in an adversarial manner. Both schemes are beneficial to the model in training, and can be readily unloaded in inference to achieve real-time detection. We establish a large in-house dataset of ESD surgery for esophageal cancer to validate the effectiveness of our proposed method. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and efficiency, achieving better detection results faster. Promising results on two downstream applications further corroborate the great potential of our method in ESD clinical practice. ",Kein DOI-Link verfügbar,2111.04733v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Gene Regulatory Network Inference from Pre-trained Single-Cell   Transcriptomics Transformer with Joint Graph Learning,1970,"  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data is a complex challenge that requires capturing the intricate relationships between genes and their regulatory interactions. In this study, we tackle this challenge by leveraging the single-cell BERT-based pre-trained transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to augment structured biological knowledge from existing GRNs. We introduce a novel joint graph learning approach that combines the rich contextual representations learned by pre-trained single-cell language models with the structured knowledge encoded in GRNs using graph neural networks (GNNs). By integrating these two modalities, our approach effectively reasons over boththe gene expression level constraints provided by the scRNA-seq data and the structured biological knowledge inherent in GRNs. We evaluate our method on human cell benchmark datasets from the BEELINE study with cell type-specific ground truth networks. The results demonstrate superior performance over current state-of-the-art baselines, offering a deeper understanding of cellular regulatory mechanisms. ",Kein DOI-Link verfügbar,2407.18181v1,Yes,intricate(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",MGE: A Training-Free and Efficient Model Generation and Enhancement   Scheme,1970,"  To provide a foundation for the research of deep learning models, the construction of model pool is an essential step. This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance. Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases. Moreover, the time consumed in generating models accounts for only 1\% of the time required for normal model training. More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in few-shot tasks. And the behavioral dissimilarity of generated models has the potential of adversarial defense. ",Kein DOI-Link verfügbar,2402.17486v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",TTM-RE: Memory-Augmented Document-Level Relation Extraction,1970,"  Document-level relation extraction aims to categorize the association between any two entities within a document. We find that previous methods for document-level relation extraction are ineffective in exploiting the full potential of large amounts of training data with varied noise levels. For example, in the ReDocRED benchmark dataset, state-of-the-art methods trained on the large-scale, lower-quality, distantly supervised training data generally do not perform better than those trained solely on the smaller, high-quality, human-annotated training data. To unlock the full potential of large-scale noisy training data for document-level relation extraction, we propose TTM-RE, a novel approach that integrates a trainable memory module, known as the Token Turing Machine, with a noisy-robust loss function that accounts for the positive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark dataset for document-level relation extraction, reveal that TTM-RE achieves state-of-the-art performance (with an absolute F1 score improvement of over 3%). Ablation studies further illustrate the superiority of TTM-RE in other domains (the ChemDisGene dataset in the biomedical domain) and under highly unlabeled settings. ",Kein DOI-Link verfügbar,2406.05906v1,Yes,potent(2)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Improved Belief Propagation Decoding Algorithms for Surface Codes,1970,"  Quantum error correction is crucial for universal fault-tolerant quantum computing. Highly accurate and low-time-complexity decoding algorithms play an indispensable role in making sure quantum error correction works. Among existing decoding algorithms, belief propagation (BP) is notable for its nearly linear time complexity and general applicability to stabilizer codes. However, BP's decoding accuracy without post-processing is unsatisfactory in most situations. This article focuses on improving the decoding accuracy of BP over GF(4) for surface codes. We first propose Momentum-BP and AdaGrad-BP, inspired by machine learning optimization techniques, to reduce oscillation in message updating and break the symmetric trapping sets. We further propose EWAInit-BP, which adaptively updates initial probabilities and provides a 1 to 3 orders of magnitude improvement over traditional BP for planar surface code, toric code, and XZZX surface code without any post-processing method, showing high decoding accuracy even under parallel scheduling. The theoretical $O(1)$ time complexity under parallel scheduling and high accuracy of EWAInit-BP make it a promising candidate for high-precision real-time decoders. Meanwhile, the ideas of the Momentum-BP, AdaGrad-BP and EWAInit-BP provide promising approaches to improve the decoding accuracy of BP to get rid of its reliance on post-processing. ",Kein DOI-Link verfügbar,2407.11523v3,Yes,notable(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Semi-supervised Transfer Learning for Evaluation of Model Classification   Performance,1970,"  In modern machine learning applications, frequent encounters of covariate shift and label scarcity have posed challenges to robust model training and evaluation. Numerous transfer learning methods have been developed to robustly adapt the model itself to some unlabeled target populations using existing labeled data in a source population. However, there is a paucity of literature on transferring performance metrics of a trained model. In this paper, we aim to evaluate the performance of a trained binary classifier on unlabeled target population based on receiver operating characteristic (ROC) analysis. We proposed $\bf S$emi-supervised $\bf T$ransfer l$\bf E$arning of $\bf A$ccuracy $\bf M$easures (STEAM), an efficient three-step estimation procedure that employs 1) double-index modeling to construct calibrated density ratio weights and 2) robust imputation to leverage the large amount of unlabeled data to improve estimation efficiency. We establish the consistency and asymptotic normality of the proposed estimators under correct specification of either the density ratio model or the outcome model. We also correct for potential overfitting bias in the estimators in finite samples with cross-validation. We compare our proposed estimators to existing methods and show reductions in bias and gains in efficiency through simulations. We illustrate the practical utility of the proposed method on evaluating prediction performance of a phenotyping model for Rheumatoid Arthritis (RA) on a temporally evolving EHR cohort. ",Kein DOI-Link verfügbar,2208.07927v2,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak   Supervision,1970,"  We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus (2020-03-13). This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines. CORD-NER annotation is a combination of four sources with different NER methods. The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool. Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples. We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system. ",Kein DOI-Link verfügbar,2003.12218v5,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",MEGClass: Extremely Weakly Supervised Text Classification via   Mutually-Enhancing Text Granularities,1970,"  Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document's most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods. ",Kein DOI-Link verfügbar,2304.01969v2,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and   Multi-View Transformer,1970,"  Deciphering the intricacies of the human brain has captivated curiosity for centuries. Recent strides in Brain-Computer Interface (BCI) technology, particularly using motor imagery, have restored motor functions such as reaching, grasping, and walking in paralyzed individuals. However, unraveling natural language from brain signals remains a formidable challenge. Electroencephalography (EEG) is a non-invasive technique used to record electrical activity in the brain by placing electrodes on the scalp. Previous studies of EEG-to-text decoding have achieved high accuracy on small closed vocabularies, but still fall short of high accuracy when dealing with large open vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy of open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG pre-training to enhance the learning of semantics from EEG signals and proposes a multi-view transformer to model the EEG signal processing by different spatial regions of the brain. Experiments show that EEG2TEXT has superior performance, outperforming the state-of-the-art baseline methods by a large margin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great potential for a high-performance open-vocabulary brain-to-text system to facilitate communication. ",Kein DOI-Link verfügbar,2405.02165v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Hybrid HMM Decoder For Convolutional Codes By Joint Trellis-Like   Structure and Channel Prior,1970,"  The anti-interference capability of wireless links is a physical layer problem for edge computing. Although convolutional codes have inherent error correction potential due to the redundancy introduced in the data, the performance of the convolutional code is drastically degraded due to multipath effects on the channel. In this paper, we propose the use of a Hidden Markov Model (HMM) for the reconstruction of convolutional codes and decoding by the Viterbi algorithm. Furthermore, to implement soft-decision decoding, the observation of HMM is replaced by Gaussian mixture models (GMM). Our method provides superior error correction potential than the standard method because the model parameters contain channel state information (CSI). We evaluated the performance of the method compared to standard Viterbi decoding by numerical simulation. In the multipath channel, the hybrid HMM decoder can achieve a performance gain of 4.7 dB and 2 dB when using hard-decision and soft-decision decoding, respectively. The HMM decoder also achieves significant performance gains for the RSC code, suggesting that the method could be extended to turbo codes. ",https://doi.org/10.1109/TCCN.2022.3220766,2210.14749v2,Yes,potent(2)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",MKDTI: Predicting drug-target interactions via multiple kernel fusion on   graph attention network,1970,"  Drug-target relationships may now be predicted computationally using bioinformatics data, which is a valuable tool for understanding pharmacological effects, enhancing drug development efficiency, and advancing related research. A number of structure-based, ligand-based and network-based approaches have now emerged. Furthermore, the integration of graph attention networks with intricate drug target studies is an application area of growing interest. In our work, we formulate a model called MKDTI by extracting kernel information from various layer embeddings of a graph attention network. This combination improves the prediction ability with respect to novel drug-target relationships. We first build a drug-target heterogeneous network using heterogeneous data of drugs and targets, and then use a self-enhanced multi-head graph attention network to extract potential features in each layer. Next, we utilize embeddings of each layer to computationally extract kernel matrices and fuse multiple kernel matrices. Finally, we use a Dual Laplacian Regularized Least Squares framework to forecast novel drug-target entity connections. This prediction can be facilitated by integrating the kernel matrix associated with the drug-target. We measured our model's efficacy using AUPR and AUC. Compared to the benchmark algorithms, our model outperforms them in the prediction outcomes. In addition, we conducted an experiment on kernel selection. The results show that the multi-kernel fusion approach combined with the kernel matrix generated by the graph attention network provides complementary insights into the model. The fusion of this information helps to enhance the accuracy of the predictions. ",Kein DOI-Link verfügbar,2407.10055v1,Yes,"intricate(1), potent(1)"
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Reconstructing bifurcation diagrams of chaotic circuits with reservoir   computing,1970,"  Model-free reconstruction of the bifurcation diagrams of Chua's circuits by the technique of parameter-aware reservoir computing is investigated. We demonstrate that: (1) reservoir computer can be utilized as a noise filter to recover the system trajectory from noisy signals; (2) for a single Chua circuit, the machine trained by the noisy time series measured at several sampling states is capable of reconstructing the whole bifurcation diagram of the circuit with a high precision; (3) for two coupled chaotic Chua circuits of mismatched parameters, the machine trained by the noisy time series measured at several coupling strengths is able to anticipate the variation of the synchronization degree of the coupled circuits with respect to the coupling strength over a wide range. The studies verify the capability of the technique of parameter-aware reservoir computing in learning the dynamics of chaotic circuits from noisy signals, signifying the potential application of this technique in reconstructing the bifurcation diagram of real-world chaotic systems. ",Kein DOI-Link verfügbar,2309.09986v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise   Resource Sharing,1970,"  Deep learning (DL) has demonstrated significant success across diverse fields, leading to the construction of dedicated GPU accelerators within GPU clusters for high-quality training services. Efficient scheduler designs for such clusters are vital to reduce operational costs and enhance resource utilization. While recent schedulers have shown impressive performance in optimizing DL job performance and cluster utilization through periodic reallocation or selection of GPU resources, they also encounter challenges such as preemption and migration overhead, along with potential DL accuracy degradation. Nonetheless, few explore the potential benefits of GPU sharing to improve resource utilization and reduce job queuing times. Motivated by these insights, we present a job scheduling model allowing multiple jobs to share the same set of GPUs without altering job training settings. We introduce SJF-BSBF (shortest job first with best sharing benefit first), a straightforward yet effective heuristic scheduling algorithm. SJF-BSBF intelligently selects job pairs for GPU resource sharing and runtime settings (sub-batch size and scheduling time point) to optimize overall performance while ensuring DL convergence accuracy through gradient accumulation. In experiments with both physical DL workloads and trace-driven simulations, even as a preemption-free policy, SJF-BSBF reduces the average job completion time by 27-33\% relative to the state-of-the-art preemptive DL schedulers. Moreover, SJF-BSBF can wisely determine the optimal resource sharing settings, such as the sharing time point and sub-batch size for gradient accumulation, outperforming the aggressive GPU sharing approach (baseline SJF-FFS policy) by up to 17\% in large-scale traces. ",Kein DOI-Link verfügbar,2407.13088v1,Yes,potent(2)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Direct Detections of Dark Matter in the Presence of Non-standard   Neutrino Interactions,1970,"  In this paper we investigate impacts of non-standard neutrino interactions (NSIs) to the limitations on the discovery potential of dark matter in direct detection experiments. New neutrino floors are derived taking into account current upper bounds on the effective couplings of various NSIs. Our study shows that the neutrino floors of the standard model neutral current interactions can be significantly changed in the presence of vector-current NSI and scalar-current NSI, and the neutrino floors can be raised up to about ${\cal O}(20\%)$ in the presence of pseudo-scalar-current NSI, and there are almost no impacts to the neutrino floors from the axial-vector NSI and the tensor NSI. We suggest combining the dark matter direct detection experiments with the coherent elastic neutrino nucleus scattering experiments to hunt for new physics behind the signal of nuclear recoil in the future. ",https://doi.org/10.1088/1475-7516/2019/08/010,1904.11214v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Review of Learning-Assisted Power System Optimization,1970,"  With dramatic breakthroughs in recent years, machine learning is showing great potential to upgrade the toolbox for power system optimization. Understanding the strength and limitation of machine learning approaches is crucial to decide when and how to deploy them to boost the optimization performance. This paper pays special attention to the coordination between machine learning approaches and optimization models, and carefully evaluates how such data-driven analysis may improve the rule-based optimization. The typical references are selected and categorized into four groups: the boundary parameter improvement, the optimization option selection, the surrogate model, and the hybrid model. This taxonomy provides a novel perspective to elaborate the latest research progress and development. We further compare the design patterns of different categories, and discuss several key challenges and opportunities as well. Deep integration between machine learning approaches and optimization models is expected to become the most promising technical trend. ",https://doi.org/10.17775/CSEEJPES.2020.03070,2007.00210v2,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",ConFL: Constraint-guided Fuzzing for Machine Learning Framework,1970,"  As machine learning gains prominence in various sectors of society for automated decision-making, concerns have risen regarding potential vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing these frameworks is a daunting task due to their intricate implementation. Previous research on fuzzing ML frameworks has struggled to effectively extract input constraints and generate valid inputs, leading to extended fuzzing durations for deep execution or revealing the target crash.   In this paper, we propose ConFL, a constraint-guided fuzzer for ML frameworks. ConFL automatically extracting constraints from kernel codes without the need for any prior knowledge. Guided by the constraints, ConFL is able to generate valid inputs that can pass the verification and explore deeper paths of kernel codes. In addition, we design a grouping technique to boost the fuzzing efficiency.   To demonstrate the effectiveness of ConFL, we evaluated its performance mainly on Tensorflow. We find that ConFL is able to cover more code lines, and generate more valid inputs than state-of-the-art (SOTA) fuzzers. More importantly, ConFL found 84 previously unknown vulnerabilities in different versions of Tensorflow, all of which were assigned with new CVE ids, of which 3 were critical-severity and 13 were high-severity. We also extended ConFL to test PyTorch and Paddle, 7 vulnerabilities are found to date. ",Kein DOI-Link verfügbar,2307.05642v1,Yes,"intricate(1), potent(1)"
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",KnobTree: Intelligent Database Parameter Configuration via Explainable   Reinforcement Learning,1970,"  Databases are fundamental to contemporary information systems, yet traditional rule-based configuration methods struggle to manage the complexity of real-world applications with hundreds of tunable parameters. Deep reinforcement learning (DRL), which combines perception and decision-making, presents a potential solution for intelligent database configuration tuning. However, due to black-box property of RL-based method, the generated database tuning strategies still face the urgent problem of lack explainability. Besides, the redundant parameters in large scale database always make the strategy learning become unstable. This paper proposes KnobTree, an interpertable framework designed for the optimization of database parameter configuration. In this framework, an interpertable database tuning algorithm based on RL-based differentatial tree is proposed, which building a transparent tree-based model to generate explainable database tuning strategies. To address the problem of large-scale parameters, We also introduce a explainable method for parameter importance assessment, by utilizing Shapley Values to identify parameters that have significant impacts on database performance. Experiments conducted on MySQL and Gbase8s databases have verified exceptional transparency and interpretability of the KnobTree model. The good property makes generated strategies can offer practical guidance to algorithm designers and database administrators. Moreover, our approach also slightly outperforms the existing RL-based tuning algorithms in aspects such as throughput, latency, and processing time. ",Kein DOI-Link verfügbar,2406.15073v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",A Generic Multi-Player Transformation Algorithm for Solving Large-Scale   Zero-Sum Extensive-Form Adversarial Team Games,1970,"  Many recent practical and theoretical breakthroughs focus on adversarial team multi-player games (ATMGs) in ex ante correlation scenarios. In this setting, team members are allowed to coordinate their strategies only before the game starts. Although there existing algorithms for solving extensive-form ATMGs, the size of the game tree generated by the previous algorithms grows exponentially with the number of players. Therefore, how to deal with large-scale zero-sum extensive-form ATMGs problems close to the real world is still a significant challenge. In this paper, we propose a generic multi-player transformation algorithm, which can transform any multi-player game tree satisfying the definition of AMTGs into a 2-player game tree, such that finding a team-maxmin equilibrium with correlation (TMECor) in large-scale ATMGs can be transformed into solving NE in 2-player games. To achieve this goal, we first introduce a new structure named private information pre-branch, which consists of a temporary chance node and coordinator nodes and aims to make decisions for all potential private information on behalf of the team members. We also show theoretically that NE in the transformed 2-player game is equivalent TMECor in the original multi-player game. This work significantly reduces the growth of action space and nodes from exponential to constant level. This enables our work to outperform all the previous state-of-the-art algorithms in finding a TMECor, with 182.89, 168.47, 694.44, and 233.98 significant improvements in the different Kuhn Poker and Leduc Poker cases (21K3, 21K4, 21K6 and 21L33). In addition, this work first practically solves the ATMGs in a 5-player case which cannot be conducted by existing algorithms. ",Kein DOI-Link verfügbar,2307.01441v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Do We Really Need a Complex Agent System? Distill Embodied Agent into a   Single Model,1970,"  With the power of large language models (LLMs), open-ended embodied agents can flexibly understand human instructions, generate interpretable guidance strategies, and output executable actions. Nowadays, Multi-modal Language Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer perception to entity agents and allowing embodied agents to perceive world-understanding tasks more delicately. However, existing works: 1) operate independently by agents, each containing multiple LLMs, from perception to action, resulting in gaps between complex tasks and execution; 2) train MLMs on static data, struggling with dynamics in open-ended scenarios; 3) input prior knowledge directly as prompts, suppressing application flexibility. We propose STEVE-2, a hierarchical knowledge distillation framework for open-ended embodied tasks, characterized by 1) a hierarchical system for multi-granular task division, 2) a mirrored distillation method for parallel simulation data, and 3) an extra expert model for bringing additional knowledge into parallel simulation. After distillation, embodied agents can complete complex, open-ended tasks without additional expert guidance, utilizing the performance and knowledge of a versatile MLM. Extensive evaluations on navigation and creation tasks highlight the superior performance of STEVE-2 in open-ended tasks, with $1.4 \times$ - $7.3 \times$ in performance. ",Kein DOI-Link verfügbar,2404.04619v1,Yes,versatile(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",Recursive expansion of Tanner graph: a method to construct stabilizer   codes with high coding rate,1970,"  Quantum stabilizer codes face the problem of low coding rate. In this article, following the idea of recursively expanding Tanner graph proposed in our previous work, we try to construct new stabilizer codes with high coding rate, and propose XZ-type Tanner-graph-recursive-expansion (XZ-TGRE) code and Tanner-graph-recursive-expansion hypergraph product (TGRE-HP) code. XZ-TGRE code have zero asymptotic coding rate, but its coding rate tends to zero extremely slowly with the growth of code length. Under the same code length, its coding rate is much higher than that of surface code. The coding rate of TGRE-HP is the constant 0.2, which is the highest constant coding rate of stabilizer codes to our best knowledge. We prove that the code distance of XZ-TGRE code scales as $O(log(N))$, and that of TGRE-HP code scales as $O(\log \sqrt{N})$, where $N$ is the code length. Moreover, the code capacity noise threshold of XZ-TGRE code is around 0.078, and that of TGRE-HP code is around 0.096. This articles shows that the idea of recursively expanding Tanner graph might have potential to construct quantum codes with good performance. ",Kein DOI-Link verfügbar,2402.07823v2,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",AI for Biomedicine in the Era of Large Language Models,1970,"  The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this survey, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation ",Kein DOI-Link verfügbar,2403.15673v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",PathoLM: Identifying pathogenicity from the DNA sequence through the   Genome Foundation Model,1970,"  Pathogen identification is pivotal in diagnosing, treating, and preventing diseases, crucial for controlling infections and safeguarding public health. Traditional alignment-based methods, though widely used, are computationally intense and reliant on extensive reference databases, often failing to detect novel pathogens due to their low sensitivity and specificity. Similarly, conventional machine learning techniques, while promising, require large annotated datasets and extensive feature engineering and are prone to overfitting. Addressing these challenges, we introduce PathoLM, a cutting-edge pathogen language model optimized for the identification of pathogenicity in bacterial and viral sequences. Leveraging the strengths of pre-trained DNA models such as the Nucleotide Transformer, PathoLM requires minimal data for fine-tuning, thereby enhancing pathogen detection capabilities. It effectively captures a broader genomic context, significantly improving the identification of novel and divergent pathogens. We developed a comprehensive data set comprising approximately 30 species of viruses and bacteria, including ESKAPEE pathogens, seven notably virulent bacterial strains resistant to antibiotics. Additionally, we curated a species classification dataset centered specifically on the ESKAPEE group. In comparative assessments, PathoLM dramatically outperforms existing models like DciPatho, demonstrating robust zero-shot and few-shot capabilities. Furthermore, we expanded PathoLM-Sp for ESKAPEE species classification, where it showed superior performance compared to other advanced deep learning methods, despite the complexities of the task. ",Kein DOI-Link verfügbar,2406.13133v1,Yes,pivotal(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University",STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft,1970,"  Building an embodied agent system with a large language model (LLM) as its core is a promising direction. Due to the significant costs and uncontrollable factors associated with deploying and training such agents in the real world, we have decided to begin our exploration within the Minecraft environment. Our STEVE Series agents can complete basic tasks in a virtual environment and more challenging tasks such as navigation and even creative tasks, with an efficiency far exceeding previous state-of-the-art methods by a factor of $2.5\times$ to $7.3\times$. We begin our exploration with a vanilla large language model, augmenting it with a vision encoder and an action codebase trained on our collected high-quality dataset STEVE-21K. Subsequently, we enhanced it with a Critic and memory to transform it into a complex system. Finally, we constructed a hierarchical multi-agent system. Our recent work explored how to prune the agent system through knowledge distillation. In the future, we will explore more potential applications of STEVE agents in the real world. ",Kein DOI-Link verfügbar,2406.11247v1,Yes,potent(1)
0000-0002-3528-5785,Xuan Wang,"Fudan University, Huashan Hospital Fudan University","Deep Learning Meets OBIA: Tasks, Challenges, Strategies, and   Perspectives",1970,"  Deep learning has gained significant attention in remote sensing, especially in pixel- or patch-level applications. Despite initial attempts to integrate deep learning into object-based image analysis (OBIA), its full potential remains largely unexplored. In this article, as OBIA usage becomes more widespread, we conducted a comprehensive review and expansion of its task subdomains, with or without the integration of deep learning. Furthermore, we have identified and summarized five prevailing strategies to address the challenge of deep learning's limitations in directly processing unstructured object data within OBIA, and this review also recommends some important future research directions. Our goal with these endeavors is to inspire more exploration in this fascinating yet overlooked area and facilitate the integration of deep learning into OBIA processing workflows. ",Kein DOI-Link verfügbar,2408.01607v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Programmable In-Network Obfuscation of Traffic,1970,"  Recent advances in programmable switch hardware offer a fresh opportunity to protect user privacy. This paper presents PINOT, a lightweight in-network anonymity solution that runs at line rate within the memory and processing constraints of hardware switches. PINOT encrypts a client's IPv4 address with an efficient encryption scheme to hide the address from downstream ASes and the destination server. PINOT is readily deployable, requiring no end-user software or cooperation from networks other than the trusted network where it runs. We implement a PINOT prototype on the Barefoot Tofino switch, deploy PINOT in a campus network, and present results on protecting user identity against public DNS, NTP, and WireGuard VPN services. ",Kein DOI-Link verfügbar,2006.00097v1,Yes,fresh(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Quantum Long Short-Term Memory for Drug Discovery,1970,"  Quantum computing combined with machine learning (ML) is an extremely promising research area, with numerous studies demonstrating that quantum machine learning (QML) is expected to solve scientific problems more effectively than classical ML. In this work, we successfully apply QML to drug discovery, showing that QML can significantly improve model performance and achieve faster convergence compared to classical ML. Moreover, we demonstrate that the model accuracy of the QML improves as the number of qubits increases. We also introduce noise to the QML model and find that it has little effect on our experimental conclusions, illustrating the high robustness of the QML model. This work highlights the potential application of quantum computing to yield significant benefits for scientific advancement as the qubit quantity increase and quality improvement in the future. ",Kein DOI-Link verfügbar,2407.19852v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained   Language Models,1970,"  Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC . ",Kein DOI-Link verfügbar,2203.02167v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Renewable Energy-Aware Information-Centric Networking,1970,"  The ICT industry today is placed as one of the major consumers of energy, where recent reports have also shown that the industry is a major contributor to global carbon emissions. While renewable energy-aware data centers have been proposed, these solutions have certain limitations. The primary limitation is due to the design of data centers which focus on large-size facilities located in selected locations. This paper addresses this problem, by utilizing in-network caching with each router having storage and being powered by renewable energy sources (wind and solar). Besides placing contents closer to end users, utilizing in-network caching could potentially increase probability of capturing renewable energy in diverse geographical locations. Our proposed solution is dual- layered: on the first layer a distributed gradient-based routing protocol is used to discover the paths along routers that are powered by the highest renewable energy, and on the second layer, a caching mechanism will pull the contents from the data centre and place them on routers of the paths that are discovered by our routing protocol. Through our experiments on a testbed utilizing real meteorological data, our proposed solution has demonstrated increased quantity of renewable energy consumption, while reducing the workload on the data centers. ",Kein DOI-Link verfügbar,1412.6382v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Quantitative Analysis of Community Evolution in Developer Social   Networks Around Open Source Software Projects,1970,"  Understanding the evolution of communities in developer social networks (DSNs) around open source software (OSS) projects can provide valuable insights about the socio-technical process of OSS development. Existing studies show the evolutionary behaviors of social communities can effectively be described using patterns including split, shrink, merge, expand, emerge, and extinct. However, existing pattern-based approaches are limited in supporting quantitative analysis, and are potentially problematic for using the patterns in a mutually exclusive manner when describing community evolution. In this work, we propose that different patterns can occur simultaneously between every pair of communities during the evolution, just in different degrees. Four entropy-based indices are devised to measure the degree of community split, shrink, merge, and expand, respectively, which can provide a comprehensive and quantitative measure of community evolution in DSNs. The indices have properties desirable to quantify community evolution including monotonicity, and bounded maximum and minimum values that correspond to meaningful cases. They can also be combined to describe more patterns such as community emerge and extinct. We conduct experiments with real-world OSS projects to evaluate the validity of the proposed indices. The results suggest the proposed indices can effectively capture community evolution, and are consistent with existing approaches in detecting evolution patterns in DSNs with an accuracy of 94.1\%. The results also show that the indices are useful in predicting OSS team productivity with an accuracy of 0.718. In summary, the proposed approach is among the first to quantify the degree of community evolution with respect to different patterns, which is promising in supporting future research and applications about DSNs and OSS development. ",Kein DOI-Link verfügbar,2205.09935v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Human Image Generation: A Comprehensive Survey,1970,"  Image and video synthesis has become a blooming topic in computer vision and machine learning communities along with the developments of deep generative models, due to its great academic and application value. Many researchers have been devoted to synthesizing high-fidelity human images as one of the most commonly seen object categories in daily lives, where a large number of studies are performed based on various models, task settings and applications. Thus, it is necessary to give a comprehensive overview on these variant methods on human image generation. In this paper, we divide human image generation techniques into three paradigms, i.e., data-driven methods, knowledge-guided methods and hybrid methods. For each paradigm, the most representative models and the corresponding variants are presented, where the advantages and characteristics of different methods are summarized in terms of model architectures. Besides, the main public human image datasets and evaluation metrics in the literature are summarized. Furthermore, due to the wide application potentials, the typical downstream usages of synthesized human images are covered. Finally, the challenges and potential opportunities of human image generation are discussed to shed light on future research. ",https://doi.org/10.1145/3665869,2212.08896v3,Yes,potent(2)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Stabilizing Policy Gradients for Stochastic Differential Equations via   Consistency with Perturbation Process,1970,"  Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset. ",Kein DOI-Link verfügbar,2403.04154v2,Yes,versatile(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Bridging Text and Molecule: A Survey on Multimodal Frameworks for   Molecule,1970,"  Artificial intelligence has demonstrated immense potential in scientific research. Within molecular science, it is revolutionizing the traditional computer-aided paradigm, ushering in a new era of deep learning. With recent progress in multimodal learning and natural language processing, an emerging trend has targeted at building multimodal frameworks to jointly model molecules with textual domain knowledge. In this paper, we present the first systematic survey on multimodal frameworks for molecules research. Specifically,we begin with the development of molecular deep learning and point out the necessity to involve textual modality. Next, we focus on recent advances in text-molecule alignment methods, categorizing current models into two groups based on their architectures and listing relevant pre-training tasks. Furthermore, we delves into the utilization of large language models and prompting techniques for molecular tasks and present significant applications in drug discovery. Finally, we discuss the limitations in this field and highlight several promising directions for future research. ",Kein DOI-Link verfügbar,2403.13830v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Practical Constrained Optimization of Auction Mechanisms in E-Commerce   Sponsored Search Advertising,1970,"  Sponsored search in E-commerce platforms such as Amazon, Taobao and Tmall provides sellers an effective way to reach potential buyers with most relevant purpose. In this paper, we study the auction mechanism optimization problem in sponsored search on Alibaba's mobile E-commerce platform. Besides generating revenue, we are supposed to maintain an efficient marketplace with plenty of quality users, guarantee a reasonable return on investment (ROI) for advertisers, and meanwhile, facilitate a pleasant shopping experience for the users. These requirements essentially pose a constrained optimization problem. Directly optimizing over auction parameters yields a discontinuous, non-convex problem that denies effective solutions. One of our major contribution is a practical convex optimization formulation of the original problem. We devise a novel re-parametrization of auction mechanism with discrete sets of representative instances. To construct the optimization problem, we build an auction simulation system which estimates the resulted business indicators of the selected parameters by replaying the auctions recorded from real online requests. We summarized the experiments on real search traffics to analyze the effects of fidelity of auction simulation, the efficacy under various constraint targets and the influence of regularization. The experiment results show that with proper entropy regularization, we are able to maximize revenue while constraining other business indicators within given ranges. ",Kein DOI-Link verfügbar,1807.11790v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Improving Description-based Person Re-identification by   Multi-granularity Image-text Alignments,1970,"  Description-based person re-identification (Re-id) is an important task in video surveillance that requires discriminative cross-modal representations to distinguish different people. It is difficult to directly measure the similarity between images and descriptions due to the modality heterogeneity (the cross-modal problem). And all samples belonging to a single category (the fine-grained problem) makes this task even harder than the conventional image-description matching task. In this paper, we propose a Multi-granularity Image-text Alignments (MIA) model to alleviate the cross-modal fine-grained problem for better similarity evaluation in description-based person Re-id. Specifically, three different granularities, i.e., global-global, global-local and local-local alignments are carried out hierarchically. Firstly, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. Secondly, the global-local alignment employs the potential relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. Thirdly, as for the local-local alignment, we match visual human parts with noun phrases in the Bi-directional Fine-grained Matching (BFM) module. The whole network combining multiple granularities can be end-to-end trained without complex pre-processing. To address the difficulties in training the combination of multiple granularities, an effective step training strategy is proposed to train these granularities step-by-step. Extensive experiments and analysis have shown that our method obtains the state-of-the-art performance on the CUHK-PEDES dataset and outperforms the previous methods by a significant margin. ",Kein DOI-Link verfügbar,1906.09610v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",CvFormer: Cross-view transFormers with Pre-training for fMRI Analysis of   Human Brain,1970,"  In recent years, functional magnetic resonance imaging (fMRI) has been widely utilized to diagnose neurological disease, by exploiting the region of interest (RoI) nodes as well as their connectivities in human brain. However, most of existing works only rely on either RoIs or connectivities, neglecting the potential for complementary information between them. To address this issue, we study how to discover the rich cross-view information in fMRI data of human brain. This paper presents a novel method for cross-view analysis of fMRI data of the human brain, called Cross-view transFormers (CvFormer). CvFormer employs RoI and connectivity encoder modules to generate two separate views of the human brain, represented as RoI and sub-connectivity tokens. Then, basic transformer modules can be used to process the RoI and sub-connectivity tokens, and cross-view modules integrate the complement information across two views. Furthermore, CvFormer uses a global token for each branch as a query to exchange information with other branches in cross-view modules, which only requires linear time for both computational and memory complexity instead of quadratic time. To enhance the robustness of the proposed CvFormer, we propose a two-stage strategy to train its parameters. To be specific, RoI and connectivity views can be firstly utilized as self-supervised information to pre-train the CvFormer by combining it with contrastive learning and then fused to finetune the CvFormer using label information. Experiment results on two public ABIDE and ADNI datasets can show clear improvements by the proposed CvFormer, which can validate its effectiveness and superiority. ",Kein DOI-Link verfügbar,2309.07940v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Distributed data analytics,1970,"  Machine Learning (ML) techniques have begun to dominate data analytics applications and services. Recommendation systems are a key component of online service providers. The financial industry has adopted ML to harness large volumes of data in areas such as fraud detection, risk-management, and compliance. Deep Learning is the technology behind voice-based personal assistants, etc. Deployment of ML technologies onto cloud computing infrastructures has benefited numerous aspects of our daily life. The advertising and associated online industries in particular have fuelled a rapid rise the in deployment of personal data collection and analytics tools. Traditionally, behavioural analytics relies on collecting vast amounts of data in centralised cloud infrastructure before using it to train machine learning models that allow user behaviour and preferences to be inferred. A contrasting approach, distributed data analytics, where code and models for training and inference are distributed to the places where data is collected, has been boosted by two recent, ongoing developments: increased processing power and memory capacity available in user devices at the edge of the network, such as smartphones and home assistants; and increased sensitivity to the highly intrusive nature of many of these devices and services and the attendant demands for improved privacy. Indeed, the potential for increased privacy is not the only benefit of distributing data analytics to the edges of the network: reducing the movement of large volumes of data can also improve energy efficiency, helping to ameliorate the ever increasing carbon footprint of our digital infrastructure, enabling much lower latency for service interactions than is possible when services are cloud-hosted. These approaches often introduce challenges in privacy, utility, and efficiency trade-offs, while having to ensure fruitful user engagement. ",Kein DOI-Link verfügbar,2203.14088v1,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification,1970,"  Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in accuracy improvement, let alone explainability, a critical capability of fact verification systems. Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant, high-quality dataset. Previous datasets either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EXFEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionally, we demonstrate a novel baseline system on our EX-FEVER dataset, showcasing document retrieval, explanation generation, and claim verification, and validate the significance of our dataset. Furthermore, we highlight the potential of utilizing Large Language Models in the fact verification task. We hope our dataset could make a significant contribution by providing ample opportunities to explore the integration of natural language explanations in the domain of fact verification. ",Kein DOI-Link verfügbar,2310.09754v3,Yes,potent(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Translational control of gene expression via interacting feedback loops,1970,"  Translation is a key step in the synthesis of proteins. Accordingly, cells have evolved an intricate array of control mechanisms to regulate this process. By constructing a multi-component mathematical framework for translation we uncover how translation may be controlled via interacting feedback loops. Our results reveal that this interplay gives rise to a remarkable range of protein synthesis dynamics, including oscillations, step-change and bistability. This suggests that cells may have recourse to a much richer set of control mechanisms than was previously understood. ",https://doi.org/10.1103/PhysRevE.100.050402,1904.03064v1,Yes,intricate(1)
0000-0003-3010-6845,Liang Wang,"Fudan University, Huashan Hospital Fudan University",Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion,1970,"  How to better fuse cross-modal features is the core issue of RGB-T tracking. Some previous methods either insufficiently fuse RGB and TIR features, or depend on intermediaries containing information from both modalities to achieve cross-modal information interaction. The former does not fully exploit the potential of using only RGB and TIR information of the template or search region for channel and spatial feature fusion, and the latter lacks direct interaction between the template and search area, which limits the model's ability to fully exploit the original semantic information of both modalities. To alleviate these limitations, we explore how to improve the performance of a visual Transformer by using direct fusion of cross-modal channels and spatial features, and propose CSTNet. CSTNet uses ViT as a backbone and inserts cross-modal channel feature fusion modules (CFM) and cross-modal spatial feature fusion modules (SFM) for direct interaction between RGB and TIR features. The CFM performs parallel joint channel enhancement and joint multilevel spatial feature modeling of RGB and TIR features and sums the features, and then globally integrates the sum feature with the original features. The SFM uses cross-attention to model the spatial relationship of cross-modal features and then introduces a convolutional feedforward network for joint spatial and channel integration of multimodal features. We retrain the model with CSNet as the pre-training weights in the model with CFM and SFM removed, and propose CSTNet-small, which achieves 36% reduction in parameters and 24% reduction in Flops, and 50% speedup with a 1-2% performance decrease. Comprehensive experiments show that CSTNet achieves state-of-the-art performance on three public RGB-T tracking benchmarks. Code is available at https://github.com/LiYunfengLYF/CSTNet. ",Kein DOI-Link verfügbar,2405.03177v2,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Interpretable learning of voltage for electrode design of multivalent   metal-ion batteries,1970,"  Deep learning (DL) has indeed emerged as a powerful tool for rapidly and accurately predicting materials properties from big data, such as the design of current commercial Li-ion batteries. However, its practical utility for multivalent metal-ion batteries (MIBs), the most promising future solution of large-scale energy storage, is limited due to the scarce MIB data availability and poor DL model interpretability. Here, we develop an interpretable DL model as an effective and accurate method for learning electrode voltages of multivalent MIBs (divalent magnesium, calcium, zinc, and trivalent aluminum) at small dataset limits (150~500). Using the experimental results as validation, our model is much more accurate than machine-learning models which usually are better than DL in the small dataset regime. Besides the high accuracy, our feature-engineering-free DL model is explainable, which automatically extracts the atom covalent radius as the most important feature for the voltage learning by visualizing vectors from the layers of the neural network. The presented model potentially accelerates the design and optimization of multivalent MIB materials with fewer data and less domain-knowledge restriction, and is implemented into a publicly available online tool kit in http://batteries.2dmatpedia.org/ for the battery community. ",Kein DOI-Link verfügbar,2201.04493v1,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",GEB-1.3B: Open Lightweight Large Language Model,1970,"  Recently developed large language models (LLMs) such as ChatGPT, Claude, and Llama have demonstrated impressive abilities, and even surpass human-level performance in several tasks. Despite their success, the resource-intensive demands of these models, requiring significant computational power for both training and inference, limit their deployment to high-performance servers. Additionally, the extensive calculation requirements of the models often lead to increased latency in response times. With the increasing need for LLMs to operate efficiently on CPUs, research about lightweight models that are optimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a lightweight LLM trained on 550 billion tokens in both Chinese and English languages. We employ novel training techniques, including ROPE, Group-Query-Attention, and FlashAttention-2, to accelerate training while maintaining model performance. Additionally, we fine-tune the model using 10 million samples of instruction data to enhance alignment. GEB-1.3B exhibits outstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU, outperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B. Notably, the FP32 version of GEB-1.3B achieves commendable inference times on CPUs, with ongoing efforts to further enhance speed through advanced quantization techniques. The release of GEB-1.3B as an open-source model marks a significant contribution to the development of lightweight LLMs, promising to foster further research and innovation in the field. ",Kein DOI-Link verfügbar,2406.09900v1,Yes,commendable(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Short range correlations in the extended quantum molecular dynamics   model,1970,"  Short-range potential has been added into an extended quantum molecular dynamics (EQMD) model. The RMS radius, binding energy and momentum distribution of $^{12}$C with different initial structures and short-range potential parameters have been checked. The separation energy of $^{12}$C(p,2p)$^{11}$B reaction is calculated and compared with the experimental data, it indicates that our modified EQMD model can be taken as a reliable tool for studying proton pair knock-out reaction. Furthermore, the short range correlation effects on emission time and momentum spectrum of two protons are discussed. Finally, the momentum correlation function of the emitted proton pair is calculated by the Lednicky and Lyuboshitz's analytical method. The result explains that short-range repulsion causes high momentum tail and weakens the momentum correlation function. ",https://doi.org/10.1103/PhysRevC.105.014603,2112.10348v1,Yes,potent(2)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Differentially Private Natural Language Models: Recent Advances and   Future Directions,1970,"  Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient perturbation based methods, embedding vector perturbation based methods, and ensemble model based methods. We also discuss some challenges and future directions. ",Kein DOI-Link verfügbar,2301.09112v2,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Data Storage: Review of Heusler Compounds,1970,"  In the recent decade, the family of Heusler compounds has attracted tremendous scientific and technological interest in the field of spintronics. This is essentially due to their exceptional magnetic properties, which qualify them as promising functional materials in various data-storage devices, such as giant-magnetoresistance spin valves, magnetic tunnel junctions, and spin-transfer torque devices. In this article, we provide a comprehensive review on the applications of the Heusler family in magnetic data storage. In addition to their important roles in the performance improvement of these devices, we also try to point out the challenges as well as possible solutions, of the current Heusler-based devices. We hope that this review would spark further investigation efforts into efficient incorporation of this eminent family of materials into data storage applications by fully arousing their intrinsic potential. ",https://doi.org/10.1142/S201032471230006X,1301.5455v2,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",GSDA: Generative Adversarial Network-based Semi-Supervised Data   Augmentation for Ultrasound Image Classification,1970,"  Medical Ultrasound (US) is one of the most widely used imaging modalities in clinical practice, but its usage presents unique challenges such as variable imaging quality. Deep Learning (DL) models can serve as advanced medical US image analysis tools, but their performance is greatly limited by the scarcity of large datasets. To solve the common data shortage, we develop GSDA, a Generative Adversarial Network (GAN)-based semi-supervised data augmentation method. GSDA consists of the GAN and Convolutional Neural Network (CNN). The GAN synthesizes and pseudo-labels high-resolution, high-quality US images, and both real and synthesized images are then leveraged to train the CNN. To address the training challenges of both GAN and CNN with limited data, we employ transfer learning techniques during their training. We also introduce a novel evaluation standard that balances classification accuracy with computational time. We evaluate our method on the BUSI dataset and GSDA outperforms existing state-of-the-art methods. With the high-resolution and high-quality images synthesized, GSDA achieves a 97.9% accuracy using merely 780 images. Given these promising results, we believe that GSDA holds potential as an auxiliary tool for medical US analysis. ",Kein DOI-Link verfügbar,2203.06184v4,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Giant enhancement of anomalous Hall effect in Cr modulation-doped   non-collinear antiferromagnetic Mn3Sn thin films,1970,"  We report on Cr doping effect in Mn3Sn polycrystalline films with both uniform and modulation doping. It is found that Cr doping with low concentration does not cause notable changes to the structural and magnetic properties of Mn3Sn, but it significantly enhances the anomalous Hall conductivity, particularly for modulation-doped samples at low temperature. A Hall conductivity as high as 184.8 {\Omega}-1 cm-1 is obtained for modulation-doped samples at 50 K, in a sharp contrast to vanishingly small values for undoped samples at the same temperature. We attribute the enhancement to the change of Fermi level induced by Cr doping ",Kein DOI-Link verfügbar,2107.00959v1,Yes,notable(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",A Group Fairness Lens for Large Language Models,1970,"  The rapid advancement of large language models has revolutionized various applications but also raised crucial concerns about their potential to perpetuate biases and unfairness when deployed in social media contexts. Evaluating LLMs' potential biases and fairness has become crucial, as existing methods rely on limited prompts focusing on just a few groups, lacking a comprehensive categorical perspective. In this paper, we propose evaluating LLM biases from a group fairness lens using a novel hierarchical schema characterizing diverse social groups. Specifically, we construct a dataset, GFair, encapsulating target-attribute combinations across multiple dimensions. In addition, we introduce statement organization, a new open-ended text generation task, to uncover complex biases in LLMs. Extensive evaluations of popular LLMs reveal inherent safety concerns. To mitigate the biases of LLM from a group fairness perspective, we pioneer a novel chain-of-thought method GF-Think to mitigate biases of LLMs from a group fairness perspective. Experimental results demonstrate its efficacy in mitigating bias in LLMs to achieve fairness. ",Kein DOI-Link verfügbar,2312.15478v1,Yes,potent(2)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Phosphorene and Transition Metal Dichalcogenide 2D Heterojunctions:   Application in Excitonic Solar Cells,1970,"  Using the first-principles GW-Bethe-Salpeter equation method, here we study the excited-state properties, including quasi-particle band structures and optical spectra, of phosphorene, a two-dimensional (2D) atomic layer of black phosphorus. The quasi-particle band gap of monolayer phosphorene is 2.15 eV and its optical gap is 1.6 eV, which is suitable for excitonic thin film solar cell applications. Next, this potential application is analysed by considering type-II heterostructures with single layered phosphorene and transition metal dichalcogenides (TMDs). These heterojunctions have a potential maximum power conversion efficiency of up to 12\%, which can be further enhanced to 20\% by strain engineering. Our results show that phosphorene is not only a promising new material for use in nanoscale electronics, but also in optoelectronics. ",Kein DOI-Link verfügbar,1507.07343v1,Yes,potent(2)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Tungsten Boride: a 2D Multiple Dirac Semimetal for Hydrogen Evolution   Reaction,1970,"  Here, we propose a two-dimensional tungsten boride (WB4) lattice, with the Gibbs free energy for the adsorption of atomic hydrogen, tending to be the ideal value of 0 eV at 3% strained state, to host a better hydrogen evolution reaction activity. Based on first-principles calculations, it is demonstrated that the multiple d-p-pi and d-p-sigma Dirac conjugations of WB4 lattice ensures its excellent electronic transport characteristics. Meanwhile, coupling with the d-orbitals of W, the p-orbitals of borophene subunits in WB4 lattice can modulate the d band center to get a good HER performance. Our results not only provide a versatile platform for hosting multiple Dirac semimetal states with a sandwich configuration, but also offer a guiding principle for discovering the relationship between intrinsic properties of the active centre and the catalytic activity of metal layer from the emerging field of low-dimensional noble-metal-free lattices. ",https://doi.org/10.1039/C9TC01862J,1901.09664v1,Yes,versatile(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",MedAugment: Universal Automatic Data Augmentation Plug-in for Medical   Image Analysis,1970,"  Data augmentation (DA) has been widely leveraged in computer vision to alleviate the data shortage, whereas the DA in medical image analysis (MIA) faces multiple challenges. The prevalent DA approaches in MIA encompass conventional DA, synthetic DA, and automatic DA. However, utilizing these approaches poses various challenges such as experience-driven design and intensive computation cost. Here, we propose an efficient and effective automatic DA method termed MedAugment. We propose a pixel augmentation space and spatial augmentation space and exclude the operations that can break medical details and features, such as severe color distortions or structural alterations that can compromise image diagnostic value. Besides, we propose a novel sampling strategy by sampling a limited number of operations from the two spaces. Moreover, we present a hyperparameter mapping relationship to produce a rational augmentation level and make the MedAugment fully controllable using a single hyperparameter. These configurations settle the differences between natural and medical images, such as high sensitivity to certain attributes such as brightness and posterize. Extensive experimental results on four classification and four segmentation datasets demonstrate the superiority of MedAugment. Compared with existing approaches, the proposed MedAugment serves as a more suitable yet general processing pipeline for medical images without producing color distortions or structural alterations and involving negligible computational overhead. We emphasize that our method can serve as a plugin for arbitrary projects without any extra training stage, thereby holding the potential to make a valuable contribution to the medical field, particularly for medical experts without a solid foundation in deep learning. Code is available at https://github.com/NUS-Tim/MedAugment. ",Kein DOI-Link verfügbar,2306.17466v4,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition,1970,"  Palmprint recently shows great potential in recognition applications as it is a privacy-friendly and stable biometric. However, the lack of large-scale public palmprint datasets limits further research and development of palmprint recognition. In this paper, we propose a novel realistic pseudo-palmprint generation (RPG) model to synthesize palmprints with massive identities. We first introduce a conditional modulation generator to improve the intra-class diversity. Then an identity-aware loss is proposed to ensure identity consistency against unpaired training. We further improve the B\'ezier palm creases generation strategy to guarantee identity independence. Extensive experimental results demonstrate that synthetic pretraining significantly boosts the recognition model performance. For example, our model improves the state-of-the-art B\'ezierPalm by more than $5\%$ and $14\%$ in terms of TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol. When accessing only $10\%$ of the real training data, our method still outperforms ArcFace with $100\%$ real training data, indicating that we are closer to real-data-free palmprint recognition. ",Kein DOI-Link verfügbar,2307.14016v3,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Topological skyrmions in monolayer multiferroic MoPtGe2S6,1970,"  Two-dimensional (2D) multiferroic materials with coexisting ferroelectricity and ferromagnetism have garnered substantial attention for their intriguing physical properties and diverse promising applications in spintronics. For example, multiferroic materials with electronically controlled broken central symmetry provide a versatile platform for designing and manipulating topological skyrmions and diverse spintronic applications. Here, we investigate the complex magnetic properties of room-temerature multiferroic material MoPtGe2S6 and its electrical control of topological skyrmions using first-principles calculations and atomistic micromagnetic simulations. A sizable Dzyaloshinskii-Moriya interaction (DMI) (2.1 meV) is found in the multiferroic material MoPtGe2S6 with an electrically polarized ground state. The magnetic skyrmions can be stabilized in monolayer MoPtGe2S6 under zero magnetic field, and the chirality of skyrmions can be reversed with electric field-induced flipping of electrical polarization due to the reversed chirality of the DMI. Furthermore, an external magnetic fielc can reverse the magnetization direction and topological charge of the skyrmions as well as tune the size of skyrmions. These results demonstrate that the monolayer MoPtGe2S6 can enrich the 2D skyrmion community and pave the way for electronically controlled spintronic devices. ",Kein DOI-Link verfügbar,2402.15983v1,Yes,versatile(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Lightweight equivariant interaction graph neural network for accurate   and efficient interatomic potential and force predictions,1970,"  In modern computational materials science, deep learning has shown the capability to predict interatomic potentials, thereby supporting and accelerating conventional simulations. However, existing models typically sacrifice either accuracy or efficiency. Moreover, lightweight models are highly demanded for offering simulating systems on a considerably larger scale at reduced computational costs. A century ago, Felix Bloch demonstrated how leveraging the equivariance of the translation operation on a crystal lattice (with geometric symmetry) could significantly reduce the computational cost of determining wavefunctions and accurately calculate material properties. Here, we introduce a lightweight equivariant interaction graph neural network (LEIGNN) that can enable accurate and efficient interatomic potential and force predictions in crystals. Rather than relying on higher-order representations, LEIGNN employs a scalar-vector dual representation to encode equivariant features. By extracting both local and global structures from vector representations and learning geometric symmetry information, our model remains lightweight while ensuring prediction accuracy and robustness through the equivariance. Our results show that LEIGNN consistently outperforms the prediction performance of the representative baselines and achieves significant efficiency across diverse datasets, which include catalysts, molecules, and organic isomers. Finally, to further validate the predicted interatomic potentials from our model, we conduct classical molecular dynamics (MD) and ab initio MD simulation across various systems, including solid, liquid, and gas. It is found that LEIGNN can achieve the accuracy of ab initio MD and retain the computational efficiency of classical MD across all examined systems, demonstrating its accuracy, efficiency, and universality. ",Kein DOI-Link verfügbar,2311.02869v6,Yes,potent(3)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Polar Rectification Effect in Electro-Fatigued SrTiO3 Based Junctions,1970,"  Rectifying semiconductor junctions are crucial to electronic devices. They convert alternating current into direct one by allowing unidirectional charge flows. In analogy to the current-flow rectification for itinerary electrons, here, a polar rectification that based on the localized oxygen vacancies (OVs) in a Ti/fatigued-SrTiO3 (fSTO) Schottky junction is first demonstrated. The fSTO with OVs is produced by an electro-degradation process. The different movability of localized OVs and itinerary electrons in the fSTO yield a unidirectional electric polarization at the interface of the junction under the coaction of external and built-in electric fields. Moreover, the fSTO displays a pre-ferroelectric state located between paraelectric and ferroelectric phases. The pre-ferroelectric state has three sub-states and can be easily driven into a ferroelectric state by external electric field. These observations open up opportunities for potential polar devices and may underpin many useful polar-triggered electronic phenomena. ",Kein DOI-Link verfügbar,2003.03244v1,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Tunable Rashba spin-orbit coupling and its interplay with multiorbital   effect and magnetic ordering at oxide interfaces,1970,"  The complex oxide heterostructures such as LaAlO3/SrTiO3 (LAO/STO) interface are paradigmatic platforms to explore emerging multi-degrees of freedom coupling and the associated exotic phenomena. In this study, we reveal the effects of multiorbital and magnetic ordering on Rashba spin-orbit coupling (SOC) at the LAO/STO (001) interface. Based on first-principles calculations, we show that the Rashba spin splitting near the conduction band edge can be tuned substantially by the interfacial insulator-metal transition due to the multiorbital effect of the lowest t_2g bands. We further unravel a competition between Rashba SOC and intrinsic magnetism, in which the Rashba SOC induced spin polarization is suppressed by the interfacial magnetic ordering. These results deepen our understanding of intricate electronic and magnetic reconstruction at the perovskite oxide interfaces and shed light on the engineering of oxide heterostructures for all-oxides-based spintronic devices. ",https://doi.org/10.1103/PhysRevB.104.155152,2101.07586v1,Yes,intricate(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University",Van der Waals Spin-Orbit Torque Antiferromagnetic Memory,1970,"  The technique of conventional ferromagnet/heavy-metal spin-orbit torque (SOT) offers significant potential for enhancing the efficiency of magnetic memories. However, it faces fundamental physical limitations, including hunting effects from the metallic layer, broken symmetry for enabling antidamping switching, spin scattering caused by interfacial defects, and sensitivity to stray magnetic fields. To address these issues, we here propose a van der Waals (vdW) field-free SOT antiferromagnetic memory using a vdW bilayer LaBr$_2$ (an antiferromagnet with perpendicular magnetic anisotropy) and a monolayer T$_d$ phase WTe$_2$ (a Weyl semimetal with broken inversion symmetry). By systematically employing density functional theory in conjunction with non-equilibrium Green's function methods and macrospin simulations, we demonstrate that the proposed vdW SOT devices exhibit remarkably low critical current density approximately 10 MA/cm$^2$ and rapid field-free magnetization switching in 250 ps. This facilitates excellent write performance with extremely low energy consumption. Furthermore, the device shows a significantly low read error rate, as evidenced by a high tunnel magnetoresistance ratio of up to 4250%. The superior write and read performance originates from the unique strong on-site (insulating phase) and off-site (magnetic phase) Coulomb interactions in electride LaBr$_2$, a large non-zero z-component polarization in WTe$_2$, and the proximity effect between them. ",Kein DOI-Link verfügbar,2310.02805v1,Yes,potent(1)
0000-0002-8944-5959,Lei Shen,"Fudan University, Huashan Hospital Fudan University","High Spectral-Efficiency, Ultra-low MIMO SDM Transmission over a   Field-Deployed Multi-Core OAM Fiber",1970,"  Few-mode multi-core fiber (FM-MCF) based Space-Division Multiplexing (SDM) systems possess the potential to maximize the number of multiplexed spatial channels per fiber by harnessing both the space (fiber cores) and mode (optical mode per core) dimensions. However, to date, no SDM transmissions over field-deployed FM-MCFs in realistic outdoor settings have been reported, which contrasts with SDM schemes demonstrated using single-mode multi-core fibers (SM-MCFs) installed in practical fiber cable ducts. In this paper, we present the successful demonstration of bidirectional SDM transmission over a 5-km field-deployed seven ring-core fiber (7-RCF) with a cladding diameter of 178 ${\mu}$m, achieving a Spectral Efficiency (SE) of 2$\times$201.6 bit/s/Hz. This work establishes a new record for the highest SE attained in SDM demonstrations utilizing field-deployed fiber cables, achieving an approximate 10x increase compared to the SE of reported field-deployed optical fiber cable transmission systems. Notably, these results are realized through the utilization of small-scale modular 4$\times$4 multiple-input multiple-output (MIMO) processing with a time-domain equalization (TDE) tap number not exceeding 15, maintaining a complexity per unit capacity comparable to that of MIMO equalization in SDM demonstrations employing weakly coupled SM-MCF cables. These results underscore the significant potential for achieving heightened SE and expanding capacity per individual fiber using SDM techniques in practical applications. ",Kein DOI-Link verfügbar,2407.01552v1,Yes,potent(2)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Casimir repulsive-attractive transition between liquid-separated   dielectric metamaterial and metal,1970,"  We study the repulsive-attractive transition, regarded as stable equilibrium, between gold and dielectric metamaterial based on Mie resonance immersed in various fluids due to the interplay of gravity, buoyancy and Casimir force among different geometries consisting of parallel plates and spheres levitated over substrates. A wider range of separation distance of stable equilibrium is obtained with Mie metamaterial than natural materials. We investigate the relationship between separation distance of stable equilibrium and constructive parameters of Mie metamaterial and geometric parameters of the system, and provide simple rules to tune the equilibrium position by modifying constructive parameters of Mie metamaterial. Particularly, the effect of permeability of Mie metamaterial on equilibrium separation is also considered. Our work is promising for potential applications in frictionless suspension in micro/nanofabrication technologies. ",https://doi.org/10.1103/PhysRevB.98.035410,1804.09011v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Experimentally demonstration of the repulsive Casimir force in the   gold-cyclohexane-PTFE system,1970,"  The experimentally demonstration of Casimir force transition from attraction to repulsion is still challenging. Herein, the Casimir forces for a sphere above a plate immersed in different liquids were precisely measured using Atomic force microscope, and the long-range repulsive Casimir force in the gold-cyclohexane-PTFE system is observed for the first time. The experimental data are consistent with the calculation by Lifshitz theory, which offers the direct evidence for the system of {\epsilon}1<{\epsilon}3<{\epsilon}2. It further verifies the reasonability of van Zwol et al. dielectric model to describe the intervening fluids. This study is promising for potential applications on quantum levitation and frictionless devices in MEMS and NEMS by Casimir repulsion. ",Kein DOI-Link verfügbar,1911.09938v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Investigation of the particle-particle interaction effects in the cosmic   Zevatron based on cyclotron auto-resonance by particle-in-cell simulations,1970,"  Cyclotron autoresonance acceleration has been recently advanced as a potential mechanism for accelerating nuclei to ZeV energies (1 ZeV = $10^{21}$ eV). All results have been based on single- and many-particle calculations employing analytic solutions to the relativistic equations of motion in the combined magnetic and radiation fields, excluding effects related to the particle-particle interactions. Here, results from many-particle calculations and Particle-In-Cell (PIC) simulations, are presented which lend support to the single-particle investigations. Each single-particle result is found to lie well within one standard deviation about the ensemble average obtained from the corresponding many-particle calculation. The PIC simulations show that, even for number densities far exceeding those employed in the non-interacting case, the energy gain drops markedly due to the particle-particle interactions, over the first $\sim 8~ mm$ of the acceleration length. Together with the substantial attenuation, this finding supports the conclusion that the particle-particle interaction effects can be negligibly small over acceleration lengths of typically many kilometers. ",Kein DOI-Link verfügbar,2106.05787v2,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Observing the nodal-line conversion determined by the relative homotopy,1970,"  Directly identifying the non-Abelian nodal-line semimetals (NASM) is quite challenging because nodal-line semimetals typically do not possess topologically protected boundary modes. Here, by reconstructing the correspondence between the bulk states of Hermitian systems and circuit voltage modes through gauge scale potential, the temporal topolectrical circuits (TTC) for evidencing NASM are proposed. Following the logical progress of discovering NASM, we start by demonstrating the relative homotopy group of two-band models using TTC, which can faithfully determine the conversion rules between the nodes in and out of the non-local-symmetry invariant subspace. Next, we show that those rules dramatically change with the consideration of the additional band, historically leading to the arising of the NASM. Also, we demonstrate the unique non-Abelian constrained nodal configuration -- earring nodal lines. Our results established NASM for further investigating topological line degeneracies, and proposed TTC will be a versatile platform for exploring nodal-line semimetals. ",Kein DOI-Link verfügbar,2206.11547v1,Yes,"versatile(1), potent(1)"
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive   and Dynamic Stock Investment Prediction,1970,"  The stock market is a crucial component of the financial system, but predicting the movement of stock prices is challenging due to the dynamic and intricate relations arising from various aspects such as economic indicators, financial reports, global news, and investor sentiment. Traditional sequential methods and graph-based models have been applied in stock movement prediction, but they have limitations in capturing the multifaceted and temporal influences in stock price movements. To address these challenges, the Multi-relational Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a discrete dynamic graph to comprehensively capture multifaceted relations among stocks and their evolution over time. The representation generated from the graph offers a complete perspective on the interrelationships among stocks and associated entities. Additionally, the power of the Transformer structure is leveraged to encode the temporal evolution of multiplex relations, providing a dynamic and effective approach to predicting stock investment. Further, our proposed MDGNN framework achieves the best performance in public datasets compared with state-of-the-art (SOTA) stock investment methods. ",Kein DOI-Link verfügbar,2402.06633v1,Yes,intricate(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University","A new ferromagnetic semiconductor system of Eu$_{1-x}$Sr$_x$AgP $(x =   0.0-0.6)$ compounds: Crystallographic, magnetic, and magneto-resistive   properties",1970,"  Adjusting chemical pressure through doping is a highly effective method for customizing the chemical and physical properties of materials, along with their respective phase diagrams, thereby uncovering novel quantum phenomena. Here, we successfully synthesized Sr-doped Eu$_{1-x}$Sr$_x$AgP $(x = 0.0-0.6)$ and conducted a comprehensive investigation involving crystallography, magnetization, heat capacity, and magnetoresistance. Utilizing X-ray diffraction and PPMS DynaCool measurements, we studied Eu$_{1-x}$Sr$_x$AgP in detail. The hexagonal structure of parent EuAgP at room temperature, with the $P6_3/mmc$ space group, remains unaltered, while the lattice constants expand. The magnetic phase transition from paramagnetism to ferromagnetism, as temperature decreases, is suppressed through the gradual introduction of strontium doping. Heat capacity measurements reveal a shift from magnon-dominated to predominantly phonon and electron contributions near the ferromagnetic phase with increasing doping levels. The resistivity-temperature relationship displays distinct characteristics, emphasizing the impact of Sr doping on modifying charge transport. Magnetoresistance measurements uncover novel phenomena, illustrating the adjustability of magnetoresistance through Sr doping. Notably, Sr doping results in both positive magnetoresistance (up to 20\%) and negative magnetoresistance (approximately -60\%). The resistivity and magnetic phase diagram were established for the first time, revealing the pronounced feasibility of Sr doping in modulating EuAgP's resistivity. This study has provided valuable insights into the intricate interplay between structural modifications and diverse physical properties. The potential for technological advancements and the exploration of novel quantum states make Sr-doped Eu$_{1-x}$Sr$_x$AgP a compelling subject for continued research in the field of applied physics. ",https://doi.org/10.1016/j.mtcomm.2024.109197,2405.08357v1,Yes,"intricate(1), potent(1)"
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Isospin coupling-channel decomposition of nuclear symmetry energy in   covariant density functional theory,1970,"  The isospin coupling-channel decomposition of the potential energy density functional is carried out within the covariant density functional theory, and their isospin and density dependence in particular the influence on the symmetry energy is studied. It is found that both isospin-singlet and isospin-triplet components of the potential energy play the dominant role in deciding the symmetry energy, especially when the Fock diagram is introduced. The results illustrate a quite different mechanism to the origin of the symmetry energy from the microscopic Brueckner-Hartree-Fock theory, and demonstrate the importance of the Fork diagram in the CDF theory, especially from the isoscalar mesons, in the isospin properties of the in-medium nuclear force at high density. ",https://doi.org/10.1088/0954-3899/42/9/095101,1411.6274v1,Yes,potent(2)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Shear-wave manipulation by embedded soft devices,1970,"  Hyperelastic transformation theory has proven shear-wave manipulation devices with various functions can be designed by utilizing neo-Hookean material with appropriate pre-deformation. However, it is still elusive that how can such devices match with the background medium in which they embedded. In this work, we present a systematic formulation of the transmission and reflection of elastic waves at the interface between un-deformed and pre-deformed hyperelastic materials. With the combination of theoretical analyses and numerical simulations, we specifically investigate the shear-wave propagation from an un-deformed neo-Hookean material to the one subject to different homogeneous deformations. Among three typical deformation modes, we found ""constrained"" uniaxial tension and simple shear guarantee total transmission, whereas ""ordinary"" uniaxial tension and hydrostatic compression cause wave reflection. On this basis, three embedded shear-wave manipulation devices, including a unidirectional cloak, a splicable beam bend, and a concave lens, are proposed and verified through numerical simulations. This work may pave the way for the design and realization of soft-matter-based wave control devices. Potential applications can be anticipated in nondestructive testing, structure impact protection, biomedical imaging, and soft robotics. ",Kein DOI-Link verfügbar,1909.00684v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Diagnosing Batch Normalization in Class Incremental Learning,1970,"  Extensive researches have applied deep neural networks (DNNs) in class incremental learning (Class-IL). As building blocks of DNNs, batch normalization (BN) standardizes intermediate feature maps and has been widely validated to improve training stability and convergence. However, we claim that the direct use of standard BN in Class-IL models is harmful to both the representation learning and the classifier training, thus exacerbating catastrophic forgetting. In this paper we investigate the influence of BN on Class-IL models by illustrating such BN dilemma. We further propose BN Tricks to address the issue by training a better feature extractor while eliminating classification bias. Without inviting extra hyperparameters, we apply BN Tricks to three baseline rehearsal-based methods, ER, DER++ and iCaRL. Through comprehensive experiments conducted on benchmark datasets of Seq-CIFAR-10, Seq-CIFAR-100 and Seq-Tiny-ImageNet, we show that BN Tricks can bring significant performance gains to all adopted baselines, revealing its potential generality along this line of research. ",Kein DOI-Link verfügbar,2202.08025v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Enhancing Underwater Imaging with 4-D Light Fields: Dataset and Method,1970,"  In this paper, we delve into the realm of 4-D light fields (LFs) to enhance underwater imaging plagued by light absorption, scattering, and other challenges. Contrasting with conventional 2-D RGB imaging, 4-D LF imaging excels in capturing scenes from multiple perspectives, thereby indirectly embedding geometric information. This intrinsic property is anticipated to effectively address the challenges associated with underwater imaging. By leveraging both explicit and implicit depth cues present in 4-D LF images, we propose a progressive, mutually reinforcing framework for underwater 4-D LF image enhancement and depth estimation. Specifically, our framework explicitly utilizes estimated depth information alongside implicit depth-related dynamic convolutional kernels to modulate output features. The entire framework decomposes this complex task, iteratively optimizing the enhanced image and depth information to progressively achieve optimal enhancement results. More importantly, we construct the first 4-D LF-based underwater image dataset for quantitative evaluation and supervised training of learning-based methods, comprising 75 underwater scenes and 3675 high-resolution 2K pairs. To craft vibrant and varied underwater scenes, we build underwater environments with various objects and adopt several types of degradation. Through extensive experimentation, we showcase the potential and superiority of 4-D LF-based underwater imaging vis-a-vis traditional 2-D RGB-based approaches. Moreover, our method effectively corrects color bias and achieves state-of-the-art performance. The dataset and code will be publicly available at https://github.com/linlos1234/LFUIE. ",Kein DOI-Link verfügbar,2408.17339v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Direct observation of energy band attraction effect in non-Hermitian   systems,1970,"  The energy band attraction (EBA) caused by the non-orthogonal eigenvectors is a unique phenomenon in the non-Hermitian (NH) system. However, restricted by the required tight-binding approximation and meticulously engineered complex potentials, such effect has not been experimentally demonstrated. Here, an experimentally verifiable model is proposed based on the photonic counterpart of the all-dielectric Mie-resonator lattice in a parallel-plate transmission line. Through theoretical derivation, we directly connect the transmission spectra with eigenvalues and eigenvectors of the NH Hamiltonians. By precisely tuning the resonance loss of the Mie-resonators, the evolution of the EBA effect in two-level NH systems, from gapped bands, gapless bands to flat bands, is directly observed for the first time. Furthermore, such effect can be extended to a graphene-like two-dimensional NH system. Our works show a metamaterial approach towards NH topological photonics and offer a deeper understanding of band theory in open systems. ",https://doi.org/10.1103/PhysRevLett.125.137703,2004.14744v1,Yes,"meticulous(1), potent(1), meticulously(1)"
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Progress in Water-Based Metamaterial Absorber: A Review,1970,"  Increasing attention on microwave ultra-broadband metamaterial absorbers has been paid due to their promising applications. While most microwave ultra-broadband metamaterial absorbers developed so far are based on metallic resonant structures, dispersive dielectric water-based metamaterial opens a simpler and more versatile route for the construction of polarization- and angle- insensitive ultra-broadband absorption. Here, we review the recent progresses of water-based metamaterial absorber by providing an illustration of the mechanisms to realize ultra-broadband, tunable and multi-functional absorption. We also address the further development direction and some potential novel applications. ",Kein DOI-Link verfügbar,2112.13183v1,Yes,"versatile(1), potent(1)"
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Tunable terahertz coherent perfect absorption in a monolayer graphene,1970,"  Coherent perfect absorber (CPA) was proposed as the time-reversed counterpart to laser: a resonator containing lossy medium instead of gain medium can absorb the coherent optical fields completely. Here, we exploit a monolayer graphene to realize the CPA in a non-resonant manner. It is found that quasi-CPA point exists in the terahertz regime for suspending monolayer graphene, and the CPA can be implemented with the assistant of proper phase modulation among two incident beams at the quasi-CPA frequencies. The graphene based CPA is found of broadband angular selectivity: CPA point splits into two frequency bands for the orthogonal $s$ and $p$ polarizations at oblique incidence, and the two bands cover a wide frequency range starting from zero frequency. Furthermore, the coherent absorption can be tuned substantially by varying the gate-controlled Fermi energy. The findings of CPA with non-resonant graphene sheet can be generalized for potential applications in terahertz/infrared detections and signal processing with two-dimensional optoelectronic materials. ",https://doi.org/10.1364/OL.39.006269,1407.7961v2,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Controllable light capsules employing modified Bessel-Gauss beams,1970,"  We report, in theory and experiment, on a novel class of controlled light capsules with nearly perfect darkness, directly employing intrinsic properties of modified Bessel-Gauss beams. These beams are able to naturally create three-dimensional bottle-shaped region during propagation as long as the parameters are properly chosen. Remarkably, the optical bottle can be controlled to demonstrate various geometries through tuning the beam parameters, thereby leading to an adjustable light capsule. We provide a detailed insight into the theoretical origin and characteristics of the light capsule derived from modified Bessel-Gauss beams. Moreover, a binary digital micromirror device (DMD) based scheme is first employed to shape the bottle beams by precise amplitude and phase manipulation. Further, we demonstrate their ability for optical trapping of core-shell magnetic microparticles, which play a particular role in biomedical research, with holographic optical tweezers. Therefore, our observations provide a new route for generating and controlling bottle beams and will widen the potentials for micromanipulation of absorbing particles, aerosols or even individual atoms. ",Kein DOI-Link verfügbar,1605.03544v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Zincophilic armor: Phytate ammonium as a multifunctional additive for   enhanced performance in aqueous zinc-ion batteries,1970,"  Corrosion and the formation of by-products resulting from parasitic side reactions, as well as random dendrite growth, pose significant challenges for aqueous zinc-ion batteries (AZIBs). In this study, phytate ammonium is introduced into the traditional dilute Zinc sulfate electrolyte as a multi-functional additive. Leveraging the inherent zincophilic nature of the phytic anion, a protective layer is formed on the surface of the zinc anode. This layer can effectively manipulate the deposition process, mitigate parasitic reactions, and reduce the accumulation of detrimental by-products. Additionally, the competitive deposition between dissociated ammonium ions and Zn2+ promotes uniform deposition, thereby alleviating dendrite growth. Consequently, the modified electrolyte with a lower volume addition exhibits superior performance. The zinc symmetric battery demonstrates much more reversible plating/stripping, sustaining over 2000 hours at 5 mA cm-2 and 1 mA h cm-2. A high average deposition/stripping efficiency of 99.83% is achieved, indicating the significant boosting effect and practical potential of our strategy for high-performance aqueous zinc-ion batteries. ",https://doi.org/10.1016/j.cej.2024.151111,2404.05165v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Blind Image Deconvolution by Generative-based Kernel Prior and   Initializer via Latent Encoding,1970,"  Blind image deconvolution (BID) is a classic yet challenging problem in the field of image processing. Recent advances in deep image prior (DIP) have motivated a series of DIP-based approaches, demonstrating remarkable success in BID. However, due to the high non-convexity of the inherent optimization process, these methods are notorious for their sensitivity to the initialized kernel. To alleviate this issue and further improve their performance, we propose a new framework for BID that better considers the prior modeling and the initialization for blur kernels, leveraging a deep generative model. The proposed approach pre-trains a generative adversarial network-based kernel generator that aptly characterizes the kernel priors and a kernel initializer that facilitates a well-informed initialization for the blur kernel through latent space encoding. With the pre-trained kernel generator and initializer, one can obtain a high-quality initialization of the blur kernel, and enable optimization within a compact latent kernel manifold. Such a framework results in an evident performance improvement over existing DIP-based BID methods. Extensive experiments on different datasets demonstrate the effectiveness of the proposed method. ",Kein DOI-Link verfügbar,2407.14816v1,Yes,aptly(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Video Rain/Snow Removal by Transformed Online Multiscale Convolutional   Sparse Coding,1970,"  Video rain/snow removal from surveillance videos is an important task in the computer vision community since rain/snow existed in videos can severely degenerate the performance of many surveillance system. Various methods have been investigated extensively, but most only consider consistent rain/snow under stable background scenes. Rain/snow captured from practical surveillance camera, however, is always highly dynamic in time with the background scene transformed occasionally. To this issue, this paper proposes a novel rain/snow removal approach, which fully considers dynamic statistics of both rain/snow and background scenes taken from a video sequence. Specifically, the rain/snow is encoded as an online multi-scale convolutional sparse coding (OMS-CSC) model, which not only finely delivers the sparse scattering and multi-scale shapes of real rain/snow, but also well encodes their temporally dynamic configurations by real-time ameliorated parameters in the model. Furthermore, a transformation operator imposed on the background scenes is further embedded into the proposed model, which finely conveys the dynamic background transformations, such as rotations, scalings and distortions, inevitably existed in a real video sequence. The approach so constructed can naturally better adapt to the dynamic rain/snow as well as background changes, and also suitable to deal with the streaming video attributed its online learning mode. The proposed model is formulated in a concise maximum a posterior (MAP) framework and is readily solved by the ADMM algorithm. Compared with the state-of-the-art online and offline video rain/snow removal methods, the proposed method achieves better performance on synthetic and real videos datasets both visually and quantitatively. Specifically, our method can be implemented in relatively high efficiency, showing its potential to real-time video rain/snow removal. ",Kein DOI-Link verfügbar,1909.06148v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Angle-dependent pair production in the polarized two-photon   Breit-Wheeler process,1970,"  The advent of laser-driven high-intensity $\gamma$-photon beams has opened up new opportunities for designing advanced photon-photon colliders. Such colliders have the potential to produce a large yield of linear Breit-Wheeler (LBW) pairs in a single shot, which offers a unique platform for studying the polarized LBW process. In our recent work [Phys. Rev. D 105, L071902(2022)], we investigated the polarization characteristics of LBW pair production in CP $\gamma$-photon collisions. To fully clarify the polarization effects involving both CP and LP $\gamma$-photons, here we further investigate the LBW process using the polarized cross section with explicit azimuthal-angle dependence due to the base rotation of photon polarization vectors. We accomplished this by defining a new spin basis for positrons and electrons, which enables us to decouple the transverse and longitudinal spin components of $e^\pm$. By means of analytical calculations and Monte Carlo simulations, we find that the linear polarization of photon can induce the highly angle-dependent pair yield and polarization distributions. The comprehensive knowledge of the polarized LBW process will also open up avenues for investigating the higher-order photon-photon scattering, the laser-driven quantum electrodynamic plasmas and the high-energy astrophysics. ",https://doi.org/10.1103/PhysRevD.107.096013,2304.04367v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Tunable mid-infrared coherent perfect absorption in a graphene   meta-surface,1970,"  We exploited graphene nanoribbons based meta-surface to realize coherent perfect absorption (CPA) in the mid-infrared regime. It was shown that quasi-CPA frequencies, at which CPA can be demonstrated with proper phase modulations, exist for the graphene meta-surface with strong resonant behaviors. The CPA can be tuned substantially by merging the geometric design of the meta-surface and the electrical tunability of graphene. Furthermore, we found that the graphene nanoribbon meta-surface based CPA is realizable with experimental graphene data. The findings of CPA with graphene meta-surface can be generalized for potential applications in optical detections and signal processing with two-dimensional optoelectronic materials. ",https://doi.org/10.1038/srep13956,1502.07435v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Custom Edge-Element FEM Solver and its Application to Eddy-Current   Simulation of Realistic 2M-Element Human Brain Phantom,1970,"  Extensive research papers of three-dimensional computational techniques are widely used for the investigation of human brain pathophysiology. Eddy current analyzing could provide an indication of conductivity change within a biological body. A significant obstacle to current trend analyses is the development of a numerically stable and efficiency-finite element scheme that performs well at low frequency and does not require a large number of degrees of freedom. Here, a custom finite element method (FEM) solver based on edge elements is proposed using the weakly coupled theory, which separates the solution into two steps. First, the background field (the magnetic vector potential on each edge) is calculated and stored. Then, the electric scalar potential on each node is obtained by FEM based on Galerkin formulations. Consequently, the electric field and eddy current distribution in the object can be obtained. This solver is more efficient than typical commercial solvers since it reduces the vector eddy current equation to a scalar one, and reduces the meshing domain to just the eddy current region. It can therefore tackle complex eddy current calculations for models with much larger numbers of elements, such as those encountered in eddy current computation in biological tissues. An example is presented with a realistic human brain mesh of 2 million elements. In addition, with this solver, the equivalent magnetic field induced from the excitation coil is applied, and therefore there is no need to mesh the excitation coil. In combination, these significantly increase the efficiency of the solver. ",https://doi.org/10.1002/bem.22148,1906.01513v1,Yes,potent(2)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Brilliant circularly polarized $γ$-ray sources via single-shot   laser plasma interaction,1970,"  Circularly polarized (CP) $\gamma$-ray sources are versatile for broad applications in nuclear physics, high-energy physics and astrophysics. The laser-plasma based particle accelerators provide accessibility for much higher flux $\gamma$-ray sources than conventional approaches, in which, however, the circular polarization properties of emitted $\gamma$-photons are used to be neglected. In this letter, we show that brilliant CP $\gamma$-ray beams can be generated via the combination of laser plasma wakefield acceleration and plasma mirror techniques. In weakly nonlinear Compton scattering scheme with moderate laser intensities, the helicity of the driving laser can be transferred to the emitted $\gamma$-photons, and their average polarization degree can reach about $\sim 37\%$ ($21\%$) with a peak brilliance of $\gtrsim 10^{21}~$photons/(s $\cdot$ mm$^2 \cdot$ mrad$^2 \cdot$ 0.1% BW) around 1~MeV (100~MeV). Moreover, our proposed method is easily feasible and robust with respect to the laser and plasma parameters. ",https://doi.org/10.1364/OL.462612,2111.11012v1,Yes,versatile(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Evidencing non-Bloch dynamics in temporal topolectrical circuits,1970,"  One of the core concepts from the non-Hermitian skin effect is the extended complex wavevectors (CW) in the generalized Brillouin zone (GBZ), while the origin of CW remains elusive, and further experimental demonstration of GBZ is still lacking. We show that the bulk states of an open quantum system dynamically governed by the Lindblad master equation exhibit non-Bloch evolution which results in CW. Experimentally, we present temporal topolectrical circuits to serve as simulators for the dynamics of an open system. By reconstructing the correspondence between the bulk states of an open system and circuit voltage modes through gauge scale potentials in the circuit, the non-Bloch evolution is demonstrated. Facilitated by the simulators and proper approach to characterize the non-Bloch band proposed here, the GBZ is confirmed. Our work may advance the investigation of the dissipative topological modes and provide a versatile platform for exploring the unique evolution and topology for both closed and open systems. ",https://doi.org/10.1103/PhysRevB.107.064307,2206.11542v1,Yes,"versatile(1), potent(1)"
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Predicting densities and elastic moduli of SiO2-based glasses by machine   learning,1970,"  Chemical design of SiO2-based glasses with high elastic moduli and low weight is of great interest. However, it is difficult to find a universal expression to predict the elastic moduli according to the glass composition before synthesis since the elastic moduli are a complex function of interatomic bonds and their ordering at different length scales. Here we show that the densities and elastic moduli of SiO2-based glasses can be efficiently predicted by machine learning (ML) techniques across a complex compositional space with multiple (>10) types of additive oxides besides SiO2. Our machine learning approach relies on a training set generated by high-throughput molecular dynamic (MD) simulations, a set of elaborately constructed descriptors that bridges the empirical statistical modeling with the fundamental physics of interatomic bonding, and a statistical learning/predicting model developed by implementing least absolute shrinkage and selection operator with a gradient boost machine (GBM-LASSO). The predictions of the ML model are comprehensively compared and validated with a large amount of both simulation and experimental data. By just training with a dataset only composed of binary and ternary glass samples, our model shows very promising capabilities to predict the density and elastic moduli for k-nary SiO2-based glasses beyond the training set. As an example of its potential applications, our GBM-LASSO model was used to perform a rapid and low-cost screening of many (~105) compositions of a multicomponent glass system to construct a compositional-property database that allows for a fruitful overview on the glass density and elastic properties. ",Kein DOI-Link verfügbar,1911.02416v1,Yes,potent(1)
0000-0001-8949-1923,Qian Zhao,"Fudan University, Huashan Hospital Fudan University",Electron Self-Polarization in a Quantum-Radiation-Dominated Beam-Plasma   Interaction,1970,"  We propose a feasible method for producing an ultrarelativistic kiloampere spin-polarized electron beam through a multi-GeV unpolarized dense electron beam grazing a solid target composed of two foil layers. In this scheme, a strong asymmetric plasma field is generated by plasma electron backflows between the two foils, leading to a significant radiation via the nonlinear Compton scatter. We show that by positioning the target at an appropriate tilt angle, the electrons emitting high-energy photons can gain high polarization and be reflected above the target. Moreover, the double-layer target setup not only strengthens the asymmetric plasma field but also induces a plasma bubble that further focuses the reflected electrons and reshape their polarization distribution. Our particle-in-cell simulations demonstrate the generation of a dense polarized electron beam with an average polarization exceeding 30% and a conversion rate greater than 10%. Through further selection of energy and angle, a picocoulomb electron beam with a current of approximately 0.4 kA and a polarization degree of 65% can be obtained at a energy peak of 1.5 GeV within 35$\times$35 mrad$^2$ angular spread and 10% energy spread. Such polarized electron beams have potential applications in high-energy physics and laboratory astrophysics. ",Kein DOI-Link verfügbar,2408.08563v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Content Significance Distribution of Sub-Text Blocks in Articles and Its   Application to Article-Organization Assessment,1970,"  We explore how to capture the significance of a sub-text block in an article and how it may be used for text mining tasks. A sub-text block is a sub-sequence of sentences in the article. We formulate the notion of content significance distribution (CSD) of sub-text blocks, referred to as CSD of the first kind and denoted by CSD-1. In particular, we leverage Hugging Face's SentenceTransformer to generate contextual sentence embeddings, and use MoverScore over text embeddings to measure how similar a sub-text block is to the entire text. To overcome the exponential blowup on the number of sub-text blocks, we present an approximation algorithm and show that the approximated CSD-1 is almost identical to the exact CSD-1. Under this approximation, we show that the average and median CSD-1's for news, scholarly research, argument, and narrative articles share the same pattern. We also show that under a certain linear transformation, the complement of the cumulative distribution function of the beta distribution with certain values of $\alpha$ and $\beta$ resembles a CSD-1 curve. We then use CSD-1's to extract linguistic features to train an SVC classifier for assessing how well an article is organized. Through experiments, we show that this method achieves high accuracy for assessing student essays. Moreover, we study CSD of sentence locations, referred to as CSD of the second kind and denoted by CSD-2, and show that average CSD-2's for different types of articles possess distinctive patterns, which either conform common perceptions of article structures or provide rectification with minor deviation. ",https://doi.org/10.5220/0012232600003598,2311.01673v2,Yes,scholarly(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Probe nuclear structure using the anisotropic flow at the Large Hadron   Collider,1970,"  Recent studies have shown that the shape and radial profile of the colliding nuclei have strong influences on the initial condition of the heavy ion collisions and the subsequent development of the anisotropic flow. Using A Multi-Phase Transport model (AMPT) model, we investigated the impact of nuclear quadrupole deformation $\beta_2$ and nuclear diffuseness $a_0$ of $^{129}$Xe on various of flow observables in Xe--Xe collisions at $\sqrtnn =$ 5.44 TeV. We found that $\beta_2$ has a strong influence on central collisions while $a_0$ mostly influences the mid-central collisions. The relative change of flow observables induced by a change in $\beta_2$ and $a_0$ are also found to be insensitive to the values of parameters controlling the strength of the interaction among final state particles. Our study demonstrates the potential for constraining the initial condition of heavy ion collisions using future system scans at the LHC. ",https://doi.org/10.1140/epja/s10050-023-01194-2,2309.09663v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Setup for measuring the optical propagation time in matter,1970,"  Optical propagation time in matter could reveal fruitful information, such as the velocity of light and the sample's refractive index. In this paper, we build a simple and robust setup for measuring the optical propagation time in matter for a known distance, the system uses high frequency square signal as the signal carrier, and a lock-in amplifier is employed to obtain the phase difference between the reference square signal and the other one penetrating the sample, in this way the optical time of flight in matter can be obtained by a background subtraction process. Primary experimental result confirms the feasibility of the newly proposed measuring theory, which can be used to measure easily in high-speed the speed of light and the refractive index of optical transparent material, compared with the currently popular measuring technique using oscilloscope, potential advantage of our proposed method employing lock-in amplifier is that high accuracy are promising, and in contrast with the presently most popular method for measuring the sample's refractive index based on the minimum deviation angle, superiority of our suggested method is the easy preparation of the sample, the convenient operability and the fast measuring speed. ",Kein DOI-Link verfügbar,1709.05208v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",INeAT: Iterative Neural Adaptive Tomography,1970,"  Computed Tomography (CT) with its remarkable capability for three-dimensional imaging from multiple projections, enjoys a broad range of applications in clinical diagnosis, scientific observation, and industrial detection. Neural Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on neural radiance field for CT, and it demonstrates superior performance compared to traditional methods. However, it still faces challenges when dealing with the substantial perturbations and pose shifts encountered in CT scanning processes. Here, we propose a neural rendering method for CT reconstruction, named Iterative Neural Adaptive Tomography (INeAT), which incorporates iterative posture optimization to effectively counteract the influence of posture perturbations in data, particularly in cases involving significant posture variations. Through the implementation of a posture feedback optimization strategy, INeAT iteratively refines the posture corresponding to the input images based on the reconstructed 3D volume. We demonstrate that INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in scenarios with significant pose disturbances. Furthermore, we show that our INeAT maintains comparable reconstruction performance to stable-state acquisitions even using data from unstable-state acquisitions, which significantly reduces the time required for CT scanning and relaxes the stringent requirements on imaging hardware systems, underscoring its immense potential for applications in short-time and low-cost CT technology. ",Kein DOI-Link verfügbar,2311.01653v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Distilling Autoregressive Models to Obtain High-Performance   Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference   Speed,1970,"  Neural construction models have shown promising performance for Vehicle Routing Problems (VRPs) by adopting either the Autoregressive (AR) or Non-Autoregressive (NAR) learning approach. While AR models produce high-quality solutions, they generally have a high inference latency due to their sequential generation nature. Conversely, NAR models generate solutions in parallel with a low inference latency but generally exhibit inferior performance. In this paper, we propose a generic Guided Non-Autoregressive Knowledge Distillation (GNARKD) method to obtain high-performance NAR models having a low inference latency. GNARKD removes the constraint of sequential generation in AR models while preserving the learned pivotal components in the network architecture to obtain the corresponding NAR models through knowledge distillation. We evaluate GNARKD by applying it to three widely adopted AR models to obtain NAR VRP solvers for both synthesized and real-world instances. The experimental results demonstrate that GNARKD significantly reduces the inference time (4-5 times faster) with acceptable performance drop (2-3\%). To the best of our knowledge, this study is first-of-its-kind to obtain NAR VRP solvers from AR ones through knowledge distillation. ",Kein DOI-Link verfügbar,2312.12469v2,Yes,pivotal(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Reconfigurable anisotropy and functional transformations with   VO$_{2}$-based metamaterial electric circuits,1970,"  We demonstrate an innovative multifunctional artificial material that combines exotic metamaterial properties and the environmentally responsive nature of phase change media. The tunable metamaterial is designed with the aid of two interwoven coordinate-transformation equations and implemented with a network of thin film resistors and vanadium dioxide ($VO_{2}$). The strong temperature dependence of $VO_{2}$ electrical conductivity results in a relevant modification of the resistor network behavior, and we provide experimental evidence for a reconfigurable metamaterial electric circuit (MMEC) that not only mimics a continuous medium but is also capable of responding to thermal stimulation through dynamic variation of its spatial anisotropy. Upon external temperature change the overall effective functionality of the material switches between a ""truncated-cloak"" and ""concentrator"" for electric currents. Possible applications may include adaptive matching resistor networks, multifunctional electronic devices, and equivalent artificial materials in the magnetic domain. Additionally, the proposed technology could also be relevant for thermal management of integrated circuits ",https://doi.org/10.1103/PhysRevB.91.134105,1405.7743v1,Yes,innovative(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Quantum simulation with hybrid tensor networks,1970,"  Tensor network theory and quantum simulation are respectively the key classical and quantum computing methods in understanding quantum many-body physics. Here, we introduce the framework of hybrid tensor networks with building blocks consisting of measurable quantum states and classically contractable tensors, inheriting both their distinct features in efficient representation of many-body wave functions. With the example of hybrid tree tensor networks, we demonstrate efficient quantum simulation using a quantum computer whose size is significantly smaller than the one of the target system. We numerically benchmark our method for finding the ground state of 1D and 2D spin systems of up to $8\times 8$ and $9\times 8$ qubits with operations only acting on $8+1$ and $9+1$ qubits,~respectively. Our approach sheds light on simulation of large practical problems with intermediate-scale quantum computers, with potential applications in chemistry, quantum many-body physics, quantum field theory, and quantum gravity thought experiments. ",https://doi.org/10.1103/PhysRevLett.127.040501,2007.00958v2,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Exploring the Nuclear Shape Phase Transition in Ultra-Relativistic   $^{129}$Xe+$^{129}$Xe Collisions at the LHC,1970,"  The shape phase transition for certain isotope or isotone chains, associated with the quantum phase transition of finite nuclei, is an intriguing phenomenon in nuclear physics. A notable case is the Xe isotope chain, where the structure transits from a $\gamma$-soft rotor to a spherical vibrator, with the second-order shape phase transition occurring in the vicinity of $^{128-130}$Xe. In this letter, we focus on investigating the $\gamma$-soft deformation of $^{129}$Xe associated with the second-order shape phase transition by constructing novel correlators for ultra-relativistic $^{129}$Xe+$^{129}$Xe collisions. In particular, our iEBE-VISHNU model calculations show that the $v_2^2-[p_T]$ correlation $\rho_{2}$ and the mean transverse momentum fluctuation $\Gamma_{p_T}$, which were previously interpreted as the evidence for the rigid triaxial deformation of $^{129}$Xe, can also be well explained by the $\gamma$-soft deformation of $^{129}$Xe. We also propose two novel correlators $\rho_{4,2}$ and $\rho_{2,4}$, which carry non-trivial higher-order correlations and show unique capabilities to distinguish between the $\gamma$-soft and the rigid triaxial deformation of $^{129}$Xe in $^{129}$Xe+$^{129}$Xe collisions at the LHC. The present study also provides a novel way to explore the second-order shape phase transition of finite nuclei with ultra-relativistic heavy ion collisions. ",Kein DOI-Link verfügbar,2403.07441v1,Yes,notable(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Nuclear cluster structure effect in $^{16}$O+$^{16}$O collisions at the   top RHIC energy,1970,"  The impact of nuclear structure has garnered considerable attention in the high-energy nuclear physics community in recent years. This work focuses on studying the potential nuclear cluster structure in $^{16}\text{O}$ nuclei using anisotropic flow observables in $\rm O+O$ collisions at 200 GeV. Employing an improved AMPT model with various cluster structure configurations, we find that an extended effective parton formation time is necessary to align with the recent STAR experimental data. In addition, we reveal that the presented flow observables serve as sensitive probes for differentiating configurations of $\alpha$-clustering of $^{16}\text{O}$ nuclei. The systematic AMPT calculations presented in this paper, along with comprehensive comparisons to forthcoming experimental measurements at RHIC and the LHC, pave the way for a novel approach to investigate the $\alpha$-clustering structure of $^{16}\text{O}$ nuclei using $\rm O+O$ collisions at the ultra-relativistic energies. ",Kein DOI-Link verfügbar,2404.09780v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Giant optical nonlinearity of Fermi polarons in atomically thin   semiconductors,1970,"  Realizing strong nonlinear optical responses is a long-standing goal of both fundamental and technological importance. Recently significant efforts have focused on exploring excitons in solids as a pathway to achieving nonlinearities even down to few-photon levels. However, a crucial tradeoff arises as strong light-matter interactions require large oscillator strength and short radiative lifetime of the excitons, which limits their interaction strength and nonlinearity. Here we experimentally demonstrate strong nonlinear optical responses by exploiting the coupling between excitons and carriers in an atomically thin semiconductor of trilayer tungsten diselenide. By controlling the electric field and electrostatic doping of the trilayer, we observe the hybridization between intralayer and interlayer excitons along with the formation of Fermi polarons due to the interactions between excitons and free carriers. We find substantial optical nonlinearity can be achieved under both continuous wave and pulsed laser excitation, where the resonance of the hole-doped Fermi polaron blueshifts by as much as ~10 meV. Intriguingly, we observe a remarkable asymmetry in the optical nonlinearity between electron and hole doping, which is tunable by the applied electric field. We attribute these features to the strong interactions between excitons and free charges with optically induced valley polarization. Our results establish that atomically thin heterostructures are a highly versatile platform for engineering nonlinear optical response with applications to classical and quantum optoelectronics, and open avenues for exploring many-body physics in hybrid Fermionic-Bosonic systems. ",Kein DOI-Link verfügbar,2306.11199v1,Yes,versatile(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Efficient Characterizations of Multiphoton States with an Ultra-thin   Optical Device,1970,"  Metasurface enables the generation and manipulation of multiphoton entanglement with flat optics, providing a more efficient platform for large-scale photonic quantum information processing. Here, we show that a single metasurface optical device would allow more efficient characterizations of multiphoton entangled states, such as shadow tomography, which generally requires fast and complicated control of optical setups to perform information-complete measurements, a demanding task using conventional optics. The compact and stable device here allows implementations of general positive observable value measures with a reduced sample complexity and significantly alleviates the experimental complexity to implement shadow tomography. Integrating self-learning and calibration algorithms, we observe notable advantages in the reconstruction of multiphoton entanglement, including using fewer measurements, having higher accuracy, and being robust against experimental imperfections. Our work unveils the feasibility of metasurface as a favorable integrated optical device for efficient characterization of multiphoton entanglement, and sheds light on scalable photonic quantum technologies with ultra-thin optical devices. ",https://doi.org/10.1038/s41467-024-48213-4,2308.07067v2,Yes,notable(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Directly Estimating Mixed-State Entanglement with Bell Measurement   Assistance,1970,"  Entanglement plays a fundamental role in quantum physics and information processing. Here, we develop an unbiased estimator for mixed-state entanglement in the few-shot scenario and directly estimate it using random unitary evolution in a photonic system. As a supplement to traditional projective measurements, we incorporate Bell measurements on qubit-pairs, enriching the previous randomized measurement scheme, which is no-go in this task with only local unitary evolution. The scheme is scalable to n-qubits via Bell measurements on qubit-pairs. The estimator can be derived directly from a few consecutive outcomes while exhibiting greater robustness to system errors and noise compared to schemes based on shadow estimation. We find that, under a fixed measurement resource, the way with more versatile measurement settings with fewer repeats per setting is more efficient. Our protocol and demonstration advance the direct characterization of quantum states in practice. ",Kein DOI-Link verfügbar,2405.20696v2,Yes,versatile(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Continuous 3D Myocardial Motion Tracking via Echocardiography,1970,"  Myocardial motion tracking stands as an essential clinical tool in the prevention and detection of cardiovascular diseases (CVDs), the foremost cause of death globally. However, current techniques suffer from incomplete and inaccurate motion estimation of the myocardium in both spatial and temporal dimensions, hindering the early identification of myocardial dysfunction. To address these challenges, this paper introduces the Neural Cardiac Motion Field (NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model the 3D structure and the comprehensive 6D forward/backward motion of the heart. This method surpasses pixel-wise limitations by offering the capability to continuously query the precise shape and motion of the myocardium at any specific point throughout the cardiac cycle, enhancing the detailed analysis of cardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF operates without the need for paired datasets, and its optimization is self-supervised through the physics knowledge priors in both space and time dimensions, ensuring compatibility with both 2D and 3D echocardiogram video inputs. Experimental validations across three representative datasets support the robustness and innovative nature of the NeuralCMF, marking significant advantages over existing state-of-the-art methods in cardiac imaging and motion tracking. ",https://doi.org/10.1109/TMI.2024.3419780,2310.02792v2,Yes,innovative(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Measurement Induced Magic Resources,1970,"  Magic states and magic gates are crucial for achieving universal computation, but some important questions about how magic resources should be implemented to attain quantum advantage have remained unexplored, for instance, in the context of Measurement-based Quantum Computation (MQC) with only single-qubit measurements. This work bridges the gap between MQC and the resource theory of magic by introducing the concept of ``invested'' and ``potential"" magic resources. The former quantifies the magic cost associated with the MQC framework, serving both as a witness of magic resources and an upper bound for the realization of a desired unitary transformation. Potential magic resources represent the maximum achievable magic resource in a given graph structure defining the MQC. We utilize these concepts to analyze the magic resource requirements of the Quantum Fourier Transform (QFT) and provide a fresh perspective on the universality of MQC of different resource states, highlighting the crucial role of non-Pauli measurements for injecting magic. We demonstrate experimentally our theoretical predictions in a high-fidelity four-photon setup and demonstrate the efficiency of MQC in generating magic states, surpassing the limitations of conventional magic state injection methods. Our findings pave the way for future research exploring magic resource optimization and novel distillation schemes within the MQC framework, contributing to the advancement of fault-tolerant universal quantum computation. ",Kein DOI-Link verfügbar,2408.01980v3,Yes,"potent(2), fresh(1)"
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Signatures of bilayer Wigner crystals in a transition metal   dichalcogenide heterostructure,1970,"  A Wigner crystal, a regular electron lattice arising from strong correlation effects, is one of the earliest predicted collective electronic states. This many-body state exhibits quantum and classical phase transitions and has been proposed as a basis for quantum information processing applications. In semiconductor platforms, two-dimensional Wigner crystals have been observed under magnetic field or moir\'e-based lattice potential where the electron kinetic energy is strongly suppressed. Here, we report bilayer Wigner crystal formation without a magnetic or confinement field in atomically thin MoSe$_2$ bilayers separated by hexagonal boron nitride. We observe optical signatures of robust correlated insulating states formed at symmetric (1:1) and asymmetric (4:1 and 7:1) electron doping of the two MoSe$_2$ layers at cryogenic temperatures. We attribute these features to the bilayer Wigner crystals formed from two commensurate triangular electron lattices in each layer, stabilized via inter-layer interaction. These bilayer Wigner crystal phases are remarkably stable and undergo quantum and thermal melting transitions above a critical electron density of up to $6 \times10^{12}$ cm$^{-2}$ and at temperatures of ~40 K. Our results demonstrate that atomically thin semiconductors provide a promising new platform for realizing strongly correlated electronic states, probing their electronic and magnetic phase transitions, and developing novel applications in quantum electronics and optoelectronics. ",https://doi.org/10.1038/s41586-021-03560-w,2010.03037v1,Yes,potent(1)
0000-0001-9250-0982,You Zhou,"Fudan University, Zhongshan Hospital Fudan University",Moiré Excitons Correlated with Superlattice Structure in Twisted   WSe$_2$/WSe$_2$ Homobilayers,1970,"  Moir\'e superlattices in twisted van der Waals materials constitute a promising platform for engineering electronic and optical properties. However, a major obstacle to fully understanding these systems and harnessing their potential is the limited ability to correlate the local moir\'e structure with optical properties. By using a recently developed scanning electron microscopy technique to image twisted WSe$_2$/WSe$_2$ bilayers, we directly correlate increasing moir\'e periodicity with the emergence of two distinct exciton species. These can be tuned individually through electrostatic gating, and feature different valley coherence properties. Our observations can be understood as resulting from an array of two intralayer exciton species residing in alternating locations in the superlattice, and illuminate the influence of the moir\'e potential on lateral exciton motion. They open up new avenues for controlling exciton arrays in twisted TMDs, with applications in quantum optoelectronics and explorations of novel many body systems. ",https://doi.org/10.1038/s41563-020-00873-5,1912.06955v1,Yes,potent(2)
0000-0003-3206-7851,James Williams,King's College London,Normal Subgroups of Powerful $p$ -groups,1970,"  In this note we show that if $p$ is an odd prime and $G$ is a powerful $p$-group with $N\leq G^{p}$ and $N$ normal in $G$, then $N$ is powerfully nilpotent. An analogous result is proved for $p=2$ when $N\leq G^{4}$. ",Kein DOI-Link verfügbar,1908.07030v1,Yes,potent(1)
0000-0003-3206-7851,James Williams,King's College London,Omegas of Agemos in Powerful Groups,1970,"  In this note we show that for any powerful $p$-group $G$, the subgroup $\Omega_{i}(G^{p^{j}})$ is powerfully nilpotent for all $i,j\geq1$ when $p$ is an odd prime, and $i\geq1$, $j\geq2$ when $p=2$. We provide an example to show why this modification is needed in the case $p=2$. Furthermore we obtain a bound on the powerful nilpotency class of $\Omega_{i}(G^{p^{j}})$. We give an example to show that powerfully nilpotent characteristic subgroups of powerful $p$-groups need not be strongly powerful. ",https://doi.org/10.22108/IJGT.2019.113217.1507,1811.00977v2,Yes,potent(2)
0000-0003-3206-7851,James Williams,King's College London,On finite $p$-groups with powerful subgroups,1970,"  In this paper we investigate the structure of finite $p$-groups with the property that every subgroup of index $p^i$ is powerful for some $i$. For odd primes $p$, we show that under certain conditions these groups must be potent. Then, motivated by a question of Mann, we investigate in detail the case when all maximal subgroups are powerful. We show that for odd $p$ any finite $p$-group $G$ with all maximal subgroups powerful has a regular power structure - with precisely one exceptional case which is a $3$-group of maximal class and order $81$. To show this counterexample is unique we use a computational approach. We briefly discuss the case $p=2$ and some generalisations. ",Kein DOI-Link verfügbar,2101.05720v1,Yes,potent(1)
0000-0003-3206-7851,James Williams,King's College London,Powerfully nilpotent groups of rank 2 or small order,1970,  In this paper we continue the study of powerfully nilpotent groups. These are powerful $p$-groups possessing a central series of a special kind. To each such group one can attach a powerful nilpotency class that leads naturally to the notion of a powerful coclass and classification in terms of an ancestry tree. In this paper we will give a full classification of powerfully nilpotent groups of rank $2$. The classification will then be used to arrive at a precise formula for the number of powerfully nilpotent groups of rank $2$ and order $p^{n}$. We will also give a detailed analysis of the ancestry tree for these groups. The second part of the paper is then devoted to a full classification of powerfully nilpotent groups of order up to $p^{6}$. ,Kein DOI-Link verfügbar,2002.02694v1,Yes,potent(4)
0000-0003-3206-7851,James Williams,King's College London,Powerfully nilpotent groups,1970,"  We introduce a special class of powerful $p$-groups that we call powerfully nilpotent groups that are finite $p$-groups that possess a central series of a special kind. To these we can attach the notion of a powerful nilpotence class that leads naturally to a classification in terms of an `ancestry tree' and powerful coclass. We show that there are finitely many powerfully nilpotent $p$-groups of each given powerful coclass and develop some general theory for this class of groups. We also determine the growth of powerfully nilpotent groups of exponent $p^{2}$ and order $p^{n}$ where $p$ is odd. The number of these is $f(n)=p^{\alpha n^{3}+o(n^{3})}$ where $\alpha=\frac{9+4\sqrt{2}}{394}$. For the larger class of all powerful groups of exponent $p^{2}$ and order $p^{n}$, where $p$ is odd, the number is $p^{\frac{2}{27}n^{3}+o(n^{3})}$. Thus here the class of powerfully nilpotent $p$-groups is large while sparse within the larger class of powerful $p$-groups. ",Kein DOI-Link verfügbar,1811.00962v1,Yes,potent(4)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Implicit semantic-based personalized micro-videos recommendation,1970,"  With the rapid development of mobile Internet and big data, a huge amount of data is generated in the network, but the data that users are really interested in a very small portion. To extract the information that users are interested in from the huge amount of data, the information overload problem needs to be solved. In the era of mobile internet, the user's characteristics and other information should be combined in the massive amount of data to quickly and accurately recommend content to the user, as far as possible to meet the user's personalized needs. Therefore, there is an urgent need to realize high-speed and effective retrieval in tens of thousands of micro-videos. Video data content contains complex meanings, and there are intrinsic connections between video data. For multimodal information, subspace coding learning is introduced to build a coding network from public potential representations to multimodal feature information, taking into account the consistency and complementarity of information under each modality to obtain a public representation of the complete eigenvalue. An end-to-end reordering model based on deep learning and attention mechanism, called interest-related product similarity model based on multimodal data, is proposed for providing top-N recommendations. The multimodal feature learning module, interest-related network module and product similarity recommendation module together form the new model.By conducting extensive experiments on publicly accessible datasets, the results demonstrate the state-of-the-art performance of our proposed algorithm and its effectiveness. ",Kein DOI-Link verfügbar,2205.03297v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Affective Digital Twins for Digital Human: Bridging the Gap in   Human-Machine Affective Interaction,1970,"  In recent years, metaverse and digital humans have become important research and industry areas of focus. However, existing digital humans still lack realistic affective traits, making emotional interaction with humans difficult. Grounded in the developments of artificial intelligence, human-computer interaction, virtual reality, and affective computing, this paper proposes the concept and technical framework of ""Affective Digital Twins for Digital Human"" based on the philosophy of digital twin technology. The paper discusses several key technical issues including affective modeling, affective perception, affective encoding, and affective expression. Based on this, the paper conducts a preliminary imagination of the future application prospects of affective digital twins for digital human, while considering potential problems that may need to be addressed. ",Kein DOI-Link verfügbar,2308.10207v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Topological $p_{x}+ip_{y}$ Superfluid Phase of a Dipolar Fermi Gas in a   2D Optical Lattice,1970,"  In a dipolar Fermi gas, the anisotropic interaction between electric dipoles can be turned into an effectively attractive interaction in the presence of a rotating electric field. We show that the topological $p_{x}+ip_{y}$ superfluid phase can be realized in a single-component dipolar Fermi gas trapped in a 2D square optical lattice with this attractive interaction at low temperatures. The $p_{x}+ip_{y}$ superfluid state has potential applications for topological quantum computing. We obtain the phase diagram of this system at zero temperature. In the weak-coupling limit, the p-wave superfluid phase is stable for all filling factors. As the interaction strength increases, it is stable close to filling factors $n=0$ or $n=1$, and phase separation takes place in between. When the interaction strength is above a threshold, the system is phase separated for any $0<n<1$. The transition temperature of the $p_{x}+ip_{y}$ superfluid state is estimated and the implication for experiments is discussed. ",https://doi.org/10.1103/PhysRevA.86.031603,1202.4924v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Metric Residual Networks for Sample Efficient Goal-Conditioned   Reinforcement Learning,1970,"  Goal-conditioned reinforcement learning (GCRL) has a wide range of potential real-world applications, including manipulation and navigation problems in robotics. Especially in such robotics tasks, sample efficiency is of the utmost importance for GCRL since, by default, the agent is only rewarded when it reaches its goal. While several methods have been proposed to improve the sample efficiency of GCRL, one relatively under-studied approach is the design of neural architectures to support sample efficiency. In this work, we introduce a novel neural architecture for GCRL that achieves significantly better sample efficiency than the commonly-used monolithic network architecture. The key insight is that the optimal action-value function Q^*(s, a, g) must satisfy the triangle inequality in a specific sense. Furthermore, we introduce the metric residual network (MRN) that deliberately decomposes the action-value function Q(s,a,g) into the negated summation of a metric plus a residual asymmetric component. MRN provably approximates any optimal action-value function Q^*(s,a,g), thus making it a fitting neural architecture for GCRL. We conduct comprehensive experiments across 12 standard benchmark environments in GCRL. The empirical results demonstrate that MRN uniformly outperforms other state-of-the-art GCRL neural architectures in terms of sample efficiency. ",Kein DOI-Link verfügbar,2208.08133v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Stability of Branched Flow from a Quantum Point Contact,1970,"  In classically chaotic systems, small differences in initial conditions are exponentially magnified over time. However, it was observed experimentally that the (necessarily quantum) ""branched flow"" pattern of electron flux from a quantum point contact (QPC) traveling over a random background potential in two-dimensional electron gases(2DEGs) remains substantially invariant to large changes in initial conditions. Since such a potential is classically chaotic and unstable to changes in initial conditions, it was conjectured that the origin of the observed stability is purely quantum mechanical, with no classical analog. In this paper, we show that the observed stability is a result of the physics of the QPC and the nature of the experiment. We show that the same stability can indeed be reproduced classically, or quantum mechanically. In addition, we explore the stability of the branched flow with regards to changes in the eigenmodes of quantum point contact. ",https://doi.org/10.1103/PhysRevLett.111.236804,1309.1814v3,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Chiral orbital magnetism of $p$-orbital bosons in optical lattices,1970,"  Chiral magnetism is a fascinating quantum phenomena that has been found in low-dimensional magnetic materials. It is not only interesting for understanding the concept of chirality, but also important for potential applications in spintronics. Past studies show that chiral magnets require both lack of the inversion symmetry and spin-orbit coupling to induce the Dzyaloshinskii-Moriya (DM) interaction. Here we report that the combination of inversion symmetry breaking and quantum degeneracy of orbital degrees of freedom will provide a new paradigm to achieve the chiral orbital magnetism. By means of the density matrix renormalization group (DMRG) calculation, we demonstrate that the chiral orbital magnetism can be found when considering bosonic atoms loaded in the $p$-band of an optical lattice in the Mott regime. The high tunability of our scheme is also illustrated through simply manipulating the inversion symmetry of the system for the cold atom experimental conditions. ",https://doi.org/10.1103/PhysRevLett.121.015303,1710.08145v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Deep Object Co-segmentation via Spatial-Semantic Network Modulation,1970,"  Object co-segmentation is to segment the shared objects in multiple relevant images, which has numerous applications in computer vision. This paper presents a spatial and semantic modulated deep network framework for object co-segmentation. A backbone network is adopted to extract multi-resolution image features. With the multi-resolution features of the relevant images as input, we design a spatial modulator to learn a mask for each image. The spatial modulator captures the correlations of image feature descriptors via unsupervised learning. The learned mask can roughly localize the shared foreground object while suppressing the background. For the semantic modulator, we model it as a supervised image classification task. We propose a hierarchical second-order pooling module to transform the image features for classification use. The outputs of the two modulators manipulate the multi-resolution features by a shift-and-scale operation so that the features focus on segmenting co-object regions. The proposed model is trained end-to-end without any intricate post-processing. Extensive experiments on four image co-segmentation benchmark datasets demonstrate the superior accuracy of the proposed method compared to state-of-the-art methods. ",Kein DOI-Link verfügbar,1911.12950v1,Yes,intricate(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Anharmonicity Induced Supersolidity In Spin-Orbit Coupled Bose-Einstein   Condensates,1970,"  Supersolid, a fascinating quantum state of matter, features novel phenomena such as the non-classical rotational inertia and transport anomalies. It is a long standing issue of the coexistence of superfluidity and broken translational symmetry in condensed matter physics. By recent experimental advances to create tunable synthetic spin-orbit coupling in ultracold gases, such highly controllable atomic systems would provide new possibilities to access supersolidity with no counterpart in solids. Here we report that the combination of anharmonicity of trapping potential and spin-orbit coupling will provide a new paradigm to achieve supersolids. By means of imaginary time evolution of the Gross-Pitaevskii equation, we demonstrate that a supersolid state can be found when considering a trapped Rashba-type spin-orbit coupled bosonic atoms loaded in a one-dimensional optical lattice. Furthermore, a skyrmion-anti-skyrmion lattice is associated with the appearance of such supersoildity, indicating the topological nontrivial properties of our proposed supersolids. ",https://doi.org/10.1103/PhysRevA.102.033328,1909.11871v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Lorentz Force Correction and Radiation Frequency Property of Charged   Particles in Magnetic Dipole,1970,"  By concern of compression of charge density field, the corrected Lorentz force formula and consequent inference is presented. And further radiation frequency property of an individual charge density field in magnetic dipole is analyzed respectively for radiant property of the charged particle and the emitted electromagnetic wave transfer property between the moving radiant source and observer. As results, the behavior and radiation frequency property of the electron beam in magnetic dipole is interpreted upon the individual's behavior and property. At final, the potential application is put forward for wider interest. ",Kein DOI-Link verfügbar,physics/0304023v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data,1970,"  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play important regulatory roles in post-transcriptional gene regulation by inhibiting the translation of the mRNA into proteins or otherwise cleaving the target mRNA. Inferring miRNA targets provides useful information for understanding the roles of miRNA in biological processes that are potentially involved in complex diseases. Statistical methodologies for point estimation, such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm, have been proposed to identify the interactions of miRNA and mRNA based on sequence and expression data. In this paper, we propose using the Bayesian LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the interactions between miRNA and mRNA using expression data. The proposed Bayesian methods explore the posterior distributions for those parameters required to model the miRNA-mRNA interactions. These approaches can be used to observe the inferred effects of the miRNAs on the targets by plotting the posterior distributions of those parameters. For comparison purposes, the Least Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO (nLASSO), and the proposed Bayesian approaches were applied to four public datasets. We concluded that nLASSO and nBLASSO perform best in terms of sensitivity and specificity. Compared to the point estimate algorithms, which only provide single estimates for those parameters, the Bayesian methods are more meaningful and provide credible intervals, which take into account the uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore, Bayesian methods naturally provide statistical significance to select convincing inferred interactions, while point estimate algorithms require a manually chosen threshold, which is less meaningful, to choose the possible interactions. ",Kein DOI-Link verfügbar,1210.3456v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Constraining the anomalous Higgs boson coupling in $H$+$γ$   production,1970,"  Higgs boson production in association with a photon ($H$+$\gamma$) offers a promising channel to test the Higgs boson to photon coupling at various energy scales. Its potential sensitivity to anomalous couplings of the Higgs boson has not been explored with the proton-proton collision data. In this paper, we reinterpret the latest ATLAS $H$+$\gamma$ resonance search results within the Standard Model effective field theory (EFT) framework, using 36.1 fb$^{-1}$ of proton-proton collision data recorded with the ATLAS detector at $\sqrt{s}=13$ TeV. Constraints on the Wilson coefficients of dimension-six EFT operators related to the Higgs boson to photon coupling are provided for the first time in the $H$+$\gamma$ final state at the LHC. ",https://doi.org/10.1088/1674-1137/43/4/043001,1811.02261v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",A Lifelong Learning Approach to Mobile Robot Navigation,1970,"  This paper presents a self-improving lifelong learning framework for a mobile robot navigating in different environments. Classical static navigation methods require environment-specific in-situ system adjustment, e.g. from human experts, or may repeat their mistakes regardless of how many times they have navigated in the same environment. Having the potential to improve with experience, learning-based navigation is highly dependent on access to training resources, e.g. sufficient memory and fast computation, and is prone to forgetting previously learned capability, especially when facing different environments. In this work, we propose Lifelong Learning for Navigation (LLfN) which (1) improves a mobile robot's navigation behavior purely based on its own experience, and (2) retains the robot's capability to navigate in previous environments after learning in new ones. LLfN is implemented and tested entirely onboard a physical robot with a limited memory and computation budget. ",Kein DOI-Link verfügbar,2007.14486v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Dual-Role AoI-based Incentive Mechanism for HD map Crowdsourcing,1970,"  A high-quality fresh high-definition (HD) map is vital in enhancing transportation efficiency and safety in autonomous driving. Vehicle-based crowdsourcing offers a promising approach for updating HD maps. However, recruiting crowdsourcing vehicles involves making the challenging tradeoff between the HD map freshness and recruitment costs. Existing studies on HD map crowdsourcing often (1) prioritize maximizing spatial coverage and (2) overlook the dual role of crowdsourcing vehicles in HD maps, as vehicles serve both as contributors and customers of HD maps. This motivates us to propose the Dual-Role Age of Information (AoI) based Incentive Mechanism (DRAIM) to address these issues. % Specifically, we propose the trajectory age of information, incorporating the expected AoI of the HD map and the trajectory, to quantify a vehicle's HD map usage utility, which is freshness- and trajectory-dependent. DRAIM aims to achieve the company's tradeoff between freshness and recruitment costs. ",Kein DOI-Link verfügbar,2405.00353v1,Yes,fresh(4)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Real-world challenges for multi-agent reinforcement learning in   grid-interactive buildings,1970,"  Building upon prior research that highlighted the need for standardizing environments for building control research, and inspired by recently introduced challenges for real life reinforcement learning control, here we propose a non-exhaustive set of nine real world challenges for reinforcement learning control in grid-interactive buildings. We argue that research in this area should be expressed in this framework in addition to providing a standardized environment for repeatability. Advanced controllers such as model predictive control and reinforcement learning (RL) control have both advantages and disadvantages that prevent them from being implemented in real world problems. Comparisons between the two are rare, and often biased. By focusing on the challenges, we can investigate the performance of the controllers under a variety of situations and generate a fair comparison. As a demonstration, we implement the offline learning challenge in CityLearn and study the impact of different levels of domain knowledge and complexity of RL algorithms. We show that the sequence of operations utilized in a rule based controller (RBC) used for offline training affects the performance of the RL agents when evaluated on a set of four energy flexibility metrics. Longer offline learning from an optimized RBC leads to improved performance in the long run. RL agents that learn from a simplified RBC risk poorer performance as the offline learning period increases. We also observe no impact on performance from information sharing amongst agents. We call for a more interdisciplinary effort of the research community to address the real world challenges, and unlock the potential of grid-interactive building ",https://doi.org/10.1016/j.egyai.2022.100202,2112.06127v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",A Critical Review of Inductive Logic Programming Techniques for   Explainable AI,1970,"  Despite recent advances in modern machine learning algorithms, the opaqueness of their underlying mechanisms continues to be an obstacle in adoption. To instill confidence and trust in artificial intelligence systems, Explainable Artificial Intelligence has emerged as a response to improving modern machine learning algorithms' explainability. Inductive Logic Programming (ILP), a subfield of symbolic artificial intelligence, plays a promising role in generating interpretable explanations because of its intuitive logic-driven framework. ILP effectively leverages abductive reasoning to generate explainable first-order clausal theories from examples and background knowledge. However, several challenges in developing methods inspired by ILP need to be addressed for their successful application in practice. For example, existing ILP systems often have a vast solution space, and the induced solutions are very sensitive to noises and disturbances. This survey paper summarizes the recent advances in ILP and a discussion of statistical relational learning and neural-symbolic algorithms, which offer synergistic views to ILP. Following a critical review of the recent advances, we delineate observed challenges and highlight potential avenues of further ILP-motivated research toward developing self-explanatory artificial intelligence systems. ",Kein DOI-Link verfügbar,2112.15319v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Spatial enantioseparation of gaseous chiral molecules,1970,"  We explore the spatial enantioseparation of gaseous chiral molecules for the cyclic three-level systems coupled with three electromagnetic fields. Due to molecular rotations, the specific requirements of the polarization directions of the three electromagnetic fields lead to the space-dependent part of the overall phase of the coupling strengths. Thus, the overall phase of the coupling strengths, which differs with $\pi$ for the enantiomers in the cyclic three-level model of chiral molecules, varies intensely in the length scale of the typical wavelength of the applied electromagnetic fields. Under the induced gauge potentials resulting from the space-dependent part of the overall phase and the space-dependent intensities of coupling strengths, we further show spatial enantioseparation for typical parameters of gaseous chiral molecules. ",https://doi.org/10.1103/PhysRevA.104.013113,2103.01758v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Coalescence induced late departure of bubbles improves water   electrolysis efficiency,1970,"  In this study, we examine the effects of bubble collisions and coalescence on electrolysis performance using micro-electrodes in a 0.5 M H2SO4. We observe that the addition of electrolytes such as HClO4, despite increasing conductivity, significantly reduces energy conversion efficiency by inhibiting bubble coalescence. A phase diagram illustrates the trade-off between improved conductivity and inhibition of coalescence with varying concentrations of supporting electrolytes (HClO4 or Na2SO4) at different electrolysis current. Contrary to conventional understanding, we find that larger bubbles resulting from coalescence enhance efficiency. This can be attributed to the early release of smaller bubbles, approximately 10 micron in size, facilitated by the coalescence of the surface microbubble and the just-detached large bubble. Additionally, the merge of two bubbles, at a critical velocity of ~2 m/s, enhances agitation at the electrode surface that significantly improves electrolyte transport within the stagnant layer. These findings challenge conventional perspectives on bubble detachment size and suggest innovative approaches to optimize energy consumption in electrolysis, especially in systems where the bubble coalescence is inhibited, such as alkaline water electrolysis and the chlor-alkali industry. ",Kein DOI-Link verfügbar,2403.12064v1,Yes,innovative(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Tricritical point and solid/liquid/gas phase transition of higher   dimensional AdS black hole in massive gravity,1970,"  By considering the fifth order term of the interaction potential in massive gravity theory, we study the $P-V$ critical behaviors of AdS black hole in $d \geq 7$ dimensional space-time, and find the tricritical point and the solid/liquid/gas phase transition in addition to the Van der Waals-like phase and the reentrant phase transition of the system. The critical phenomena of black holes depend crucially on the number $n$ of interaction potential terms. ",Kein DOI-Link verfügbar,1810.07885v1,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Video Saliency Prediction Using Enhanced Spatiotemporal Alignment   Network,1970,"  Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map. The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released. ",Kein DOI-Link verfügbar,2001.00292v1,Yes,intricate(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",An Iterative Co-Training Transductive Framework for Zero Shot Learning,1970,"  In zero-shot learning (ZSL) community, it is generally recognized that transductive learning performs better than inductive one as the unseen-class samples are also used in its training stage. How to generate pseudo labels for unseen-class samples and how to use such usually noisy pseudo labels are two critical issues in transductive learning. In this work, we introduce an iterative co-training framework which contains two different base ZSL models and an exchanging module. At each iteration, the two different ZSL models are co-trained to separately predict pseudo labels for the unseen-class samples, and the exchanging module exchanges the predicted pseudo labels, then the exchanged pseudo-labeled samples are added into the training sets for the next iteration. By such, our framework can gradually boost the ZSL performance by fully exploiting the potential complementarity of the two models' classification capabilities. In addition, our co-training framework is also applied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector is proposed to pick out the most likely unseen-class samples before class-level classification to alleviate the bias problem in GZSL. Extensive experiments on three benchmarks show that our proposed methods could significantly outperform about $31$ state-of-the-art ones. ",Kein DOI-Link verfügbar,2203.16041v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Enhancing Essay Scoring with Adversarial Weights Perturbation and   Metric-specific AttentionPooling,1970,"  The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.   To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adversarial Weights Perturbation (AWP) and Metric-specific AttentionPooling (6 kinds of AP) for each label in the competition.   The primary focus of this research is to investigate the impact of hyperparameters, particularly the adversarial learning rate, on the performance of the model. By fine-tuning the hyperparameter tuning process, including the influence of 6AP and AWP, the resulting models can provide more accurate evaluations of language proficiency and support tailored learning tasks for ELLs. This work has the potential to significantly benefit ELLs by improving their English language proficiency and facilitating their educational journey. ",Kein DOI-Link verfügbar,2401.05433v1,Yes,"innovative(1), potent(1)"
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Toward Agile Maneuvers in Highly Constrained Spaces: Learning from   Hallucination,1970,"  While classical approaches to autonomous robot navigation currently enable operation in certain environments, they break down in tightly constrained spaces, e.g., where the robot needs to engage in agile maneuvers to squeeze between obstacles. Recent machine learning techniques have the potential to address this shortcoming, but existing approaches require vast amounts of navigation experience for training, during which the robot must operate in close proximity to obstacles and risk collision. In this paper, we propose to side-step this requirement by introducing a new machine learning paradigm for autonomous navigation called learning from hallucination (LfH), which can use training data collected in completely safe environments to compute navigation controllers that result in fast, smooth, and safe navigation in highly constrained environments. Our experimental results show that the proposed LfH system outperforms three autonomous navigation baselines on a real robot and generalizes well to unseen environments, including those based on both classical and machine learning techniques. ",Kein DOI-Link verfügbar,2007.14479v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",VI-OOD: A Unified Representation Learning Framework for Textual   Out-of-distribution Detection,1970,"  Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \url{https://github.com/liam0949/LLM-OOD}. ",Kein DOI-Link verfügbar,2404.06217v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Bloch bound state of spin-orbit-coupled fermions in an optical lattice,1970,"  Understanding fundamentals of few-body physics provides an interesting bottom-up approach for the clarification of many-body properties. The remarkable experimental progress in realizing spin-orbit coupling (SOC) in optical Raman lattices offers a renewed thrust towards discovering novel few-body features induced by the interplay between SOC and optical lattices. Using the Wilson renormalization method to account for high-band effects, we study the low-energy two-body scattering processes of spin-$1/2$ fermions in spin-orbit coupled optical lattices. We demonstrate that, under weak SOC, adding a small lattice potential would destabilize shallow two-body bound states, contrary to conventional wisdom. On the other hand, when lattice is sufficiently deep, two-body bound states are always stabilized by increasing the lattice depth. This intriguing non-monotonic behavior of the bound-state stability derives from the competition between SOC and optical lattices, and can be explained by analyzing the low-energy density of states. We also discuss the impact of high-band effects on such a behavior, as well as potential experimental detections. ",https://doi.org/10.1103/PhysRevA.99.012703,1806.02478v2,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Continuous-time Gaussian Process Trajectory Generation for Multi-robot   Formation via Probabilistic Inference,1970,"  In this paper, we extend a famous motion planning approach GPMP2 to multi-robot cases, yielding a novel centralized trajectory generation method for the multi-robot formation. A sparse Gaussian Process model is employed to represent the continuous-time trajectories of all robots as a limited number of states, which improves computational efficiency due to the sparsity. We add constraints to guarantee collision avoidance between individuals as well as formation maintenance, then all constraints and kinematics are formulated on a factor graph. By introducing a global planner, our proposed method can generate trajectories efficiently for a team of robots which have to get through a width-varying area by adaptive formation change. Finally, we provide the implementation of an incremental replanning algorithm to demonstrate the online operation potential of our proposed framework. The experiments in simulation and real world illustrate the feasibility, efficiency and scalability of our approach. ",Kein DOI-Link verfügbar,2010.13148v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Automatic Extraction of Medication Names in Tweets as Named Entity   Recognition,1970,"  Social media posts contain potentially valuable information about medical conditions and health-related behavior. Biocreative VII Task 3 focuses on mining this information by recognizing mentions of medications and dietary supplements in tweets. We approach this task by fine tuning multiple BERT-style language models to perform token-level classification, and combining them into ensembles to generate final predictions. Our best system consists of five Megatron-BERT-345M models and achieves a strict F1 score of 0.764 on unseen test data. ",Kein DOI-Link verfügbar,2111.15641v1,Yes,potent(1)
0000-0001-7351-6014,Alexandra Parker,King's College London,A prototype software framework for transferable computational health   economic models and its early application in youth mental health,1970,"  We are developing an economic model to explore multiple topics in Australian youth mental health policy. We want that model to be readily transferable to other jurisdictions. We developed a software framework for authoring transparent, reusable and updatable Computational Health Economic Models (CHEMs) (the software files that implement health economic models). We specified framework user requirements of a template CHEM module that facilitates modular model implementations, a simple programming syntax and tools for authoring new CHEM modules, supplying CHEMs with data, reporting reproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM project website. We implemented the framework as six development version code libraries in the programming language R that integrate with online services for software development and research data archiving. We used the framework to author five development version R libraries of CHEM modules focused on utility mapping in youth mental health. These modules provide tools for variable validation, dataset description, multi-attribute instrument scoring, construction of mapping models, reporting of mapping studies and making out of sample predictions. We assessed these CHEM module libraries as mostly meeting transparency, reusability and updatability criteria that we have previously developed, but requiring more detailed documentation and unit testing of individual modules. Our software framework has potential value as a prototype for future tools to support the development of transferable CHEMs. ",https://doi.org/10.1007/s40273-024-01378-8,2310.14138v2,Yes,potent(1)
0000-0003-4259-724X,Mark Watson,The University of Queensland,"Relativistic Wind Farm Effect: Possibly Turbulent Flow of a Charged,   Massless Relativistic Fluid in Graphene",1970,"  At low Reynolds numbers, the wind flow in the wake of a single wind turbine is generally not turbulent. However, turbines in wind farms affect each other's wakes so that a turbulent flow can arise. In the present work, an analogue of this effect for the massless charge carrier flow around obstacles in graphene is outlined. We use a relativistic hydrodynamic simulation to analyze the flow in a sample containing impurities. Depending on the density of impurities in the sample, we indeed find evidence for potentially turbulent flow and discuss experimental consequences. ",https://doi.org/10.1063/5.0093429,2202.07839v1,Yes,potent(1)
0000-0003-4259-724X,Mark Watson,The University of Queensland,Turbulence in Two-Dimensional Relativistic Hydrodynamic Systems with a   Lattice Boltzmann Model,1970,"  Using a Lattice Boltzmann hydrodynamic computational modeler to simulate relativistic fluid systems we explore turbulence in two-dimensional relativistic flows. We first a give a pedagogical description of the phenomenon of turbulence and its characteristics in a two-dimensional system. The classical Lattice Boltzmann Method and its extension to relativistic fluid systems is then described. The model is tested against a system incorporating a random stirring force in k-space and then applied to a realistic sample of graphene.   Part I: We investigate the relativistic adaptation of the Lattice Boltzmann Method reproducing a turbulent, two-dimensional, massless hydrodynamic system with a zero-averaged stirring force randomly generated in momentum space. The numeric formulation is evaluated and the flow characteristics produced are compared to properties of classical turbulence. The model can reasonably be expected to offer quantitative simulations of charged fluid flows in two-dimensional relativistic fluid systems.   Part II: At low Reynolds numbers, the wind flow in the wake of a single wind turbine is generally not turbulent. However, turbines in wind farms affect each other's wakes so that a turbulent flow can arise. An analogue of this effect for the massless charge carrier flow around obstacles in graphene is outlined. We use a relativistic hydrodynamic simulation to analyze the flow in a sample containing impurities. Depending on the density of impurities in the sample, we indeed find evidence for a potentially turbulent flow and discuss experimental consequences. ",Kein DOI-Link verfügbar,2205.04658v1,Yes,potent(1)
0000-0003-4259-724X,Mark Watson,The University of Queensland,Length-scale study in deep learning prediction for non-small cell lung   cancer brain metastasis,1970,"  Deep learning assisted digital pathology has the potential to impact clinical practice in significant ways. In recent studies, deep neural network (DNN) enabled analysis outperforms human pathologists. Increasing sizes and complexity of the DNN architecture generally improves performance at the cost of DNN's explainability. For pathology, this lack of DNN explainability is particularly problematic as it hinders the broader clinical interpretation of the pathology features that may provide physiological disease insights. To better assess the features that DNN uses in developing predictive algorithms to interpret digital microscopic images, we sought to understand the role of resolution and tissue scale and here describe a novel method for studying the predictive feature length-scale that underpins a DNN's predictive power. We applied the method to study a DNN's predictive capability in the case example of brain metastasis prediction from early-stage non-small-cell lung cancer biopsy slides. The study highlights the DNN attention in the brain metastasis prediction targeting both cellular scale (resolution) and tissue scale features on H&E-stained histological whole slide images. At the cellular scale, we see that DNN's predictive power is progressively increased at higher resolution (i.e., lower resolvable feature length) and is largely lost when the resolvable feature length is longer than 5 microns. In addition, DNN uses more macro-scale features (maximal feature length) associated with tissue organization/architecture and is optimized when assessing visual fields larger than 41 microns. This study for the first time demonstrates the length-scale requirements necessary for optimal DNN learning on digital whole slide images. ",Kein DOI-Link verfügbar,2406.00555v1,Yes,potent(1)
0000-0001-6739-4145,Shazia Sadiq,The University of Queensland,Leveraging Multi-aspect Time-related Influence in Location   Recommendation,1970,"  Point-Of-Interest (POI) recommendation aims to mine a user's visiting history and find her/his potentially preferred places. Although location recommendation methods have been studied and improved pervasively, the challenges w.r.t employing various influences including temporal aspect still remain. Inspired by the fact that time includes numerous granular slots (e.g. minute, hour, day, week and etc.), in this paper, we define a new problem to perform recommendation through exploiting all diversified temporal factors. In particular, we argue that most existing methods only focus on a limited number of time-related features and neglect others. Furthermore, considering a specific granularity (e.g. time of a day) in recommendation cannot always apply to each user or each dataset. To address the challenges, we propose a probabilistic generative model, named after Multi-aspect Time-related Influence (MATI) to promote POI recommendation. We also develop a novel optimization algorithm based on Expectation Maximization (EM). Our MATI model firstly detects a user's temporal multivariate orientation using her check-in log in Location-based Social Networks(LBSNs). It then performs recommendation using temporal correlations between the user and proposed locations. Our method is adaptable to various types of recommendation systems and can work efficiently in multiple time-scales. Extensive experimental results on two large-scale LBSN datasets verify the effectiveness of our method over other competitors. ",Kein DOI-Link verfügbar,1701.00595v1,Yes,potent(1)
0000-0001-6739-4145,Shazia Sadiq,The University of Queensland,To Predict or to Reject: Causal Effect Estimation with Uncertainty on   Networked Data,1970,"  Due to the imbalanced nature of networked observational data, the causal effect predictions for some individuals can severely violate the positivity/overlap assumption, rendering unreliable estimations. Nevertheless, this potential risk of individual-level treatment effect estimation on networked data has been largely under-explored. To create a more trustworthy causal effect estimator, we propose the uncertainty-aware graph deep kernel learning (GraphDKL) framework with Lipschitz constraint to model the prediction uncertainty with Gaussian process and identify unreliable estimations. To the best of our knowledge, GraphDKL is the first framework to tackle the violation of positivity assumption when performing causal effect estimation with graphs. With extensive experiments, we demonstrate the superiority of our proposed method in uncertainty-aware causal effect estimation on networked data. ",Kein DOI-Link verfügbar,2309.08165v1,Yes,potent(1)
0000-0001-6739-4145,Shazia Sadiq,The University of Queensland,Poisoning Attacks against Recommender Systems: A Survey,1970,"  Modern recommender systems (RS) have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common datasets. The library is released at https://github.com/CoderWZW/ARLib. ",Kein DOI-Link verfügbar,2401.01527v3,Yes,potent(1)
0000-0001-6739-4145,Shazia Sadiq,The University of Queensland,Poisoning Attacks and Defenses in Recommender Systems: A Survey,1970,"  Modern recommender systems (RS) have profoundly enhanced user experience across digital platforms, yet they face significant threats from poisoning attacks. These attacks, aimed at manipulating recommendation outputs for unethical gains, exploit vulnerabilities in RS through injecting malicious data or intervening model training. This survey presents a unique perspective by examining these threats through the lens of an attacker, offering fresh insights into their mechanics and impacts. Concretely, we detail a systematic pipeline that encompasses four stages of a poisoning attack: setting attack goals, assessing attacker capabilities, analyzing victim architecture, and implementing poisoning strategies. The pipeline not only aligns with various attack tactics but also serves as a comprehensive taxonomy to pinpoint focuses of distinct poisoning attacks. Correspondingly, we further classify defensive strategies into two main categories: poisoning data filtering and robust training from the defender's perspective. Finally, we highlight existing limitations and suggest innovative directions for further exploration in this field. ",Kein DOI-Link verfügbar,2406.01022v2,Yes,"innovative(1), fresh(1)"
0000-0001-6739-4145,Shazia Sadiq,The University of Queensland,Unveiling Vulnerabilities of Contrastive Recommender Systems to   Poisoning Attacks,1970,"  Contrastive learning (CL) has recently gained prominence in the domain of recommender systems due to its great ability to enhance recommendation accuracy and improve model robustness. Despite its advantages, this paper identifies a vulnerability of CL-based recommender systems that they are more susceptible to poisoning attacks aiming to promote individual items. Our analysis indicates that this vulnerability is attributed to the uniform spread of representations caused by the InfoNCE loss. Furthermore, theoretical and empirical evidence shows that optimizing this loss favors smooth spectral values of representations. This finding suggests that attackers could facilitate this optimization process of CL by encouraging a more uniform distribution of spectral values, thereby enhancing the degree of representation dispersion. With these insights, we attempt to reveal a potential poisoning attack against CL-based recommender systems, which encompasses a dual-objective framework: one that induces a smoother spectral value distribution to amplify the InfoNCE loss's inherent dispersion effect, named dispersion promotion; and the other that directly elevates the visibility of target items, named rank promotion. We validate the threats of our attack model through extensive experimentation on four datasets. By shedding light on these vulnerabilities, our goal is to advance the development of more robust CL-based recommender systems. The code is available at \url{https://github.com/CoderWZW/ARLib}. ",Kein DOI-Link verfügbar,2311.18244v2,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,On coupled envelope evolution equations in the Hamiltonian theory of   nonlinear surface gravity waves,1970,"  This paper presents a novel theoretical framework in the Hamiltonian theory of nonlinear surface gravity waves. The envelope of surface elevation and the velocity potential on the free water surface are introduced in the framework, which are shown to be a new pair of canonical variables. Using the two envelopes as the main unknowns, coupled envelope evolution equations (CEEEs) are derived based on a perturbation expansion. Similar to the High Order Spectral method, the CEEEs can be derived up to arbitrary order in wave steepness. In contrast, they have a temporal scale as slow as the rate of change of a wave spectrum and allow for the wave fields prescribed on a computational (spatial) domain with a much larger size and with spacing longer than the characteristic wavelength at no expense of accuracy and numerical efficiency. The energy balance equation is derived based on the CEEEs. The nonlinear terms in the CEEEs are in a form of the separation of wave harmonics, due to which an individual term is shown to have clear physical meanings in terms of whether or not it is able to force free waves which obey the dispersion relation. Both the nonlinear terms that can only lead to the forcing of bound waves and these which are capable of forcing free waves are demonstrated, with the latter through the analysis of the quartet and quintet resonant interactions of linear waves. The relations between the CEEEs and two other existing theoretical frameworks are established, including the theory for a train of Stokes waves up to second order in wave steepness [Fenton, J. waterway, Port, Coast. & Ocean Eng., 111, 2, 1985] and a semi-analytical framework for three-dimensional weakly nonlinear surface waves with arbitrary bandwidth and large directional spreading by Li & Li [Phys. Fluids, 33, 7, 2021]. ",https://doi.org/10.1017/jfm.2023.205,2303.09276v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Mass Concentration of Two-Spinless Fermi Systems with Attractive   Interactions,1970,"  We study the two-spinless mass-critical Fermi systems with attractive interactions and trapping potentials. We prove that ground states of the system exist, if and only if the strength $a$ of attractive interactions satisfies $0<a<a_2^*$, where $0<a_2^*<+\infty$ is the best constant of a dual finite-rank Lieb-Thirring inequality. By the blow-up analysis of many-fermion systems, we show that ground states of the system concentrate at the flattest minimum points of the trapping potential $V(x)$ as $a\nearrow a_2^*$. ",Kein DOI-Link verfügbar,2403.17591v1,Yes,potent(2)
0000-0003-2626-6723,Yan Li,The University of Queensland,Policy Mirror Descent Inherently Explores Action Space,1970,"  Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evaluation operator, namely truncated on-policy Monte Carlo (TOMC), attains an $\tilde{\mathcal{O}}(\mathcal{H}_{\mathcal{D}}/\epsilon^2)$ sample complexity, where $\mathcal{H}_{\mathcal{D}}$ mildly depends on the effective horizon and the size of the action space with properly chosen Bregman divergence (e.g., Tsallis divergence). SPMD with TOMC also exhibits stronger convergence properties in that it controls the optimality gap with high probability rather than in expectation. In contrast to explicit exploration, these new policy gradient methods can prevent repeatedly committing to potentially high-risk actions when searching for optimal policies. ",Kein DOI-Link verfügbar,2303.04386v2,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Structural and electronic properties of hybrid graphene and boron   nitride nanostructures on Cu,1970,"  Recently, two-dimensional nanostructures consisting of alternating graphene and boron nitride (BN) domains have been synthesized. These systems possess interesting electronic and mechanical properties, with potential applications in electronics and optical devices. Here, we perform a first-principles investigation of models of BN-C hybrid monolayers and nanoribbons deposited on the Cu(111) surface, a substrate used for their growth in said experiments. For the sake of comparison, we also consider BN and BC2N nanostructures. We show that BN and BC2N monolayers bind weakly to Cu(111), whereas monolayers with alternating domains interact strongly with the substrate at the B-C interface, due to the presence of localized interface states. This binding leads to a deformation of the monolayers and sizable n-doping. Nanoribbons exhibit a similar behaviour. Furthermore, they also interact significantly with the substrate at the edge, even in the case of passivated edges. These findings suggest a route to tune the band gap and doping level of BN-C hybrid models based on the interplay between nanostructuring and substrate-induced effects. ",https://doi.org/10.1103/PhysRevB.88.045317,1308.2306v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Asteroseismology of the pulsating extremely low-mass white dwarf SDSS   J111215.82+111745.0: a model with $p$-mode pulsations consistent with the   observations,1970,"  SDSS J111215.82+111745.0 is the second pulsating extremely low-mass white dwarf discovered. Two short-period pulsations, 107.56 and 134.275 s, were detected on this star, which would be the first observed pressure mode ($p$-mode) pulsations observed on a white dwarf. While the two potential $p$-modes have yet to be confirmed, they make SDSS J111215.82+111745.0 an interesting object. In this work, we analyzed the whole set of seven periods observed on SDSS J111215.82+111745.0. We adopt three independent period-spacing tests to reveal a roughly 93.4 s mean period spacing of $\ell=1$ $g$-modes, which gives added credence to the $\ell=1$ identifications. Then we perform asteroseismic modeling for this star, in which the H chemical profile is taken as a variable. The stellar parameters $M=0.1650\pm0.0137$ $M_\odot$ and $T_\mathrm{eff}=9750\pm560$ K are determined from the best-fit model and the H/He chemical profiles are also defined. The two suspected $p$-modes are also well represented in the best-fit model, and both the stellar parameters and the pulsation frequencies are in good agreement with the values derived from spectroscopy. ",https://doi.org/10.3847/1538-4357/aca533,2211.12011v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Metal-Semiconductor Transition in Armchair Carbon Nanotubes by Symmetry   Breaking,1970,"  The electronic band structure of armchair carbon nanotubes may be considerably modified by potentials with angular dependence. Different angular modes V_q ~ cos(q*theta) have been studied within a tight-binding scheme. Using symmetry arguments, we demonstrate a bandgap opening in these metallic nanotubes when certain selection rules are satisfied for both potential and nanotube structure. We estimate the bandgap opening as a function of both the external potential strength and the nanotube radius and suggest an effective mechanism of metal-semiconductor transition by combination of different forms of perturbations. ",https://doi.org/10.1063/1.1811792,cond-mat/0409464v2,Yes,potent(3)
0000-0003-2626-6723,Yan Li,The University of Queensland,Metal-Semiconductor Transition and Fermi Velocity Renormalization in   Metallic Carbon Nanotubes,1970,"  Angular perturbations modify the band structure of armchair (and other metallic) carbon nanotubes by breaking the tube symmetry and may induce a metal-semiconductor transition when certain selection rules are satisfied. The symmetry requirements apply for both the nanotube and the perturbation potential, as studied within a nonorthogonal $\pi$-orbital tight-binding method. Perturbations of two categories are considered: an on-site electrostatic potential and a lattice deformation which changes the off-site hopping integrals. Armchair nanotubes are proved to be robust against the metal-semiconductor transition in second-order perturbation theory due to their high symmetry, but can develop a nonzero gap by extending the perturbation series to higher orders or by combining potentials of different types. An assumption of orthogonality between $\pi$ orbitals is shown to lead to an accidental electron-hole symmetry and extra selection rules that are weakly broken in the nonorthogonal theory. These results are further generalized to metallic nanotubes of arbitrary chirality. ",https://doi.org/10.1103/PhysRevB.73.035415,cond-mat/0511414v1,Yes,potent(3)
0000-0003-2626-6723,Yan Li,The University of Queensland,Some Liouville theorems for the fractional Laplacian,1970,"  In this paper, we prove the following result. Let $\alpha$ be any real number between $0$ and $2$. Assume that $u$ is a solution of $$ \left\{\begin{array}{ll} (-\Delta)^{\alpha/2} u(x) = 0 , \;\; x \in \mathbb{R}^n ,\\ \displaystyle\underset{|x| \to \infty}{\underline{\lim}} \frac{u(x)}{|x|^{\gamma}} \geq 0 , \end{array} \right. $$ for some $0 \leq \gamma \leq 1$ and $\gamma < \alpha$. Then $u$ must be constant throughout $\mathbb{R}^n$. This is a Liouville Theorem for $\alpha$-harmonic functions under a much weaker condition.   For this theorem we have two different proofs by using two different methods: One is a direct approach using potential theory. The other is by Fourier analysis as a corollary of the fact that the only $\alpha$-harmonic functions are affine. ",https://doi.org/10.1016/j.na.2014.11.003,1407.5559v4,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,"Comment on ""Clustering by fast search and find of density peaks""",1970,"  In [1], a clustering algorithm was given to find the centers of clusters quickly. However, the accuracy of this algorithm heavily depend on the threshold value of d-c. Furthermore, [1] has not provided any efficient way to select the threshold value of d-c, that is, one can have to estimate the value of d_c depend on one's subjective experience. In this paper, based on the data field [2], we propose a new way to automatically extract the threshold value of d_c from the original data set by using the potential entropy of data field. For any data set to be clustered, the most reasonable value of d_c can be objectively calculated from the data set by using our proposed method. The same experiments in [1] are redone with our proposed method on the same experimental data set used in [1], the results of which shows that the problem to calculate the threshold value of d_c in [1] has been solved by using our method. ",Kein DOI-Link verfügbar,1501.04267v2,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Modularized Bilinear Koopman Operator for Modeling and Predicting   Transients of Microgrids,1970,"  Modularized Koopman Bilinear Form (M-KBF) is presented to model and predict the transient dynamics of microgrids in the presence of disturbances. As a scalable data-driven approach, M-KBF divides the identification and prediction of the high-dimensional nonlinear system into the individual study of subsystems; and thus, alleviating the difficulty of intensively handling high volume data and overcoming the curse of dimensionality. For each subsystem, Koopman bilinear form is applied to efficiently identify its model by developing eigenfunctions via the extended dynamic mode decomposition method with an eigenvalue-based order truncation. Extensive tests show that M-KBF can provide accurate transient dynamics prediction for the nonlinear microgrids and verify the plug-and-play modeling and prediction function, which offers a potent tool for identifying high-dimensional systems. The modularity feature of M-KBF enables the provision of fast and precise prediction for the microgrid operation and control, paving the way towards online applications. ",Kein DOI-Link verfügbar,2205.03214v2,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,K-energy on polarized compactifications of Lie groups,1970,"  In this paper, we study Mabuchi's K-energy on a compactification M of a reductive Lie group G, which is a complexification of its maximal compact subgroup K. We give a criterion for the properness of K-energy on the space of K \times K-invariant Kahler potentials. In particular, it turns to give an alternative proof of Delcroix's theorem for the existence of Kahler-Einstein metrics in case of Fano manifolds M . We also study the existence of minimizers of K-energy for general Kahler classes of M. ",Kein DOI-Link verfügbar,1701.00306v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Estimates of the early EM emission from compact binary mergers,1970,"  Compact binary mergers that involve at least one neutron star, either binary neutron star or black hole--neutron star coalescences, are thought to be the potential sources of electromagnetic emission due to the material ejected during the merger or those left outside the central object after the merger. Since the intensity of these electromagnetic transients decay rapidly with time, one should pay more attention to early emissions from such events, which are useful in revealing the nature of these mergers. In this work, we study the early emission of kilonovae, short $\gamma$-ray bursts and cocoons that could be produced in those mergers. We estimate their luminosities and time scales as functions of the chirp mass which is the most readily constrained parameter from the gravitational wave detections of these events. We focus on the range of chirp mass as $1.3M_{\odot} -2.7M_{\odot}$ which is compatible with one of the merging component being a so-called `mass gap' black hole. We show that the electromagnetic observation of these transients could be used to distinguish the types of the mergers when the detected chirp mass falls in the range of $1.5M_{\odot}-1.7M_{\odot}$. Applying our analysis to the sub-threshold GRB GBM-190816, we found that for this particular event the effective spin should be larger than 0.6 and the mass of the heavier object might be larger than 5.5$M_{\odot}$ for the SFHo equation of state. ",https://doi.org/10.3847/1538-4357/abe462,2102.04901v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,A Unified and General Framework for Continual Learning,1970,"  Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnectedness through a shared underlying optimization objective. Moreover, the proposed general framework introduces an innovative concept called refresh learning, specifically designed to enhance the CL performance. This novel approach draws inspiration from neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information. In essence, refresh learning operates by initially unlearning current data and subsequently relearning it. It serves as a versatile plug-in that seamlessly integrates with existing CL methods, offering an adaptable and effective enhancement to the learning process. Extensive experiments on CL benchmarks and theoretical analysis demonstrate the effectiveness of the proposed refresh learning. Code is available at \url{https://github.com/joey-wang123/CL-refresh-learning}. ",Kein DOI-Link verfügbar,2403.13249v1,Yes,"innovative(1), versatile(1), fresh(4)"
0000-0003-2626-6723,Yan Li,The University of Queensland,Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic   Displays,1970,"  Recently, deep learning-based computer-generated holography (CGH) has demonstrated tremendous potential in three-dimensional (3D) displays and yielded impressive display quality. However, most existing deep learning-based CGH techniques can only generate holograms of 1080p resolution, which is far from the ultra-high resolution (16K+) required for practical virtual reality (VR) and augmented reality (AR) applications to support a wide field of view and large eye box. One of the major obstacles in current CGH frameworks lies in the limited memory available on consumer-grade GPUs which could not facilitate the generation of higher-definition holograms. To overcome the aforementioned challenge, we proposed a divide-conquer-and-merge strategy to address the memory and computational capacity scarcity in ultra-high-definition CGH generation. This algorithm empowers existing CGH frameworks to synthesize higher-definition holograms at a faster speed while maintaining high-fidelity image display quality. Both simulations and experiments were conducted to demonstrate the capabilities of the proposed framework. By integrating our strategy into HoloNet and CCNNs, we achieved significant reductions in GPU memory usage during the training period by 64.3\% and 12.9\%, respectively. Furthermore, we observed substantial speed improvements in hologram generation, with an acceleration of up to 3$\times$ and 2 $\times$, respectively. Particularly, we successfully trained and inferred 8K definition holograms on an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore, we conducted full-color optical experiments to verify the effectiveness of our method. We believe our strategy can provide a novel approach for memory- and time-efficient holographic displays. ",Kein DOI-Link verfügbar,2404.10777v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Block Policy Mirror Descent,1970,"  In this paper, we present a new policy gradient (PG) methods, namely the block policy mirror descent (BPMD) method for solving a class of regularized reinforcement learning (RL) problems with (strongly)-convex regularizers. Compared to the traditional PG methods with a batch update rule, which visits and updates the policy for every state, BPMD method has cheap per-iteration computation via a partial update rule that performs the policy update on a sampled state. Despite the nonconvex nature of the problem and a partial update rule, we provide a unified analysis for several sampling schemes, and show that BPMD achieves fast linear convergence to the global optimality. In particular, uniform sampling leads to comparable worst-case total computational complexity as batch PG methods. A necessary and sufficient condition for convergence with on-policy sampling is also identified. With a hybrid sampling scheme, we further show that BPMD enjoys potential instance-dependent acceleration, leading to improved dependence on the state space and consequently outperforming batch PG methods. We then extend BPMD methods to the stochastic setting, by utilizing stochastic first-order information constructed from samples. With a generative model, $\tilde{\mathcal{O}}(\left\lvert \mathcal{S}\right\rvert \left\lvert \mathcal{A}\right\rvert /\epsilon)$ (resp. $\tilde{\mathcal{O}}(\left\lvert \mathcal{S}\right\rvert \left\lvert \mathcal{A} \right\rvert /\epsilon^2)$) sample complexities are established for the strongly-convex (resp. non-strongly-convex) regularizers, where $\epsilon$ denotes the target accuracy. To the best of our knowledge, this is the first time that block coordinate descent methods have been developed and analyzed for policy optimization in reinforcement learning, which provides a new perspective on solving large-scale RL problems. ",Kein DOI-Link verfügbar,2201.05756v3,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Inverse scattering problem with a bare state,1970,"  In hadron physics, molecular-like multihadron states can interact with compact multiquark states. The latter are modeled as bare states in the Hilbert space of a potential model. In this work, we study several potential models relevant to the bare state, and solve their inverse scattering problems. The first model, called ""cc"", is a separable potential model. We show that it can approximate (single-channel short-range) $S$-wave near-threshold physics with an error of $\mathcal{O}(\beta^3/M_V^3)$, where $\beta$ sets the maximum momentum of the near-threshold region and $M_V$ is the typical scale of the potential. The second model, called ""bc"", serves as the bare-state-dominance approximation, where interaction between continuum states is ignored. Under this model, even though the bare state is always crucial for a bound state's generation, a shallow bound state naturally tends to have a small bare-state proportion. Therefore, we need other quantities to quantify the importance of the bare state. The last model, called ""bcc"", is a combination of the first two models. This model not only serves as a correction to the bare-state-dominance approximation, but can also be used to understand the interplay between quark and hadron degrees of freedom. This model naturally leads to the presence of a Castillejo-Dalitz-Dyson (CDD) zero. We consider the energy decomposition of a bound state. The potential ratio of the bare-continuum interaction to the continuum self-interaction is proposed to understand how the bound state is generated. Model independently, an inequality for the potential ratio is derived. Based on the model ""bcc"", the CDD zero can be used to estimate the potential ratio. Finally, we apply these studies to the deuteron, $\rho$ meson, and $D_{s0}^*(2317)$, and analyze their properties. ",https://doi.org/10.1103/PhysRevD.105.116024,2204.05510v2,Yes,potent(7)
0000-0003-2626-6723,Yan Li,The University of Queensland,A Climate Change Vulnerability Assessment Framework: A Spatial Approach,1970,"  Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions. ",Kein DOI-Link verfügbar,2108.09762v1,Yes,innovative(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Existence and Uniqueness of Constraint Minimizers for the Planar   Schrodinger-Poisson System with Logarithmic Potentials,1970,"  In this paper, we study constraint minimizers $u$ of the planar Schr\""odinger-Poisson system with a logarithmic convolution potential $\ln |x|\ast u^2$ and a logarithmic external potential $V(x)=\ln (1+|x|^2)$, which can be described by the $L^2$-critical constraint minimization problem with a subcritical perturbation. We prove that there is a threshold $\rho ^* \in (0,\infty)$ such that constraint minimizers exist if and only if $0<\rho<\rho^*$. In particular, the local uniqueness of positive constraint minimizers as $\rho\nearrow\rho^*$ is analyzed by overcoming the sign-changing property of the logarithmic convolution potential and the non-invariance under translations of the logarithmic external potential. ",Kein DOI-Link verfügbar,2212.00234v1,Yes,potent(4)
0000-0003-2626-6723,Yan Li,The University of Queensland,Motion-enhanced Holography,1970,"  Holographic displays, which enable pixel-level depth control and aberration correction, are considered the key technology for the next-generation virtual reality (VR) and augmented reality (AR) applications. However, traditional holographic systems suffer from limited spatial bandwidth product (SBP), which makes them impossible to reproduce \textit{realistic} 3D displays. Time-multiplexed holography creates different speckle patterns over time and then averages them to achieve a speckle-free 3D display. However, this approach requires spatial light modulators (SLMs) with ultra-fast refresh rates, and current algorithms cannot update holograms at such speeds. To overcome the aforementioned challenge, we proposed a novel architecture, motion-enhanced holography, that achieves \textit{realistic} 3D holographic displays without artifacts by continuously shifting a special hologram. We introduced an iterative algorithm to synthesize motion-enhanced holograms and demonstrated that our method achieved a 10 dB improvement in the peak signal-to-noise ratio (PSNR) of 3D focal stacks in numerical simulations compared to traditional holographic systems. Furthermore, we validated this idea in optical experiments utilizing a high-speed and high-precision programmable three-axis displacement stage to display full-color and high-quality 3D focal stacks. ",Kein DOI-Link verfügbar,2401.12537v1,Yes,fresh(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Approximate dispersion relations for waves on arbitrary shear flows,1970,"  An approximate dispersion relation is derived and presented for linear surface waves atop a shear current whose magnitude and direction can vary arbitrarily with depth. The approximation, derived to first order of deviation from potential flow, is shown to produce good approximations at all wavelengths for a wide range of naturally occuring shear flows as well as widely used model flows. The relation reduces in many cases to a 3D generalization of the much used approximation by Skop [1987], developed further by Kirby & Chen [1989], but is shown to be more robust, succeeding in situations where the Kirby & Chen model fails. The two approximations incur the same numerical cost and difficulty.   While the Kirby & Chen approximation is excellent for a wide range of currents, the exact criteria for its applicability have not been known. We explain the apparently serendipitous success of the latter and derive proper conditions of applicability for both approximate dispersion relations. Our new model has a greater range of applicability.   A second order approximation is also derived. It greatly improves accuracy, which is shown to be important in difficult cases. It has an advantage over the corresponding 2nd order expression proposed by Kirby \& Chen that its criterion of accuracy is explicitly known, which is not currently the case for the latter to our knowledge. Our 2nd order term is also arguably significantly simpler to implement, and more physically transparent, than its sibling due to Kirby & Chen. ",https://doi.org/10.1002/2017JC012994,1711.02914v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots:   A Survey,1970,"  Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, etc. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond. ",https://doi.org/10.1145/3487893,2103.12019v2,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,PIDGeuN: Graph Neural Network-Enabled Transient Dynamics Prediction of   Networked Microgrids Through Full-Field Measurement,1970,"  A Physics-Informed Dynamic Graph Neural Network (PIDGeuN) is presented to accurately, efficiently and robustly predict the nonlinear transient dynamics of microgrids in the presence of disturbances. The graph-based architecture of PIDGeuN provides a natural representation of the microgrid topology. Using only the state information that is practically measurable, PIDGeuN employs a time delay embedding formulation to fully reproduce the system dynamics, avoiding the dependency of conventional methods on internal dynamic states such as controllers. Based on a judiciously designed message passing mechanism, the PIDGeuN incorporates two physics-informed techniques to improve its prediction performance, including a physics-data-infusion approach to determining the inter-dependencies between buses, and a loss term to respect the known physical law of the power system, i.e., the Kirchhoff's law, to ensure the feasibility of the model prediction. Extensive tests show that PIDGeuN can provide accurate and robust prediction of transient dynamics for nonlinear microgrids over a long-term time period. Therefore, the PIDGeuN offers a potent tool for the modeling of large scale networked microgrids (NMs), with potential applications to predictive or preventive control in real time applications for the stable and resilient operations of NMs. ",Kein DOI-Link verfügbar,2204.08557v1,Yes,potent(2)
0000-0003-2626-6723,Yan Li,The University of Queensland,A Quantum Algorithm Based Heuristic to Hide Sensitive Itemsets,1970,"  Quantum devices use qubits to represent information, which allows them to exploit important properties from quantum physics, specifically superposition and entanglement. As a result, quantum computers have the potential to outperform the most advanced classical computers. In recent years, quantum algorithms have shown hints of this promise, and many algorithms have been proposed for the quantum domain. There are two key hurdles to solving difficult real-world problems on quantum computers. The first is on the hardware front -- the number of qubits in the most advanced quantum systems is too small to make the solution of large problems practical. The second involves the algorithms themselves -- as quantum computers use qubits, the algorithms that work there are fundamentally different from those that work on traditional computers. As a result of these constraints, research has focused on developing approaches to solve small versions of problems as proofs of concept -- recognizing that it would be possible to scale these up once quantum devices with enough qubits become available. Our objective in this paper is along the same lines. We present a quantum approach to solve a well-studied problem in the context of data sharing. This heuristic uses the well-known Quantum Approximate Optimization Algorithm (QAOA). We present results on experiments involving small datasets to illustrate how the problem could be solved using quantum algorithms. The results show that the method has potential and provide answers close to optimal. At the same time, we realize there are opportunities for improving the method further. ",Kein DOI-Link verfügbar,2402.08055v1,Yes,potent(2)
0000-0003-2626-6723,Yan Li,The University of Queensland,Controllable selective coupling of Dyakonov surface wave at liquid   crystal based interface,1970,"  Highly directional and lossless surface wave has significant potential applications in the two-dimensional photonic circuits and devices. Here we experimentally demonstrate a selective Dyakonov surface wave coupling at the interface between a transparent polycarbonate material and nematic liquid crystal 5CB. By controlling the anisotropy of the nematic liquid crystal with an applied magnetic field, a single ray at a certain incident angle from a diverged incident beam can be selectively coupled into surface wave. The implementation of this property may lead to a new generation of on-chip integrated optics and two-dimensional photonic devices. ",https://doi.org/10.1103/PhysRevApplied.13.024024,1907.12181v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,High-speed impulsive stimulated Brillouin microscopy,1970,"  Brillouin microscopy, which maps elastic modulus from the frequency shift of scattered light, has evolved to a faster speed for the investigation of rapid biomechanical changes. Impulsive stimulated Brillouin scattering (ISBS) spectroscopy has the potential to speed up measurement through the resonant amplification interaction from pulsed excitation and time-domain continuous detection. However, significant progress has not been achieved due to the limitation in signal-to-noise ratio (SNR) and the corresponding need for excessive averaging to maintain high spectral precision. Moreover, the limited spatial resolution also hinders its application in mechanical imaging. Here, by scrutinizing the SNR model, we design a high-speed ISBS microscope through multi-parameter optimization including phase, reference power, and acquisition time. Leveraging this, with the further assistance of the Matrix Pencil method for data processing, three-dimensional mechanical images are mapped under multiple contrast mechanisms for a millimeter-scale polydimethylsiloxane pattern immersed in methanol, enabling the identification of these two transparent materials without any contact or labeling. Our experimental results demonstrate the capability to maintain high spectral precision and resolution at a sub-millisecond integration time for one pixel. With a two-order improvement in the speed and a tenfold improvement in the spatial resolution over the state-of-the-art systems, this method makes it possible for ISBS microscopes to sensitively investigate rapid mechanical changes in time and space. ",Kein DOI-Link verfügbar,2312.03396v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Benchmarking Domain Generalization on EEG-based Emotion Recognition,1970,"  Electroencephalography (EEG) based emotion recognition has demonstrated tremendous improvement in recent years. Specifically, numerous domain adaptation (DA) algorithms have been exploited in the past five years to enhance the generalization of emotion recognition models across subjects. The DA methods assume that calibration data (although unlabeled) exists in the target domain (new user). However, this assumption conflicts with the application scenario that the model should be deployed without the time-consuming calibration experiments. We argue that domain generalization (DG) is more reasonable than DA in these applications. DG learns how to generalize to unseen target domains by leveraging knowledge from multiple source domains, which provides a new possibility to train general models. In this paper, we for the first time benchmark state-of-the-art DG algorithms on EEG-based emotion recognition. Since convolutional neural network (CNN), deep brief network (DBN) and multilayer perceptron (MLP) have been proved to be effective emotion recognition models, we use these three models as solid baselines. Experimental results show that DG achieves an accuracy of up to 79.41\% on the SEED dataset for recognizing three emotions, indicting the potential of DG in zero-training emotion recognition when multiple sources are available. ",Kein DOI-Link verfügbar,2204.09016v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Higgs algebraic symmetry of screened system in a spherical geometry,1970,"  The orbits and the dynamical symmetries for the screened Coulomb potentials and isotropic harmonic oscillators have been studied by Wu and Zeng [Z. B. Wu and J. Y. Zeng, Phys. Rev. A 62,032509 (2000)]. We find the similar properties in the responding systems in a spherical space, whose dynamical symmetries are described by Higgs Algebra. There exists a conserved aphelion and perihelion vector, which, together with angular momentum, constitute the generators of the geometrical symmetry group at the aphelia and perihelia points $(\dot{r}=0)$. ",https://doi.org/10.1088/0253-6102/60/3/04,1208.3382v1,Yes,potent(1)
0000-0003-2626-6723,Yan Li,The University of Queensland,Axial Symmetry of Normalized Solutions for Magnetic Gross-Pitaevskii   Equations with Anharmonic Potentials,1970,"  This paper is concerned with normalized solutions of the magnetic focusing Gross-Pitaevskii equations with anharmonic potentials in $\R^N$, where $N=2,3$. The existence of axially symmetric solutions is constructed as the parameter $a>0$ satisfies $a \to a_*(N)$, where $a_*(N)\geq0$ is a critical constant depending only on $N$. We further prove that up to the constant phase and rotational transformation, normalized concentrating solutions as $a\to a_*(N)$ must be unique and axially symmetric. As a byproduct, we also obtain that for the case $N=3$, the normalized concentrating solution as $a\to a_*(3)$ is free of vortices, where the anharmonic potential is non-radially symmetric. ",Kein DOI-Link verfügbar,2310.00556v1,Yes,potent(2)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Meson Spectral Functions at Finite Temperature and Isospin Density with   Functional Renormalization Group,1970,"  The pion superfluid and the corresponding Goldstone and soft modes are investigated in two-flavor quark-meson model with functional renormalization group. By solving the flow equations for the effective potential and the meson two-point functions at finite temperature and isospin density, the critical temperature for the superfluid increases sizeably in comparison with solving the flow equation for the potential only. The spectral function for the soft mode shows clearly a transition from meson gas to quark gas with increasing temperature and a crossover from BEC to BCS pairing of quarks with increasing isospin density. ",https://doi.org/10.1103/PhysRevD.96.014006,1703.01035v1,Yes,potent(2)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Fermion spectral function in hot strongly interacting matter from the   functional renormalization group,1970,"  We present the first calculation of fermion spectral function at finite temperature in quark-meson model in the framework of the functional renormalization group (FRG). We compare the results in two truncations, after first evolving flow equation of effective potential, we investigate the spectral function either by taking the IR values as input to calculate one-loop self-energy or by taking the scale-dependent values as input to evolve the flow equation of the fermion two-point function. The latter one is a self-consistent procedure in the framework of FRG. In both truncations, we find a multi-peak structure in the spectral function, indicating quark collective excitations realized in terms of the Landau damping. However, in contrast to fermion zero-mode in the one-loop truncation, we find a fermion soft-mode in the self-consistent truncation, which approaches the zero-mode as temperature increases. ",https://doi.org/10.1103/PhysRevD.98.094031,1808.08535v1,Yes,potent(1)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Relativistic Borromean States,1970,"  In this work the existence of Borromean states has been discussed for bosonic and fermionic cases in both the relativistic and non-relativistic limits from the 3-momentum shell renormalization. With the linear bosonic model we checked the existence of Efimov-like states in the bosonic system. In both limits a geometric series of singularities are found in the 3-boson interaction vertex, while the energy ratio is reduced by around 70\% in the relativistic limit because of the anti-particle contribution. Motivated by the quark-diquark model in heavy baryon studies, we have carefully examined the p-wave quark-diquark interaction and found an isolated Borromean pole at finite energy scale. This may indicate a special baryonic state of light quarks in high energy quark matters. In other cases trivial results are obtained as expected. In relativistic limit, for both bosonic and fermionic cases, potential Borromean states are independent of the mass, which means the results would be valid even in zero-mass limit as well. ",https://doi.org/10.1088/1674-1137/abe197,2004.05997v1,Yes,potent(1)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Accelerating Coordinate Descent via Active Set Selection for Device   Activity Detection for Multi-Cell Massive Random Access,1970,"  We propose a computationally efficient algorithm for the device activity detection problem in the multi-cell massive multi-input multi-output (MIMO) system, where the active devices transmit their signature sequences to multiple BSs in multiple cells and all the BSs cooperate to detect the active devices. The device activity detection problem has been formulated as a maximum likelihood maximization (MLE) problem in the literature. The state-of-the-art algorithm for solving the problem is the (random) coordinate descent (CD) algorithm. However, the CD algorithm fails to exploit the special sparsity structure of the solution of the device activity detection problem, i.e., most of devices are not active in each time slot. In this paper, we propose a novel active set selection strategy to accelerate the CD algorithm and propose an efficient active set CD algorithm for solving the considered problem. Specifically, at each iteration, the proposed active set CD algorithm first selects a small subset of all devices, namely the active set, which contains a few devices that contribute the most to the deviation from the first-order optimality condition of the MLE problem thus potentially can provide the most improvement to the objective function, then applies the CD algorithm to perform the detection for the devices in the active set. Simulation results show that the proposed active set CD algorithm significantly outperforms the state-of-the-art CD algorithm in terms of the computational efficiency. ",https://doi.org/10.1109/SPAWC51858.2021.9593150,2104.12984v2,Yes,potent(1)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,An Efficient Active Set Algorithm for Covariance Based Joint Data and   Activity Detection for Massive Random Access with Massive MIMO,1970,"  This paper proposes a computationally efficient algorithm to solve the joint data and activity detection problem for massive random access with massive multiple-input multiple-output (MIMO). The BS acquires the active devices and their data by detecting the transmitted preassigned nonorthogonal signature sequences. This paper employs a covariance based approach that formulates the detection problem as a maximum likelihood estimation (MLE) problem. To efficiently solve the problem, this paper designs a novel iterative algorithm with low complexity in the regime where the device activity pattern is sparse $\unicode{x2013}$ a key feature that existing algorithmic designs have not previously exploited for reducing complexity. Specifically, at each iteration, the proposed algorithm focuses on only a small subset of all potential sequences, namely the active set, which contains a few most likely active sequences (i.e., transmitted sequences by all active devices), and performs the detection for the sequences in the active set. The active set is carefully selected at each iteration based on the current detection result and the first-order optimality condition of the MLE problem. Simulation results show that the proposed active set algorithm enjoys significantly better computational efficiency (in terms of the CPU time) than the state-of-the-art algorithms. ",https://doi.org/10.1109/ICASSP39728.2021.9413525,2102.03490v1,Yes,potent(1)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Graph-Structured Speculative Decoding,1970,"  Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly surpassing standard speculative decoding. ",Kein DOI-Link verfügbar,2407.16207v1,Yes,"innovative(1), potent(1)"
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Innovative RIS Prototyping Enhancing Wireless Communication with   Real-Time Spot Beam Tracking and OAM Wavefront Manipulation,1970,"  This paper presents a sophisticated reconfigurable metasurface architecture that introduces an advanced concept of flexible full-array space-time wavefront manipulation with enhanced dynamic capabilities. The practical 2-bit phase-shifting unit cell on the RIS is distinguished by its ability to maintain four stable phase states, each with ${90^ \circ }$ differences, and features an insertion loss of less than 0.6 dB across a bandwidth of 200 MHz. All reconfigurable units are equipped with meticulously designed control circuits, governed by an intelligent core composed of multiple Micro-Controller Units (MCUs), enabling rapid control response across the entire RIS array. Owing to the capability of each unit cell on the metasurface to independently switch states, the entire RIS is not limited to controlling general beams with specific directional patterns, but also generates beams with more complex structures, including multi-focus 3D spot beams and vortex beams. This development substantially broadens its applicability across various industrial wireless transmission scenarios. Moreover, by leveraging the rapid-respond space-time coding and the full-array independent programmability of the RIS prototyping operating at 10.7 GHz, we have demonstrated that: 1) The implementation of 3D spot beams scanning facilitates dynamic beam tracking and real-time communication under the indoor near-field scenario; 2) The rapid wavefront rotation of vortex beams enables precise modulation of signals within the Doppler domain, showcasing an innovative approach to wireless signal manipulation; 3) The beam steering experiments for blocking users under outdoor far-field scenarios, verifying the beamforming capability of the RIS board. ",Kein DOI-Link verfügbar,2407.19379v1,Yes,"innovative(1), meticulous(1), meticulously(1)"
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Octopus: Embodied Vision-Language Programmer from Environmental Feedback,1970,"  Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community. ",Kein DOI-Link verfügbar,2310.08588v1,Yes,intricate(1)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,Model Composition for Multimodal Large Language Models,1970,"  Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities. ",Kein DOI-Link verfügbar,2402.12750v2,Yes,versatile(2)
0009-0006-5366-2763,Ziyue Wang,The University of Queensland,"Domain generalization across tumor types, laboratories, and species --   insights from the 2022 edition of the Mitosis Domain Generalization Challenge",1970,"  Recognition of mitotic figures in histologic tumor specimens is highly relevant to patient outcome assessment. This task is challenging for algorithms and human experts alike, with deterioration of algorithmic performance under shifts in image representations. Considerable covariate shifts occur when assessment is performed on different tumor types, images are acquired using different digitization devices, or specimens are produced in different laboratories. This observation motivated the inception of the 2022 challenge on MItosis Domain Generalization (MIDOG 2022). The challenge provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for mitotic figure detection provided by nine challenge participants on ten independent domains. Ground truth for mitotic figure detection was established in two ways: a three-expert consensus and an independent, immunohistochemistry-assisted set of labels. This work represents an overview of the challenge tasks, the algorithmic strategies employed by the participants, and potential factors contributing to their success. With an $F_1$ score of 0.764 for the top-performing team, we summarize that domain generalization across various tumor domains is possible with today's deep learning-based recognition pipelines. However, we also found that domain characteristics not present in the training set (feline as new species, spindle cell shape as new morphology and a new scanner) led to small but significant decreases in performance. When assessed against the immunohistochemistry-assisted reference standard, all methods resulted in reduced recall scores, but with only minor changes in the order of participants in the ranking. ",https://doi.org/10.1016/j.media.2024.103155,2309.15589v2,Yes,potent(1)
0000-0002-3361-5365,Boya Zhang,The University of Queensland,Enhancing Adversarial Robustness via Score-Based Optimization,1970,"  Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed. ",Kein DOI-Link verfügbar,2307.04333v3,Yes,potent(1)
0000-0002-6722-5445,Thomas Davidson,The University of Queensland,Racial Bias in Hate Speech and Abusive Language Detection Datasets,1970,"  Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect. ",Kein DOI-Link verfügbar,1905.12516v1,Yes,potent(1)
0000-0002-6722-5445,Thomas Davidson,The University of Queensland,COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive   Statements,1970,"  Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance ""your English is very good"" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors. ",Kein DOI-Link verfügbar,2306.01985v2,Yes,potent(1)
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Pyramid Hierarchical Transformer for Hyperspectral Image Classification,1970,"  The traditional Transformer model encounters challenges with variable-length input sequences, particularly in Hyperspectral Image Classification (HSIC), leading to efficiency and scalability concerns. To overcome this, we propose a pyramid-based hierarchical transformer (PyFormer). This innovative approach organizes input data hierarchically into segments, each representing distinct abstraction levels, thereby enhancing processing efficiency for lengthy sequences. At each level, a dedicated transformer module is applied, effectively capturing both local and global context. Spatial and spectral information flow within the hierarchy facilitates communication and abstraction propagation. Integration of outputs from different levels culminates in the final input representation. Experimental results underscore the superiority of the proposed method over traditional approaches. Additionally, the incorporation of disjoint samples augments robustness and reliability, thereby highlighting the potential of our approach in advancing HSIC.   The source code is available at https://github.com/mahmad00/PyFormer. ",Kein DOI-Link verfügbar,2404.14945v1,Yes,"innovative(1), potent(1)"
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Importance of Disjoint Sampling in Conventional and Transformer Models   for Hyperspectral Image Classification,1970,"  Disjoint sampling is critical for rigorous and unbiased evaluation of state-of-the-art (SOTA) models. When training, validation, and test sets overlap or share data, it introduces a bias that inflates performance metrics and prevents accurate assessment of a model's true ability to generalize to new examples. This paper presents an innovative disjoint sampling approach for training SOTA models on Hyperspectral image classification (HSIC) tasks. By separating training, validation, and test data without overlap, the proposed method facilitates a fairer evaluation of how well a model can classify pixels it was not exposed to during training or validation. Experiments demonstrate the approach significantly improves a model's generalization compared to alternatives that include training and validation data in test data. By eliminating data leakage between sets, disjoint sampling provides reliable metrics for benchmarking progress in HSIC. Researchers can have confidence that reported performance truly reflects a model's capabilities for classifying new scenes, not just memorized pixels. This rigorous methodology is critical for advancing SOTA models and their real-world application to large-scale land mapping with Hyperspectral sensors.   The source code is available at https://github.com/mahmad00/Disjoint-Sampling-for-Hyperspectral-Image-Classification. ",Kein DOI-Link verfügbar,2404.14944v1,Yes,innovative(1)
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Transformers Fusion across Disjoint Samples for Hyperspectral Image   Classification,1970,"  3D Swin Transformer (3D-ST) known for its hierarchical attention and window-based processing, excels in capturing intricate spatial relationships within images. Spatial-spectral Transformer (SST), meanwhile, specializes in modeling long-range dependencies through self-attention mechanisms. Therefore, this paper introduces a novel method: an attentional fusion of these two transformers to significantly enhance the classification performance of Hyperspectral Images (HSIs). What sets this approach apart is its emphasis on the integration of attentional mechanisms from both architectures. This integration not only refines the modeling of spatial and spectral information but also contributes to achieving more precise and accurate classification results. The experimentation and evaluation of benchmark HSI datasets underscore the importance of employing disjoint training, validation, and test samples. The results demonstrate the effectiveness of the fusion approach, showcasing its superiority over traditional methods and individual transformers. Incorporating disjoint samples enhances the robustness and reliability of the proposed methodology, emphasizing its potential for advancing hyperspectral image classification. ",Kein DOI-Link verfügbar,2405.01095v1,Yes,"intricate(1), potent(1)"
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Scalable Approximation Algorithm for Network Immunization,1970,"  The problem of identifying important players in a given network is of pivotal importance for viral marketing, public health management, network security and various other fields of social network analysis. In this work we find the most important vertices in a graph G = (V,E) to immunize so as the chances of an epidemic outbreak is minimized. This problem is directly relevant to minimizing the impact of a contagion spread (e.g. flu virus, computer virus and rumor) in a graph (e.g. social network, computer network) with a limited budget (e.g. the number of available vaccines, antivirus software, filters). It is well known that this problem is computationally intractable (it is NP-hard). In this work we reformulate the problem as a budgeted combinational optimization problem and use techniques from spectral graph theory to design an efficient greedy algorithm to find a subset of vertices to be immunized. We show that our algorithm takes less time compared to the state of the art algorithm. Thus our algorithm is scalable to networks of much larger sizes than best known solutions proposed earlier. We also give analytical bounds on the quality of our algorithm. Furthermore, we evaluate the efficacy of our algorithm on a number of real world networks and demonstrate that the empirical performance of algorithm supplements the theoretical bounds we present, both in terms of approximation guarantees and computational efficiency. ",Kein DOI-Link verfügbar,1711.00784v1,Yes,pivotal(1)
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Multi Sensor-based Implicit User Identification,1970,"  Smartphones have ubiquitously integrated into our home and work environments, however, users normally rely on explicit but inefficient identification processes in a controlled environment. Therefore, when a device is stolen, a thief can have access to the owner's personal information and services against the stored passwords. As a result of this potential scenario, this work proposes an automatic legitimate user identification system based on gait biometrics extracted from user walking patterns captured by a smartphone. A set of preprocessing schemes is applied to calibrate noisy and invalid samples and augment the gait-induced time and frequency domain features, then further optimized using a non-linear unsupervised feature selection method. The selected features create an underlying gait biometric representation able to discriminate among individuals and identify them uniquely. Different classifiers (i.e. Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Bagging, and Extreme Learning Machine (ELM)) are adopted to achieve accurate legitimate user identification. Extensive experiments on a group of $16$ individuals in an indoor environment show the effectiveness of the proposed solution: with $5$ to $70$ samples per window, KNN and bagging classifiers achieve $87-99\%$ accuracy, $82-98\%$ for ELM, and $81-94\%$ for SVM. The proposed pipeline achieves a $100\%$ true positive and $0\%$ false-negative rate for almost all classifiers. ",Kein DOI-Link verfügbar,1706.01739v3,Yes,potent(1)
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,Anomaly Detection in DevOps Toolchain,1970,"  The tools employed in the DevOps Toolchain generates a large quantity of data that is typically ignored or inspected only in particular occasions, at most. However, the analysis of such data could enable the extraction of useful information about the status and evolution of the project. For example, metrics like the ""lines of code added since the last release"" or ""failures detected in the staging environment"" are good indicators for predicting potential risks in the incoming release. In order to prevent problems appearing in later stages of production, an anomaly detection system can operate in the staging environment to compare the current incoming release with previous ones according to predefined metrics. The analysis is conducted before going into production to identify anomalies which should be addressed by human operators that address false-positive and negatives that can appear. In this paper, we describe a prototypical implementation of the aforementioned idea in the form of a ""proof of concept"". The current study effectively demonstrates the feasibility of the approach for a set of implemented functionalities. ",Kein DOI-Link verfügbar,1909.12682v1,Yes,potent(1)
0000-0002-2626-9214,Muhammad Ahmad,Shanghai Jiao Tong University,A Comprehensive Survey for Hyperspectral Image Classification: The   Evolution from Conventional to Transformers,1970,"  Hyperspectral Image Classification (HSC) is a challenging task due to the high dimensionality and complex nature of Hyperspectral (HS) data. Traditional Machine Learning approaches while effective, face challenges in real-world data due to varying optimal feature sets, subjectivity in human-driven design, biases, and limitations. Traditional approaches encounter the curse of dimensionality, struggle with feature selection and extraction, lack spatial information consideration, exhibit limited robustness to noise, face scalability issues, and may not adapt well to complex data distributions. In recent years, DL techniques have emerged as powerful tools for addressing these challenges. This survey provides a comprehensive overview of the current trends and future prospects in HSC, focusing on the advancements from DL models to the emerging use of Transformers. We review the key concepts, methodologies, and state-of-the-art approaches in DL for HSC. We explore the potential of Transformer-based models in HSC, outlining their benefits and challenges. We also delve into emerging trends in HSC, as well as thorough discussions on Explainable AI and Interoperability concepts along with Diffusion Models (image denoising, feature extraction, and image fusion). Additionally, we address several open challenges and research questions pertinent to HSC. Comprehensive experimental results have been undertaken using three HS datasets to verify the efficacy of various conventional DL models and Transformers. Finally, we outline future research directions and potential applications that can further enhance the accuracy and efficiency of HSC. The Source code is available at \url{https://github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024}. ",Kein DOI-Link verfügbar,2404.14955v3,Yes,potent(2)
0000-0003-2482-8104,Armando Zhu,Shanghai Jiao Tong University,Utilizing Deep Learning to Optimize Software Development Processes,1970,"  This study explores the application of deep learning technologies in software development processes, particularly in automating code reviews, error prediction, and test generation to enhance code quality and development efficiency. Through a series of empirical studies, experimental groups using deep learning tools and control groups using traditional methods were compared in terms of code error rates and project completion times. The results demonstrated significant improvements in the experimental group, validating the effectiveness of deep learning technologies. The research also discusses potential optimization points, methodologies, and technical challenges of deep learning in software development, as well as how to integrate these technologies into existing software development workflows. ",https://doi.org/10.5281/zenodo.11004006,2404.13630v2,Yes,potent(1)
0000-0001-6826-1635,Yanjing Wang,Shanghai Jiao Tong University,An Epistemic Interpretation of Tensor Disjunction,1970,"  This paper aims to give an epistemic interpretation to the tensor disjunction in dependence logic, through a rather surprising connection to the so-called weak disjunction in Medvedev's early work on intermediate logic under the Brouwer-Heyting-Kolmogorov (BHK)-interpretation. We expose this connection in the setting of inquisitive logic with tensor disjunction discussed by Ciardelli and Barbero (2019}, but from an epistemic perspective. More specifically, we translate the propositional formulae of inquisitive logic with tensor into modal formulae in a powerful epistemic language of ""knowing how"" following the proposal by Wang (2021). We give a complete axiomatization of the logic of our full language based on Fine's axiomatization of S5 modal logic with propositional quantifiers. Finally, we generalize the tensor operator with parameters $k$ and $n$, which intuitively captures the epistemic situation that one knows $n$ potential answers to $n$ questions and is sure $k$ answers of them must be correct. The original tensor disjunction is the special case when $k=1$ and $n=2$. We show that the generalized tensor operators do not increase the expressive power of our logic, the inquisitive logic, and propositional dependence logic, though most of these generalized tensors are not uniformly definable in these logics, except in our dynamic epistemic logic of knowing how. ",Kein DOI-Link verfügbar,2203.13970v1,Yes,potent(1)
0000-0002-9445-8981,Hailong Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",Determining gravitational wave radiation from close galaxy pairs using a   binary population synthesis approach,1970,"  Context. The early phase of the coalescence of supermassive black hole (SMBH) binaries from their host galaxies provides a guaranteed source of low-frequency (nHz-$\mu$Hz) gravitational wave (GW) radiation by pulsar timing observations. These types of GW sources would survive the coalescing and be potentially identifiable. Aims. We aim to provide an outline of a new method for detecting GW radiation from individual SMBH systems based on the Sloan Digital Sky Survey (SDSS) observational results, which can be verified by future observations. Methods. Combining the sensitivity of the international Pulsar Timing Array (PTA) and the Square Kilometer Array (SKA) detectors, we used a binary population synthesis (BPS) approach to determine GW radiation from close galaxy pairs under the assumption that SMBHs formed at the core of merged galaxies. We also performed second post-Newtonian approximation methods to estimate the variation of the strain amplitude with time. Results. We find that the value of the strain amplitude \emph{h} varies from about $10^{-14}$ to $10^{-17}$ using the observations of 20 years, and we estimate that about 100 SMBH sources can be detected with the SKA detector. ",https://doi.org/10.1051/0004-6361/201117501,1203.5892v1,Yes,potent(1)
0000-0002-9445-8981,Hailong Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",LenslessFace: An End-to-End Optimized Lensless System for   Privacy-Preserving Face Verification,1970,"  Lensless cameras, innovatively replacing traditional lenses for ultra-thin, flat optics, encode light directly onto sensors, producing images that are not immediately recognizable. This compact, lightweight, and cost-effective imaging solution offers inherent privacy advantages, making it attractive for privacy-sensitive applications like face verification. Typical lensless face verification adopts a two-stage process of reconstruction followed by verification, incurring privacy risks from reconstructed faces and high computational costs. This paper presents an end-to-end optimization approach for privacy-preserving face verification directly on encoded lensless captures, ensuring that the entire software pipeline remains encoded with no visible faces as intermediate results. To achieve this, we propose several techniques to address unique challenges from the lensless setup which precludes traditional face detection and alignment. Specifically, we propose a face center alignment scheme, an augmentation curriculum to build robustness against variations, and a knowledge distillation method to smooth optimization and enhance performance. Evaluations under both simulation and real environment demonstrate our method outperforms two-stage lensless verification while enhancing privacy and efficiency. Project website: \url{lenslessface.github.io}. ",Kein DOI-Link verfügbar,2406.04129v1,Yes,"innovative(1), innovatively(1)"
0000-0002-9078-9062,Yuanyuan Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",Estimating Cluster Masses from SDSS Multi-band Images with Transfer   Learning,1970,"  The total masses of galaxy clusters characterize many aspects of astrophysics and the underlying cosmology. It is crucial to obtain reliable and accurate mass estimates for numerous galaxy clusters over a wide range of redshifts and mass scales. We present a transfer-learning approach to estimate cluster masses using the ugriz-band images in the SDSS Data Release 12. The target masses are derived from X-ray or SZ measurements that are only available for a small subset of the clusters. We designed a semi-supervised deep learning model consisting of two convolutional neural networks. In the first network, a feature extractor is trained to classify the SDSS photometric bands. The second network takes the previously trained features as inputs to estimate their total masses. The training and testing processes in this work depend purely on real observational data. Our algorithm reaches a mean absolute error (MAE) of 0.232 dex on average and 0.214 dex for the best fold. The performance is comparable to that given by redMaPPer, 0.192 dex. We have further applied a joint integrated gradient and class activation mapping method to interpret such a two-step neural network. The performance of our algorithm is likely to improve as the size of training dataset increases. This proof-of-concept experiment demonstrates the potential of deep learning in maximizing the scientific return of the current and future large cluster surveys. ",https://doi.org/10.1093/mnras/stac725,2203.06288v1,Yes,potent(1)
0000-0002-9078-9062,Yuanyuan Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",AI-Based Beam-Level and Cell-Level Mobility Management for High Speed   Railway Communications,1970,"  High-speed railway (HSR) communications are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. The high speed of trains creates rapidly time-varying wireless channels, increases the signaling overhead, and reduces the system throughput, making it difficult to meet the growing and stringent needs of HSR applications. In this article, we explore artificial intelligence (AI)-based beam-level and cell-level mobility management suitable for HSR communications, including the use cases, inputs, outputs, and key performance indicators (KPI)s of AI models. Particularly, in comparison to traditional down-sampling spatial beam measurements, we show that the compressed spatial multi-beam measurements via compressive sensing lead to improved spatial-temporal beam prediction. Moreover, we demonstrate the performance gains of AI-assisted cell handover over traditional mobile handover mechanisms. In addition, we observe that the proposed approaches to reduce the measurement overhead achieve comparable radio link failure performance with the traditional approach that requires all the beam measurements of all cells, while the former methods can save 50% beam measurement overhead. ",Kein DOI-Link verfügbar,2407.04336v1,Yes,pivotal(1)
0000-0002-9078-9062,Yuanyuan Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG   Reconstruction from Millimeter-Wave Radar,1970,"  Radar-based contactless cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple the cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses pure data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model for domain transformation, and then a novel deep learning framework called radarODE is designed to fuse the temporal and morphological features extracted from radar signals and generate ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the dataset, the proposed radarODE achieves better performance compared with the benchmark in terms of missed detection rate, root mean square error, Pearson correlation coefficient with the improvement of 9%, 16% and 19%, respectively. The validation results imply that radarODE is capable of recovering ECG signals from radar signals with high fidelity and can be potentially implemented in real-life scenarios. ",Kein DOI-Link verfügbar,2408.01672v1,Yes,potent(1)
0000-0002-9078-9062,Yuanyuan Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",Grant Free MIMO-NOMA with Differential Modulation for Machine Type   Communications,1970,"  This paper considers a challenging scenario of machine type communications, where we assume internet of things (IoT) devices send short packets sporadically to an access point (AP) and the devices are not synchronized in the packet level. High transmission efficiency and low latency are concerned. Motivated by the great potential of multiple-input multiple-output non-orthogonal multiple access (MIMO-NOMA) in massive access, we design a grant-free MIMO-NOMA scheme, and in particular differential modulation is used so that expensive channel estimation at the receiver (AP) can be bypassed. The receiver at AP needs to carry out active device detection and multi-device data detection. The active user detection is formulated as the estimation of the common support of sparse signals, and a message passing based sparse Bayesian learning (SBL) algorithm is designed to solve the problem. Due to the use of differential modulation, we investigate the problem of non-coherent multi-device data detection, and develop a message passing based Bayesian data detector, where the constraint of differential modulation is exploited to drastically improve the detection performance, compared to the conventional non-coherent detection scheme. Simulation results demonstrate the effectiveness of the proposed active device detector and non-coherent multi-device data detector. ",Kein DOI-Link verfügbar,2112.07097v2,Yes,potent(1)
0000-0002-9078-9062,Yuanyuan Zhang,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine","Luminescence properties and phase transformation of broadband NIR   emitting A2(WO4)3:Cr3+ (A=Al3+, Sc3+) phosphors toward NIR spectroscopy   applications",1970,"  The synthesis, structural, and luminescence properties have been carried out for Cr3+-activated Al2(WO4)3 (AWO) and Sc2(WO4)3 (SWO) phosphors for application in pc-NIR LED. Upon blue excitation, these compounds are capable of exhibiting broadband NIR emission stems primarily from 4T2-->4A2 transition in the range of 670-1200 nm (maxima ~808 nm, FWHM ~140 nm) for AWO:Cr and of 700-1300 nm (maxima ~870 nm, FWHM ~164 nm) for SWO:Cr. The significant shift of NIR emission is attributed to the substitution of AlO6 with larger ScO6 octahedrons. To gain insight into the luminescence the crystal field strength, Racah parameters, nephelauxetic effect, and electron-phonon coupling have been analyzed based on spectroscopic results. The electron-phonon coupling parameter S for SWO:Cr was determined to be 11.5, twice as large as that for AWO:Cr, which is in accordance with its strong thermal quenching. The abrupt changes occurring at 275 K in temperature-dependent luminescence spectra and decay lifetime of AWO:Cr is associated with temperature-driven phase transformation from low-temperature monoclinic to high-temperature orthorhombic phase. Pressure induced amorphization of AWO:Cr at pressures higher than 25 kbar was confirmed by employing high pressure evolution of Raman spectra. A high-power NIR pc-LED, fabricated by coating AWO:0.04Cr on a commercial 470 nm LED chip, shows good performance with an output power of 17.1 mW driven by a current of 320 mA, revealing potential application of studied materials for NIR light source. ",https://doi.org/10.1016/j.jlumin.2022.119445,2403.15148v1,Yes,potent(1)
0000-0002-7017-8782,Hanjun Li,"Shanghai Jiao Tong University, Shanghai Jiao Tong University School of Medicine",Multimodal Label Relevance Ranking via Reinforcement Learning,1970,"  Conventional multi-label recognition methods often focus on label confidence, frequently overlooking the pivotal role of partial order relations consistent with human preference. To resolve these issues, we introduce a novel method for multimodal label relevance ranking, named Label Relevance Ranking with Proximal Policy Optimization (LR\textsuperscript{2}PPO), which effectively discerns partial order relations among labels. LR\textsuperscript{2}PPO first utilizes partial order pairs in the target domain to train a reward model, which aims to capture human preference intrinsic to the specific scenario. Furthermore, we meticulously design state representation and a policy loss tailored for ranking tasks, enabling LR\textsuperscript{2}PPO to boost the performance of label relevance ranking model and largely reduce the requirement of partial order annotation for transferring to new scenes. To assist in the evaluation of our approach and similar methods, we further propose a novel benchmark dataset, LRMovieNet, featuring multimodal labels and their corresponding partial order data. Extensive experiments demonstrate that our LR\textsuperscript{2}PPO algorithm achieves state-of-the-art performance, proving its effectiveness in addressing the multimodal label relevance ranking problem. Codes and the proposed LRMovieNet dataset are publicly available at \url{https://github.com/ChazzyGordon/LR2PPO}. ",Kein DOI-Link verfügbar,2407.13221v1,Yes,"meticulous(1), pivotal(1), meticulously(1)"
0000-0001-5308-6977,Dongliang Zhang,Shanghai Jiao Tong University,On the Age of Information for AMP based Grant-Free Random Access,1970,"  With the rapid development of Internet of Things (IoT), massive devices are deployed, which poses severe challenges on access networks due to limited communication resources. When massive users contend for access, the information freshness gets worse caused by increasing collisions. It could be fatal for information freshness sensing scenarios, such as remote monitoring systems or self-driving systems, in which information freshness plays a critical part. In this paper, by taking the Age of Information (AoI) as the primary performance indicator, the information freshness using AMP-based grant-free scheme is investigated and compared with grant-based scheme. Base on the analysis, a user scheduling strategy with sleep threshold and forcing active threshold is proposed to further reduce average AoI (AAoI). Numerical results reveal that the AMP-based grant-free scheme can provide sufficient access capability with less pilot resources, and it is robust to the fluctuation of the number of active users. That ensures that the AMP-based grant-free scheme can keep the AAoI at a low level. It is also shown that the proposed threshold strategy can effectively improve the information freshness. ",Kein DOI-Link verfügbar,2207.04712v1,Yes,fresh(5)
0000-0002-0186-9475,Philippe Mounaix,Institut Polytechnique de Paris,Wave localization does not affect the breakdown of a Schrödinger-type   amplifier driven by the square of a Gaussian field,1970,"  We study the divergence of the solution to a Schr\""odinger-type amplifier driven by the square of a Gaussian noise in presence of a random potential. We follow the same approach as Mounaix, Collet, and Lebowitz (MCL) in terms of a distributional formulation of the amplified field and the use of the Paley-Wiener theorem [Commun. Math. Phys. {\bf 264}, 741-758 (2006) and {\bf 280}, 281-283 (2008)]. Our results show that the divergence is not affected by the random potential, in the sense that it occurs at exactly the same coupling constant as what was found by MCL without a potential. It follows {\it a fortiori} that the breakdown of the amplifier is not affected by the possible existence of a localized regime in the amplification free limit. ",Kein DOI-Link verfügbar,0911.3744v2,Yes,potent(3)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Statistical inference for pairwise comparison models,1970,"  Pairwise comparison models have been widely used for utility evaluation and ranking across various fields. The increasing scale of problems today underscores the need to understand statistical inference in these models when the number of subjects diverges, a topic currently lacking in the literature except in a few special instances. To partially address this gap, this paper establishes a near-optimal asymptotic normality result for the maximum likelihood estimator in a broad class of pairwise comparison models, as well as a non-asymptotic convergence rate for each individual subject under comparison. The key idea lies in identifying the Fisher information matrix as a weighted graph Laplacian, which can be studied via a meticulous spectral analysis. Our findings provide a unified theory for performing statistical inference in a wide range of pairwise comparison models beyond the Bradley--Terry model, benefiting practitioners with theoretical guarantees for their use. Simulations utilizing synthetic data are conducted to validate the asymptotic normality result, followed by a hypothesis test using a tennis competition dataset. ",Kein DOI-Link verfügbar,2401.08463v2,Yes,meticulous(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Gap Completion in Point Cloud Scene occluded by Vehicles using SGC-Net,1970,"  Recent advances in mobile mapping systems have greatly enhanced the efficiency and convenience of acquiring urban 3D data. These systems utilize LiDAR sensors mounted on vehicles to capture vast cityscapes. However, a significant challenge arises due to occlusions caused by roadside parked vehicles, leading to the loss of scene information, particularly on the roads, sidewalks, curbs, and the lower sections of buildings. In this study, we present a novel approach that leverages deep neural networks to learn a model capable of filling gaps in urban scenes that are obscured by vehicle occlusion. We have developed an innovative technique where we place virtual vehicle models along road boundaries in the gap-free scene and utilize a ray-casting algorithm to create a new scene with occluded gaps. This allows us to generate diverse and realistic urban point cloud scenes with and without vehicle occlusion, surpassing the limitations of real-world training data collection and annotation. Furthermore, we introduce the Scene Gap Completion Network (SGC-Net), an end-to-end model that can generate well-defined shape boundaries and smooth surfaces within occluded gaps. The experiment results reveal that 97.66% of the filled points fall within a range of 5 centimeters relative to the high-density ground truth point cloud scene. These findings underscore the efficacy of our proposed model in gap completion and reconstructing urban scenes affected by vehicle occlusions. ",https://doi.org/10.1016/j.isprsjprs.2024.07.009,2407.08290v1,Yes,innovative(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Sensing-Enhanced Secure Communication: Joint Time Allocation and   Beamforming Design,1970,"  The integration of sensing and communication enables wireless communication systems to serve environment-aware applications. In this paper, we propose to leverage sensing to enhance physical layer security (PLS) in multiuser communication systems in the presence of a suspicious target. To this end, we develop a two-phase framework to first estimate the location of the potential eavesdropper by sensing and then utilize the estimated information to enhance PLS for communication. In particular, in the first phase, a dual-functional radar and communication (DFRC) base station (BS) exploits a sensing signal to mitigate the sensing information uncertainty of the potential eavesdropper. Then, in the second phase, to facilitate joint sensing and secure communication, the DFRC BS employs beamforming and artificial noise to enhance secure communication. The design objective is to maximize the system sum rate while alleviating the information leakage by jointly optimizing the time allocation and beamforming policy. Capitalizing on monotonic optimization theory, we develop a two-layer globally optimal algorithm to reveal the performance upper bound of the considered system. Simulation results show that the proposed scheme achieves a significant sum rate gain over two baseline schemes that adopt existing techniques. Moreover, our results unveil that ISAC is a promising paradigm for enhancing secure communication in wireless networks. ",Kein DOI-Link verfügbar,2312.15231v1,Yes,potent(2)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Interference Mitigation for Network-Level ISAC: An Optimization   Perspective,1970,"  Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users. One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network. However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled. Among them, interference management is one of the most significant ones. In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems. In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference. Then, we present several promising techniques that can be utilized to suppress specific types of interference. For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods. Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective. Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC. Finally, we highlight several promising future research directions for the design of ISAC systems. ",Kein DOI-Link verfügbar,2402.09974v1,Yes,potent(2)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,The Effective Field Theory of Dark Matter Direct Detection,1970,"  We extend and explore the general non-relativistic effective theory of dark matter (DM) direct detection. We describe the basic non-relativistic building blocks of operators and discuss their symmetry properties, writing down all Galilean-invariant operators up to quadratic order in momentum transfer arising from exchange of particles of spin 1 or less. Any DM particle theory can be translated into the coefficients of an effective operator and any effective operator can be simply related to most general description of the nuclear response. We find several operators which lead to novel nuclear responses. These responses differ significantly from the standard minimal WIMP cases in their relative coupling strengths to various elements, changing how the results from different experiments should be compared against each other. Response functions are evaluated for common DM targets - F, Na, Ge, I, and Xe - using standard shell model techniques. We point out that each of the nuclear responses is familiar from past studies of semi-leptonic electroweak interactions, and thus potentially testable in weak interaction studies. We provide tables of the full set of required matrix elements at finite momentum transfer for a range of common elements, making a careful and fully model-independent analysis possible. Finally, we discuss embedding non-relativistic effective theory operators into UV models of dark matter. ",https://doi.org/10.1088/1475-7516/2013/02/004,1203.3542v3,Yes,potent(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Micromobility Trip Origin and Destination Inference Using General   Bikeshare Feed Specification (GBFS) Data,1970,"  Emerging micromobility services (e.g., e-scooters) have a great potential to enhance urban mobility but more knowledge on their usage patterns is needed. The General Bikeshare Feed Specification (GBFS) data are a possible source for examining micromobility trip patterns, but efforts are needed to infer trips from the GBFS data. Existing trip inference methods are usually based upon the assumption that the vehicle ID of a micromobility option (e-scooter or e-bike) does not change, and so they cannot deal with data with vehicle IDs that change over time. In this study, we propose a comprehensive package of algorithms to infer trip origins and destinations from GBFS data with different types of vehicle ID. We implement the algorithms in Washington DC by analyzing one-week (last week of February 2020) of GBFS data published by six vendors, and we evaluate the inference accuracy of the proposed algorithms by R-squared, mean absolute error, and sum absolute error. We find that the R-squared measure is larger than 0.9 and the MAE measure is smaller than 2 when the algorithms are evaluated with a 400m*400m grid, and the absolute errors are relatively larger in the downtown area. The accuracy of the trip-inference algorithms is sufficiently high for most practical applications. ",Kein DOI-Link verfügbar,2010.12006v1,Yes,potent(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,A setup for extreme-ultraviolet ultrafast angle-resolved photoelectron   spectroscopy at 50-kHz repetition rate,1970,"  Time- and angle-resolved photoelectron spectroscopy (trARPES) is a powerful method to track the ultrafast dynamics of quasiparticles and electronic bands in energy and momentum space. We present a setup for trARPES with 22.3 eV extreme-ultraviolet (XUV) femtosecond pulses at 50-kHz repetition rate, which enables fast data acquisition and access to dynamics across momentum space with high sensitivity. The design and operation of the XUV beamline, pump-probe setup, and UHV endstation are described in detail. By characterizing the effect of space-charge broadening, we determine an ultimate source-limited energy resolution of 60 meV, with typically 80-100 meV obtained at 1-2e10 photons/s probe flux on the sample. The instrument capabilities are demonstrated via both equilibrium and time-resolved ARPES studies of transition-metal dichalcogenides. The 50-kHz repetition rate enables sensitive measurements of quasiparticles at low excitation fluences in semiconducting MoSe$_2$, with an instrumental time resolution of 65 fs. Moreover, photo-induced phase transitions can be driven with the available pump fluence, as shown by charge density wave melting in 1T-TiSe$_2$. The high repetition-rate setup thus provides a versatile platform for sensitive XUV trARPES, from quenching of electronic phases down to the perturbative limit. ",https://doi.org/10.1063/1.5079677,1811.00715v2,Yes,versatile(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Towards Autocomplete Strategies for Visualization Construction,1970,"  Constructive visualization uses physical data units - tokens - to enable non-experts to create personalized visualizations engagingly. However, its physical nature limits efficiency and scalability. One potential solution to address this issue is autocomplete. By providing automated suggestions while still allowing for manual intervention, autocomplete can expedite visualization construction while maintaining expressivity. We conduct a speculative design study to examine how people would like to interact with a visualization authoring system that supports autocomplete. Our study identifies three types of autocomplete strategies and gains insights for designing future visualization authoring tools with autocomplete functionality. A free copy of this paper and all supplemental materials are available on our online repository https://osf.io/nu4z3/?view_only=594baee54d114a99ab381886fb32a126 ",Kein DOI-Link verfügbar,2308.02679v2,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,The Dark Side of Perceptual Manipulations in Virtual Reality,1970,"  ""Virtual-Physical Perceptual Manipulations"" (VPPMs) such as redirected walking and haptics expand the user's capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user's physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities. ",https://doi.org/10.1145/3491102.3517728,2202.13200v1,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Design by Immersion: A Transdisciplinary Approach to Problem-Driven   Visualizations,1970,"  While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion. ",https://doi.org/10.1109/TVCG.2019.2934790,1908.00559v2,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Memory Manipulations in Extended Reality,1970,"  Human memory has notable limitations (e.g., forgetting) which have necessitated a variety of memory aids (e.g., calendars). As we grow closer to mass adoption of everyday Extended Reality (XR), which is frequently leveraging perceptual limitations (e.g., redirected walking), it becomes pertinent to consider how XR could leverage memory limitations (forgetting, distorting, persistence) to induce memory manipulations. As memories highly impact our self-perception, social interactions, and behaviors, there is a pressing need to understand XR Memory Manipulations (XRMMs). We ran three speculative design workshops (n=12), with XR and memory researchers creating 48 XRMM scenarios. Through thematic analysis, we define XRMMs, present a framework of their core components and reveal three classes (at encoding, pre-retrieval, at retrieval). Each class differs in terms of technology (AR, VR) and impact on memory (influencing quality of memories, inducing forgetting, distorting memories). We raise ethical concerns and discuss opportunities of perceptual and memory manipulations in XR. ",https://doi.org/10.1145/3544548.3580988,2304.02394v1,Yes,notable(1)
0000-0002-8359-5921,Mireille Sarkiss,"Télécom SudParis, Institut Polytechnique de Paris",Efficient Network Representation for GNN-based Intrusion Detection,1970,"  The last decades have seen a growth in the number of cyber-attacks with severe economic and privacy damages, which reveals the need for network intrusion detection approaches to assist in preventing cyber-attacks and reducing their risks. In this work, we propose a novel network representation as a graph of flows that aims to provide relevant topological information for the intrusion detection task, such as malicious behavior patterns, the relation between phases of multi-step attacks, and the relation between spoofed and pre-spoofed attackers activities. In addition, we present a Graph Neural Network (GNN) based framework responsible for exploiting the proposed graph structure to classify communication flows by assigning them a maliciousness score. The framework comprises three main steps that aim to embed nodes features and learn relevant attack patterns from the network representation. Finally, we highlight a potential data leakage issue with classical evaluation procedures and suggest a solution to ensure a reliable validation of intrusion detection systems performance. We implement the proposed framework and prove that exploiting the flow-based graph structure outperforms the classical machine learning-based and the previous GNN-based solutions. ",https://doi.org/10.1007/978-3-031-33488-7_20,2310.05956v1,Yes,potent(1)
0000-0002-8359-5921,Mireille Sarkiss,"Télécom SudParis, Institut Polytechnique de Paris",Secrecy Capacity-Memory Tradeoff of Erasure Broadcast Channels,1970,"  This paper derives upper and lower bounds on the secrecy capacity-memory tradeoff of a wiretap erasure broadcast channel (BC) with Kw weak receivers and Ks strong receivers, where weak receivers, respectively strong receivers, have same erasure probabilities and cache sizes. The lower bounds are achieved by schemes that meticulously combine joint cache-channel coding with wiretap coding and key-aided one-time pads. The presented upper bound holds more generally for arbitrary degraded BCs and arbitrary cache sizes. When only weak receivers have cache memories, upper and lower bounds coincide for small and large cache memories, thus providing the exact secrecy capacity-memory tradeoff for this setup. The derived bounds allow to further conclude that the secrecy capacity is positive even when the eavesdropper is stronger than all the legitimate receivers with cache memories. Moreover, they show that the secrecy capacity-memory tradeoff can be significantly smaller than its non-secure counterpart, but it grows much faster when cache memories are small. The paper also presents a lower bound on the global secrecy capacity-memory tradeoff where one is allowed to optimize the cache assignment subject to a total cache budget. It is close to the best known lower bound without secrecy constraint. For small total cache budget, the global secrecy capacity-memory tradeoff is achieved by assigning all the available cache memory uniformly over all receivers if the eavesdropper is stronger than all legitimate receivers, and it is achieved by assigning the cache memory uniformly only over the weak receivers if the eavesdropper is weaker than the strong receivers. ",Kein DOI-Link verfügbar,1801.00606v1,Yes,"meticulous(1), meticulously(1)"
0000-0002-1765-1219,Martin Krejca,"Ecole Polytechnique, Institut Polytechnique de Paris",Using random graphs to sample repulsive Gibbs point processes with   arbitrary-range potentials,1970,"  We study computational aspects of repulsive Gibbs point processes, which are probabilistic models of interacting particles in a finite-volume region of space. We introduce an approach for reducing a Gibbs point process to the hard-core model, a well-studied discrete spin system. Given an instance of such a point process, our reduction generates a random graph drawn from a natural geometric model. We show that the partition function of a hard-core model on graphs generated by the geometric model concentrates around the partition function of the Gibbs point process. Our reduction allows us to use a broad range of algorithms developed for the hard-core model to sample from the Gibbs point process and approximate its partition function. This is, to the extend of our knowledge, the first approach that deals with pair potentials of unbounded range. We compare the resulting algorithms with recently established results and study further properties of the random geometric graphs with respect to the hard-core model. ",Kein DOI-Link verfügbar,2204.01793v2,Yes,potent(1)
0000-0003-4641-6944,Shihao Ding,"Telecom Paris, Institut Polytechnique de Paris",Unveiling the dynamical diversity of quantum dot lasers subject to   optoelectronic feedback,1970,"  This paper investigates experimentally and numerically the nonlinear dynamics of an epitaxial quantum dot laser on silicon subjected to optoelectronic feedback. Experimental results showcase a diverse range of dynamics, encompassing square wave patterns, quasi-chaotic states, and mixed waveforms exhibiting fast and slow oscillations. These measurements unequivocally demonstrate that quantum dot lasers on silicon readily and stably generate a more extensive repertoire of nonlinear dynamics compared to quantum well lasers. This pronounced sensitivity of quantum dot lasers to optoelectronic feedback represents a notable departure from their inherent insensitivity to optical feedback arising from reflections. Moreover, based on the Ikeda-like model, our simulations illustrate that the inherent characteristics of quantum dot lasers on silicon enable rapid and diverse dynamic transformations in response to optoelectronic feedback. The emergence of these exotic dynamics paves the way for further applications like integrated optical clocks, optical logic, and optical computing. ",Kein DOI-Link verfügbar,2309.14056v1,Yes,notable(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Rigidity for Families of Polarized Calabi-Yau Varieties,1970,"  In this paper, we study the analogue of the Shafarevich conjecture for polarized Calabi-Yau varieties. We use variations of Hodge structures and Higgs bundles to establish a criterion for the {\it rigidity} of families. We then apply the criterion to obtain that some important and typical families of Calabi-Yau varieties are rigid, for examples., Lefschetz pencils of Calabi-Yau varieties, {\it strongly degenerated} families (not only for families of Calabi-Yau varieties), families of Calabi-Yau varieties admitting a degeneration with {\it maximal unipotent monodromy}. ",Kein DOI-Link verfügbar,math/0308034v4,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Warm Inflation with a General Form of the Dissipative Coefficient,1970,"  We propose and investigate a general form of the dissipative coefficient $\Gamma=C_{\phi}T^{m}/\phi^{m-1}$ in warm inflation. We focus on discussing the strong dissipative processes $r=\Gamma/3H\gg1$ in the thermal state of approximate equilibrium. To this toy model, we give the slow-roll conditions, the amplitude and the index of the power spectrum under the general form of dissipative coefficient. Furthermore, the monomial potential and the hybrid-like potential are analyzed specifically. We conclude that the $m=0,3$ cases are worthy further investigation especially. ",https://doi.org/10.1088/1475-7516/2009/03/023,0903.0685v1,Yes,potent(2)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",An In-depth Summary of Recent Artificial Intelligence Applications in   Drug Design,1970,"  As a promising tool to navigate in the vast chemical space, artificial intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the number of applications of several recent AI models (i.e. graph neural network (GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative adversarial network (GAN), flow and reinforcement learning (RL)) in drug design increases significantly. Many relevant literature reviews exist. However, none of them provides an in-depth summary of many applications of the recent AI models in drug design. To complement the existing literature, this survey includes the theoretical development of the previously mentioned AI models and detailed summaries of 42 recent applications of AI in drug design. Concretely, 13 of them leverage GNN for molecular property prediction and 29 of them use RL and/or deep generative models for molecule generation and optimization. In most cases, the focus of the summary is the models, their variants, and modifications for specific tasks in drug design. Moreover, 60 additional applications of AI in molecule generation and optimization are briefly summarized in a table. Finally, this survey provides a holistic discussion of the abundant applications so that the tasks, potential solutions, and challenges in AI-based drug design become evident. ",Kein DOI-Link verfügbar,2110.05478v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Unified non-Fermi liquid and superconductivity in a model of   strongly-correlated systems,1970,"  We study a model of strongly-correlated systems that incorporates phases such as Fermi liquids, non-Fermi liquids, and superconductivity, in addition to potential intertwined orders. The model describes Fermi surfaces of spinful electron gas or electron liquid coupled to bosons via an intermediate gauge field. Effectively, the coupling imposes constraints and interactions between the fermion spin and the local boson density. This grants the bosons the meaning as the Schwinger boson of the magnetic order and allows us to probe a larger phase space, rather than around the quantum critical point. We design the initial model so that after the boson and gauge fields are integrated out exactly, the resulting fermion-only effective theory only consists of several local interactions, allowing controlled weak-coupling interpretation for certain parameter regions. Consequently, we find a non-Fermi liquid whose degrees of freedom in the effective theory take the form of an \emph{emergent} Fermi surface that has a different Luttinger volume from the original fermions, therefore explicitly violates the Luttinger theorem. Further, this allows a $d$-wave superconducting instability if the effective interacting induced by boson coupling is in the relevant pairing channel. When the emergent Fermi surface undergoes a Lifshitz transition, the carrier type changes. In addition, we attribute the Mott insulating behavior in the strong coupling limit to the loss of emergent Fermi surface and argue the possibility of coexisting magnetic order. Our effective theory also suggests the possibility of charge density waves as a consequence of \emph{effective} strong repulsion and is independent of the Fermi-surface-nesting scenario. We discuss the possible optimal conditions for higher superconducting transition temperature and potential relevances and implications for realistic materials. ",Kein DOI-Link verfügbar,1711.03981v1,Yes,potent(2)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Prototype matching: children's preference for forming scientific   concepts,1970,"  Inspired by a sample lesson, this paper studies and discusses children's preferences in learning scientific concepts. In a ""Dissolution"" lesson, one of the students took the demonstration experiment of ""carmine dissolves in Water"" demonstrated by the teacher as the prototype to judge whether a new phenomenon belongs to dissolution, instead of analyzing and judging the phenomenon by using the dissolution definition. Therefore, we propose a conjecture that ""prototype matching"" may be a more preferred way for children to learn concepts than thinking through inquiry experiment, analysis, deduction, etc. To this end, we conducted a targeted test on 160 fifth grade students (all of whom had learned this lesson) from a primary school in Beijing, and used goodness of fit test to statistically analyze the results. The results showed that: 1. the Chi square of the general result is 73.865, P<0.001, indicating that children did have obvious prototype preference; 2. We ""tampered"" some of the prototypes, that is, they looked like the prototypes that the teacher had told students, but they were actually wrong. However, the results showed that children still preferred these so-called ""prototypes"" (chi square is 21.823, P<0.001). Conclusion: 1. Children have an obvious preference for ""prototype matching"" in scientific concept learning, which is not only obviously deviated from the current general understanding of science education that emphasizes discovery/inquiry construction, but also points out that there may be a priority relationship among various ways of concept organization (such as definition theory, prototype theory, schema theory, etc.). 2. Children's preference for prototypes seems to be unthinking, and they will not identify the authenticity of prototypes, which is particularly noteworthy in front-line teaching. ",Kein DOI-Link verfügbar,2212.01535v1,Yes,noteworthy(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Towards Accuracy-Fairness Paradox: Adversarial Example-based Data   Augmentation for Visual Debiasing,1970,"  Machine learning fairness concerns about the biases towards certain protected or sensitive group of people when addressing the target tasks. This paper studies the debiasing problem in the context of image classification tasks. Our data analysis on facial attribute recognition demonstrates (1) the attribution of model bias from imbalanced training data distribution and (2) the potential of adversarial examples in balancing data distribution. We are thus motivated to employ adversarial example to augment the training data for visual debiasing. Specifically, to ensure the adversarial generalization as well as cross-task transferability, we propose to couple the operations of target task classifier training, bias task classifier training, and adversarial example generation. The generated adversarial examples supplement the target task training dataset via balancing the distribution over bias variables in an online fashion. Results on simulated and real-world debiasing experiments demonstrate the effectiveness of the proposed solution in simultaneously improving model accuracy and fairness. Preliminary experiment on few-shot learning further shows the potential of adversarial attack-based pseudo sample generation as alternative solution to make up for the training data lackage. ",Kein DOI-Link verfügbar,2007.13632v2,Yes,potent(2)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Kramers Fulde-Ferrell state and superconducting spin diode effect,1970,"  We study a novel Fulde-Ferrell equal-spin pairing state with opposite center of mass Cooper pair momentum for each spin polarization. This state respects time-reversal symmetry and is dubbed a Kramers Fulde-Ferrell (KFF) state. It can be realized in a one-dimensional system with spin-orbit coupling and nearest neighbor attraction. We find that the KFF state supports nonreciprocal spin transport for both bulk superconductor and Josephson junctions when inversion symmetry is broken. In addition to the spin Josephson diode effect, the charge transport is controlled by intriguing dynamics of bound states whose transitions can be manipulated by the length of the KFF superconductor. We discuss the relevance of the effective theory to the novel physics observed in monolayer Fe-based superconductor along line defects and domain walls, and the potential for using spins to make dissipationless superconducting spintronics. ",https://doi.org/10.1103/PhysRevB.107.224510,2209.03520v3,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Cornwall-Jackiw-Tomboulis effective field theory to nonuniversal   equation of state of an ultracold Bose gas,1970,"  The equation of state (EOS) serves as a cornerstone in elucidating the properties of quantum many-body systems. A recent highlight along this research line consists of the derivation of the nonuniversal Lee-Huang-Yang (LHY) EOS for an ultracold quantum bosonic gas with finite-range interatomic interactions using one-loop effective path-integral field theory. The purpose of this work is to extend Salasnich's pioneering work to uncover beyond-LHY corrections to the EOS by employing the Cornwall-Jackiw-Tomboulis (CJT) effective field theory, leveraging its two-loop approximation. In this end, we expand Salasnich's remarkable findings of EOS to the next leading order characterized by $\left(\rho a_{\text{s}}^{3}\right)^{2}$, with $\rho$ and $a_{\text{s}}$ being the density and the $s$-wave scattering length. Notably, we derive analytical expressions for quantum depletion and chemical potential, representing the next-to-LHY corrections to nonuniversal EOS induced by finite-range effects. Moreover, we propose an experimental protocol of observing the nonuniversal next-to-LHY corrections to the EOS by calculating fractional frequency shifts in the breathing modes. The nonuniversal beyond-LHY EOS in this work paves the way of using LHY effects in quantum simulation experiments and for investigations beyond the LHY regime. ",Kein DOI-Link verfügbar,2408.09668v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Spin-triplet pair density wave superconductors,1970,"  Recent experiments have shown that the nonzero center of mass momentum pair density wave (PDW) is a widespread phenomenon observed over different superconducting materials. However, concrete theoretical model realizations of the PDW order have remained elusive. Here, we study a one-dimensional model with nearest-neighbor pairing attraction, i.e. a spinful Kitaev chain, under generic spin-orbit couplings such that the spin-rotation symmetry is fully broken. The most general superconducting order parameter is described by a spatial dependent $\mathbf{d}_i$-vector. We show that a spin-triplet pair density wave (t-PDW) emerges in the ground state and occupies a large part of the phase diagram. The $\mathbf{d_i}$-vector of the t-PDW rotates with a pitch $Q_{\rm pdw}$ along the chain and spans an ellipsoid. The pure t-PDW is fully-gapped and a class-DIII topological superconductor with two Majorana zero modes localized at each end of the chain and protected by time-reversal symmetry. Our findings reveal unprecedented insights into the exotic pure PDW superconductor and provide a possible explanation for the one-dimensional PDW detected along domain walls in monolayer iron-based superconductor Fe(Te,Se) and potentially realizable using other quantum structures in unconventional superconductors. ",Kein DOI-Link verfügbar,2408.13886v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Solution-Set Geometry and Regularization Path of a Nonconvexly   Regularized Convex Sparse Model,1970,"  The generalized minimax concave (GMC) penalty is a nonconvex sparse regularizer which can preserve the overall-convexity of the regularized least-squares problem. In this paper, we focus on a significant instance of the GMC model termed scaled GMC (sGMC), and present various notable findings on its solution-set geometry and regularization path. Our investigation indicates that while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the $\ell_1$-norm), the sGMC model preserves many celebrated properties of the LASSO model, hence can serve as a less biased surrogate of LASSO without losing its advantages. Specifically, for a fixed regularization parameter $\lambda$, we show that the solution-set geometry, solution uniqueness and sparseness of the sGMC model can be characterized in a similar elegant way to the LASSO model (see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying $\lambda$, we prove that the sGMC solution set is a continuous polytope-valued mapping of $\lambda$. Most noticeably, our study indicates that similar to LASSO, the minimum $\ell_2$-norm regularization path of the sGMC model is continuous and piecewise linear in $\lambda$. Based on these theoretical results, an efficient regularization path algorithm is proposed for the sGMC model, extending the well-known least angle regression (LARS) algorithm for LASSO. We prove the correctness and finite termination of the proposed algorithm under a mild assumption, and confirm its correctness-in-general-situation, efficiency, and practical utility through numerical experiments. Many results in this study also contribute to the theoretical research of LASSO. ",Kein DOI-Link verfügbar,2311.18438v2,Yes,notable(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Conversational Recommender System,1970,"  A personalized conversational sales agent could have much commercial potential. E-commerce companies such as Amazon, eBay, JD, Alibaba etc. are piloting such kind of agents with their users. However, the research on this topic is very limited and existing solutions are either based on single round adhoc search engine or traditional multi round dialog system. They usually only utilize user inputs in the current session, ignoring users' long term preferences. On the other hand, it is well known that sales conversion rate can be greatly improved based on recommender systems, which learn user preferences based on past purchasing behavior and optimize business oriented metrics such as conversion rate or expected revenue. In this work, we propose to integrate research in dialog systems and recommender systems into a novel and unified deep reinforcement learning framework to build a personalized conversational recommendation agent that optimizes a per session based utility function. ",Kein DOI-Link verfügbar,1806.03277v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",A Property of the Frobenius Map of a Polynomial Ring,1970,"  Let R be a ring of polynomials in a finite number of variables over a perfect field k of characteristic p>0 and let F:R\to R be the Frobenius map of R, i.e. F(r)=r^p. We explicitly describe an R-module isomorphism Hom_R(F_*(M),N)\cong Hom_R(M,F^*(N)) for all R-modules M and N. Some recent and potential applications are discussed. ",Kein DOI-Link verfügbar,1001.2949v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Track identification and reconstruction in fast neutron detection by   MPGD,1970,"  Micro pattern gaseous detectors have been widely used in position measurements of particle detection in the last two decades. In this work a novel method of track identification and reconstruction was developed for fast neutron detection by MPGD, which in most cases requires a strong rejection of the gamma background. Based on this method, an online tracking system can be built in a FPGA-based Daq system to significantly improve both the capability of counting rate and the spatial resolution. This work also offers a potential usage in future hadron experiments such as SoLID spectrometer in Jeffereson Lab. ",Kein DOI-Link verfügbar,1512.04677v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",High-Order Fusion Graph Contrastive Learning for Recommendation,1970,"  Self-supervised learning (SSL) has recently attracted significant attention in the field of recommender systems. Contrastive learning (CL) stands out as a major SSL paradigm due to its robust ability to generate self-supervised signals. Mainstream graph contrastive learning (GCL)-based methods typically implement CL by creating contrastive views through various data augmentation techniques. Despite these methods are effective, we argue that there still exist several challenges: i) Data augmentation (e.g., discarding edges or adding noise) necessitates additional graph convolution (GCN) or modeling operations, which are highly time-consuming and potentially harm the embedding quality. ii) Existing CL-based methods use traditional CL objectives to capture self-supervised signals. However, few studies have explored obtaining CL objectives from more perspectives and have attempted to fuse the varying signals from these CL objectives to enhance recommendation performance.   To overcome these challenges, we propose a High-Order Fusion Graph Contrastive Learning (HFGCL) framework for recommendation. Specifically, we discards the data augmentations and instead high-order information from GCN process to create contrastive views. Additionally, to integrate self-supervised signals from various CL objectives, we propose an advanced CL objective. By ensuring that positive pairs are distanced from negative samples derived from both contrastive views, we effectively fuse self-supervised signals from distinct CL objectives, thereby enhancing the mutual information between positive pairs. Experimental results on three public datasets demonstrate the superior effectiveness of HFGCL compared to the state-of-the-art baselines. ",Kein DOI-Link verfügbar,2407.19692v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Generalized Semi-Holographic Universe,1970,"  We study the semi-holographic idea in context of decaying dark components. The energy flow between dark energy and the compensating dark matter is thermodynamically generalized to involve a particle number variable dark component with non-zero chemical potential. It's found that, unlike the original semi-holographic model, no cosmological constant is needed for a dynamical evolution of the universe. A transient phantom phase appears while a non-trivial dark energy-dark matter scaling solution keeps at late time, which evades the big-rip and helps to resolve the coincidence problem. For reasonable parameters, the deceleration parameter is well consistent with current observations. The original semi-holographic model is extended and it also suggests that the concordance model may be reconstructed from the semi-holographic idea. ",https://doi.org/10.1088/0256-307X/30/8/089801,1212.2360v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Modelling and Traffic Signal Control of Heterogeneous Traffic Systems,1970,"  An urban traffic system is a heterogeneous system, which consists of different types of intersections and dynamics. In this paper, we focus on one type of heterogeneous traffic network, which consists of signalized junctions and non-signalized ones, where in the latter case vehicles usually follow the first-in-first-out principle. We propose a novel model describing the dynamic behaviors of such a system and validate it via simulations in VISSIM. Upon such a new model, a signal control problem for a heterogeneous traffic network is formulated as a mixed integer programming problem, which is solved by a Lagrangian multiplier based hierarchical distributed approach. Comparisons between a homogeneous traffic system and a heterogeneous one are provided, which leaves the door open for developing a systematic planning approach on deciding what traffic junctions require signal control to ensure a good traffic control performance, thus, have a great social and economic potentials, considering that it is rather expensive to have signal control in an urban area. ",Kein DOI-Link verfügbar,1705.03713v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",SemanticAxis: Exploring Multi-attribute Data by Semantics Construction   and Ranking Analysis,1970,"  Mining the distribution of features and sorting items by combined attributes are two common tasks in exploring and understanding multi-attribute (or multivariate) data. Up to now, few have pointed out the possibility of merging these two tasks into a united exploration context and the potential benefits of doing so. In this paper, we present SemanticAxis, a technique that achieves this goal by enabling analysts to build a semantic vector in two-dimensional space interactively. Essentially, the semantic vector is a linear combination of the original attributes. It can be used to represent and explain abstract concepts implied in local (outliers, clusters) or global (general pattern) features of reduced space, as well as serving as a ranking metric for its defined concepts. In order to validate the significance of combining the above two tasks in multi-attribute data analysis, we design and implement a visual analysis system, in which several interactive components cooperate with SemanticAxis seamlessly and expand its capacity to handle complex scenarios. We prove the effectiveness of our system and the SemanticAxis technique via two practical cases. ",https://doi.org/10.1007/s12650-020-00733-z,2208.13346v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",A pedestrian hopping model and traffic light scheduling for   pedestrian-vehicle mixed-flow networks,1970,"  This paper presents a pedestrian hopping model and a traffic signal scheduling strategy with consideration of both pedestrians and vehicles in the urban traffic system. Firstly, a novel mathematical model consisting of several logic constraints is proposed to describe the pedestrian flow in the urban traffic network and its dynamics are captured by the hopping rule, which depicts the changing capacity of each time interval from one waiting zone to another. Based on the hopping mechanism, the pedestrian traffic light scheduling problems are formulated by two different performance standards: pedestrian delay and pedestrian unhappiness. Then the mathematical technique and the meta-heuristic approach are both adopted to solve the scheduling problem: Mixed integer linear programming (MILP) formulation for pedestrian delay model and discrete harmony search algorithm (DHS) for both pedestrian delay model and unhappiness model. Secondly, a mathematical model about the vehicle traffic network, which captures drivers psychological responses to the traffic light signals, is introduced. Thirdly, a traffic light scheduling strategy to minimize the trade-off of the delays between pedestrians and vehicles is proposed. Finally, we translate this traffic signal scheduling problem for both pedestrians and vehicles into a MILP problem which can be solved by several existing tools, e.g., GUROBI. Numerical simulation results are provided to illustrate the effectiveness of our real-time traffic light scheduling for pedestrian movement and the potential impact to the vehicle traffic flows by the pedestrian movement. ",Kein DOI-Link verfügbar,1705.05251v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Investigation of shallow water waves near the coast or in lake   environments via the KdV-Calogero-Bogoyavlenskii-Schiff equation,1970,"  Shallow water waves phenomena in nature attract the attention of scholars and play an important role in fields such as tsunamis, tidal waves, solitary waves, and hydraulic engineering. Hereby, fortheshallowwaterwavesphenomenainvariousnaturalenvironments, westudytheKdV-Calogero-Bogoyavlenskii-Schiff (KdV-CBS) equation. Based on the binary Bell polynomial theory, a new general bilinear B\""acklund transformation, Lax pair and infinite conservation laws of the KdV-CBS equation are derived, and it is proved that it is completely integrable in Lax pair sense. Various types of mixed solutions are constructed by using a combination of Homoclinic test method and symbolic computations. These findings have important significance for the discipline, offering vital insights into the intricate dynamics of the KdV-CBS equation. We hope that our research results could help the researchers understand the nonlinear complex phenomena of the shallow water waves in oceans, rivers and coastal areas. Furthermore, the present work can be directly applied to other nonlinear equations. ",Kein DOI-Link verfügbar,2404.18697v2,Yes,intricate(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Responsible AI: Portraits with Intelligent Bibliometrics,1970,"  Shifting the focus from principles to practical implementation, responsible artificial intelligence (AI) has garnered considerable attention across academia, industry, and society at large. Despite being in its nascent stages, this emerging field grapples with nebulous concepts and intricate knowledge frameworks. By analyzing three prevailing concepts - explainable AI, trustworthy AI, and ethical AI, this study defined responsible AI and identified its core principles. Methodologically, this study successfully demonstrated the implementation of leveraging AI's capabilities into bibliometrics for enhanced knowledge discovery and the cross-validation of experimentally examined models with domain insights. Empirically, this study investigated 17,799 research articles contributed by the AI community since 2015. This involves recognizing key technological players and their relationships, unveiling the topical landscape and hierarchy of responsible AI, charting its evolution, and elucidating the interplay between the responsibility principles and primary AI techniques. An analysis of a core cohort comprising 380 articles from multiple disciplines captures the most recent advancements in responsible AI. As one of the pioneering bibliometric studies dedicated to exploring responsible AI, this study will provide comprehensive macro-level insights, enhancing the understanding of responsible AI while furnishing valuable knowledge support for AI regulation and governance initiatives. ",Kein DOI-Link verfügbar,2405.02846v1,Yes,intricate(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Symplectic Extra-gradient Type Method for Solving General Non-monotone   Inclusion Problem,1970,"  In recent years, accelerated extra-gradient methods have attracted much attention by researchers, for solving monotone inclusion problems. A limitation of most current accelerated extra-gradient methods lies in their direct utilization of the initial point, which can potentially decelerate numerical convergence rate. In this work, we present a new accelerated extra-gradient method, by utilizing the symplectic acceleration technique. We establish the inverse of quadratic convergence rate by employing the Lyapunov function technique. Also, we demonstrate a faster inverse of quadratic convergence rate alongside its weak convergence property under stronger assumptions. To improve practical efficiency, we introduce a line search technique for our symplectic extra-gradient method. Theoretically, we prove the convergence of the symplectic extra-gradient method with line search. Numerical tests show that this adaptation exhibits faster convergence rates in practice compared to several existing extra-gradient type methods. ",Kein DOI-Link verfügbar,2406.10793v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",DouFu: A Double Fusion Joint Learning Method For Driving Trajectory   Representation,1970,"  Driving trajectory representation learning is of great significance for various location-based services, such as driving pattern mining and route recommendation. However, previous representation generation approaches tend to rarely address three challenges: 1) how to represent the intricate semantic intentions of mobility inexpensively; 2) complex and weak spatial-temporal dependencies due to the sparsity and heterogeneity of the trajectory data; 3) route selection preferences and their correlation to driving behavior. In this paper, we propose a novel multimodal fusion model, DouFu, for trajectory representation joint learning, which applies multimodal learning and attention fusion module to capture the internal characteristics of trajectories. We first design movement, route, and global features generated from the trajectory data and urban functional zones and then analyze them respectively with the attention encoder or feed forward network. The attention fusion module incorporates route features with movement features to create a better spatial-temporal embedding. With the global semantic feature, DouFu produces a comprehensive embedding for each trajectory. We evaluate representations generated by our method and other baseline models on classification and clustering tasks. Empirical results show that DouFu outperforms other models in most of the learning algorithms like the linear regression and the support vector machine by more than 10%. ",https://doi.org/10.1016/j.knosys.2022.110035,2205.08356v2,Yes,intricate(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Exploring Lexical Irregularities in Hypothesis-Only Models of Natural   Language Inference,1970,"  Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) is the task of predicting the entailment relation between a pair of sentences (premise and hypothesis). This task has been described as a valuable testing ground for the development of semantic representations, and is a key component in natural language understanding evaluation benchmarks. Models that understand entailment should encode both, the premise and the hypothesis. However, experiments by Poliak et al. revealed a strong preference of these models towards patterns observed only in the hypothesis, based on a 10 dataset comparison. Their results indicated the existence of statistical irregularities present in the hypothesis that bias the model into performing competitively with the state of the art. While recast datasets provide large scale generation of NLI instances due to minimal human intervention, the papers that generate them do not provide fine-grained analysis of the potential statistical patterns that can bias NLI models. In this work, we analyze hypothesis-only models trained on one of the recast datasets provided in Poliak et al. for word-level patterns. Our results indicate the existence of potential lexical biases that could contribute to inflating the model performance. ",Kein DOI-Link verfügbar,2101.07397v3,Yes,potent(2)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Tailor Versatile Multi-modal Learning for Multi-label Emotion   Recognition,1970,"  Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), aiming to refine multi-modal representations and enhance discriminative capacity of each label. Specifically, we design an adversarial multi-modal refinement module to sufficiently explore the commonality among different modalities and strengthen the diversity of each modality. To further exploit label-modal dependence, we devise a BERT-like cross-modal encoder to gradually fuse private and common modality representations in a granularity descent way, as well as a label-guided decoder to adaptively generate a tailored representation for each label with the guidance of label semantics. In addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in both aligned and unaligned settings, which demonstrate the superiority of TAILOR over the state-of-the-arts. Code is available at https://github.com/kniter1/TAILOR. ",Kein DOI-Link verfügbar,2201.05834v1,Yes,versatile(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Safe Policy Learning under Regression Discontinuity Designs with   Multiple Cutoffs,1970,"  The regression discontinuity (RD) design is widely used for program evaluation with observational data. The primary focus of the existing literature has been the estimation of the local average treatment effect at the existing treatment cutoff. In contrast, we consider policy learning under the RD design. Because the treatment assignment mechanism is deterministic, learning better treatment cutoffs requires extrapolation. We develop a robust optimization approach to finding optimal treatment cutoffs that improve upon the existing ones. We first decompose the expected utility into point-identifiable and unidentifiable components. We then propose an efficient doubly-robust estimator for the identifiable parts. To account for the unidentifiable components, we leverage the existence of multiple cutoffs that are common under the RD design. Specifically, we assume that the heterogeneity in the conditional expectations of potential outcomes across different groups vary smoothly along the running variable. Under this assumption, we minimize the worst case utility loss relative to the status quo policy. The resulting new treatment cutoffs have a safety guarantee that they will not yield a worse overall outcome than the existing cutoffs. Finally, we establish the asymptotic regret bounds for the learned policy using semi-parametric efficiency theory. We apply the proposed methodology to empirical and simulated data sets. ",Kein DOI-Link verfügbar,2208.13323v3,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Generative-Enhanced Heterogeneous Graph Contrastive Learning,1970,"  Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and contrastive discriminators for downstream tasks. However, data augmentation is still limited due to the graph data's integrity. Furthermore, the contrastive discriminators remain sampling bias and lack local heterogeneous information. To tackle the above limitations, we propose a novel Generative-Enhanced Heterogeneous Graph Contrastive Learning (GHGCL). Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using a masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generating hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced contrastive discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks. To reproduce our work, we have open-sourced our code at https://anonymous.4open.science/r/GC-HGNN-E50C. ",Kein DOI-Link verfügbar,2404.02810v2,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",DU-GAN: Generative Adversarial Networks with Dual-Domain U-Net Based   Discriminators for Low-Dose CT Denoising,1970,"  LDCT has drawn major attention in the medical imaging field due to the potential health risks of CT-associated X-ray radiation to patients. Reducing the radiation dose, however, decreases the quality of the reconstructed images, which consequently compromises the diagnostic performance. Various deep learning techniques have been introduced to improve the image quality of LDCT images through denoising. GANs-based denoising methods usually leverage an additional classification network, i.e. discriminator, to learn the most discriminate difference between the denoised and normal-dose images and, hence, regularize the denoising model accordingly; it often focuses either on the global structure or local details. To better regularize the LDCT denoising model, this paper proposes a novel method, termed DU-GAN, which leverages U-Net based discriminators in the GANs framework to learn both global and local difference between the denoised and normal-dose images in both image and gradient domains. The merit of such a U-Net based discriminator is that it can not only provide the per-pixel feedback to the denoising network through the outputs of the U-Net but also focus on the global structure in a semantic level through the middle layer of the U-Net. In addition to the adversarial training in the image domain, we also apply another U-Net based discriminator in the image gradient domain to alleviate the artifacts caused by photon starvation and enhance the edge of the denoised CT images. Furthermore, the CutMix technique enables the per-pixel outputs of the U-Net based discriminator to provide radiologists with a confidence map to visualize the uncertainty of the denoised results, facilitating the LDCT-based screening and diagnosis. Extensive experiments on the simulated and real-world datasets demonstrate superior performance over recently published methods both qualitatively and quantitatively. ",https://doi.org/10.1109/TIM.2021.3128703,2108.10772v2,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Helical Metal Inside a Topological Band Insulator,1970,"  Topological defects, such as domain walls and vortices, have long fascinated physicists. A novel twist is added in quantum systems like the B-phase of superfluid helium He$_3$, where vortices are associated with low energy excitations in the cores. Similarly, cosmic strings may be tied to propagating fermion modes. Can analogous phenomena occur in crystalline solids that host a plethora of topological defects? Here we show that indeed dislocation lines are associated with one dimensional fermionic excitations in a `topological insulator', a novel band insulator believed to be realized in the bulk material Bi$_{0.9}$Sb$_{0.1}$. In contrast to fermionic excitations in a regular quantum wire, these modes are topologically protected like the helical edge states of the quantum spin-Hall insulator, and not scattered by disorder. Since dislocations are ubiquitous in real materials, these excitations could dominate spin and charge transport in topological insulators. Our results provide a novel route to creating a potentially ideal quantum wire in a bulk solid. ",https://doi.org/10.1038/nphys1220,0810.5121v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",A multiple exp-function method for nonlinear differential equations and   its application,1970,"  A multiple exp-function method to exact multiple wave solutions of nonlinear partial differential equations is proposed. The method is oriented towards ease of use and capability of computer algebra systems, and provides a direct and systematical solution procedure which generalizes Hirota's perturbation scheme. With help of Maple, an application of the approach to the $3+1$ dimensional potential-Yu-Toda-Sasa-Fukuyama equation yields exact explicit 1-wave and 2-wave and 3-wave solutions, which include 1-soliton, 2-soliton and 3-soliton type solutions. Two cases with specific values of the involved parameters are plotted for each of 2-wave and 3-wave solutions. ",https://doi.org/10.1088/0031-8949/82/06/065003,1010.3324v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Identifying Restaurant Features via Sentiment Analysis on Yelp Reviews,1970,"  Many people use Yelp to find a good restaurant. Nonetheless, with only an overall rating for each restaurant, Yelp offers not enough information for independently judging its various aspects such as environment, service or flavor. In this paper, we introduced a machine learning based method to characterize such aspects for particular types of restaurants. The main approach used in this paper is to use a support vector machine (SVM) model to decipher the sentiment tendency of each review from word frequency. Word scores generated from the SVM models are further processed into a polarity index indicating the significance of each word for special types of restaurant. Customers overall tend to express more sentiment regarding service. As for the distinction between different cuisines, results that match the common sense are obtained: Japanese cuisines are usually fresh, some French cuisines are overpriced while Italian Restaurants are often famous for their pizzas. ",Kein DOI-Link verfügbar,1709.08698v1,Yes,fresh(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Low-Dose CT via Deep CNN with Skip Connection and Network in Network,1970,"  A major challenge in computed tomography (CT) is how to minimize patient radiation exposure without compromising image quality and diagnostic performance. The use of deep convolutional (Conv) neural networks for noise reduction in Low-Dose CT (LDCT) images has recently shown a great potential in this important application. In this paper, we present a highly efficient and effective neural network model for LDCT image noise reduction. Specifically, to capture local anatomical features we integrate Deep Convolutional Neural Networks (CNNs) and Skip connection layers for feature extraction. Also, we introduce parallelized $1\times 1$ CNN, called Network in Network, to lower the dimensionality of the output from the previous layer, achieving faster computational speed at less feature loss. To optimize the performance of the network, we adopt a Wasserstein generative adversarial network (WGAN) framework. Quantitative and qualitative comparisons demonstrate that our proposed network model can produce images with lower noise and more structural details than state-of-the-art noise-reduction methods. ",Kein DOI-Link verfügbar,1811.10564v2,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Stepping beyond your comfort zone: Diffusion-based network analytics for   knowledge trajectory recommendation,1970,"  Interest in tracing the research interests of scientific researchers is rising, and particularly that of predicting a researcher's knowledge trajectories beyond their current foci into potential inter-/cross-/multi-disciplinary interactions. Hence, in this study, we present a method of diffusion-based network analytics for knowledge trajectory recommendation. The method begins by constructing a heterogeneous bibliometric network consisting of a co-topic layer and a co-authorship layer. A novel link prediction approach with a diffusion strategy is then used to reflect real-world academic activity, such as knowledge sharing between co-authors or diffusing between similar research topics. This strategy differentiates the interactions occurring between homogeneous and heterogeneous nodes and weights the strengths of these interactions. Two sets of experiments - one with a local dataset and another with a global dataset - demonstrate that the proposed method is prior to selected baselines. In addition, to further examine the reliability of our method, we conducted a case study on recommending knowledge trajectories of selected information scientists and their research groups. The results demonstrate the empirical insights our method yields for individual researchers, communities, and research institutions in the information science discipline. ",Kein DOI-Link verfügbar,2205.15504v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",The relationship between the interdisciplinary activation of children's   scientific concepts and their mastery of basic knowledges: a pre study based   on reaction times,1970,"  The activation of scientific concepts (such as association) is not only an important way for children to organize scientific knowledges, but also an important way for them to learn complex concepts (such as compound concepts composed of multiple knowledge points). Inspired by the details of a primary electrical lesson, we used the E-Prime software to study the activation of concepts inside and outside the electrical unit by students with different electrical knowledge levels (taking reaction time as the main index). The results showed that: 1. the levels of basic knowledge was negatively correlated with the cross domain activation ability of concepts, that is, the worse the basic knowledge, the faster the cross domain activation speed (P<0.05); 2. The better the basic knowledges, the closer the activation behavior is to science teachers. Conclusion: 1. the poorer the basic knowledges, the stronger the cross domain association ability and the more active the thinking; 2. The reason for this phenomenon may be human's thinking is patterned; 3. Therefore, there is a potential logical contradiction in the view that divergent thinking is the psychological basis of creativity. ",Kein DOI-Link verfügbar,2212.00450v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Nontrivial worldline winding in non-Hermitian quantum systems,1970,"  Amid the growing interest in non-Hermitian quantum systems, non-interacting models have received the most attention. Here, through the stochastic series expansion quantum Monte Carlo method, we investigate non-Hermitian physics in interacting quantum systems, e.g., various non-Hermitian quantum spin chains. While calculations yield consistent numerical results under open boundary conditions, non-Hermitian quantum systems under periodic boundary conditions observe an unusual concentration of imaginary-time worldlines over nontrivial winding and require enhanced ergodicity between winding-number sectors for proper convergences. Such nontrivial worldline winding is an emergent physical phenomenon that also exists in other non-Hermitian models and analytical approaches. Alongside the non-Hermitian skin effect and the point-gap spectroscopy, it largely extends the identification and analysis of non-Hermitian topological phenomena to quantum systems with interactions, finite temperatures, biorthogonal basis, and periodic boundary conditions in a novel and controlled fashion. Finally, we study the direct physical implications of such nontrivial worldline winding, which bring additional, potentially quasi-long-range contributions to the entanglement entropy. ",https://doi.org/10.1103/PhysRevB.108.245114,2307.01260v2,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",BDC-Adapter: Brownian Distance Covariance for Better Vision-Language   Reasoning,1970,"  Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP and ALIGN, have introduced a new paradigm for learning transferable visual representations. Recently, there has been a surge of interest among researchers in developing lightweight fine-tuning techniques to adapt these models to downstream visual tasks. We recognize that current state-of-the-art fine-tuning methods, such as Tip-Adapter, simply consider the covariance between the query image feature and features of support few-shot training samples, which only captures linear relations and potentially instigates a deceptive perception of independence. To address this issue, in this work, we innovatively introduce Brownian Distance Covariance (BDC) to the field of vision-language reasoning. The BDC metric can model all possible relations, providing a robust metric for measuring feature dependence. Based on this, we present a novel method called BDC-Adapter, which integrates BDC prototype similarity reasoning and multi-modal reasoning network prediction to perform classification tasks. Our extensive experimental results show that the proposed BDC-Adapter can freely handle non-linear relations and fully characterize independence, outperforming the current state-of-the-art methods by large margins. ",Kein DOI-Link verfügbar,2309.01256v1,Yes,"innovative(1), potent(1), innovatively(1)"
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Controllable Text-to-Image Generation with GPT-4,1970,"  Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on the spatial arrangement and object positioning generation and enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. Our work, as a first attempt, shows the potential for employing LLMs to enhance the performance in computer vision tasks. ",Kein DOI-Link verfügbar,2305.18583v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Wave breaking for the generalized Fornberg-Whitham equation,1970,  This paper aims to show that the Cauchy problem of the Burgers equation with a weakly dispersive perturbation involving the Bessel potential (generalization of the Fornberg-Whitham equation) can exhibit wave breaking for initial data with large slope. We also comment on the dispersive properties of the equation. ,https://doi.org/10.1137/23M1603431,2309.10651v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",A Survey of Embodied Learning for Object-Centric Robotic Manipulation,1970,"  Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM_survey. ",Kein DOI-Link verfügbar,2408.11537v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Noether Symmetry Approach in multiple scalar fields Scenario,1970,"  In this Letter, we find suitable potentials in the multiple scalar fields scenario by using the Noether symmetry approach. We discussed three models with multiple scalar fields: N-quintessence with positive kinetic terms, N-phantom with negative kinetic terms and N-quintom with both positive and negative kinetic terms. In the N-quintessence case, the exponential potential which could be derived from several theoretic models is obtained from the Noether conditions. In the N-phantom case, the potential $\frac{V_{0}}{2}(1-\cos(\sqrt{\frac{3N}{2}}\frac{\phi}{m_{pl}}))$, which could be derived from the Pseudo Nambu-Goldstone boson model, is chosen as the Noether conditions required. In the N-quintom case, we derive a relation $DV'_{\phi q}=-\tilde{D}V'_{\phi p}$ between the potential forms for the quintessence-like fields and the phantom-like fields by using the Noether symmetry. ",https://doi.org/10.1016/j.physletb.2010.03.071,0912.0067v2,Yes,potent(4)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics","Noether Symmetry Approach in ""Cosmic Triad"" Vector Field Scenario",1970,"  To realize the accelerations in the early and late periods of our universe, we need to specify potentials for the dominant fields. In this paper, by using the Noether symmetry approach, we try to find suitable potentials in the ""cosmic triad"" vector field scenario. Because the equation of state parameter of dark energy has been constrained in the range of $-1.21\leq \omega\leq -0.89$ by observations, we derive the Noether conditions for the vector field in quintessence, phantom and quintom models, respectively. In the first two cases, constant potential solutions have been obtained. What is more, a fast decaying point-like solution with power-law potential is also found for the vector field in quintessence model. For the quintom case, we find an interesting constraint $\tilde{C}V_{p}'=-CV_{q}'$ on the field potentials, where $C$ and $\tilde{C}$ are constants related to the Noether symmetry. ",https://doi.org/10.1088/0264-9381/27/13/135019,0912.4766v2,Yes,potent(5)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Reinforcement Learning-based Joint User Scheduling and Link   Configuration in Millimeter-wave Networks,1970,"  In this paper, we develop algorithms for joint user scheduling and three types of mmWave link configuration: relay selection, codebook optimization, and beam tracking in millimeter wave (mmWave) networks. Our goal is to design an online controller that dynamically schedules users and configures their links to minimize the system delay. To solve this complex scheduling problem, we model it as a dynamic decision-making process and develop two reinforcement learning-based solutions. The first solution is based on deep reinforcement learning (DRL), which leverages the proximal policy optimization to train a neural network-based solution. Due to the potential high sample complexity of DRL, we also propose an empirical multi-armed bandit (MAB)-based solution, which decomposes the decision-making process into a sequential of sub-actions and exploits classic maxweight scheduling and Thompson sampling to decide those sub-actions. Our evaluation of the proposed solutions confirms their effectiveness in providing acceptable system delay. It also shows that the DRL-based solution has better delay performance while the MAB-based solution has a faster training process. ",Kein DOI-Link verfügbar,2207.03526v2,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Zero-inflated Smoothing Spline (ZISS) Models for Individual-level   Single-cell Temporal Data,1970,"  Recent advancements in single-cell RNA-sequencing (scRNA-seq) have enhanced our understanding of cell heterogeneity at a high resolution. With the ability to sequence over 10,000 cells per hour, researchers can collect large scRNA-seq datasets for different participants, offering an opportunity to study the temporal progression of individual-level single-cell data. However, the presence of excessive zeros, a common issue in scRNA-seq, significantly impacts regression/association analysis, potentially leading to biased estimates in downstream analysis. Addressing these challenges, we introduce the Zero Inflated Smoothing Spline (ZISS) method, specifically designed to model single-cell temporal data. The ZISS method encompasses two components for modeling gene expression patterns over time and handling excessive zeros. Our approach employs the smoothing spline ANOVA model, providing robust estimates of mean functions and zero probabilities for irregularly observed single-cell temporal data compared to existing methods in our simulation studies and real data analysis. ",Kein DOI-Link verfügbar,2401.15309v1,Yes,potent(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",TF4CTR: Twin Focus Framework for CTR Prediction via Adaptive Sample   Differentiation,1970,"  Effective feature interaction modeling is critical for enhancing the accuracy of click-through rate (CTR) prediction in industrial recommender systems. Most of the current deep CTR models resort to building complex network architectures to better capture intricate feature interactions or user behaviors. However, we identify two limitations in these models: (1) the samples given to the model are undifferentiated, which may lead the model to learn a larger number of easy samples in a single-minded manner while ignoring a smaller number of hard samples, thus reducing the model's generalization ability; (2) differentiated feature interaction encoders are designed to capture different interactions information but receive consistent supervision signals, thereby limiting the effectiveness of the encoder. To bridge the identified gaps, this paper introduces a novel CTR prediction framework by integrating the plug-and-play Twin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic Fusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR). Specifically, the framework employs the SSEM at the bottom of the model to differentiate between samples, thereby assigning a more suitable encoder for each sample. Meanwhile, the TF Loss provides tailored supervision signals to both simple and complex encoders. Moreover, the DFM dynamically fuses the feature interaction information captured by the encoders, resulting in more accurate predictions. Experiments on five real-world datasets confirm the effectiveness and compatibility of the framework, demonstrating its capacity to enhance various representative baselines in a model-agnostic manner. To facilitate reproducible research, our open-sourced code and detailed running logs will be made available at: https://github.com/salmon1802/TF4CTR. ",Kein DOI-Link verfügbar,2405.03167v2,Yes,intricate(1)
0000-0003-0515-4781,Yi Zhang,"Institut Polytechnique de Paris, Peking University School of Physics",Intent-guided Heterogeneous Graph Contrastive Learning for   Recommendation,1970,"  Contrastive Learning (CL)-based recommender systems have gained prominence in the context of Heterogeneous Graph (HG) due to their capacity to enhance the consistency of representations across different views. However, existing frameworks often neglect the fact that user-item interactions within HG are governed by diverse latent intents (e.g., brand preferences or demographic characteristics of item audiences), which are pivotal in capturing fine-grained relations. The exploration of these underlying intents, particularly through the lens of meta-paths in HGs, presents us with two principal challenges: i) How to integrate CL with intents; ii) How to mitigate noise from meta-path-driven intents.   To address these challenges, we propose an innovative framework termed Intent-guided Heterogeneous Graph Contrastive Learning (IHGCL), which designed to enhance CL-based recommendation by capturing the intents contained within meta-paths. Specifically, the IHGCL framework includes: i) a meta-path-based Dual Contrastive Learning (DCL) approach to effectively integrate intents into the recommendation, constructing intent-intent contrast and intent-interaction contrast; ii) a Bottlenecked AutoEncoder (BAE) that combines mask propagation with the information bottleneck principle to significantly reduce noise perturbations introduced by meta-paths. Empirical evaluations conducted across six distinct datasets demonstrate the superior performance of our IHGCL framework relative to conventional baseline methods. Our model implementation is available at https://github.com/wangyu0627/IHGCL. ",Kein DOI-Link verfügbar,2407.17234v2,Yes,"innovative(1), pivotal(1)"
0000-0003-4001-264X,Hongce Zhang,The Hong Kong University of Science and Technology,MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design,1970,"  In modern VLSI design flow, the register-transfer level (RTL) stage is a critical point, where designers define precise design behavior with hardware description languages (HDLs) like Verilog. Since the RTL design is in the format of HDL code, the standard way to evaluate its quality requires time-consuming subsequent synthesis steps with EDA tools. This time-consuming process significantly impedes design optimization at the early RTL stage. Despite the emergence of some recent ML-based solutions, they fail to maintain high accuracy for any given RTL design. In this work, we propose an innovative pre-synthesis PPA estimation framework named MasterRTL. It first converts the HDL code to a new bit-level design representation named the simple operator graph (SOG). By only adopting single-bit simple operators, this SOG proves to be a general representation that unifies different design types and styles. The SOG is also more similar to the target gate-level netlist, reducing the gap between RTL representation and netlist. In addition to the new SOG representation, MasterRTL proposes new ML methods for the RTL-stage modeling of timing, power, and area separately. Compared with state-of-the-art solutions, the experiment on a comprehensive dataset with 90 different designs shows accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative slack (TNS), worst negative slack (WNS), and power, respectively. ",Kein DOI-Link verfügbar,2311.08441v1,Yes,innovative(1)
0000-0002-3805-3007,Feng Yang,The Hong Kong University of Science and Technology,Intrinsic conductance of ferroelectric domain walls,1970,"  Ferroelectric domain walls hold great promise for innovative applications in ferroelectric devices. However, the underlying mechanisms behind the observed giant conductance of charged domain walls remain poorly understood. Using a first-principles approach that incorporates Boltzmann transport theory and the relaxation time approximation, we determine the carrier concentration, mobility, and conductivity of domain walls with head-to-head and tail-to-tail polarization orientations. Our systematic exploration reveals that the accumulation of carriers, particularly their concentration, plays a dominant role in the domain wall conductance mechanism. However, the observed conductance differences between head-to-head and tail-to-tail domain walls are primarily due to differences in carrier mobility. The width of the domain wall is a key factor determining the device scale. Our calculated domain wall width is significantly smaller than previously reported values. This method, not limited to a certain ferroelectric material, can be used for the optimization and application development of various domain wall materials and devices. ",Kein DOI-Link verfügbar,2404.00821v1,Yes,innovative(1)
0000-0002-3805-3007,Feng Yang,The Hong Kong University of Science and Technology,MonoDETRNext: Next-generation Accurate and Efficient Monocular 3D Object   Detection Method,1970,"  Monocular vision-based 3D object detection is crucial in various sectors, yet existing methods face significant challenges in terms of accuracy and computational efficiency. Building on the successful strategies in 2D detection and depth estimation, we propose MonoDETRNext, which seeks to optimally balance precision and processing speed. Our methodology includes the development of an efficient hybrid visual encoder, enhancement of depth prediction mechanisms, and introduction of an innovative query generation strategy, augmented by an advanced depth predictor. Building on MonoDETR, MonoDETRNext introduces two variants: MonoDETRNext-F, which emphasizes speed, and MonoDETRNext-A, which focuses on precision. We posit that MonoDETRNext establishes a new benchmark in monocular 3D object detection and opens avenues for future research. We conducted an exhaustive evaluation demonstrating the model's superior performance against existing solutions. Notably, MonoDETRNext-A demonstrated a 4.60% improvement in the AP3D metric on the KITTI test benchmark over MonoDETR, while MonoDETRNext-F showed a 2.21% increase. Additionally, the computational efficiency of MonoDETRNext-F slightly exceeds that of its predecessor. ",Kein DOI-Link verfügbar,2405.15176v1,Yes,innovative(1)
0000-0002-3805-3007,Feng Yang,The Hong Kong University of Science and Technology,An Agile Formal Specification Language Design Based on K Framework,1970,"  Formal Methods (FMs) are currently essential for verifying the safety and reliability of software systems. However, the specification writing in formal methods tends to be complex and challenging to learn, requiring familiarity with various intricate formal specification languages and verification technologies. In response to the increasing complexity of software frameworks, existing specification writing methods fall short in meeting agility requirements. To address this, this paper introduces an Agile Formal Specification Language (ASL). The ASL is defined based on the K Framework and YAML Ain't Markup Language (YAML). The design of ASL incorporates agile design principles, making the writing of formal specifications simpler, more efficient, and scalable. Additionally, a specification translation algorithm is developed, capable of converting ASL into K formal specification language that can be executed for verification. Experimental evaluations demonstrate that the proposed method significantly reduces the code size needed for specification writing, enhancing agility in formal specification writing. ",Kein DOI-Link verfügbar,2404.18515v1,Yes,intricate(1)
0000-0002-3805-3007,Feng Yang,The Hong Kong University of Science and Technology,Does image resolution impact chest X-ray based fine-grained   Tuberculosis-consistent lesion segmentation?,1970,"  Deep learning (DL) models are state-of-the-art in segmenting anatomical and disease regions of interest (ROIs) in medical images. Particularly, a large number of DL-based techniques have been reported using chest X-rays (CXRs). However, these models are reportedly trained on reduced image resolutions for reasons related to the lack of computational resources. Literature is sparse in discussing the optimal image resolution to train these models for segmenting the Tuberculosis (TB)-consistent lesions in CXRs. In this study, we investigated the performance variations using an Inception-V3 UNet model using various image resolutions with/without lung ROI cropping and aspect ratio adjustments, and (ii) identified the optimal image resolution through extensive empirical evaluations to improve TB-consistent lesion segmentation performance. We used the Shenzhen CXR dataset for the study which includes 326 normal patients and 336 TB patients. We proposed a combinatorial approach consisting of storing model snapshots, optimizing segmentation threshold and test-time augmentation (TTA), and averaging the snapshot predictions, to further improve performance with the optimal resolution. Our experimental results demonstrate that higher image resolutions are not always necessary, however, identifying the optimal image resolution is critical to achieving superior performance. ",Kein DOI-Link verfügbar,2301.04032v2,Yes,reportedly(1)
0000-0003-4694-4237,Wentai Zhang,The Hong Kong University of Science and Technology,A Deep Reinforcement Learning Approach for Global Routing,1970,"  Global routing has been a historically challenging problem in electronic circuit design, where the challenge is to connect a large and arbitrary number of circuit components with wires without violating the design rules for the printed circuit boards or integrated circuits. Similar routing problems also exist in the design of complex hydraulic systems, pipe systems and logistic networks. Existing solutions typically consist of greedy algorithms and hard-coded heuristics. As such, existing approaches suffer from a lack of model flexibility and non-optimum solutions. As an alternative approach, this work presents a deep reinforcement learning method for solving the global routing problem in a simulated environment. At the heart of the proposed method is deep reinforcement learning that enables an agent to produce an optimal policy for routing based on the variety of problems it is presented with leveraging the conjoint optimization mechanism of deep reinforcement learning. Conjoint optimization mechanism is explained and demonstrated in details; the best network structure and the parameters of the learned model are explored. Based on the fine-tuned model, routing solutions and rewards are presented and analyzed. The results indicate that the approach can outperform the benchmark method of a sequential A* method, suggesting a promising potential for deep reinforcement learning for global routing and other routing or path planning problems in general. Another major contribution of this work is the development of a global routing problem sets generator with the ability to generate parameterized global routing problem sets with different size and constraints, enabling evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches. ",Kein DOI-Link verfügbar,1906.08809v1,Yes,potent(1)
0000-0003-4694-4237,Wentai Zhang,The Hong Kong University of Science and Technology,Component Segmentation of Engineering Drawings Using Graph Convolutional   Networks,1970,"  We present a data-driven framework to automate the vectorization and machine interpretation of 2D engineering part drawings. In industrial settings, most manufacturing engineers still rely on manual reads to identify the topological and manufacturing requirements from drawings submitted by designers. The interpretation process is laborious and time-consuming, which severely inhibits the efficiency of part quotation and manufacturing tasks. While recent advances in image-based computer vision methods have demonstrated great potential in interpreting natural images through semantic segmentation approaches, the application of such methods in parsing engineering technical drawings into semantically accurate components remains a significant challenge. The severe pixel sparsity in engineering drawings also restricts the effective featurization of image-based data-driven methods. To overcome these challenges, we propose a deep learning based framework that predicts the semantic type of each vectorized component. Taking a raster image as input, we vectorize all components through thinning, stroke tracing, and cubic bezier fitting. Then a graph of such components is generated based on the connectivity between the components. Finally, a graph convolutional neural network is trained on this graph data to identify the semantic type of each component. We test our framework in the context of semantic segmentation of text, dimension and, contour components in engineering drawings. Results show that our method yields the best performance compared to recent image, and graph-based segmentation methods. ",https://doi.org/10.1016/j.compind.2023.103885,2212.00290v2,Yes,potent(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,Reduced model and nonlinear analysis of localized instabilities of   residually stressed cylinders under axial stretch,1970,"  In this paper we present a dimensional reduction to obtain a one-dimensional model to analyze localized necking or bulging in a residually stressed circular cylindrical solid. The nonlinear theory of elasticity is first specialized to obtain the equations governing the homogeneous deformation. Then, to analyze the non-homogeneous part, we include higher order correction terms of the axisymmetric displacement components leading to a three-dimensional form of the total potential energy functional. Details of the reduction to the one-dimensional form are given. We focus on a residually stressed Gent material and use numerical methods to solve the governing equations. Two loading conditions are considered. In the first, the residual stress is maintained constant, while the axial stretch is used as the loading parameter. In the second, we keep the pre-stretch constant and monotonically increase the residual stress until bifurcation occurs. We specify initial conditions, find the critical values for localized bifurcation and compute the change in radius during localized necking or bulging growth. Finally, we optimize material properties and use the one-dimensional model to simulate necking or bulging until the Maxwell values of stretch are reached. ",Kein DOI-Link verfügbar,2403.11215v1,Yes,potent(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,3D Object Detection and High-Resolution Traffic Parameters Extraction   Using Low-Resolution LiDAR Data,1970,"  Traffic volume data collection is a crucial aspect of transportation engineering and urban planning, as it provides vital insights into traffic patterns, congestion, and infrastructure efficiency. Traditional manual methods of traffic data collection are both time-consuming and costly. However, the emergence of modern technologies, particularly Light Detection and Ranging (LiDAR), has revolutionized the process by enabling efficient and accurate data collection. Despite the benefits of using LiDAR for traffic data collection, previous studies have identified two major limitations that have impeded its widespread adoption. These are the need for multiple LiDAR systems to obtain complete point cloud information of objects of interest, as well as the labor-intensive process of annotating 3D bounding boxes for object detection tasks. In response to these challenges, the current study proposes an innovative framework that alleviates the need for multiple LiDAR systems and simplifies the laborious 3D annotation process. To achieve this goal, the study employed a single LiDAR system, that aims at reducing the data acquisition cost and addressed its accompanying limitation of missing point cloud information by developing a Point Cloud Completion (PCC) framework to fill in missing point cloud information using point density. Furthermore, we also used zero-shot learning techniques to detect vehicles and pedestrians, as well as proposed a unique framework for extracting low to high features from the object of interest, such as height, acceleration, and speed. Using the 2D bounding box detection and extracted height information, this study is able to generate 3D bounding boxes automatically without human intervention. ",Kein DOI-Link verfügbar,2401.06946v1,Yes,innovative(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,EMC2A-Net: An Efficient Multibranch Cross-channel Attention Network for   SAR Target Classification,1970,"  In recent years, convolutional neural networks (CNNs) have shown great potential in synthetic aperture radar (SAR) target recognition. SAR images have a strong sense of granularity and have different scales of texture features, such as speckle noise, target dominant scatterers and target contours, which are rarely considered in the traditional CNN model. This paper proposed two residual blocks, namely EMC2A blocks with multiscale receptive fields(RFs), based on a multibranch structure and then designed an efficient isotopic architecture deep CNN (DCNN), EMC2A-Net. EMC2A blocks utilize parallel dilated convolution with different dilation rates, which can effectively capture multiscale context features without significantly increasing the computational burden. To further improve the efficiency of multiscale feature fusion, this paper proposed a multiscale feature cross-channel attention module, namely the EMC2A module, adopting a local multiscale feature interaction strategy without dimensionality reduction. This strategy adaptively adjusts the weights of each channel through efficient one-dimensional (1D)-circular convolution and sigmoid function to guide attention at the global channel wise level. The comparative results on the MSTAR dataset show that EMC2A-Net outperforms the existing available models of the same type and has relatively lightweight network structure. The ablation experiment results show that the EMC2A module significantly improves the performance of the model by using only a few parameters and appropriate cross-channel interactions. ",https://doi.org/10.1109/TGRS.2023.3285037,2208.01836v1,Yes,potent(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,Disturbance Observer for Estimating Coupled Disturbances,1970,"  High-precision control for nonlinear systems is impeded by the low-fidelity dynamical model and external disturbance. Especially, the intricate coupling between internal uncertainty and external disturbance is usually difficult to be modeled explicitly. Here we show an effective and convergent algorithm enabling accurate estimation of the coupled disturbance via combining control and learning philosophies. Specifically, by resorting to Chebyshev series expansion, the coupled disturbance is firstly decomposed into an unknown parameter matrix and two known structures depending on system state and external disturbance respectively. A Regularized Least Squares (RLS) algorithm is subsequently formalized to learn the parameter matrix by using historical time-series data. Finally, a higher-order disturbance observer (HODO) is developed to achieve a high-precision estimation of the coupled disturbance by utilizing the learned portion. The efficiency of the proposed algorithm is evaluated through extensive simulations. We believe this work can offer a new option to merge learning schemes into the control framework for addressing existing intractable control problems. ",Kein DOI-Link verfügbar,2407.13229v1,Yes,intricate(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA   Tasks? A: Self-Train on Unlabeled Images!,1970,"  Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA. ",Kein DOI-Link verfügbar,2306.03932v1,Yes,fresh(1)
0000-0001-5591-3526,Xiang Yu,The Hong Kong University of Science and Technology,Synergistic Effects of Nanoparticle Heating and Amoxicillin on H. Pylori   Inhibition,1970,"  We report the design and development of a dual-functional magnetic nanoparticle platform for potential treatment of H. pylori infection. We show that an ultralow concentration of Mn0.3Fe2.7O4@SiO2 nanoparticles subjected to a moderate AC magnetic field, without bulk heating effect, can deposit heat locally and effectively inhibit H. pylori growth and virulence in vitro. When coupled with antibiotic amoxicillin, the dual-functional amoxicillin loaded Mn0.3Fe2.7O4@SiO2 further decreases the bacteria survival rate by a factor of 7 and 5, respectively, compared to amoxicillin treatment and nanoparticle heating alone. The synergistic effect can be partially attributed to the heating induced damage to the cell membrane and protective biofilm, which may increase the permeability of antibiotics to bacteria. Our method provides a viable approach to treat H. pylori infection, with the potential of reducing side effects and enhancing the efficacy for combating drug resistant strains. ",https://doi.org/10.1016/j.jmmm.2019.04.076,1904.09672v1,Yes,potent(2)
0000-0001-6918-7158,James Lee,The Hong Kong University of Science and Technology,On the ability of CNNs to extract color invariant intensity based   features for image classification,1970,"  Convolutional neural networks (CNNs) have demonstrated remarkable success in vision-related tasks. However, their susceptibility to failing when inputs deviate from the training distribution is well-documented. Recent studies suggest that CNNs exhibit a bias toward texture instead of object shape in image classification tasks, and that background information may affect predictions. This paper investigates the ability of CNNs to adapt to different color distributions in an image while maintaining context and background. The results of our experiments on modified MNIST and FashionMNIST data demonstrate that changes in color can substantially affect classification accuracy. The paper explores the effects of various regularization techniques on generalization error across datasets and proposes a minor architectural modification utilizing the dropout regularization in a novel way that enhances model reliance on color-invariant intensity-based features for improved classification accuracy. Overall, this work contributes to ongoing efforts to understand the limitations and challenges of CNNs in image classification tasks and offers potential solutions to enhance their performance. ",Kein DOI-Link verfügbar,2307.06500v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Overcoming Medical Overuse with AI Assistance: An Experimental   Investigation,1970,"  This study evaluates the effectiveness of Artificial Intelligence (AI) in mitigating medical overtreatment, a significant issue characterized by unnecessary interventions that inflate healthcare costs and pose risks to patients. We conducted a lab-in-the-field experiment at a medical school, utilizing a novel medical prescription task, manipulating monetary incentives and the availability of AI assistance among medical students using a three-by-two factorial design. We tested three incentive schemes: Flat (constant pay regardless of treatment quantity), Progressive (pay increases with the number of treatments), and Regressive (penalties for overtreatment) to assess their influence on the adoption and effectiveness of AI assistance. Our findings demonstrate that AI significantly reduced overtreatment rates by up to 62% in the Regressive incentive conditions where (prospective) physician and patient interests were most aligned. Diagnostic accuracy improved by 17% to 37%, depending on the incentive scheme. Adoption of AI advice was high, with approximately half of the participants modifying their decisions based on AI input across all settings. For policy implications, we quantified the monetary (57%) and non-monetary (43%) incentives of overtreatment and highlighted AI's potential to mitigate non-monetary incentives and enhance social welfare. Our results provide valuable insights for healthcare administrators considering AI integration into healthcare systems. ",Kein DOI-Link verfügbar,2405.10539v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian   Monte Carlo,1970,"  Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreover, we prove that low-precision SGHMC is more robust to the quantization error compared to low-precision SGLD due to the robustness of the momentum-based update w.r.t. gradient noise. Empirically, we conduct experiments on synthetic data, and {MNIST, CIFAR-10 \& CIFAR-100} datasets, which validate our theoretical findings. Our study highlights the potential of low-precision SGHMC as an efficient and accurate sampling method for large-scale and resource-limited machine learning. ",Kein DOI-Link verfügbar,2310.16320v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,"From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital   Humanities Research and Services?",1970,"  Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application. This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation. The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research. Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books. Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation. Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies. ",Kein DOI-Link verfügbar,2404.18518v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models,1970,"  With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP. ",Kein DOI-Link verfügbar,2307.14971v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Enhancing Commentary Strategies for Imperfect Information Card Games: A   Study of Large Language Models in Guandan Commentary,1970,"  Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics. ",Kein DOI-Link verfügbar,2406.17807v3,Yes,"intricate(1), potent(1)"
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,FCNCP: A Coupled Nonnegative CANDECOMP/PARAFAC Decomposition Based on   Federated Learning,1970,"  In the field of brain science, data sharing across servers is becoming increasingly challenging due to issues such as industry competition, privacy security, and administrative procedure policies and regulations. Therefore, there is an urgent need to develop new methods for data analysis and processing that enable scientific collaboration without data sharing. In view of this, this study proposes to study and develop a series of efficient non-negative coupled tensor decomposition algorithm frameworks based on federated learning called FCNCP for the EEG data arranged on different servers. It combining the good discriminative performance of tensor decomposition in high-dimensional data representation and decomposition, the advantages of coupled tensor decomposition in cross-sample tensor data analysis, and the features of federated learning for joint modelling in distributed servers. The algorithm utilises federation learning to establish coupling constraints for data distributed across different servers. In the experiments, firstly, simulation experiments are carried out using simulated data, and stable and consistent decomposition results are obtained, which verify the effectiveness of the proposed algorithms in this study. Then the FCNCP algorithm was utilised to decompose the fifth-order event-related potential (ERP) tensor data collected by applying proprioceptive stimuli on the left and right hands. It was found that contralateral stimulation induced more symmetrical components in the activation areas of the left and right hemispheres. The conclusions drawn are consistent with the interpretations of related studies in cognitive neuroscience, demonstrating that the method can efficiently process higher-order EEG data and that some key hidden information can be preserved. ",Kein DOI-Link verfügbar,2404.11890v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Learning in Nonzero-Sum Stochastic Games with Potentials,1970,"  Multi-agent reinforcement learning (MARL) has become effective in tackling discrete cooperative game scenarios. However, MARL has yet to penetrate settings beyond those modelled by team and zero-sum games, confining it to a small subset of multi-agent systems. In this paper, we introduce a new generation of MARL learners that can handle nonzero-sum payoff structures and continuous settings. In particular, we study the MARL problem in a class of games known as stochastic potential games (SPGs) with continuous state-action spaces. Unlike cooperative games, in which all agents share a common reward, SPGs are capable of modelling real-world scenarios where agents seek to fulfil their individual goals. We prove theoretically our learning method, SPot-AC, enables independent agents to learn Nash equilibrium strategies in polynomial time. We demonstrate our framework tackles previously unsolvable tasks such as Coordination Navigation and large selfish routing games and that it outperforms the state of the art MARL baselines such as MADDPG and COMIX in such scenarios. ",Kein DOI-Link verfügbar,2103.09284v4,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided   Surgical Automation in Laparoscopic Hysterectomy,1970,"  Computer-assisted minimally invasive surgery has great potential in benefiting modern operating theatres. The video data streamed from the endoscope provides rich information to support context-awareness for next-generation intelligent surgical systems. To achieve accurate perception and automatic manipulation during the procedure, learning based technique is a promising way, which enables advanced image analysis and scene understanding in recent years. However, learning such models highly relies on large-scale, high-quality, and multi-task labelled data. This is currently a bottleneck for the topic, as available public dataset is still extremely limited in the field of CAI. In this paper, we present and release the first integrated dataset (named AutoLaparo) with multiple image-based perception tasks to facilitate learning-based automation in hysterectomy surgery. Our AutoLaparo dataset is developed based on full-length videos of entire hysterectomy procedures. Specifically, three different yet highly correlated tasks are formulated in the dataset, including surgical workflow recognition, laparoscope motion prediction, and instrument and key anatomy segmentation. In addition, we provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset. The dataset is available at https://autolaparo.github.io. ",Kein DOI-Link verfügbar,2208.02049v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Demystify Mamba in Vision: A Linear Attention Perspective,1970,"  Mamba is an effective state space model with linear computation complexity. It has recently shown impressive efficiency in dealing with high-resolution inputs across various vision tasks. In this paper, we reveal that the powerful Mamba model shares surprising similarities with linear attention Transformer, which typically underperform conventional Transformer in practice. By exploring the similarities and disparities between the effective Mamba and subpar linear attention Transformer, we provide comprehensive analyses to demystify the key factors behind Mamba's success. Specifically, we reformulate the selective state space model and linear attention within a unified formulation, rephrasing Mamba as a variant of linear attention Transformer with six major distinctions: input gate, forget gate, shortcut, no attention normalization, single-head, and modified block design. For each design, we meticulously analyze its pros and cons, and empirically evaluate its impact on model performance in vision tasks. Interestingly, the results highlight the forget gate and block design as the core contributors to Mamba's success, while the other four designs are less crucial. Based on these findings, we propose a Mamba-Like Linear Attention (MLLA) model by incorporating the merits of these two key designs into linear attention. The resulting model outperforms various vision Mamba models in both image classification and high-resolution dense prediction tasks, while enjoying parallelizable computation and fast inference speed. Code is available at https://github.com/LeapLabTHU/MLLA. ",Kein DOI-Link verfügbar,2405.16605v1,Yes,"meticulous(1), meticulously(1)"
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,FPHammer: A Device Identification Framework based on DRAM Fingerprinting,1970,"  The device fingerprinting technique extracts fingerprints based on the hardware characteristics of the device to identify the device. The primary goal of device fingerprinting is to accurately and uniquely identify a device, which requires the generated device fingerprints to have good stability to achieve long-term tracking of the target device. However, the fingerprints generated by some existing fingerprinting technologies are not stable enough or change frequently, making it impossible to track the target device for a long time. In this paper, we present FPHammer, a novel DRAM-based fingerprinting technique. The device fingerprint generated by our technique has high stability and can be used to track the device for a long time. We leverage the Rowhammer technique to repeatedly and quickly access a row in DRAM to get bit flips in its adjacent row. We then construct a physical fingerprint of the device based on the locations of the collected bit flips. The evaluation results of the uniqueness and reliability of the physical fingerprint show that it can be used to distinguish devices with the same hardware and software configuration. The experimental results on device identification demonstrate that the physical fingerprints engendered by our innovative technique are inherently linked to the entirety of the device rather than just the DRAM module. Even if the device modifies software-level parameters such as MAC address and IP address or even reinstalls the operating system, we can accurately identify the target device. This demonstrates that FPHammer can generate stable fingerprints that are not affected by software layer parameters. ",Kein DOI-Link verfügbar,2201.07597v3,Yes,innovative(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Regioselective On-Surface Synthesis of [3]Triangulene Graphene   Nanoribbons,1970,"  The integration of low-energy states into bottom-up engineered graphene nanoribbons (GNRs) is a robust strategy for realizing materials with tailored electronic band structure for nanoelectronics. Low-energy zero-modes (ZMs) can be introduced into nanographenes (NGs) by creating an imbalance between the two sublattices of graphene. This phenomenon is exemplified by the family of [n]triangulenes. Here, we demonstrate the synthesis of [3]triangulene-GNRs, a regioregular one-dimensional (1D) chain of [3]triangulenes linked by five-membered rings. Hybridization between ZMs on adjacent [3]triangulenes leads to the emergence of a narrow band gap, Eg = 0.7 eV, and topological end states that are experimentally verified using scanning tunneling spectroscopy (STS). Tight-binding and first-principles density functional theory (DFT) calculations within the local spin density approximation (LSDA) corroborate our experimental observations. Our synthetic design takes advantage of a selective on-surface head-to-tail coupling of monomer building blocks enabling the regioselective synthesis of [3]triangulene-GNRs. Detailed ab initio theory provides insight into the mechanism of on-surface radical polymerization, revealing the pivotal role of Au-C bond formation/breakage in driving selectivity. ",Kein DOI-Link verfügbar,2402.15882v1,Yes,pivotal(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,InternLM-Math: Open Math Large Language Models Toward Verifiable   Reasoning,1970,"  The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math. Our models, codes, and data are released at \url{https://github.com/InternLM/InternLM-Math}. ",Kein DOI-Link verfügbar,2402.06332v2,Yes,versatile(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,The Dawn of AI-Native EDA: Opportunities and Challenges of Large Circuit   Models,1970,"  Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts. We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities. ",Kein DOI-Link verfügbar,2403.07257v2,Yes,"intricate(1), pivotal(1)"
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,InternLM2 Technical Report,1970,"  The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution. ",Kein DOI-Link verfügbar,2403.17297v1,Yes,"innovative(1), meticulous(1), meticulously(1)"
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Study of $ψ(3686)\rightarrowΛ\barΛω$,1970,"  Based on a data sample of $(448.1\pm2.9)\times10^6$ $\psi(3686)$ events collected with the BESIII detector at the BEPCII collider, the branching fraction of $\psi(3686)\rightarrow\Lambda\bar{\Lambda}\omega$ is measured to be $\rm (3.30\pm0.34(stat.)\pm0.29(syst.))\times10^{-5}$ for the first time. In addition, the $\Lambda\omega$ (or $\bar{\Lambda}\omega$) invariant mass spectra is studied and the potential presence of excited $\Lambda$ states has been investigated. ",Kein DOI-Link verfügbar,2207.11666v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,First measurement of $ΛN$ inelastic scattering with $Λ$   from $e^{+} e^{-} \rightarrow J/ψ\to Λ\barΛ$,1970,"  Using an $e^+ e^-$ collision data sample of $(10087 \pm 44)\times10^6 ~J/\psi$ events taken at the center-of-mass energy of $3.097~\rm{GeV}$ by the BESIII detector at the BEPCII collider, the process $\Lambda+N \rightarrow \Sigma^+ + X$ is studied for the first time employing a novel method. The $\Sigma^{+}$ hyperons are produced by the collisions of $\Lambda$ hyperons from $J/\psi$ decays with nuclei in the material of the BESIII detector. The total cross section of $\Lambda + ^{9}{\rm Be} \rightarrow \Sigma^+ + X$ is measured to be $\sigma = (37.3 \pm 4.7 \pm 3.5)~{\rm mb}$ at $\Lambda$ beam momenta within $[1.057, 1.091]~{\rm GeV}/c$, where the uncertainties are statistical and systematic, respectively. This analysis is the first study of $\Lambda$-nucleon interactions at an $e^+ e^-$ collider, providing information and constraints relevant for the strong-interaction potential, the origin of color confinement, the unified model for baryon-baryon interactions, and the internal structure of neutron stars. ",https://doi.org/10.1103/PhysRevC.109.L052201,2310.00720v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Precise measurement of the branching fractions of   $J/ψ\rightarrow\barΛπ^{+}Σ^{-}+c.c.$ and   $J/ψ\rightarrow\barΛπ^{-}Σ^{+}+c.c.$,1970,"  Based on a data sample of $(10087\pm44)\times10^6$ $J/\psi$ events collected with the BESIII detector, the branching fraction of $J/\psi\rightarrow\bar{\Lambda}\pi^{+}\Sigma^{-}+c.c.$ is measured to be $(1.221\pm 0.002\pm 0.038)\times10^{-3}$, and the branching fraction of its isospin partner mode $J/\psi\rightarrow\bar{\Lambda}\pi^{-}\Sigma^{+}+c.c.$ is measured to be $(1.244\pm 0.002\pm 0.045)\times10^{-3}$ with improved precision. Here the first uncertainties are statistical and the second ones systematic. The isospin symmetry of the $\Sigma$ baryon in charmonium hadronic decay and the ""$12\%$ rule"" are tested, and no violation is found. The potential of using these channels as $\Sigma$ baryon sources for nuclear physics research is studied, and the momentum and angular distributions of these sources are provided. ",https://doi.org/10.1103/PhysRevD.108.112012,2306.10319v3,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Measurements of Born Cross Sections for $e^+e^-\to Λ_{c}^+   \barΛ_{c}(2595)^- + {\rm c.c.}$ and $e^+e^-\to Λ_{c}^+   \barΛ_{c}(2625)^- + {\rm c.c.}$ at $\sqrt{s}=$4918.0 and 4950.9 MeV,1970,"  Using $e^+e^-$ collision data collected with the BESIII detector operating at the BEPCII collider, the Born cross sections of $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2595)^- + \rm{c.c.}$ and $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2625)^- + \rm{c.c.}$ are measured for the first time at center-of-mass energies of $\sqrt{s}=4918.0$ and 4950.9 MeV. Non-zero cross sections are observed very close to the production threshold. The measured Born cross sections of $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2625)^- + \rm{c.c.}$ are about $2\sim3$ times greater than those of $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2595)^- + \rm{c.c.}$, thereby indicating that the exotic structure potentially exists in the excited charmed baryons. The Born cross sections are $15.6\pm3.1\pm0.9$ pb and $29.4\pm3.7\pm2.7$ pb for $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2595)^- + \rm{c.c.}$, and are $43.4\pm4.0\pm4.1$ pb and $76.8\pm6.5\pm4.2$ pb for $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2625)^- +\rm{c.c.}$ at $\sqrt s=4918.0$ and 4950.9 MeV, respectively. Based on the polar angle distributions of the $\bar{\Lambda}_{c}(2625)^-$ and $\Lambda_{c}(2625)^+$, the form-factor ratios $\sqrt{|G_{E}|^2 + 3|G_{M}|^2}/|G_{C}|$ are determined for $e^+e^-\to \Lambda_{c}^+ \bar{\Lambda}_{c}(2625)^- + \rm{c.c.}$ for the first time, which are $5.95\pm4.07\pm0.15$ and $0.94\pm0.32\pm0.02$ at $\sqrt s=4918.0$ and 4950.9 MeV, respectively. All of these first uncertainties are statistical and second systematic. ",https://doi.org/10.1103/PhysRevD.109.L071104,2312.08414v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Measurement of the Electromagnetic Transition Form-factors in the decays   $η'\rightarrowπ^+π^-l^+l^-$,1970,"  With a sample of $(10087\pm44)\times10^{6}$ $J/\psi$ events accumulated with the BESIII detector, we analyze the decays $\eta'\rightarrow\pi^+\pi^-l^+l^-(l=e,$ $\mu)$ via the process $J/\psi\rightarrow\gamma\eta'$. The branching fractions are measured to be $\mathcal{B}(\eta'\rightarrow\pi^+\pi^-e^+e^-)=(2.45\pm0.02(\rm{stat.})\pm0.08(\rm{syst.})) \times10^{-3}$ and $\mathcal{B}(\eta'\rightarrow\pi^+\pi^-\mu^+\mu^-)=(2.16\pm0.12(\rm{stat.})\pm0.06(\rm{syst.}))\times10^{-5}$, and the ratio is $\frac{\mathcal{B}(\eta'\rightarrow\pi^{+}\pi^{-}e^{+}e^{-})}{\mathcal{B}(\eta'\rightarrow\pi^{+}\pi^{-}\mu^{+}\mu^{-})} = 113.4\pm0.9(\rm{stat.})\pm3.7(\rm{syst.})$. In addition, by combining the $\eta'\rightarrow\pi^+\pi^-e^+e^-$ and $\eta'\rightarrow\pi^+\pi^-\mu^+\mu^-$ decays, the slope parameter of the electromagnetic transition form factor is measured to be $b_{\eta'}=1.30\pm0.19\ (\mathrm{GeV}/c^{2})^{-2}$, which is consistent with previous measurements from BESIII and theoretical predictions from the VMD model. The asymmetry in the angle between the $\pi^+\pi^-$ and $l^+l^-$ decay planes, which has the potential to reveal the $CP$-violation originating from an unconventional electric dipole transition, is also investigated. The asymmetry parameters are determined to be $\mathcal{A}_{CP}(\eta'\rightarrow\pi^+\pi^-e^+e^-)=(-0.21\pm0.73(\rm{stat.})\pm0.01(\rm{syst.}))\%$ and $\mathcal{A}_{CP}(\eta'\rightarrow\pi^+\pi^-\mu^+\mu^-)=(0.62\pm4.71(\rm{stat.})\pm0.08(\rm{syst.}))\%$, implying that no evidence of $CP$-violation is observed at the present statistics. Finally, an axion-like particle is searched for via the decay $\eta'\rightarrow\pi^+\pi^-a, a\rightarrow e^+e^-$, and upper limits of the branching fractions are presented for the mass assumptions of the axion-like particle in the range of $0-500\ \mathrm{MeV}/c^{2}$. ",Kein DOI-Link verfügbar,2402.01993v1,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Measurement of $e^+e^-\rightarrowΛ\barΛη$ from 3.5106   to 4.6988 GeV and study of $Λ\barΛ$ mass threshold enhancement,1970,"  Using data samples with a total integrated luminosity of approximately 18 fb$^{-1}$ collected by the BESIII detector operating at the BEPCII, the process $e^+e^-\rightarrow\Lambda\bar{\Lambda} \eta$ is studied at center-of-mass energies between 3.5106 and 4.6988 GeV. The Born cross section for the process $e^+e^-\rightarrow\Lambda\bar{\Lambda}\eta$ is measured. No significant structure is observed in the Born cross section line shape. An enhancement near the $\Lambda\bar{\Lambda}$ mass threshold is observed for the first time in the process. The structure can be described by an $S$-wave Breit-Wigner function. Neglecting contribution of excited $\Lambda$ states and potential interferences, the mass and width are determined to be ($2356\pm 7\pm17$) MeV/$c^2$ and ($304\pm28\pm54$) MeV, respectively, where the first uncertainties are statistical and the second are systematic. ",https://doi.org/10.1103/PhysRevD.107.112001,2211.10755v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Search for hidden-charm tetraquark with strangeness in   $e^{+}e^{-}\rightarrow K^+ D_{s}^{*-} D^{*0}+c.c.$,1970,"  We report a search for a heavier partner of the recently observed $Z_{cs}(3985)^{-}$ state, denoted as $Z_{cs}^{\prime -}$, in the process $e^{+} e^{-}\rightarrow K^{+}D_{s}^{*-}D^{* 0}+c.c.$, based on $e^+e^-$ collision data collected at the center-of-mass energies of $\sqrt{s}=4.661$, 4.682 and 4.699 GeV with the BESIII detector. The $Z_{cs}^{\prime -}$ is of interest as it is expected to be a candidate for a hidden-charm and open-strange tetraquark. A partial-reconstruction technique is used to isolate $K^+$ recoil-mass spectra, which are probed for a potential contribution from $Z_{cs}^{\prime -}\to D_{s}^{*-}D^{* 0}$ ($c.c.$). We find an excess of $Z_{cs}^{\prime -}\rightarrow D_{s}^{*-}D^{*0}$ ($c.c.$) candidates with a significance of $2.1\sigma$, after considering systematic uncertainties, at a mass of $(4123.5\pm0.7_\mathrm{stat.}\pm4.7_\mathrm{syst.})\ \mathrm{MeV}/c^{2}$. As the data set is limited in size, the upper limits are evaluated at the 90\% confidence level on the product of the Born cross sections ($\sigma^{\mathrm{Born}}$) and the branching fraction ($\mathcal{B}$) of $Z_{cs}^{\prime-}\rightarrow D_{s}^{*-}D^{* 0}$, under different assumptions of the $Z_{cs}^{\prime -}$ mass from 4.120 to 4.140 MeV and of the width from 10 to 50 MeV at the three center-of-mass energies. The upper limits of $\sigma^{\rm Born}\cdot\mathcal{B}$ are found to be at the level of $\mathcal{O}(1)$ pb at each energy. Larger data samples are needed to confirm the $Z_{cs}^{\prime -}$ state and clarify its nature in the coming years. ",https://doi.org/10.1088/1674-1137/acac69,2211.12060v2,Yes,potent(1)
0000-0002-3020-7008,Ziyi Wang,the hong kong university of science and technology,Measurement of the branching fraction of the decay $J/ψ\to p \bar{p}   η$,1970,"  A high precision measurement of the branching fraction of the decay $J/\psi \to p \bar{p} \eta$ is performed using $(10 087 \pm 44) \times 10^6$ $J/\psi$ events recorded by the {BESIII} detector at the {BEPCII} storage ring. The branching fractions of the two decays $J/\psi \to p \bar{p} \eta(\eta \to \gamma\gamma)$ and $J/\psi \to p \bar{p} \eta(\eta \to \pi^+ \pi^- \pi^0)$ are measured individually to be $\mathcal{B}(J/\psi \to p \bar{p} \eta(\eta \to \gamma\gamma)) = (1.480 \pm 0.001 \pm 0.024)\times\,10^{-3}$ and $\mathcal{B}(J/\psi \to p \bar{p} \eta(\eta \to \pi^+ \pi^- \pi^0)) = (1.557 \pm 0.003 \pm 0.038)\times\,10^{-3}$, where the first uncertainties are statistical and the second systematic. Both results are compatible within their uncorrelated systematic uncertainties. The combined result is $\mathcal{B}(J/\psi \to p \bar{p} \eta)=(1.495 \pm 0.001 \pm 0.023)\times\,10^{-3}$ where the first uncertainty is the combined statistical uncertainty and the second one the combined systematic uncertainty of both analyses, incorporating correlations between them. In addition, the $p \bar{p}$ threshold region is investigated for a potential threshold enhancement, and no evidence for one is observed. ",Kein DOI-Link verfügbar,2407.02899v1,Yes,potent(1)
0000-0002-6816-6977,Justin Juho Kim,The Hong Kong University of Science and Technology,"Exploring User Perspectives on ChatGPT: Applications, Perceptions, and   Implications for AI-Integrated Education",1970,"  To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different educational sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some users view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among concerned users. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students' social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies. ",Kein DOI-Link verfügbar,2305.13114v3,Yes,"pivotal(1), potent(2)"
0000-0002-1877-8238,Tianlong Zhang,The Hong Kong University of Science and Technology,Multifractal level sets and metric mean dimension with potential,1970,"  Let $(X,f)$ be a dynamical system with the specification property and $\varphi$ be continuous functions. In this paper, we establish some conditional variational principles for the upper and lower Bowen/packing metric mean dimension with potential of multifractal level set $K_\alpha:=\{x\in X:\lim\limits_{n\to\infty}\dfrac{1}{n}\sum\limits_{i=0}^{n-1}\varphi(f^ix)=\alpha\}.$ ",Kein DOI-Link verfügbar,2407.15027v1,Yes,potent(1)
0000-0002-1877-8238,Tianlong Zhang,The Hong Kong University of Science and Technology,Irregular set and metric mean dimension with potential,1970,"  Let $(X,f)$ be a dynamical system with the specification property and $\varphi$ be a continuous function. In this paper, we consider the multifractal irregular set   \begin{align*}   I_{\varphi}=\left\{x\in X:\lim\limits_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n-1}\varphi(f^ix)\ \text{does not exist}\right\}   \end{align*} and show that this set is either empty or carries full Bowen upper and lower metric mean dimension with potential. ",Kein DOI-Link verfügbar,2407.17079v2,Yes,potent(1)
0000-0002-1735-2331,Zhijian He,The Hong Kong University of Science and Technology,Quasi-Monte Carlo and importance sampling methods for Bayesian inverse   problems,1970,"  Importance Sampling (IS), an effective variance reduction strategy in Monte Carlo (MC) simulation, is frequently utilized for Bayesian inference and other statistical challenges. Quasi-Monte Carlo (QMC) replaces the random samples in MC with low discrepancy points and has the potential to substantially enhance error rates. In this paper, we integrate IS with a randomly shifted rank-1 lattice rule, a widely used QMC method, to approximate posterior expectations arising from Bayesian Inverse Problems (BIPs) where the posterior density tends to concentrate as the intensity of noise diminishes. Within the framework of weighted Hilbert spaces, we first establish the convergence rate of the lattice rule for a large class of unbounded integrands. This method extends to the analysis of QMC combined with IS in BIPs. Furthermore, we explore the robustness of the IS-based randomly shifted rank-1 lattice rule by determining the quadrature error rate with respect to the noise level. The effects of using Gaussian distributions and $t$-distributions as the proposal distributions on the error rate of QMC are comprehensively investigated. We find that the error rate may deteriorate at low intensity of noise when using improper proposals, such as the prior distribution. To reclaim the effectiveness of QMC, we propose a new IS method such that the lattice rule with $N$ quadrature points achieves an optimal error rate close to $O(N^{-1})$, which is insensitive to the noise level. Numerical experiments are conducted to support the theoretical results. ",Kein DOI-Link verfügbar,2403.11374v1,Yes,potent(1)
0000-0002-1735-2331,Zhijian He,The Hong Kong University of Science and Technology,FusionPortable: A Multi-Sensor Campus-Scene Dataset for Evaluation of   Localization and Mapping Accuracy on Diverse Platforms,1970,"  Combining multiple sensors enables a robot to maximize its perceptual awareness of environments and enhance its robustness to external disturbance, crucial to robotic navigation. This paper proposes the FusionPortable benchmark, a complete multi-sensor dataset with a diverse set of sequences for mobile robots. This paper presents three contributions. We first advance a portable and versatile multi-sensor suite that offers rich sensory measurements: 10Hz LiDAR point clouds, 20Hz stereo frame images, high-rate and asynchronous events from stereo event cameras, 200Hz inertial readings from an IMU, and 10Hz GPS signal. Sensors are already temporally synchronized in hardware. This device is lightweight, self-contained, and has plug-and-play support for mobile robots. Second, we construct a dataset by collecting 17 sequences that cover a variety of environments on the campus by exploiting multiple robot platforms for data collection. Some sequences are challenging to existing SLAM algorithms. Third, we provide ground truth for the decouple localization and mapping performance evaluation. We additionally evaluate state-of-the-art SLAM approaches and identify their limitations. The dataset, consisting of raw sensor easurements, ground truth, calibration data, and evaluated algorithms, will be released: https://ram-lab.com/file/site/multi-sensor-dataset. ",Kein DOI-Link verfügbar,2208.11865v1,Yes,versatile(1)
0000-0002-3222-6579,Jinglu Wang,The Hong Kong University of Science and Technology,Hybrid Instance-aware Temporal Fusion for Online Video Instance   Segmentation,1970,"  Recently, transformer-based image segmentation methods have achieved notable success against previous solutions. While for video domains, how to effectively model temporal context with the attention of object instances across frames remains an open problem. In this paper, we propose an online video instance segmentation framework with a novel instance-aware temporal fusion method. We first leverages the representation, i.e., a latent code in the global context (instance code) and CNN feature maps to represent instance- and pixel-level features. Based on this representation, we introduce a cropping-free temporal fusion approach to model the temporal consistency between video frames. Specifically, we encode global instance-specific information in the instance code and build up inter-frame contextual fusion with hybrid attentions between the instance codes and CNN feature maps. Inter-frame consistency between the instance codes are further enforced with order constraints. By leveraging the learned hybrid temporal consistency, we are able to directly retrieve and maintain instance identities across frames, eliminating the complicated frame-wise instance matching in prior methods. Extensive experiments have been conducted on popular VIS datasets, i.e. Youtube-VIS-19/21. Our model achieves the best performance among all online VIS methods. Notably, our model also eclipses all offline methods when using the ResNet-50 backbone. ",Kein DOI-Link verfügbar,2112.01695v2,Yes,notable(1)
0000-0002-3222-6579,Jinglu Wang,The Hong Kong University of Science and Technology,$\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception   Models under Perturbations,1970,"  Referring perception, which aims at grounding visual objects with multimodal referring guidance, is essential for bridging the gap between humans, who provide instructions, and the environment where intelligent systems perceive. Despite progress in this field, the robustness of referring perception models (RPMs) against disruptive perturbations is not well explored. This work thoroughly assesses the resilience of RPMs against various perturbations in both general and specific contexts. Recognizing the complex nature of referring perception tasks, we present a comprehensive taxonomy of perturbations, and then develop a versatile toolbox for synthesizing and evaluating the effects of composite disturbances. Employing this toolbox, we construct $\text{R}^2$-Bench, a benchmark for assessing the Robustness of Referring perception models under noisy conditions across five key tasks. Moreover, we propose the $\text{R}^2$-Agent, an LLM-based agent that simplifies and automates model evaluation via natural language instructions. Our investigation uncovers the vulnerabilities of current RPMs to various perturbations and provides tools for assessing model robustness, potentially promoting the safe and resilient integration of intelligent systems into complex real-world scenarios. ",Kein DOI-Link verfügbar,2403.04924v1,Yes,"versatile(1), potent(1)"
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Strong single-photon to two-photon bundles emission in spin-1   Jaynes-Cummings model,1970,"  The realization of high-quality special nonclassical states beyond strong single atom-cavity coupling regime is a fundamental element in quantum information science. Here, we study the nonclassical photon emission in a single spin-1 atom coupled to an optical cavity with constructing a spin-1 Jaynes-Cummings model. By tuning quadratic Zeeman shift, the energy-spectrum anharmonicity can be significantly enhanced with respect to the dressed-state splitting of well-resolved n-photon resonance largely increased. The photon emission exhibit high-quality single photon and two-photon bundles properties with large photon numbers in the cavity and atom driven cases, respectively. More interestingly, nonclassical optical switching from strong single-photon blockade to two-photon bundles and super-Poissonian photon emission is achieved and highly controllable by light-cavity detuning in the presence of both atom and cavity driven fields. Our proposal not only open up a new avenue for generating high-quality n-photon sources but also provide versatile applications in quantum networks and quantum metrology. ",https://doi.org/10.1063/5.0144615,2209.13390v1,Yes,versatile(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Nonclassical correlated optical multistability at low photon level for   cavity electromagnetically induced transparency,1970,"  We study the nonequilibrium dynamic behaviors in a driven-dissipative single-atom cavity electromagnetically induced transparency. The optical bistability and multistability beyond a Kerr nonlinearity are observed utilizing the optical Stark shift induced strong nonlinearity. We show that the nonequilibrium dynamical phase transition between bistability and multistability is highly tunable by the system parameters in a large parameter region. The first-order dissipative optical bistability (multistability) always corresponds to the photon-bunching quantum statistics, which indicates that the quantum fluctuations and correlations play important roles in nonequilibrium dynamics.Interestingly, bistability and multistability with photon-bunching quantum statistics occurring at extremely low steady-state cavity photon number are observed, even under a very strong cavity driven field. Furthermore, we demonstrate that the unique cavity steady-state solution of the full quantum calculation is excellently consistent with the lowest solution based on the semiclassical mean-field approach in bistability and multistability regimes when the cavity photon number is much less than unity, albeit these nonclassical quantum states should possess strong quantum fluctuations in this parameter regime. Our results pave the way to exploring nonclassical correlated optical multistability in quantum regime, which may bring exciting opportunities for potential applications from quantum information processing to quantum metrology. ",https://doi.org/10.1088/1367-2630/aca92a,2212.03402v1,Yes,"potent(1), excellently(1)"
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Machine Mindset: An MBTI Exploration of Large Language Models,1970,"  We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI. Our method, ""Machine Mindset,"" involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs. This approach ensures that models internalize these traits, offering a stable and consistent personality profile. We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits. The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications. We also open-sourced our model and part of the data at \url{https://github.com/PKU-YuanGroup/Machine-Mindset}. ",Kein DOI-Link verfügbar,2312.12999v4,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,AlignedCoT: Prompting Large Language Models via Native-Speaking   Demonstrations,1970,"  Large Language Models prompting, such as using in-context demonstrations, is a mainstream technique for invoking LLMs to perform high-performance and solid complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and has the potential for further human-machine collaborative scientific findings. However, current LLMs are delicate and elusive in prompt words and styles. And there is an unseen gap between LLM understanding and human-written prompts. This paper introduces AlignedCoT, an LLM-acquainted prompting technique that includes proficient ""native-speaking"" in in-context learning for the LLMs. Specifically, it achieves consistent and correct step-wise prompts in zero-shot scenarios by progressively probing, refining, and formatting the LLM chain of thoughts so that free from handcrafted few-shot demonstrations while maintaining the prompt quality. We conduct experiments on mathematical reasoning and commonsense reasoning. We find that LLMs with AlignedCoT perform significantly superior to them with human-crafted demonstrations. We further apply AlignedCoT for rewriting the GSM8k training set, resulting in a GSM8k-Align dataset. We observe its benefits for retrieval augmented generation. ",Kein DOI-Link verfügbar,2311.13538v4,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Strong Photon Blockade Mediated by Optical Stark Shift in a   Single-Atom-Cavity System,1970,"  We propose a theoretical scheme to achieve strong photon blockade via a single atom in cavity. By utilizing optical Stark shift, the dressed-state splitting between higher and lower branches is enhanced, which results in significant increasing for lower (higher) branch and decreasing for higher (lower) branch at the negative (positive) Stark shift, and dominates the time-evolution of photon number oscillations as well. Furthermore, the two-photon excitation is suppressed via quantum interference under optimal phase and Rabi frequency of an external microwave field. It is shown that the interplay between the quantum interference and the enhanced vacuum-Rabi splitting gives rise to a strong photon blockade for realizing high-quality single photon source beyond strong atom-cavity coupling regime. In particular, by tuning the optical Stark shift, the second-order correlation function for our scheme has three orders of magnitude smaller than Jaynes-Cummings model and correspondingly there appears a large cavity photon number as well. Our proposal may implicate exciting opportunities for potential applications on quantum network and quantum information processing. ",https://doi.org/10.1103/PhysRevApplied.12.044065,1910.04352v2,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,R-BERT-CNN: Drug-target interactions extraction from biomedical   literature,1970,"  In this research, we present our work participation for the DrugProt task of BioCreative VII challenge. Drug-target interactions (DTIs) are critical for drug discovery and repurposing, which are often manually extracted from the experimental articles. There are >32M biomedical articles on PubMed and manually extracting DTIs from such a huge knowledge base is challenging. To solve this issue, we provide a solution for Track 1, which aims to extract 10 types of interactions between drug and protein entities. We applied an Ensemble Classifier model that combines BioMed-RoBERTa, a state of art language model, with Convolutional Neural Networks (CNN) to extract these relations. Despite the class imbalances in the BioCreative VII DrugProt test corpus, our model achieves a good performance compared to the average of other submissions in the challenge, with the micro F1 score of 55.67% (and 63% on BioCreative VI ChemProt test corpus). The results show the potential of deep learning in extracting various types of DTIs. ",Kein DOI-Link verfügbar,2111.00611v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Quantum Interference Induced Photon Blockade in a Coupled Single Quantum   Dot-Cavity System,1970,"  We propose an experimental scheme to implement a strong photon blockade with a single quantum dot coupled to a nanocavity. The photon blockade effect can be tremendously enhanced by driving the cavity and the quantum dot simultaneously with two classical laser fields. This enhancement of photon blockade is ascribed to the quantum interference effect to avoid two-photon excitation of the cavity field. Comparing with Jaynes-Cummings model, the second-order correlation function at zero time delay $g^{(2)}(0)$ in our scheme can be reduced by two orders of magnitude and the system sustains a large intracavity photon number. A red (blue) cavity-light detuning asymmetry for photon quantum statistics with bunching or antibunching characteristics is also observed. The photon blockade effect has a controllable flexibility by tuning the relative phase between the two pumping laser fields and the Rabi coupling strength between the quantum dot and the pumping field. Moreover, the photon blockade scheme based on quantum interference mechanism does not require a strong coupling strength between the cavity and the quantum dot, even with the pure dephasing of the system. This simple proposal provides an effective way for potential applications in solid state quantum computation and quantum information processing. ",https://doi.org/10.1038/srep09252,1503.05344v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning,1970,"  The pervasive deployment of Large Language Models-LLMs in various sectors often neglects the nuanced requirements of individuals and small organizations, who benefit more from models precisely tailored to their specific business contexts rather than those with broadly superior general capabilities. This work introduces \textbf{AnyTaskTune}, a novel fine-tuning methodology coined as \textbf{Task-Fine-Tune}, specifically developed to elevate model performance on a diverse array of domain-specific tasks. This method involves a meticulous process to identify and define targeted sub-tasks within a domain, followed by the creation of specialized enhancement datasets for fine-tuning, thereby optimizing task-specific model performance. We conducted comprehensive fine-tuning experiments not only in the legal domain for tasks such as keyword extraction and sentence prediction but across over twenty different sub-tasks derived from the domains of finance, healthcare, law, psychology, consumer services, and human resources. To substantiate our approach and facilitate community engagement, we will open-source these bilingual task datasets. Our findings demonstrate that models fine-tuned using the \textbf{Task-Fine-Tune} methodology not only achieve superior performance on these specific tasks but also significantly outperform models with higher general capabilities in their respective domains. Our work is publicly available at \url{https://github.com/PandaVT/DataTager}. ",Kein DOI-Link verfügbar,2407.07094v1,Yes,meticulous(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Optimal Batched Best Arm Identification,1970,"  We study the batched best arm identification (BBAI) problem, where the learner's goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability $1-\delta$ for some small constant $\delta>0$ while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and runs only in at most $3$ batches. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $\delta>0$ is arbitrarily fixed), while enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends to zero. Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least $1-\delta$), which is potentially unbounded in cases where a sub-optimal arm is returned. In contrast, the complexity of Opt-BBAI does not rely on such an event. This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest. ",Kein DOI-Link verfügbar,2310.14129v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Do the Rich Get Richer? Fairness Analysis for Blockchain Incentives,1970,"  Proof-of-Work (PoW) is the most widely adopted incentive model in current blockchain systems, which unfortunately is energy inefficient. Proof-of-Stake (PoS) is then proposed to tackle the energy issue. The rich-get-richer concern of PoS has been heavily debated in the blockchain community. The debate is centered around the argument that whether rich miners possessing more stakes will obtain higher staking rewards and further increase their potential income in the future. In this paper, we define two types of fairness, i.e., expectational fairness and robust fairness, that are useful for answering this question. In particular, expectational fairness illustrates that the expected income of a miner is proportional to her initial investment, indicating that the expected return on investment is a constant. To better capture the uncertainty of mining outcomes, robust fairness is proposed to characterize whether the return on investment concentrates to a constant with high probability as time evolves. Our analysis shows that the classical PoW mechanism can always preserve both types of fairness as long as the mining game runs for a sufficiently long time. Furthermore, we observe that current PoS blockchains implement various incentive models and discuss three representatives, namely ML-PoS, SL-PoS and C-PoS. We find that (i) ML-PoS (e.g., Qtum and Blackcoin) preserves expectational fairness but may not achieve robust fairness, (ii) SL-PoS (e.g., NXT) does not protect any type of fairness, and (iii) C-PoS (e.g., Ethereum 2.0) outperforms ML-PoS in terms of robust fairness while still maintaining expectational fairness. Finally, massive experiments on real blockchain systems and extensive numerical simulations are performed to validate our analysis. ",https://doi.org/10.1145/3448016.3457285,2103.14713v2,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Learning to Iteratively Solve Routing Problems with Dual-Aspect   Collaborative Transformer,1970,"  Recently, Transformer has become a prevailing deep architecture for solving vehicle routing problems (VRPs). However, it is less effective in learning improvement models for VRP because its positional encoding (PE) method is not suitable in representing VRP solutions. This paper presents a novel Dual-Aspect Collaborative Transformer (DACT) to learn embeddings for the node and positional features separately, instead of fusing them together as done in existing ones, so as to avoid potential noises and incompatible correlations. Moreover, the positional features are embedded through a novel cyclic positional encoding (CPE) method to allow Transformer to effectively capture the circularity and symmetry of VRP solutions (i.e., cyclic sequences). We train DACT using Proximal Policy Optimization and design a curriculum learning strategy for better sample efficiency. We apply DACT to solve the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). Results show that our DACT outperforms existing Transformer based improvement models, and exhibits much better generalization performance across different problem sizes on synthetic and benchmark instances, respectively. ",Kein DOI-Link verfügbar,2110.02544v3,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,A Survey on Mixture of Experts,1970,"  Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge developments in MoE research, we have established a resource repository accessible at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts. ",Kein DOI-Link verfügbar,2407.06204v2,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Single-atom catalysts boost nitrogen electroreduction reaction,1970,"  Ammonia (NH3) is mainly produced through the traditional Haber-Bosch process under harsh conditions with huge energy consumption and massive carbon dioxide (CO2) emission. The nitrogen electroreduction reaction (NERR), as an energy-efficient and environment-friendly process of converting nitrogen (N2) to NH3 under ambient conditions, has been regarded as a promising alternative to the Haber-Bosch process and has received enormous interest in recent years. Although some exciting progress has been made, considerable scientific and technical challenges still exist in improving the NH3 yield rate and Faradic efficiency, understanding the mechanism of the reaction and promoting the wide commercialization of NERR. Single-atom catalysts (SACs) have emerged as promising catalysts because of its atomically dispersed activity sites and maximized atom efficiency, unsaturated coordination environment, and its unique electronic structure, which could significantly improve the rate of reaction and yield rate of NH3. In this review we briefly introduce the unique structural and electronic features of SACs, which contributes to comprehensively understand the reaction mechanism owing to their structural simplicity and diversity, and in turn expedite the rational design of fantastic catalysts at the atomic scale. Then, we summarize the most recent experimental and computational efforts on developing novel SACs with excellent NERR performance, including precious metal-, nonprecious metal- and nonmetal-based SACs. Finally, we present challenges and perspectives of SACs on NERR, as well as some potential means for advanced NERR catalyst. ",Kein DOI-Link verfügbar,2001.00130v4,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for   In-Context Learning,1970,"  Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive experiments, we demonstrate that DQ-LoRe significantly outperforms prior state-of-the-art methods in the automatic selection of exemplars for GPT-4, enhancing performance from 92.5% to 94.2%. Our comprehensive analysis further reveals that DQ-LoRe consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios characterized by distribution shifts. DQ-LoRe pushes the boundary of in-context learning and opens up new avenues for addressing complex reasoning challenges. Our code is released at https://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe. ",Kein DOI-Link verfügbar,2310.02954v5,Yes,intricate(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Charge state control in single InAs/GaAs quantum dots by external   electric and magnetic fields,1970,"  We report a photoluminescence (PL) spectroscopy study of charge state control in single self-assembled InAs/GaAs quantum dots by applying electric and/or magnetic fields at 4.2 K. Neutral and charged exciton complexes were observed under applied bias voltages from -0.5 V to 0.5 V by controlling the carrier tunneling. The highly negatively charged exciton emission becomes stronger with increasing pumping power, arising from the fact that electrons have a smaller effective mass than holes and are more easily captured by the quantum dots. The integrated PL intensity of negatively charged excitons is affected significantly by a magnetic field applied along the sample growth axis. This observation is explained by a reduction in the electron drift velocity caused by an applied magnetic field, which increases the probability of non-resonantly excited electrons being trapped by localized potentials at the wetting layer interface, and results in fewer electrons distributed in the quantum dots. The hole drift velocity is also affected by the magnetic field, but it is much weaker. ",https://doi.org/10.1063/1.4891828,1407.7980v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Efficient and Effective Algorithms for Revenue Maximization in Social   Advertising,1970,"  We consider the revenue maximization problem in social advertising, where a social network platform owner needs to select seed users for a group of advertisers, each with a payment budget, such that the total expected revenue that the owner gains from the advertisers by propagating their ads in the network is maximized. Previous studies on this problem show that it is intractable and present approximation algorithms. We revisit this problem from a fresh perspective and develop novel efficient approximation algorithms, both under the setting where an exact influence oracle is assumed and under one where this assumption is relaxed. Our approximation ratios significantly improve upon the previous ones. Furthermore, we empirically show, using extensive experiments on four datasets, that our algorithms considerably outperform the existing methods on both the solution quality and computation efficiency. ",Kein DOI-Link verfügbar,2107.04997v3,Yes,fresh(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Towards Making Deep Learning-based Vulnerability Detectors Robust,1970,"  Automatically detecting software vulnerabilities in source code is an important problem that has attracted much attention. In particular, deep learning-based vulnerability detectors, or DL-based detectors, are attractive because they do not need human experts to define features or patterns of vulnerabilities. However, such detectors' robustness is unclear. In this paper, we initiate the study in this aspect by demonstrating that DL-based detectors are not robust against simple code transformations, dubbed attacks in this paper, as these transformations may be leveraged for malicious purposes. As a first step towards making DL-based detectors robust against such attacks, we propose an innovative framework, dubbed ZigZag, which is centered at (i) decoupling feature learning and classifier learning and (ii) using a ZigZag-style strategy to iteratively refine them until they converge to robust features and robust classifiers. Experimental results show that the ZigZag framework can substantially improve the robustness of DL-based detectors. ",Kein DOI-Link verfügbar,2108.00669v2,Yes,innovative(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Process-Driven Autoformalization in Lean 4,1970,"  Autoformalization, the conversion of natural language mathematics into formal languages, offers significant potential for advancing mathematical reasoning. However, existing efforts are limited to formal languages with substantial online corpora and struggle to keep pace with rapidly evolving languages like Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the autoformalization capabilities of large language models (LLMs). This benchmark encompasses a comprehensive assessment of questions, answers, formal statements, and proofs. Additionally, we introduce a \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model that leverages the precise feedback from Lean 4 compilers to enhance autoformalization. Our experiments demonstrate that the PSV method improves autoformalization, enabling higher accuracy using less filtered training data. Furthermore, when fine-tuned with data containing detailed process information, PSV can leverage the data more effectively, leading to more significant improvements in autoformalization for Lean 4. Our dataset and code are available at \url{https://github.com/rookie-joe/PDA}. ",Kein DOI-Link verfügbar,2406.01940v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical   Understanding of Outdoor Scene,1970,"  Learning on 3D scene-based point cloud has received extensive attention as its promising application in many fields, and well-annotated and multisource datasets can catalyze the development of those data-driven approaches. To facilitate the research of this area, we present a richly-annotated 3D point cloud dataset for multiple outdoor scene understanding tasks and also an effective learning framework for its hierarchical segmentation task. The dataset was generated via the photogrammetric processing on unmanned aerial vehicle (UAV) images of the National University of Singapore (NUS) campus, and has been point-wisely annotated with both hierarchical and instance-based labels. Based on it, we formulate a hierarchical learning problem for 3D point cloud segmentation and propose a measurement evaluating consistency across various hierarchies. To solve this problem, a two-stage method including multi-task (MT) learning and hierarchical ensemble (HE) with consistency consideration is proposed. Experimental results demonstrate the superiority of the proposed method and potential advantages of our hierarchical annotations. In addition, we benchmark results of semantic and instance segmentation, which is accessible online at https://3d.dataset.site with the dataset and all source codes. ",https://doi.org/10.1145/3394171.3413661,2008.04968v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Longitudinal wave function control in single quantum dots with an   applied magnetic field,1970,"  Controlling single-particle wave functions in single semiconductor quantum dots is in demand to implement solid-state quantum information processing and spintronics. Normally, particle wave functions can be tuned transversely by an perpendicular magnetic field. We report a longitudinal wave function control in single quantum dots with a magnetic field. For a pure InAs quantum dot with a shape of pyramid or truncated pyramid, the hole wave function always occupies the base because of the less confinement at base, which induces a permanent dipole oriented from base to apex. With applying magnetic field along the base-apex direction, the hole wave function shrinks in the base plane. Because of the linear changing of the confinement for hole wave function from base to apex, the center of effective mass moves up during shrinking process. Due to the uniform confine potential for electrons, the center of effective mass of electrons does not move much, which results in a permanent dipole moment change and an inverted electron-hole alignment along the magnetic field direction. Manipulating the wave function longitudinally not only provides an alternative way to control the charge distribution with magnetic field but also a new method to tune electron-hole interaction in single quantum dots. ",https://doi.org/10.1038/srep08041,1501.07853v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Electronic structure calculations of twisted multi-layer graphene   superlattices,1970,"  Quantum confinement endows two-dimensional (2D) layered materials with exceptional physics and novel properties compared to their bulk counterparts. Although certain two- and few-layer configurations of graphene have been realized and studied, a systematic investigation of the properties of arbitrarily layered graphene assemblies is still lacking. We introduce theoretical concepts and methods for the processing of materials information, and as a case study, apply them to investigate the electronic structure of multi-layer graphene-based assemblies in a high-throughput fashion. We provide a critical discussion of patterns and trends in tight binding band structures and we identify specific layered assemblies using low-dispersion electronic bands as indicators of potentially interesting physics like strongly correlated behavior. A combination of data-driven models for visualization and prediction is used to intelligently explore the materials space. This work more generally aims to increase confidence in the combined use of physics-based and data-driven modeling for the systematic refinement of knowledge about 2D layered materials, with implications for the development of novel quantum devices. ",https://doi.org/10.1088/2053-1583/ab8f62,2001.11633v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Proving Theorems Recursively,1970,"  Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation. However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs. To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover. Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture. Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels. This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels. Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods. POETRY on miniF2F achieves an average proving success rate improvement of 5.1%. Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26. ",Kein DOI-Link verfügbar,2405.14414v1,Yes,potent(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Optical Extinctions of Inter-Arm Molecular Clouds in M31: A Pilot Study   for the Upcoming CSST Observations,1970,"  Recent sub-millimeter dust thermal emission observations have unveiled a significant number of inter-arm massive molecular clouds in M31.However,the effectiveness of this technique is limited to its sensitivity,making it challenging to study more distant galaxies.This study introduces an alternative approach,utilizing optical extinctions derived from space-based telescopes,with a focus on the forthcoming China Space Station Telescope(CSST).We first demonstrate the capability of this method by constructing dust extinction maps for 17 inter-arm massive molecular clouds in M31 using the Panchromatic Hubble Andromeda Treasury(PHAT) data.Our analysis reveals that inter-arm massive molecular clouds with an optical extinction(AV) greater than 1.6 mag exhibit a notable AV excess,facilitating their identification.The majority of these inter-arm massive molecular clouds show an AV around 1 mag,aligning with measurements from our JCMT data.Further validation using a mock CSST RGB star catalog confirms the method's effectiveness.We show that the derived AV values using CSST z and y photometries align more closely with the input values.Molecular clouds with AV>1.6 mag can also be identified using the CSST mock data.We thus claim that future CSST observation could provide an effective way for the detection of inter-arm massive molecular clouds with significant optical extinction in nearby galaxies. ",https://doi.org/10.1088/1674-4527/ad47dc,2405.19710v1,Yes,notable(1)
0000-0002-0785-707X,Jing Tang,The Hong Kong University of Science and Technology,Large g factor in bilayer WS$_2$ flakes,1970,"  The valley of transition metal dichalcogenides provides an additional platform to manipulate spin due to its unique selection rule. Normally, intralayer optical transitions in magnetic field show a Zeeman splitting with g factor of about $-4$. Here we report remarkable valley Zeeman effect exhibited by splitting of excitonic emission in a bilayer WS$_{2}$, with a value of g factor as large as $-16.5$. The observed large g factor results from the interlayer recombination, as the conduction band and valence band are modified in opposite directions by magnetic field in different layers. The interlayer recombination is due to the defect induced inversion symmetry breaking, which is theoretically not accessible in ideal bilayer WS$_{2}$ with inversion symmetry. Large g factor of interlayer emission offers potential benefits for future optical spin control and detection. ",https://doi.org/10.1063/1.5087440,1903.09329v1,Yes,potent(1)
0000-0002-7961-793X,Cheng Cheng,The Hong Kong University of Science and Technology,Constraint on inflation model from BICEP2 and WMAP 9-year data,1970,"  Even though Planck data released in 2013 (P13) is not compatible with Background Imaging of Cosmic Extragalactic Polarization (B2) and some local cosmological observations, including Supernova Legacy Survey (SNLS) samples and $H_0$ prior from Hubble Space Telescope (HST) etc, Wilkinson Microwaves Anisotropy Probe 9-year data (W9) is consistent with all of them in the base six-parameter $\Lambda$CDM+tensor cosmology quite well. In this letter, we adopt the combinations of B2+W9 and B2+W9+SNLS+BAO+HST to constrain the cosmological parameters in the base six-parameter $\Lambda$CDM+tensor model with $n_t=-r/8$, where r and $n_t$ are the tensor-to-scalar ratio and the tilt of relic gravitational wave spectrum, and BAO denotes Baryon Acoustic Oscillation. We find that the Harrison-Zel'dovich (HZ) scale invariant scalar power spectrum is consistent with both data combinations, chaotic inflation is marginally disfavored by the data at around $2\sigma$ level, but the power-law inflation model and the inflation model with inverse power-law potential can fit the data nicely. ",https://doi.org/10.1142/S0218271815410011,1404.1230v2,Yes,potent(1)
0000-0002-7961-793X,Cheng Cheng,The Hong Kong University of Science and Technology,Activating Wider Areas in Image Super-Resolution,1970,"  The prevalence of convolution neural networks (CNNs) and vision transformers (ViTs) has markedly revolutionized the area of single-image super-resolution (SISR). To further boost the SR performances, several techniques, such as residual learning and attention mechanism, are introduced, which can be largely attributed to a wider range of activated area, that is, the input pixels that strongly influence the SR results. However, the possibility of further improving SR performance through another versatile vision backbone remains an unresolved challenge. To address this issue, in this paper, we unleash the representation potential of the modern state space model, i.e., Vision Mamba (Vim), in the context of SISR. Specifically, we present three recipes for better utilization of Vim-based models: 1) Integration into a MetaFormer-style block; 2) Pre-training on a larger and broader dataset; 3) Employing complementary attention mechanism, upon which we introduce the MMA. The resulting network MMA is capable of finding the most relevant and representative input pixels to reconstruct the corresponding high-resolution images. Comprehensive experimental analysis reveals that MMA not only achieves competitive or even superior performance compared to state-of-the-art SISR methods but also maintains relatively low memory and computational overheads (e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the scale of 2). Furthermore, MMA proves its versatility in lightweight SR applications. Through this work, we aim to illuminate the potential applications of state space models in the broader realm of image processing rather than SISR, encouraging further exploration in this innovative direction. ",Kein DOI-Link verfügbar,2403.08330v1,Yes,"innovative(1), versatile(1), potent(2)"
0000-0002-7961-793X,Cheng Cheng,The Hong Kong University of Science and Technology,Phase-resolved imaging of edge-mode spin waves using scanning   transmission x-ray microscopy,1970,"  We have imaged the excitation of small-amplitude spin-wave eigenmodes, localized within $\sim$ 100 nm of the vertices of nanoscale Ni$_{81}$Fe$_{19}$ ellipses, using time-resolved scanning transmission x-ray microscopy (STXM) at 2 GHz and resolution of 70 nm. Taking advantage of the buried-layer sensitivity of STXM, we find that the magnetization precession at the two vertices changes from predominantly in-phase to out-of-phase in samples with and without a conductive layer deposited over the ellipses. As a plausible interpretation for the reversal in phase, we propose that unexpectedly strong Oersted fields are generated in the discontinuous overlayer, although effects of edge roughness cannot be fully excluded. The results demonstrate the capabilities of STXM to image small-amplitude, GHz magnetization dynamics with the potential to map rf magnetic fields on the nanoscale. ",https://doi.org/10.1016/j.jmmm.2016.09.096,1404.6350v2,Yes,potent(1)
0000-0002-7961-793X,Cheng Cheng,The Hong Kong University of Science and Technology,Constraint on the primordial gravitational waves from the joint analysis   of BICEP2 and Planck HFI 353 GHz dust polarization data,1970,"  We make a joint analysis of BICEP2 and recently released Planck HFI 353 GHz dust polarization data, and find that there is no evidence for the primordial gravitational waves and the bound on the tensor-to-scalar ratio becomes $r<0.083$ at $95%$ confidence level in the base $\Lambda$CDM + tensor model. Extending to the model with running of scalar spectral index, the bound is a little bit relaxed to $r<0.116$ at $95%$ confidence level. Our results imply that the inflation model with a single monomial potential is marginally disfavored at around $95%$ confidence level. Especially, the $m^2\phi^2/2$ inflation model is disfavored at more than $2\sigma$ level. However, the Starobinsky inflation model gives a nice fit. ",https://doi.org/10.1088/1475-7516/2014/12/044,1409.7025v3,Yes,potent(1)
0000-0002-7961-793X,Cheng Cheng,The Hong Kong University of Science and Technology,Spatially Distributed Sampling and Reconstruction,1970,"  A spatially distributed system contains a large amount of agents with limited sensing, data processing, and communication capabilities. Recent technological advances have opened up possibilities to deploy spatially distributed systems for signal sampling and reconstruction. In this paper, we introduce a graph structure for a distributed sampling and reconstruction system by coupling agents in a spatially distributed system with innovative positions of signals. A fundamental problem in sampling theory is the robustness of signal reconstruction in the presence of sampling noises. For a distributed sampling and reconstruction system, the robustness could be reduced to the stability of its sensing matrix. In a traditional centralized sampling and reconstruction system, the stability of the sensing matrix could be verified by its central processor, but the above procedure is infeasible in a distributed sampling and reconstruction system as it is decentralized. In this paper, we split a distributed sampling and reconstruction system into a family of overlapping smaller subsystems, and we show that the stability of the sensing matrix holds if and only if its quasi-restrictions to those subsystems have uniform stability. This new stability criterion could be pivotal for the design of a robust distributed sampling and reconstruction system against supplement, replacement and impairment of agents, as we only need to check the uniform stability of affected subsystems. In this paper, we also propose an exponentially convergent distributed algorithm for signal reconstruction, that provides a suboptimal approximation to the original signal in the presence of bounded sampling noises. ",Kein DOI-Link verfügbar,1511.08541v1,Yes,"innovative(1), pivotal(1)"
0000-0001-9674-2903,Jiaying Wu,"Imperial College London, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology - Guangzhou Campus",Fake News in Sheep's Clothing: Robust Fake News Detection Against   LLM-Empowered Style Attacks,1970,"  It is commonly perceived that fake news and real news exhibit distinct writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the advent of powerful Large Language Models (LLMs) has empowered malicious actors to mimic the style of trustworthy news sources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals that LLM-camouflaged fake news content significantly undermines the effectiveness of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), implying a severe vulnerability to stylistic variations. To address this, we introduce SheepDog, a style-robust fake news detector that prioritizes content over style in determining news veracity. SheepDog achieves this resilience through (1) LLM-empowered news reframings that inject style diversity into the training process by customizing articles to match different styles; (2) a style-agnostic training scheme that ensures consistent veracity predictions across style-diverse reframings; and (3) content-focused veracity attributions that distill content-centric guidelines from LLMs for debunking fake news, offering supplementary cues and potential intepretability that assist veracity prediction. Extensive experiments on three real-world benchmarks demonstrate SheepDog's style robustness and adaptability to various backbones. ",https://doi.org/10.1145/3637528.3671977,2310.10830v2,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Unique Neutrino Mass Operator at any Mass Dimension,1970,"  When the standard model is viewed as a low energy effective theory, the neutrinos can obtain mass from higher dimensional operators. It has been known for long that such an operator first appears at mass dimension five and that it is unique. Here we show that the effective neutrino mass operator at every higher dimension is unique. This general claim is established using Young tableau, and illustrated by exhausting all potentially different operators at dimension seven. The result is relevant to the search of new physics effects beyond neutrino mass that can arise at a relatively low energy scale. ",https://doi.org/10.1016/j.physletb.2010.10.005,1009.1692v2,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Probabilistically Masked Language Model Capable of Autoregressive   Generation in Arbitrary Word Order,1970,"  Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU tasks. ",Kein DOI-Link verfügbar,2004.11579v1,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,A theoretical investigation on the transport properties of armchair   biphenylene nanoribbons,1970,"  Armchair biphenylene nanoribbons are investigated by using density functional theory. The nanoribbon that contains one biphenylene subunit in a unit cell is a semiconductor with a direct band gap larger than 1 eV, while that containing four biphenylene subunits is a metal. The semiconducting nanoribbon has high electron mobility of 57174 cm2V-1s-1, superior to armchair graphene nanoribbons. Negative differential resistance behavior is observed in two electronic devices composed of the semiconducting and metallic nanoribbons. The on/off ratios are in the order of 10^3. All these indicate that armchair biphenylene nanoribbons are potential candidates for ultra-small logic devices. ",https://doi.org/10.1016/j.cplett.2016.02.011,1512.01004v1,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Theoretical prediction of two-dimensional CrOF sheet as a ferromagnetic   semiconductor or a half-metal,1970,"  Two-dimensional chromium oxide fluoride CrOF sheet was studied based on density functional theory. The investigation indicates that the CrOF sheet is an intrinsic ferromagnetic semiconductor. The calculated low cleavage energy implies that the ferromagnetic semiconductor can be exfoliated from its bulk form. The corresponding Curie temperature is 150 K. In particular, the Curie temperature increases up to 410 K under hole doping and the CrOF sheet becomes a half-metal. The versatile electronic and magnetic properties indicate that the two-dimensional CrOF sheet can be a promising candidate for next-generation spintronic devices. ",https://doi.org/10.1016/j.chemphys.2018.08.007,1804.01621v1,Yes,versatile(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Dark matter and LHC phenomenology of a scale invariant scotogenic model,1970,"  We study the phenomenology of a model that addresses the neutrino mass, dark matter, and generation of the electroweak scale in a single framework. Electroweak symmetry breaking is realized via the Coleman-Weinberg mechanism in a classically scale invariant theory, while the neutrino mass is generated radiatively through interactions with dark matter in a typically scotogenic manner. The model introduces a scalar triplet and singlet and a vector-like fermion doublet that carry an odd parity of $Z_2$, and an even parity scalar singlet that helps preserve classical scale invariance. We sample over the parameter space by taking into account various experimental constraints from the dark matter relic density and direct detection, direct scalar searches, neutrino mass, and charged lepton flavor violating decays. We then examine by detailed simulations possible signatures at the LHC to find some benchmark points of the free parameters. We find that the future high-luminosity LHC will have a significant potential in detecting new physics signals in the dilepton channel. ",https://doi.org/10.1088/1674-1137/43/10/103102,1811.01180v3,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Theoretical investigation on electronic properties and carrier   mobilities of armchair graphyne nanoribbons,1970,"  Seven types of armchair graphyne nanoribbons are investigated with HSE06 functional. The quantum confinements in the graphyne nanoribbons open or increase the band gaps of the corresponding two-dimensional graphynes, which is crucial to high on/off ratio in electronic device operation. The major carrier mobilities of the graphyne nanoribbons with high percentage of sp hybridized carbon atoms are very large. The sparse linking pattern results in small number of frontier crystal orbitals and small deformation potential constants, which are responsible for the large carrier mobilities. Some graphyne nanoribbons have band gaps larger than 0.4 eV. Meanwhile, they have both high hole and electron mobilities. These benefit current complementary circuit with low power dissipation. Especially, the hole and electron mobilities of 14,14,18-graphyne nanoribbons are more than an order larger than those of the armchair graphene nanoribbons, indicating that they have potential applications in high speed electronic devices. ",https://doi.org/10.1016/j.chemphys.2015.05.029,1410.4629v1,Yes,potent(2)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,A mathematical form of force-free magnetosphere equation around Kerr   black holes and its application to Meissner effect,1970,"  Based on the Lagrangian of the steady axisymmetric force-free magnetosphere (FFM) equation around Kerr black holes(KBHs), we find that the FFM equation can be rewritten in a new form as $f_{,rr} / (1-\mu^{2}) + f_{,\mu\mu} / \Delta + K(f(r,\mu),r,\mu) = 0$, where $\mu = -\cos\theta$. By coordinate transformation, the form of the above equation can be given by $s_{,yy} + s_{,zz} + D(s(y,z),y,z) = 0$. Based on the form, we prove finally that the Meissner effect is not possessed by a KBH-FFM with the condition where $d\omega/d A_{\phi} \leqslant 0$ and $H_{\phi}(dH_{\phi}/dA_{\phi}) \geqslant 0$, here $A_{\phi}$ is the $\phi$ component of the vector potential $\vec{A}$, $\omega$ is the angular velocity of magnetic fields and ${H_{\phi}}$ corresponds to twice the poloidal electric current. ",https://doi.org/10.1016/j.physletb.2016.06.047,1603.08411v3,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Renormalization Group Evolution of Dimension-seven Operators in Standard   Model Effective Field Theory and Relevant Phenomenology,1970,"  We showed in a previous publication that there are six independent dimension-seven operators violating both lepton and baryon numbers ($L=-B=1$) and twelve ones violating lepton but preserving baryon number ($L=2,~B=0$) in standard model effective field theory, and we calculated one-loop renormalization for the former six operators. In this work we continue our efforts on renormalization of the operators. It turns out this could become subtle because the operators are connected by nontrivial relations when fermion flavors are counted. This kind of relations does not appear in lower dimensional operators. We show how we can extract anomalous dimension matrix for a flavor-specified basis of operators from counterterms computed for the above flavor-blind operators without introducing singular inverse Yukawa coupling matrices. As a phenomenological application, we investigate renormalization group effects on nuclear neutrinoless double $\beta$ decay. We also discuss very briefly its analog in the meson sector, $K^\pm\to\pi^\mp\mu^\pm\mu^\pm$, and indicate potential difficulties to compute its decay width. ",https://doi.org/10.1007/JHEP03(2019)179,1901.10302v2,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Revisiting general dark matter-bound-electron interactions,1970,"  In this letter we revisit general dark matter (DM)-bound-electron interactions studied previously in the influential work of [Catena et al., Phys. Rev. Res. 2, 033195 (2020)]. We derive the DM-electron response functions and find a crucial minus sign was missed for the second atomic response function $W_2$ defined in that work. The minus sign has significant phenomenological consequences when explaining experimental bounds on specific DM scenarios. Furthermore, for the most general DM-electron nonrelativistic or relativistic interactions for DM with spin up to one, we find there are three DM response functions ($a_{0,1,2}$) whose corresponding atomic response functions ($\widetilde W_{0,1,2}$) are linear combinations of the four response functions ($W_{1,2,3,4}$) given in that work, $$ \widetilde W_0 = W_1, \, \widetilde W_2 = |\mathbf{v}_0^\perp|^2 W_1+ W_3 - 2 {m_e\, \mathbf{q}\cdot \mathbf{v}_0^\perp \over \mathbf{q}^2} W_2,\, \widetilde W_3 = { (\mathbf{q}\cdot \mathbf{v}_0^\perp)^2 \over \mathbf{q}^2} W_1 + {m_e^2 \over \mathbf{q}^2}W_4 - 2 {m_e\, \mathbf{q}\cdot \mathbf{v}_0^\perp \over \mathbf{q}^2} W_2. $$ Due to the minus sign correction for $W_2$, there can be significant cancellations between the $W_2$ and $W_{3,4}$ terms, so that $\widetilde W_{2,3}$ are dominated by the usual response function $W_1$ in some cases. Ignoring the sign could thus result in misinterpretation of the experimental data in some DM scenarios. As an example, we show that the recent XENON1T constraint on the fermionic DM anapole moment is weakened by a factor of 2 or so. Many DM scenarios involving DM or electron axial-vector current can yield $W_2$ and thus are potentially affected by the sign. ",Kein DOI-Link verfügbar,2405.04855v1,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,Phenomenology in Minimal Cascade Seesaw for Neutrino Mass,1970,"  We make a comprehensive analysis on the phenomenology in the minimal version of cascade seesaw for tiny neutrino mass. The seesaw induces at tree level a neutrino mass operator at dimension nine, by introducing a quadruple scalar $\Phi$ of hypercharge unity and a quintuple fermion $\Sigma$ of hypercharge zero. We work in a framework that handles the complicated Yukawa couplings in a nice way without losing generality. All mixing matrices are essentially expressed in terms of the vacuum expectation value of the quadruple scalar v_\Phi, a free complex parameter, and known neutrino parameters. We show that the low-energy lepton flavor violating transitions of the charged leptons set strong constraints on the free parameters. The constraints have a significant impact on collider physics, and are incorporated in our signal analysis at the LHC. We investigate the signatures of new particles by surveying all potentially important channels. We find that the 4j2\ell^\pm signal is most important for the detection of the scalars and the 2\ell^{\pm}2\ell^{\mp}2j, 3\ell^{\pm}\ell^{\mp}2j and 3\ell^{\pm}2\ell^{\mp}+\cancel{E_T} signals are quite promising for the fermions. ",https://doi.org/10.1103/PhysRevD.89.115024,1403.2040v2,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language   Model Pre-training,1970,"  The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of ""data commonness"", a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs. ",Kein DOI-Link verfügbar,2407.06654v1,Yes,potent(1)
0000-0002-0414-439X,Yi Liao,The Hong Kong University of Science and Technology Library,A resource-efficient deep learning framework for low-dose brain PET   image reconstruction and analysis,1970,"  18F-fluorodeoxyglucose (18F-FDG) Positron Emission Tomography (PET) imaging usually needs a full-dose radioactive tracer to obtain satisfactory diagnostic results, which raises concerns about the potential health risks of radiation exposure, especially for pediatric patients. Reconstructing the low-dose PET (L-PET) images to the high-quality full-dose PET (F-PET) ones is an effective way that both reduces the radiation exposure and remains diagnostic accuracy. In this paper, we propose a resource-efficient deep learning framework for L-PET reconstruction and analysis, referred to as transGAN-SDAM, to generate F-PET from corresponding L-PET, and quantify the standard uptake value ratios (SUVRs) of these generated F-PET at whole brain. The transGAN-SDAM consists of two modules: a transformer-encoded Generative Adversarial Network (transGAN) and a Spatial Deformable Aggregation Module (SDAM). The transGAN generates higher quality F-PET images, and then the SDAM integrates the spatial information of a sequence of generated F-PET slices to synthesize whole-brain F-PET images. Experimental results demonstrate the superiority and rationality of our approach. ",Kein DOI-Link verfügbar,2202.06548v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Generalized vector potential and Trace Theorem for Lipschitz domains,1970,"  The vector potential is a fundamental concept widely applied across various fields. This paper presents an existence theorem of a vector potential for divergence-free functions in $W^{m,p}(\mathbb{R}^N,\mathbb{T})$ with general $m,p,N$. Based on this theorem, one can establish the space decomposition theorem for functions in $W^{m,p}_0(\operatorname{curl};\Omega,\mathbb{R}^N)$ and the trace theorem for functions in $W^{m,p}(\Omega)$ within the Lipschitz domain $\Omega \subset \mathbb{R}^N$. The methods of proof employed in this paper are straightforward, natural, and consistent. ",Kein DOI-Link verfügbar,2405.05228v1,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Potential precision of a direct measurement of the Higgs boson total   width at a muon colliderr,1970,"  In the light of the discovery of a 126 GeV Standard-Model-like Higgs boson at the LHC, we evaluate the achievable accuracies for direct measurements of the width, mass, and the s-channel resonant production cross section of the Higgs boson at a proposed muon collider. We find that with a beam energy resolution of R=0.01% (0.003%) and integrated luminosity of 0.5 fb^{-1} (1 fb^{-1}), a muon collider would enable us to determine the Standard-Model-like Higgs width to +/- 0.35 MeV (+/- 0.15 MeV) by combining two complementary channels of the WW^* and b\bar b final states. A non-Standard-Model Higgs with a broader width is also studied. The unparalleled accuracy potentially attainable at a muon collider would test the Higgs interactions to a high precision. ",https://doi.org/10.1103/PhysRevD.87.033007,1210.7803v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Long-lived particles at the LHC: catching them in time,1970,"  We explore the physics potential of using precision timing information at the LHC in searches for long-lived particles (LLPs). In comparison with the light Standard Model particles, the decay products of massive LLPs arrive at detectors with time delays around the nanosecond scale. We propose new strategies to take advantage of this time delay feature by using initial state radiation to timestamp the collision event and require at least one LLP to decay within the detector. This search strategy is effective for a broad range of models. In addition to outlining this general approach, we demonstrate its effectiveness with the projected reach for two benchmark scenarios: Higgs decaying into a pair of LLPs, and pair production of long-lived neutralinos in the gauge mediated supersymmetry breaking models. Our strategy increases the sensitivity to the lifetime of the LLP by two orders of magnitude or more and particularly exhibits a better behavior with a linear dependence on lifetime in the large lifetime region compared to traditional LLP searches. The timing information significantly reduces the Standard Model background and provides a powerful new dimension for LLP searches. ",https://doi.org/10.1103/PhysRevLett.122.131801,1805.05957v3,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Interference in the $gg\rightarrow h \rightarrow γγ$ On-Shell   Rate and the Higgs Boson Total Width,1970,"  We consider interference between the Higgs signal and QCD background in $gg\rightarrow h \rightarrow \gamma\gamma$ and its effect on the on-shell Higgs rate. The existence of sizable strong phases leads to destructive interference of about 2% of the on-shell cross section in the Standard Model. This effect can be enhanced by beyond the standard model physics. In particular, since it scales differently from the usual rates, the presence of interference allows indirect limits to be placed on the Higgs width in a novel way, using on-shell rate measurements. Our study motivates further QCD calculations to reduce uncertainties. We discuss potential width-sensitive observables, both using total and differential rates and find that the HL-LHC can potentially indirectly probe widths of order tens of MeV. ",https://doi.org/10.1103/PhysRevLett.119.181801,1704.08259v1,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Millicharged Particles in Liquid Argon Neutrino Experiments,1970,"  We investigate the potential of Liquid Argon (LAr) neutrino detectors to search for millicharged particles, a well-motivated extension of the standard model. Detectors located downstream of an intense proton beam that is striking a target may be exposed to a large flux of millicharged particles. Millicharged particles interact primarily through low momentum exchange producing electron recoil events near detector threshold. Recently, sub-MeV detection capabilities were demonstrated by the Fermilab ArgoNeuT detector, a small LAr detector which was exposed to the NuMI neutrino beam. Despite high background rates and its small size, we show that ArgoNeuT is capable of probing unexplored parameter space with its existing dataset. In particular, we show that the excellent spatial resolution in LAr detectors allows rejecting backgrounds by requiring two soft hits that are aligned with the upstream target. We further discuss the prospects of these types of searches in future larger LAr neutrino detectors such as the DUNE near detector. ",https://doi.org/10.1007/JHEP07(2019)170,1902.03246v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Probing Electroweak Phase Transition via Enhanced Di-Higgs Production,1970,"  We consider a singlet extension of the Standard Model (SM) with a spontaneous $Z_2$ breaking and study the gluon-gluon fusion production of the heavy scalar, with subsequent decay into a pair of SM-like Higgs bosons. We find that an on-shell interference effect can notably enhance the resonant di-Higgs production rate up to 40\%. In addition, consistently taking into account both the on-shell and off-shell interference effects between the heavy scalar and the SM di-Higgs diagrams significantly improves the HL-LHC and HE-LHC reach in this channel. As an example, within an effective field theory analysis in an explicitly $Z_2$ breaking scenario, we further discuss the potential to probe the parameter region compatible with a first order electroweak phase transition. Our analysis is applicable for general potentials of the singlet extension of the SM as well as for more general resonance searches. ",https://doi.org/10.1103/PhysRevD.97.095032,1801.00794v2,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),A New Approach to Electroweak Symmetry Non-Restoration,1970,"  Electroweak symmetry non-restoration up to high temperatures well above the electroweak scale offers new alternatives for baryogenesis. We propose a new approach for electroweak symmetry non-restoration via an inert Higgs sector that couples to the Standard Model Higgs as well as an extended scalar singlet sector. We implement renormalization group improvements and thermal resummation, necessary to evaluate the effective potential spanning over a broad range of energy scales and temperatures. We present examples of benchmark scenarios that allow for electroweak symmetry non-restoration all the way up to hundreds of TeV temperatures, and also feature suppressed sphaleron washout factors down to the electroweak scale. Our method for transmitting the Standard Model broken electroweak symmetry to an inert Higgs sector has several intriguing implications for (electroweak) baryogenesis, early universe thermal histories, and can be scrutinized through Higgs physics phenomenology and electroweak precision measurements at the HL-LHC. ",https://doi.org/10.1103/PhysRevD.104.055016,2104.00638v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Enhancing Searches for Heavy QCD Axions via Dimuon Final States,1970,"  Heavy QCD axions are well-motivated extensions of the QCD axion that address the quality problem while still solving the strong CP problem. Owing to the gluon coupling, critical for solving the strong CP problem, these axions can be produced in significant numbers in beam dump and collider environments for axion decay constants as large as PeV, relevant for addressing the axion quality problem. In addition, if these axions have leptonic couplings, they can give rise to long-lived decay into lepton pairs, in particular, dominantly into muons above the dimuon threshold and below the GeV scale in a broad class of axion models. Considering existing constraints, primarily from rare meson decays, we demonstrate that current and future neutrino facilities and long-lived particle searches have the potential to probe significant parts of the heavy QCD axion parameter space via dimuon final states. ",https://doi.org/10.1007/JHEP02(2023)111,2210.02462v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Higgs Width and Couplings at High Energy Muon Colliders with Forward   Muon Detection,1970,"  We propose a novel method using the $ZZ$-fusion channel and forward muon detection at high-energy muon colliders to address the challenge of the Higgs coupling-width degeneracy. Our approach enables inclusive Higgs rate measurement to 0.75% at 10~TeV muon collider, breaking the coupling-width degeneracy. Results indicate the potential to refine Higgs coupling to sub-percent levels and estimate its total width within (-0.41%, +2.1%). Key insights include the effectiveness of forward muon tagging in signal-background separation despite broad recoil mass distribution due to muon energy reconstruction and beam energy spread. The study emphasizes the significance of muon rapidity coverage up to $|\eta (\mu)|<6$, enhancing measurement precision. Our findings highlight the unique capabilities of high-energy lepton colliders for model-independent Higgs coupling determination and lay the groundwork for future advancements in muon collider technology and Higgs physics research. ",Kein DOI-Link verfügbar,2401.08756v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Sleptonic SUSY: From UV Framework to IR Phenomenology,1970,"  We study an attractive scenario, ""Sleptonic SUSY"", which reconciles the $125$ GeV Higgs scalar and the non-observation of superpartners thus far with potentially pivotal roles for slepton phenomenology: providing viable ongoing targets for LHC discovery, incorporating a co-annihilation partner for detectable thermal relic dark matter, and capable of mediating the potential muon $g-2$ anomaly. This is accomplished by a modestly hierarchical spectrum, with sub-TeV sleptons and electroweakinos and with multi-TeV masses for the other new states. We study new elements in the UV MSSM realization of Sleptonic SUSY based on higher-dimensional sequestering and the synergy between the resulting gaugino-mediation, hypercharge $D$-term mediation and Higgs-mediation of SUSY-breaking, so as to more fully capture the range of possibilities. This framework stands out by harmoniously solving the flavor, CP and $\mu - B\mu$ problems of the supersymmetric paradigm. We discuss its extension to orbifold GUTs, including gauge-coupling and $b$-tau unification. We also develop a non-minimal model with extra Higgs fields, in which the electroweak vacuum is more readily cosmologically stable against decay to a charge-breaking vacuum, allowing a broader range of sleptonic spectra than in the MSSM alone. We survey the rich set of signals possible at the LHC and future colliders, covering both $R$-parity conservation and violation, as well as for dark matter detection. While the multi-TeV squarks imply a Little Hierarchy Problem, intriguingly, small changes in parameter space to improve naturalness result in dramatic phase transitions to either electroweak-preservation or charge-breaking. In a Multiverse setting, the modest unnaturalness may then be explained by the ""principle of living dangerously"". ",https://doi.org/10.1007/JHEP09(2022)142,2203.01796v3,Yes,"pivotal(1), potent(2)"
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Enhancing Transferability of Targeted Adversarial Examples: A   Self-Universal Perspective,1970,"  Transfer-based targeted adversarial attacks against black-box deep neural networks (DNNs) have been proven to be significantly more challenging than untargeted ones. The impressive transferability of current SOTA, the generative methods, comes at the cost of requiring massive amounts of additional data and time-consuming training for each targeted label. This results in limited efficiency and flexibility, significantly hindering their deployment in practical applications. In this paper, we offer a self-universal perspective that unveils the great yet underexplored potential of input transformations in pursuing this goal. Specifically, transformations universalize gradient-based attacks with intrinsic but overlooked semantics inherent within individual images, exhibiting similar scalability and comparable results to time-consuming learning over massive additional data from diverse classes. We also contribute a surprising empirical insight that one of the most fundamental transformations, simple image scaling, is highly effective, scalable, sufficient, and necessary in enhancing targeted transferability. We further augment simple scaling with orthogonal transformations and block-wise applicability, resulting in the Simple, faSt, Self-universal yet Strong Scale Transformation (S$^4$ST) for self-universal TTA. On the ImageNet-Compatible benchmark dataset, our method achieves a 19.8% improvement in the average targeted transfer success rate against various challenging victim models over existing SOTA transformation methods while only consuming 36% time for attacking. It also outperforms resource-intensive attacks by a large margin in various challenging settings. ",Kein DOI-Link verfügbar,2407.15683v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Bayesian Poisson Log-normal Model with Regularized Time Structure for   Mortality Projection of Multi-population,1970,"  The improvement of mortality projection is a pivotal topic in the diverse branches related to insurance, demography, and public policy. Motivated by the thread of Lee-Carter related models, we propose a Bayesian model to estimate and predict mortality rates for multi-population. This new model features in information borrowing among populations and properly reflecting variations of data. It also provides a solution to a long-time overlooked problem: model selection for dependence structures of population-specific time parameters. By introducing a Dirac spike function, simultaneous model selection and estimation for population-specific time effects can be achieved without much extra computation cost. We use the Japanese mortality data from Human Mortality Database to illustrate the desirable properties of our model. ",Kein DOI-Link verfügbar,2010.04775v1,Yes,pivotal(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Diagnosis of a New Neutral Gauge Boson at the LHC and ILC for Snowmass   2013,1970,"  A U(1)' or Z' is generic in many scenarios of physics beyond the Standard Model, such as string theory compactifications, GUTs, extra-dimensions, compositeness, dynamical electroweak symmetry breaking, dark-sector models, etc. We study the potential of probing a TeV-scale Z' with electroweak couplings in future experiments. In particular, we focus on two scenarios: (1) If a Z' is discovered at the LHC, what is the potential of measuring its mass and width and to distinguish between benchmark models utilizing various observables, especially asymmetries, at a high luminosity LHC and the ILC. (2) If the Z' is not accessible as a clear resonance signal, what is the exclusion reach at the ILC. ",Kein DOI-Link verfügbar,1308.2738v1,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Radiative Return for Heavy Higgs Boson at a Muon Collider,1970,"  Higgs boson properties could be studied with a high accuracy at a muon collider via the s-channel resonant production. We consider the situation where the center-of-mass energy of the muon collider is off the resonance above the Higgs mass. We discuss the discovery potential for a generic heavy Higgs boson ($H$) and compare different production mechanisms, including the ""radiative return"" ($\gamma H$), $Z$-boson associated production ($ZH$) and heavy Higgs pair production ($HA$). These production mechanisms do not sensitively rely on a priori knowledge of the heavy Higgs boson mass. We include various types of Two Higgs Doublet Models for the comparison. We conclude that the radiative return process could provide an important option for both the heavy Higgs discovery and direct measurement of invisible decays at a high energy muon collider. ",https://doi.org/10.1103/PhysRevD.91.015008,1408.5912v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),WIMPs at High Energy Muon Colliders,1970,"  The Weakly Interacting Massive Particle (WIMP) paradigm is one of the most compelling scenarios for particle dark matter (DM). We show in this paper that a high energy muon collider can make decisive statements about the WIMP DM, and this should serve as one of its main physics driver cases. We demonstrate this by employing the DM as the lightest member of an electroweak (EW) multiplet, which is a simple, yet one of the most challenging WIMP scenarios given its minimal collider signature and high thermal target mass scale of 1 TeV$-$23 TeV. We perform a first study of the reach of high energy muon colliders, focusing on the simple, inclusive and conservative signals with large missing mass, through the mono-photon, VBF di-muon and a novel mono-muon channel. Using these inclusive signals, it is possible to cover the thermal targets of doublet and triplet with a 10 TeV muon collider. Higher energies, 14 TeV$-$75 TeV, would ensure a $5 \sigma$ reach above the thermal targets for the higher EW multiplets. We also estimate the reach of a search for disappearing tracks, demonstrating the potential significant enhancement of the sensitivity. ",https://doi.org/10.1103/PhysRevD.103.075004,2009.11287v3,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Probing Flavor Non-Universal Theories Through Higgs Physics at the LHC   and Future Colliders,1970,"  We explored the possibility that Higgs coupling to new physics violates flavor universality. In particular, we parameterize such models with dimension-six effective operators which modify the coupling between the first generation quarks, Higgs boson, and $Z$ boson. Through the use of boosted Higgsstrahlung events at both the HL-LHC and potential future hadron colliders, as well as existing ATLAS data for background estimates, we projected constraints on the scale of new physics as a function of the Wilson coefficient. The high energy $Zh$ process will provide unique information about these class of operators, and the sensitivity is competitive with the LEP electroweak precision measurements. We include different scenarios of the overall systematic uncertainties and the PDF uncertainties when presenting the projected sensitivities. We also discuss the constraints from FCNCs to these flavor-violating models and the complementarity of the exotic Higgs decay to the $Zh$ process. ",https://doi.org/10.1103/PhysRevD.101.035045,1909.04549v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with   Positive Forward Transfer,1970,"  Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. We discover a large discrepancy in different input embeddings by performing singular value decomposition (SVD) on input embeddings. This discrepancy results in the model learning irrelevant information for old and pre-trained tasks, leading to catastrophic forgetting and negative forward transfer. To address these issues, we propose Prompt Tuning with Positive Forward Transfer (Fwd-Prompt), a prompt-based method that projects the prompt gradient to the residual space to minimize interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research illuminates the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT. ",Kein DOI-Link verfügbar,2401.09181v3,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Learning towards Minimum Hyperspherical Energy,1970,"  Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization. ",Kein DOI-Link verfügbar,1805.09298v9,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Enhancing Sensitivities to Long-lived Particles with High Granularity   Calorimeters at the LHC,1970,"  The search for long-lived particles (LLP) is an exciting physics opportunity in the upcoming runs of the Large Hadron Collider. In this paper, we focus on a new search strategy of using the High Granularity Calorimeter (HGCAL), part of the upgrade of the CMS detector, in such searches. In particular, we demonstrate that the high granularity of the calorimeter with this upgrade, which allows us to see ""shower tracks"" in the calorimeter, can play a crucial role in identifying the signal and suppressing the background. We study the potential reach of the HGCAL using a signal model in which the Standard Model Higgs boson decays into a pair of LLPs, $h \to XX$. After carefully estimating the Standard Model QCD and the misreconstructed fake-track backgrounds, we give the projected reach for both a more conservative vector boson fusion trigger and a novel displaced-track-based trigger. Our results show that the best reach for the Higgs decay branching ratio, BR$(h \to XX)$, in the vector boson fusion channel is about $\mathcal{O}(10^{-4})$ with lifetime $c\tau_X \sim 0.1$--$1$ meters, while for the gluon gluon fusion channel it is about $\mathcal{O}(10^{-5}\text{--}10^{-6})$ for similar lifetimes. Alternatively, for an LLP with $c\tau_X \sim 10^3$ meters, the HGCAL based search should be able to probe BR$(h\to XX)$ down to a few $\times 10^{-4}$($10^{-2}$) in the gluon gluon fusion (vector boson fusion) channels, respectively. In comparison with these previous searches, our new search shows enhanced sensitivity in complementary regions of the LLP parameter space. ",https://doi.org/10.1007/JHEP11(2020)066,2005.10836v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Top Yukawa Coupling Determination at High Energy Muon Collider,1970,"  The Top Yukawa coupling profoundly influences several core mysteries linked to the electroweak scale and the Higgs boson. We study the feasibility of measuring the Top Yukawa coupling at high-energy muon colliders by examining the high-energy dynamics of the weak boson fusion to top quark pair processes. A deviation of the Top Yukawa coupling from the Standard Model would lead modified $V V \rightarrow t\bar{t}$ process, violating unitarity at high energy. Our analysis reveals that utilizing a muon collider with a center-of-mass energy of 10 TeV and an integrated luminosity of 10 ab$^{-1}$ allows us to investigate the Top Yukawa coupling with a precision surpassing 1.5\%, more than one order of magnitude better than the precision from $t\bar t h$ channel at muon colliders. This precision represents a notable enhancement compared to the anticipated sensitivities of the High-Luminosity LHC (3.4\%) and those at muon colliders derived from the $t\bar{t} H$ process. ",Kein DOI-Link verfügbar,2308.06323v2,Yes,notable(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Accidental Suppression of Wilson Coefficients in Higgs Coupling,1970,"  Higgs couplings are essential probes for physics beyond the Standard Model (BSM) since they can be modified by new physics, such as through the Higgs portal interaction $|H|^2\mathcal{O}$. These modifications influence Higgs interactions via dimension-6 operators of the form $ \left(\partial |H|^2\right)^2$ and $|H|^6$, which are generally expected to be of comparable size. This paper discusses a phenomenon of accidental suppression, where the $|H|^6$ coupling is significantly smaller than $\left(\partial |H|^2\right)^2$. This suppression, arising from the truncation of the tree-level effective potential, lacks a clear symmetry explanation but persists in portal models. This paper aims to inspire further studies on additional instances of accidental suppression without symmetry explanations or a general framework to characterize such suppression. We also discuss constraints, at the HL-LHC and future colliders, on the Wilson coefficients of the two dimension-6 operators for various benchmark scenarios of the concrete model. ",Kein DOI-Link verfügbar,2408.08948v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Remote estimation of geologic composition using interferometric   synthetic-aperture radar in California's Central Valley,1970,"  California's Central Valley is the national agricultural center, producing 1/4 of the nation's food. However, land in the Central Valley is sinking at a rapid rate (as much as 20 cm per year) due to continued groundwater pumping. Land subsidence has a significant impact on infrastructure resilience and groundwater sustainability. In this study, we aim to identify specific regions with different temporal dynamics of land displacement and find relationships with underlying geological composition. Then, we aim to remotely estimate geologic composition using interferometric synthetic aperture radar (InSAR)-based land deformation temporal changes using machine learning techniques. We identified regions with different temporal characteristics of land displacement in that some areas (e.g., Helm) with coarser grain geologic compositions exhibited potentially reversible land deformation (elastic land compaction). We found a significant correlation between InSAR-based land deformation and geologic composition using random forest and deep neural network regression models. We also achieved significant accuracy with 1/4 sparse sampling to reduce any spatial correlations among data, suggesting that the model has the potential to be generalized to other regions for indirect estimation of geologic composition. Our results indicate that geologic composition can be estimated using InSAR-based land deformation data. In-situ measurements of geologic composition can be expensive and time consuming and may be impractical in some areas. The generalizability of the model sheds light on high spatial resolution geologic composition estimation utilizing existing measurements. ",Kein DOI-Link verfügbar,2212.04813v1,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),A Light QCD Axion with Hilltop Misalignment,1970,"  We study the cosmological evolution of a light QCD axion and identify the parameter space to obtain the correct relic dark matter abundance. The axion potential is flattened at the origin, corresponding to the only minimum, while it is unsuppressed at $\pi$. These potential features arise by assuming a mirror sector with the strong CP phase $\bar\theta$ shifted by $\pi$ compared to the SM sector, which allows the mirror axion potential to be tuned against the usual QCD axion potential. Before the QCD phase transition, assuming the mirror sector is decoupled and much colder than the SM thermal bath, the mirror sector potential dominates, causing the axion to initially roll to a temporary minimum at $\pi$. However, after the QCD phase transition, the potential minimum changes, and the axion relaxes from the newly created ""hilltop"" near $\pi$ to the CP-conserving minimum at the origin. As the axion adiabatically tracks this shift in the potential minimum through the QCD phase transition, with non-adiabatic evolution near $\pi$ and 0, it alters the usual prediction of the dark matter abundance. Consequently, this ""hilltop"" misalignment mechanism opens new regions of axion parameter space, with the correct relic abundance while still solving the strong CP problem, that could be explored in future experiments. ",Kein DOI-Link verfügbar,2407.12930v1,Yes,potent(7)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Regularizing Neural Networks via Minimizing Hyperspherical Energy,1970,"  Inspired by the Thomson problem in physics where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy, hyperspherical energy minimization has demonstrated its potential in regularizing neural networks and improving their generalization power. In this paper, we first study the important role that hyperspherical energy plays in neural network training by analyzing its training dynamics. Then we show that naively minimizing hyperspherical energy suffers from some difficulties due to highly non-linear and non-convex optimization as the space dimensionality becomes higher, therefore limiting the potential to further improve the generalization. To address these problems, we propose the compressive minimum hyperspherical energy (CoMHE) as a more effective regularization for neural networks. Specifically, CoMHE utilizes projection mappings to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different designs for the projection mapping, we propose several distinct yet well-performing variants and provide some theoretical guarantees to justify their effectiveness. Our experiments show that CoMHE consistently outperforms existing regularization methods, and can be easily applied to different neural networks. ",Kein DOI-Link verfügbar,1906.04892v2,Yes,potent(3)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Predicting Gradient is Better: Exploring Self-Supervised Learning for   SAR ATR with a Joint-Embedding Predictive Architecture,1970,"  The growing Synthetic Aperture Radar (SAR) data has the potential to build a foundation model through Self-Supervised Learning (SSL) methods, which can achieve various SAR Automatic Target Recognition (ATR) tasks with pre-training in large-scale unlabeled data and fine-tuning in small labeled samples. SSL aims to construct supervision signals directly from the data, which minimizes the need for expensive expert annotation and maximizes the use of the expanding data pool for a foundational model. This study investigates an effective SSL method for SAR ATR, which can pave the way for a foundation model in SAR ATR. The primary obstacles faced in SSL for SAR ATR are the small targets in remote sensing and speckle noise in SAR images, corresponding to the SSL approach and signals. To overcome these challenges, we present a novel Joint-Embedding Predictive Architecture for SAR ATR (SAR-JEPA), which leverages local masked patches to predict the multi-scale SAR gradient representations of unseen context. The key aspect of SAR-JEPA is integrating SAR domain features to ensure high-quality self-supervised signals as target features. Besides, we employ local masks and multi-scale features to accommodate the various small targets in remote sensing. By fine-tuning and evaluating our framework on three target recognition datasets (vehicle, ship, and aircraft) with four other datasets as pre-training, we demonstrate its outperformance over other SSL methods and its effectiveness with increasing SAR data. This study showcases the potential of SSL for SAR target recognition across diverse targets, scenes, and sensors.Our codes and weights are available in \url{https://github.com/waterdisappear/SAR-JEPA. ",Kein DOI-Link verfügbar,2311.15153v5,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),A Survey on Time-Series Pre-Trained Models,1970,"  Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work. ",Kein DOI-Link verfügbar,2305.10716v1,Yes,potent(2)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),A Data-driven Human Responsibility Management System,1970,"  An ideal safe workplace is described as a place where staffs fulfill responsibilities in a well-organized order, potential hazardous events are being monitored in real-time, as well as the number of accidents and relevant damages are minimized. However, occupational-related death and injury are still increasing and have been highly attended in the last decades due to the lack of comprehensive safety management. A smart safety management system is therefore urgently needed, in which the staffs are instructed to fulfill responsibilities as well as automating risk evaluations and alerting staffs and departments when needed. In this paper, a smart system for safety management in the workplace based on responsibility big data analysis and the internet of things (IoT) are proposed. The real world implementation and assessment demonstrate that the proposed systems have superior accountability performance and improve the responsibility fulfillment through real-time supervision and self-reminder. ",Kein DOI-Link verfügbar,2012.03190v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Probing the Electroweak Phase Transition with Exotic Higgs Decays,1970,"  An essential goal of the Higgs physics program at the LHC and beyond is to explore the nature of the Higgs potential and shed light on the mechanism of electroweak symmetry breaking. An important class of models defining the strength and order of the electroweak phase transition is driven by the Higgs boson coupling to a light new state. This Snowmass white paper points out the existence of a region of parameter space where a strongly first order electroweak phase transition is compatible with exotic decays of the SM-like Higgs boson. A dedicated search for exotic Higgs decays can actively explore this framework at the Large Hadron Collider (LHC), while future exotic Higgs decay searches at the high-luminosity LHC and future Higgs factories will be vital to conclusively probe the scenario. ",Kein DOI-Link verfügbar,2203.08206v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),The Philosopher's Stone: Trojaning Plugins of Large Language Models,1970,"  Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers, an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses LLM-enhanced paraphrasing to polish benchmark poisoned datasets. In contrast, in the absence of a dataset, FUSION leverages an over-poisoning procedure to transform a benign adaptor. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can execute malware to control system (e.g., LLM-driven robot) or launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we design and evaluate three potential defenses, yet none proved entirely effective in safeguarding against our attacks. ",Kein DOI-Link verfügbar,2312.00374v2,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),First attempt of directionality reconstruction for atmospheric neutrinos   in a large homogeneous liquid scintillator detector,1970,"  The directionality information of incoming neutrinos is essential to atmospheric neutrino oscillation analysis since it is directly related to the oscillation baseline length. Large homogeneous liquid scintillator detectors, while offering excellent energy resolution, are traditionally very limited in their capabilities of measuring event directionality. In this paper, we present a novel directionality reconstruction method for atmospheric neutrino events in large homogeneous liquid scintillator detectors based on waveform analysis and machine learning techniques. We demonstrate for the first time that such detectors can achieve good direction resolution and potentially play an important role in future atmospheric neutrino oscillation measurements. ",Kein DOI-Link verfügbar,2310.06281v1,Yes,potent(1)
0000-0001-9197-0229,Zhen Liu,The Hong Kong University of Science and Technology (Guangzhou),Exotic Decays of the 125 GeV Higgs Boson,1970,"  We perform an extensive survey of non-standard Higgs decays that are consistent with the 125 GeV Higgs-like resonance. Our aim is to motivate a large set of new experimental analyses on the existing and forthcoming data from the Large Hadron Collider (LHC). The explicit search for exotic Higgs decays presents a largely untapped discovery opportunity for the LHC collaborations, as such decays may be easily missed by other searches. We emphasize that the Higgs is uniquely sensitive to the potential existence of new weakly coupled particles and provide a unified discussion of a large class of both simplified and complete models that give rise to characteristic patterns of exotic Higgs decays. We assess the status of exotic Higgs decays after LHC Run 1. In many cases we are able to set new nontrivial constraints by reinterpreting existing experimental analyses. We point out that improvements are possible with dedicated analyses and perform some preliminary collider studies. We prioritize the analyses according to their theoretical motivation and their experimental feasibility. This document is accompanied by a website that will be continuously updated with further information: http://exotichiggs.physics.sunysb.edu. ",https://doi.org/10.1103/PhysRevD.90.075004,1312.4992v6,Yes,potent(1)
0009-0006-2142-4664,Tianshuo Xu,The Hong Kong University of Science and Technology (Guangzhou),Uncovering the Over-smoothing Challenge in Image Super-Resolution:   Entropy-based Quantification and Contrastive Optimization,1970,"  PSNR-oriented models are a critical class of super-resolution models with applications across various fields. However, these models tend to generate over-smoothed images, a problem that has been analyzed previously from the perspectives of models or loss functions, but without taking into account the impact of data properties. In this paper, we present a novel phenomenon that we term the center-oriented optimization (COO) problem, where a model's output converges towards the center point of similar high-resolution images, rather than towards the ground truth. We demonstrate that the strength of this problem is related to the uncertainty of data, which we quantify using entropy. We prove that as the entropy of high-resolution images increases, their center point will move further away from the clean image distribution, and the model will generate over-smoothed images. Implicitly optimizing the COO problem, perceptual-driven approaches such as perceptual loss, model structure optimization, or GAN-based methods can be viewed. We propose an explicit solution to the COO problem, called Detail Enhanced Contrastive Loss (DECLoss). DECLoss utilizes the clustering property of contrastive learning to directly reduce the variance of the potential high-resolution distribution and thereby decrease the entropy. We evaluate DECLoss on multiple super-resolution benchmarks and demonstrate that it improves the perceptual quality of PSNR-oriented models. Moreover, when applied to GAN-based methods, such as RaGAN, DECLoss helps to achieve state-of-the-art performance, such as 0.093 LPIPS with 24.51 PSNR on 4x downsampled Urban100, validating the effectiveness and generalization of our approach. ",https://doi.org/10.1109/TPAMI.2024.3378704,2201.01034v2,Yes,potent(1)
0009-0006-2142-4664,Tianshuo Xu,The Hong Kong University of Science and Technology (Guangzhou),Systematic Investigation of Sparse Perturbed Sharpness-Aware   Minimization Optimizer,1970,"  Deep neural networks often suffer from poor generalization due to complex and non-convex loss landscapes. Sharpness-Aware Minimization (SAM) is a popular solution that smooths the loss landscape by minimizing the maximized change of training loss when adding a perturbation to the weight. However, indiscriminate perturbation of SAM on all parameters is suboptimal and results in excessive computation, double the overhead of common optimizers like Stochastic Gradient Descent (SGD). In this paper, we propose Sparse SAM (SSAM), an efficient and effective training scheme that achieves sparse perturbation by a binary mask. To obtain the sparse mask, we provide two solutions based on Fisher information and dynamic sparse training, respectively. We investigate the impact of different masks, including unstructured, structured, and $N$:$M$ structured patterns, as well as explicit and implicit forms of implementing sparse perturbation. We theoretically prove that SSAM can converge at the same rate as SAM, i.e., $O(\log T/\sqrt{T})$. Sparse SAM has the potential to accelerate training and smooth the loss landscape effectively. Extensive experimental results on CIFAR and ImageNet-1K confirm that our method is superior to SAM in terms of efficiency, and the performance is preserved or even improved with a perturbation of merely 50\% sparsity. Code is available at https://github.com/Mi-Peng/Systematic-Investigation-of-Sparse-Perturbed-Sharpness-Aware-Minimization-Optimizer. ",Kein DOI-Link verfügbar,2306.17504v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Nonparametric Bayesian Lomax delegate racing for survival analysis with   competing risks,1970,"  We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR. ",Kein DOI-Link verfügbar,1810.08564v2,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Implicit Distributional Reinforcement Learning,1970,"  To improve the sample efficiency of policy-gradient based reinforcement learning algorithms, we propose implicit distributional actor-critic (IDAC) that consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a flexible policy distribution. We adopt a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. Moreover, we use the SIA to provide a semi-implicit policy distribution, which mixes the policy parameters with a reparameterizable distribution that is not constrained by an analytic density function. In this way, the policy's marginal distribution is implicit, providing the potential to model complex properties such as covariance structure and skewness, but its parameter and entropy can still be estimated. We incorporate these features with an off-policy algorithm framework to solve problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms these baselines in most tasks. Python code is provided. ",Kein DOI-Link verfügbar,2007.06159v2,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),BayCount: A Bayesian Decomposition Method for Inferring Tumor   Heterogeneity using RNA-Seq Counts,1970,"  Tumor is heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical to precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Through extensive simulation studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data, we show that BayCount is able to accurately estimate the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically and clinically meaningful insights. ",Kein DOI-Link verfügbar,1702.07981v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Bayesian Attention Belief Networks,1970,"  Attention-based neural networks have achieved state-of-the-art results on a wide range of tasks. Most such models use deterministic attention while stochastic attention is less explored due to the optimization difficulties or complicated model design. This paper introduces Bayesian attention belief networks, which construct a decoder network by modeling unnormalized attention weights with a hierarchy of gamma distributions, and an encoder network by stacking Weibull distributions with a deterministic-upward-stochastic-downward structure to approximate the posterior. The resulting auto-encoding networks can be optimized in a differentiable way with a variational lower bound. It is simple to convert any models with deterministic attention, including pretrained ones, to the proposed Bayesian attention belief networks. On a variety of language understanding tasks, we show that our method outperforms deterministic attention and state-of-the-art stochastic attention in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our method on neural machine translation and visual question answering, showing great potential of incorporating our method into various attention-related tasks. ",Kein DOI-Link verfügbar,2106.05251v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling,1970,"  For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that generates photo-realistic images in not only a multi-scale low-to-high-resolution manner, but also a hierarchical-semantic coarse-to-fine fashion. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, VHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of image-text multi-modality learning and generation tasks. ",Kein DOI-Link verfügbar,1905.08622v3,Yes,versatile(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Score identity Distillation: Exponentially Fast Distillation of   Pretrained Diffusion Models for One-Step Generation,1970,"  We introduce Score identity Distillation (SiD), an innovative data-free method that distills the generative capabilities of pretrained diffusion models into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fr\'echet inception distance (FID) during distillation but also approaches or even exceeds the FID performance of the original teacher diffusion models. By reformulating forward diffusion processes as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four benchmark datasets, the SiD algorithm demonstrates high iteration efficiency during distillation and surpasses competing distillation approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the benchmarks for efficiency and effectiveness in diffusion distillation but also in the broader field of diffusion-based generation. The PyTorch implementation is available at https://github.com/mingyuanzhou/SiD ",Kein DOI-Link verfügbar,2404.04057v3,Yes,innovative(2)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Alignment Attention by Matching Key and Query Distributions,1970,"  The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks. ",Kein DOI-Link verfügbar,2110.12567v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Bayesian Attention Modules,1970,"  Attention modules, as simple and effective tools, have not only enabled deep neural networks to achieve state-of-the-art results in many domains, but also enhanced their interpretability. Most current models use deterministic attention modules due to their simplicity and ease of optimization. Stochastic counterparts, on the other hand, are less popular despite their potential benefits. The main reason is that stochastic attention often introduces optimization issues or requires significant model changes. In this paper, we propose a scalable stochastic version of attention that is easy to implement and optimize. We construct simplex-constrained attention distributions by normalizing reparameterizable distributions, making the training process differentiable. We learn their parameters in a Bayesian framework where a data-dependent prior is introduced for regularization. We apply the proposed stochastic attention modules to various attention-based models, with applications to graph node classification, visual question answering, image captioning, machine translation, and language understanding. Our experiments show the proposed method brings consistent improvements over the corresponding baselines. ",Kein DOI-Link verfügbar,2010.10604v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Contextual Dropout: An Efficient Sample-Dependent Dropout Module,1970,"  Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity. Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes. In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation. ",Kein DOI-Link verfügbar,2103.04181v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),A Non-negative VAE:the Generalized Gamma Belief Network,1970,"  The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limitation, we introduce the generalized gamma belief network (Generalized GBN) in this paper, which extends the original linear generative model to a more expressive non-linear generative model. Since the parameters of the Generalized GBN no longer possess an analytic conditional posterior, we further propose an upward-downward Weibull inference network to approximate the posterior distribution of the latent variables. The parameters of both the generative model and the inference network are jointly trained within the variational inference framework. Finally, we conduct comprehensive experiments on both expressivity and disentangled representation learning tasks to evaluate the performance of the Generalized GBN against state-of-the-art Gaussian variational autoencoders serving as baselines. ",Kein DOI-Link verfügbar,2408.03388v2,Yes,"notable(1), potent(1)"
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),"Weibull Racing Survival Analysis with Competing Events, Left Truncation,   and Time-varying Covariates",1970,"  We propose Bayesian nonparametric Weibull delegate racing (WDR) for survival analysis with competing events and achieve both model interpretability and flexibility. Utilizing a natural mechanism of surviving competing events, we assume a race among a potentially infinite number of sub-events. In doing this, WDR accommodates nonlinear covariate effects with no need of data transformation. Moreover, WDR is able to handle left truncation, time-varying covariates, different types of censoring, and missing event times or types. We develop an efficient MCMC algorithm based on Gibbs sampling for Bayesian inference and provide an \texttt{R} package. Synthetic data analysis and comparison with benchmark approaches demonstrate WDR's outstanding performance and parsimonious nonlinear modeling capacity. In addition, we analyze two real data sets and showcase advantages of WDR. Specifically, we study time to death of three types of lymphoma and show the potential of WDR in modeling nonlinear covariate effects and discovering new diseases. We also use WDR to investigate the age at onset of mild cognitive impairment and interpret the accelerating or decelerating effects of biomarkers on the progression of Alzheimer's disease. ",Kein DOI-Link verfügbar,1911.01827v4,Yes,potent(2)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),AutoML-GPT: Automatic Machine Learning with GPT,1970,"  AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning, and predicted training log. By leveraging {\ours}'s robust language capabilities and the available AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets. This approach achieves remarkable results in computer vision, natural language processing, and other challenging areas. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many AI tasks. ",Kein DOI-Link verfügbar,2305.02499v1,Yes,intricate(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Priors for Random Count Matrices Derived from a Family of Negative   Binomial Processes,1970,"  We define a family of probability distributions for random count matrices with a potentially unbounded number of rows and columns. The three distributions we consider are derived from the gamma-Poisson, gamma-negative binomial, and beta-negative binomial processes. Because the models lead to closed-form Gibbs sampling update equations, they are natural candidates for nonparametric Bayesian priors over count matrices. A key aspect of our analysis is the recognition that, although the random count matrices within the family are defined by a row-wise construction, their columns can be shown to be i.i.d. This fact is used to derive explicit formulas for drawing all the columns at once. Moreover, by analyzing these matrices' combinatorial structure, we describe how to sequentially construct a column-i.i.d. random count matrix one row at a time, and derive the predictive distribution of a new row count vector with previously unseen features. We describe the similarities and differences between the three priors, and argue that the greater flexibility of the gamma- and beta- negative binomial processes, especially their ability to model over-dispersed, heavy-tailed count data, makes these well suited to a wide variety of real-world applications. As an example of our framework, we construct a naive-Bayes text classifier to categorize a count vector to one of several existing random count matrices of different categories. The classifier supports an unbounded number of features, and unlike most existing methods, it does not require a predefined finite vocabulary to be shared by all the categories, and needs neither feature selection nor parameter tuning. Both the gamma- and beta- negative binomial processes are shown to significantly outperform the gamma-Poisson process for document categorization, with comparable performance to other state-of-the-art supervised text classification algorithms. ",Kein DOI-Link verfügbar,1404.3331v3,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Variational Graph Recurrent Neural Networks,1970,"  Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction. ",Kein DOI-Link verfügbar,1908.09710v3,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Improving In-Context Learning in Diffusion Models with Visual   Context-Modulated Prompts,1970,"  In light of the remarkable success of in-context learning in large language models, its potential extension to the vision domain, particularly with visual foundation models like Stable Diffusion, has sparked considerable interest. Existing approaches in visual in-context learning frequently face hurdles such as expensive pretraining, limiting frameworks, inadequate visual comprehension, and limited adaptability to new tasks. In response to these challenges, we introduce improved Prompt Diffusion (iPromptDiff) in this study. iPromptDiff integrates an end-to-end trained vision encoder that converts visual context into an embedding vector. This vector is subsequently used to modulate the token embeddings of text prompts. We show that a diffusion-based vision foundation model, when equipped with this visual context-modulated text guidance and a standard ControlNet structure, exhibits versatility and robustness across a variety of training tasks and excels in in-context learning for novel vision tasks, such as normal-to-image or image-to-line transformations. The effectiveness of these capabilities relies heavily on a deep visual understanding, which is achieved through relevant visual demonstrations processed by our proposed in-context learning architecture. ",Kein DOI-Link verfügbar,2312.01408v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Contrastive Factor Analysis,1970,"  Factor analysis, often regarded as a Bayesian variant of matrix factorization, offers superior capabilities in capturing uncertainty, modeling complex dependencies, and ensuring robustness. As the deep learning era arrives, factor analysis is receiving less and less attention due to their limited expressive ability. On the contrary, contrastive learning has emerged as a potent technique with demonstrated efficacy in unsupervised representational learning. While the two methods are different paradigms, recent theoretical analysis has revealed the mathematical equivalence between contrastive learning and matrix factorization, providing a potential possibility for factor analysis combined with contrastive learning. Motivated by the interconnectedness of contrastive learning, matrix factorization, and factor analysis, this paper introduces a novel Contrastive Factor Analysis framework, aiming to leverage factor analysis's advantageous properties within the realm of contrastive learning. To further leverage the interpretability properties of non-negative factor analysis, which can learn disentangled representations, contrastive factor analysis is extended to a non-negative version. Finally, extensive experimental validation showcases the efficacy of the proposed contrastive (non-negative) factor analysis methodology across multiple key properties, including expressiveness, robustness, interpretability, and accurate uncertainty estimation. ",Kein DOI-Link verfügbar,2407.21740v2,Yes,potent(2)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Bayesian multi-domain learning for cancer subtype discovery from   next-generation sequencing count data,1970,"  Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without ""negative transfer"" effects often seen in existing multi-task learning and transfer learning methods. ",Kein DOI-Link verfügbar,1810.09433v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Knowledge-Aware Bayesian Deep Topic Model,1970,"  We propose a Bayesian generative model for incorporating prior domain knowledge into hierarchical topic modeling. Although embedded topic models (ETMs) and its variants have gained promising performance in text analysis, they mainly focus on mining word co-occurrence patterns, ignoring potentially easy-to-obtain prior topic hierarchies that could help enhance topic coherence. While several knowledge-based topic models have recently been proposed, they are either only applicable to shallow hierarchies or sensitive to the quality of the provided prior knowledge. To this end, we develop a novel deep ETM that jointly models the documents and the given prior knowledge by embedding the words and topics into the same space. Guided by the provided knowledge, the proposed model tends to discover topic hierarchies that are organized into interpretable taxonomies. Besides, with a technique for adapting a given graph, our extended version allows the provided prior topic structure to be finetuned to match the target corpus. Extensive experiments show that our proposed model efficiently integrates the prior knowledge and improves both hierarchical topic discovery and document representation. ",Kein DOI-Link verfügbar,2209.14228v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Bayesian Gamma-Negative Binomial Modeling of Single-Cell RNA Sequencing   Data,1970,"  Background: Single-cell RNA sequencing (scRNA-seq) is a powerful profiling technique at the single-cell resolution. Appropriate analysis of scRNA-seq data can characterize molecular heterogeneity and shed light into the underlying cellular process to better understand development and disease mechanisms. The unique analytic challenge is to appropriately model highly over-dispersed scRNA-seq count data with prevalent dropouts (zero counts), making zero-inflated dimensionality reduction techniques popular for scRNA-seq data analyses. Employing zero-inflated distributions, however, may place extra emphasis on zero counts, leading to potential bias when identifying the latent structure of the data. Results: In this paper, we propose a fully generative hierarchical gamma-negative binomial (hGNB) model of scRNA-seq data, obviating the need for explicitly modeling zero inflation. At the same time, hGNB can naturally account for covariate effects at both the gene and cell levels to identify complex latent representations of scRNA-seq data, without the need for commonly adopted pre-processing steps such as normalization. Efficient Bayesian model inference is derived by exploiting conditional conjugacy via novel data augmentation techniques. Conclusion: Experimental results on both simulated data and several real-world scRNA-seq datasets suggest that hGNB is a powerful tool for cell cluster discovery as well as cell lineage inference. ",Kein DOI-Link verfügbar,1908.00650v1,Yes,potent(1)
0009-0003-3395-162X,Mingyuan Zhou,The Hong Kong University of Science and Technology (Guangzhou),Multichannel Electrophysiological Spike Sorting via Joint Dictionary   Learning & Mixture Modeling,1970,"  We propose a construction for joint feature learning and clustering of multichannel extracellular electrophysiological data across multiple recording periods for action potential detection and discrimination (""spike sorting""). Our construction improves over the previous state-of-the art principally in four ways. First, via sharing information across channels, we can better distinguish between single-unit spikes and artifacts. Second, our proposed ""focused mixture model"" (FMM) elegantly deals with units appearing, disappearing, or reappearing over multiple recording days, an important consideration for any chronic experiment. Third, by jointly learning features and clusters, we improve performance over previous attempts that proceeded via a two-stage (""frequentist"") learning process. Fourth, by directly modeling spike rate, we improve detection of sparsely spiking neurons. Moreover, our Bayesian construction seamlessly handles missing data. We present state-of-the-art performance without requiring manually tuning of many hyper-parameters on both a public dataset with partial ground truth and a new experimental dataset. ",Kein DOI-Link verfügbar,1304.0542v2,Yes,potent(1)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),Safety-Critical Traffic Control by Connected Automated Vehicles,1970,"  Connected automated vehicles (CAVs) have shown great potential in improving traffic throughput and stability. Although various longitudinal control strategies have been developed for CAVs to achieve string stability in mixed-autonomy traffic systems, the potential impact of these controllers on safety has not yet been fully addressed. This paper proposes safety-critical traffic control (STC) by CAVs -- a strategy that allows a CAV to stabilize the traffic behind it, while maintaining safety relative to both the preceding vehicle and the following connected human-driven vehicles (HDVs). Specifically, we utilize control barrier functions (CBFs) to impart collision-free behavior with formal safety guarantees to the closed-loop system. The safety of both the CAV and HDVs is incorporated into the framework through a quadratic program-based controller, that minimizes deviation from a nominal stabilizing traffic controller subject to CBF-based safety constraints. Considering that some state information of the following HDVs may be unavailable to the CAV, we employ state observer-based CBFs for STC. Finally, we conduct extensive numerical simulations -- that include vehicle trajectories from real data -- to demonstrate the efficacy of the proposed approach in achieving string stable and, at the same time, provably safe traffic. ",https://doi.org/10.1016/j.trc.2023.104230,2301.04833v1,Yes,potent(2)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),Regulator Design for a Congested Continuum Traffic Model with   App-Routing Instability,1970,"  In this paper, we propose a control design methodology for a linearized continuum traffic model in the congested regime. The continuum traffic flow on a highway is modeled using a linearized quasilinear hyperbolic partial differential equation model known as the Aw-Rascle-Zhang (ARZ) model. The linear traffic model is augmented with a novel non-local boundary condition representing car influx due to the use of routing apps such as Google Maps and Waze. The routing apps act as real-time previews for highway traffic, introducing potentially destabilizing feedback in the app-based navigation decision process, necessitating the development of a feedback controller. We first study small-time H^1 solutions of the linearized model with the addition of the app-routing for sufficiently small initial data. We introduce an extended, multi-tiered boundary control design based upon the method of infinite-dimensional backstepping. Using an intermediate decoupling transformation, we account for the non-local boundary condition arising from routing app feedback. We study the existence of the extended backstepping method by characterizing the existence of the companion kernels associated with the backstepping method. Finally, we study the linear ARZ model with the app-routing extension under the designed feedback, and show that for sufficiently small H^1 data, the equilibrium congestion solution is exponentially stable and guarantees the existence of closed-loop solutions on the infinite time interval. ",Kein DOI-Link verfügbar,1911.02713v1,Yes,potent(1)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),A Distributionally Robust Optimisation Approach to Fair Credit Scoring,1970,"  Credit scoring has been catalogued by the European Commission and the Executive Office of the US President as a high-risk classification task, a key concern being the potential harms of making loan approval decisions based on models that would be biased against certain groups. To address this concern, recent credit scoring research has considered a range of fairness-enhancing techniques put forward by the machine learning community to reduce bias and unfair treatment in classification systems. While the definition of fairness or the approach they follow to impose it may vary, most of these techniques, however, disregard the robustness of the results. This can create situations where unfair treatment is effectively corrected in the training set, but when producing out-of-sample classifications, unfair treatment is incurred again. Instead, in this paper, we will investigate how to apply Distributionally Robust Optimisation (DRO) methods to credit scoring, thereby empirically evaluating how they perform in terms of fairness, ability to classify correctly, and the robustness of the solution against changes in the marginal proportions. In so doing, we find DRO methods to provide a substantial improvement in terms of fairness, with almost no loss in performance. These results thus indicate that DRO can improve fairness in credit scoring, provided that further advances are made in efficiently implementing these systems. In addition, our analysis suggests that many of the commonly used fairness metrics are unsuitable for a credit scoring setting, as they depend on the choice of classification threshold. ",Kein DOI-Link verfügbar,2402.01811v1,Yes,potent(1)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),Intention-Aware Planner for Robust and Safe Aerial Tracking,1970,"  Autonomous target tracking with quadrotors has wide applications in many scenarios, such as cinematographic follow-up shooting or suspect chasing. Target motion prediction is necessary when designing the tracking planner. However, the widely used constant velocity or constant rotation assumption can not fully capture the dynamics of the target. The tracker may fail when the target happens to move aggressively, such as sudden turn or deceleration. In this paper, we propose an intention-aware planner by additionally considering the intention of the target to enhance safety and robustness in aerial tracking applications. Firstly, a designated intention prediction method is proposed, which combines a user-defined potential assessment function and a state observation function. A reachable region is generated to specifically evaluate the turning intentions. Then we design an intention-driven hybrid A* method to predict the future possible positions for the target. Finally, an intention-aware optimization approach is designed to generate a spatial-temporal optimal trajectory, allowing the tracker to perceive unexpected situations from the target. Benchmark comparisons and real-world experiments are conducted to validate the performance of our method. ",Kein DOI-Link verfügbar,2309.08854v4,Yes,potent(1)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),CapsuleBot: A Novel Compact Hybrid Aerial-Ground Robot with Two   Actuated-wheel-rotors,1970,"  This paper presents the design, modeling, and experimental validation of CapsuleBot, a compact hybrid aerial-ground vehicle designed for long-term covert reconnaissance. CapsuleBot combines the manoeuvrability of bicopter in the air with the energy efficiency and noise reduction of ground vehicles on the ground. To accomplish this, a structure named actuated-wheel-rotor has been designed, utilizing a sole motor for both the unilateral rotor tilting in the bicopter configuration and the wheel movement in ground mode. CapsuleBot comes equipped with two of these structures, enabling it to attain hybrid aerial-ground propulsion with just four motors. Importantly, the decoupling of motion modes is achieved without the need for additional drivers, enhancing the versatility and robustness of the system. Furthermore, we have designed the full dynamics and control for aerial and ground locomotion based on the bicopter model and the two-wheeled self-balancing vehicle model. The performance of CapsuleBot has been validated through experiments. The results demonstrate that CapsuleBot produces 40.53% less noise in ground mode and consumes 99.35% less energy, highlighting its potential for long-term covert reconnaissance applications. ",Kein DOI-Link verfügbar,2309.09224v1,Yes,potent(1)
0000-0002-9324-0200,Huan Yu,The Hong Kong University of Science and Technology (Guangzhou),Roller-Quadrotor: A Novel Hybrid Terrestrial/Aerial Quadrotor with   Unicycle-Driven and Rotor-Assisted Turning,1970,"  The Roller-Quadrotor is a novel quadrotor that combines the maneuverability of aerial drones with the endurance of ground vehicles. This work focuses on the design, modeling, and experimental validation of the Roller-Quadrotor. Flight capabilities are achieved through a quadrotor configuration, with four thrust-providing actuators. Additionally, rolling motion is facilitated by a unicycle-driven and rotor-assisted turning structure. By utilizing terrestrial locomotion, the vehicle can overcome rolling and turning resistance, thereby conserving energy compared to its flight mode. This innovative approach not only tackles the inherent challenges of traditional rotorcraft but also enables the vehicle to navigate through narrow gaps and overcome obstacles by taking advantage of its aerial mobility. We develop comprehensive models and controllers for the Roller-Quadrotor and validate their performance through experiments. The results demonstrate its seamless transition between aerial and terrestrial locomotion, as well as its ability to safely navigate through gaps half the size of its diameter. Moreover, the terrestrial range of the vehicle is approximately 2.8 times greater, while the operating time is about 41.2 times longer compared to its aerial capabilities. These findings underscore the feasibility and effectiveness of the proposed structure and control mechanisms for efficient navigation through challenging terrains while conserving energy. ",Kein DOI-Link verfügbar,2303.00668v3,Yes,innovative(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),Calibration in Deep Learning: A Survey of the State-of-the-Art,1970,"  Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent advances in calibrating deep models. In this survey, we review the state-of-the-art calibration methods and their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classify into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also cover recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions. ",Kein DOI-Link verfügbar,2308.01222v3,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),An Energy Stable BDF2 Fourier Pseudo-Spectral Numerical Scheme for the   Square Phase Field Crystal Equation,1970,"  In this paper we propose and analyze an energy stable numerical scheme for the square phase field crystal (SPFC) equation, a gradient flow modeling crystal dynamics at the atomic scale in space but on diffusive scales in time. In particular, a modification of the free energy potential to the standard phase field crystal model leads to a composition of the 4-Laplacian and the regular Laplacian operators. To overcome the difficulties associated with this highly nonlinear operator, we design numerical algorithms based on the structures of the individual energy terms. A Fourier pseudo-spectral approximation is taken in space, in such a way that the energy structure is respected, and summation-by-parts formulae enable us to study the discrete energy stability for such a high-order spatial discretization. In the temporal approximation, a second order BDF stencil is applied, combined with an appropriate extrapolation for the concave diffusion term(s). A second order artificial Douglas-Dupont-type regularization term is added to ensure energy stability, and a careful analysis leads to the artificial linear diffusion coming at an order lower that that of surface diffusion term. Such a choice leads to reduced numerical dissipation. At a theoretical level, the unique solvability, energy stability are established, and an optimal rate convergence analysis is derived in the $\ell^\infty (0,T; \ell^2) \cap \ell^2 (0,T; H_N^3)$ norm. In the numerical implementation, the preconditioned steepest descent (PSD) iteration is applied to solve for the composition of the highly nonlinear 4-Laplacian term and the standard Laplacian term, and a geometric convergence is assured for such an iteration. Finally, a few numerical experiments are presented, which confirm the robustness and accuracy of the proposed scheme. ",https://doi.org/10.4208/cicp.2019.js60.10,1906.12255v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),High-Q Lithium Niobate Microcavities and Their Applications,1970,"  Lithium niobate (LN) is an excellent nonlinear optical and electro-optic material that has found many applications in classical nonlinear optics, optical fiber communications and quantum photonics. Here we review the recent development of thin-film LN technology that has allowed the miniaturization of LN photonic devices and microcavities with ultrahigh quality factors. We discuss the design principle of LN devices that makes use of the largest nonlinear coefficients, various device fabrication approaches and resulting device performances, and the current and potential applications of LN microcavities. ",https://doi.org/10.1142/8964,2012.05802v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),A second order accurate scalar auxiliary variable (SAV) numerical method   for the square phase field crystal equation,1970,"  In this paper we propose and analyze a second order accurate (in time) numerical scheme for the square phase field crystal (SPFC) equation, a gradient flow modeling crystal dynamics at the atomic scale in space but on diffusive scales in time. Its primary difference with the standard phase field crystal model is an introduction of the 4-Laplacian term in the free energy potential, which in turn leads to a much higher degree of nonlinearity. To make the numerical scheme linear while preserving the nonlinear energy stability, we make use of the scalar auxiliary variable (SAV) approach, in which a second order backward differentiation formula (BDF) is applied in the temporal stencil. Meanwhile, a direct application of the SAV method faces certain difficulties, due to the involvement of the 4-Laplacian term, combined with a derivation of the lower bound of the nonlinear energy functional. In the proposed numerical method, an appropriate decomposition for the physical energy functional is formulated, so that the nonlinear energy part has a well-established global lower bound, and the rest terms lead to constant-coefficient diffusion terms with positive eigenvalues. In turn, the numerical scheme could be very efficiently implemented by constant-coefficient Poisson-like type solvers (via FFT), and energy stability is established by introducing an auxiliary variable, and an optimal rate convergence analysis is provided for the proposed SAV method. A few numerical experiments are also presented, which confirm the efficiency and accuracy of the proposed scheme. ",Kein DOI-Link verfügbar,2012.15133v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),Uncertainty Estimation and Calibration with Finite-State Probabilistic   RNNs,1970,"  Uncertainty quantification is crucial for building reliable and trustable machine learning systems. We propose to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, our proposed method offers several advantages in different settings. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the exploration-exploitation trade-off in reinforcement learning. ",Kein DOI-Link verfügbar,2011.12010v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),State-Regularized Recurrent Neural Networks to Extract Automata and   Explain Predictions,1970,"  Recurrent neural networks are a widely used class of neural architectures. They have, however, two shortcomings. First, they are often treated as black-box models and as such it is difficult to understand what exactly they learn as well as how they arrive at a particular prediction. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the purpose of automata extraction; (2) non-regular languages such as balanced parentheses and palindromes where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition and text categorisation. We show that state-regularization (a) simplifies the extraction of finite state automata that display an RNN's state transition dynamic; (b) forces RNNs to operate more like automata with external memory and less like finite state machines, which potentiality leads to a more structural memory; (c) leads to better interpretability and explainability of RNNs by leveraging the probabilistic finite state transition mechanism over time steps. ",Kein DOI-Link verfügbar,2212.05178v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),A positivity-preserving and convergent numerical scheme for the binary   fluid-surfactant system,1970,"  In this paper, we develop a first order (in time) numerical scheme for the binary fluid surfactant phase field model. The free energy contains a double-well potential, a nonlinear coupling entropy and a Flory-Huggins potential. The resulting coupled system consists of two Cahn-Hilliard type equations. This system is solved numerically by finite difference spatial approximation, in combination with convex splitting temporal discretization. We prove the proposed scheme is unique solvable, positivity-preserving and unconditionally energy stable. In addition, an optimal rate convergence analysis is provided for the proposed numerical scheme, which will be the first such result for the binary fluid-surfactant system. Newton iteration is used to solve the discrete system. Some numerical experiments are performed to validate the accuracy and energy stability of the proposed scheme. ",Kein DOI-Link verfügbar,2102.08105v1,Yes,potent(2)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),Numerical methods for Porous Medium Equation by an Energetic Variational   Approach,1970,"  We study numerical methods for porous media equation (PME). There are two important characteristics: the finite speed propagation of the free boundary and the potential waiting time, which make the problem not easy to handle. Based on different dissipative energy laws, we develop two numerical schemes by an energetic variational approach. Firstly, based on $f \log f$ as the total energy form of the dissipative law, we obtain the trajectory equation, and then construct a fully discrete scheme. It is proved that the scheme is uniquely solvable on an admissible convex set by taking the advantage of the singularity of the total energy. Next, based on $\frac{1}{2 f}$ as the total energy form of the dissipation law, we construct a linear numerical scheme for the corresponding trajectory equation. Both schemes preserve the corresponding discrete dissipation law. Meanwhile, under some smoothness assumption, it is proved, by a higher order expansion technique, that both schemes are second-order convergent in space and first-order convergent in time. Each scheme yields a good approximation for the solution and the free boundary. No oscillation is observed for the numerical solution around the free boundary. Furthermore, the waiting time problem could be naturally treated, which has been a well-known difficult issue for all the existence methods. Due to its linear nature, the second scheme is more efficient. ",https://doi.org/10.1016/j.jcp.2019.01.055,1806.10775v2,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),Convergence Analysis of Structure-Preserving Numerical Methods Based on   Slotboom Transformation for the Poisson--Nernst--Planck Equations,1970,"  The analysis of structure-preserving numerical methods for the Poisson--Nernst--Planck (PNP) system has attracted growing interests in recent years. In this work, we provide an optimal rate convergence analysis and error estimate for finite difference schemes based on the Slotboom reformulation. Different options of mobility average at the staggered mesh points are considered in the finite-difference spatial discretization, such as the harmonic mean, geometric mean, arithmetic mean, and entropic mean. A semi-implicit temporal discretization is applied, which in turn results in a non-constant coefficient, positive-definite linear system at each time step. A higher order asymptotic expansion is applied in the consistency analysis, and such a higher order consistency estimate is necessary to control the discrete maximum norm of the concentration variables. In convergence estimate, the harmonic mean for the mobility average, which turns out to bring lots of convenience in the theoretical analysis, is taken for simplicity, while other options of mobility average would also lead to the desired error estimate, with more technical details involved. As a result, an optimal rate convergence analysis on concentrations, electric potential, and ionic fluxes is derived, which is the first such results for the structure-preserving numerical schemes based on the Slotboom reformulation. It is remarked that the convergence analysis leads to a theoretical justification of the conditional energy dissipation analysis, which relies on the maximum norm bounds of the concentration and the gradient of the electric potential. Some numerical results are also presented to demonstrate the accuracy and structure-preserving performance of the associated schemes. ",Kein DOI-Link verfügbar,2202.10931v1,Yes,potent(2)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),A third order BDF energy stable linear scheme for the no-slope-selection   thin film model,1970,"  In this paper we propose and analyze a (temporally) third order accurate backward differentiation formula (BDF) numerical scheme for the no-slope-selection (NSS) equation of the epitaxial thin film growth model, with Fourier pseudo-spectral discretization in space. The surface diffusion term is treated implicitly, while the nonlinear chemical potential is approximated by a third order explicit extrapolation formula for the sake of solvability. In addition, a third order accurate Douglas-Dupont regularization term, in the form of $-A \Delta t^2 \Delta_N^2 ( u^{n+1} - u^n)$, is added in the numerical scheme. A careful energy stability estimate, combined with Fourier eigenvalue analysis, results in the energy stability in a modified version, and a theoretical justification of the coefficient $A$ becomes available. As a result of this energy stability analysis, a uniform in time bound of the numerical energy is obtained. And also, the optimal rate convergence analysis and error estimate are derived in details, in the $\ell^\infty (0,T; \ell^2) \cap \ell^2 (0,T; H_h^2)$ norm, with the help of a linearized estimate for the nonlinear error terms. %This convergence estimate is the first such result for a third order accurate scheme for a gradient flow. Some numerical simulation results are presented to demonstrate the efficiency of the numerical scheme and the third order convergence. The long time simulation results for $\varepsilon=0.02$ (up to $T=3 \times 10^5$) have indicated a logarithm law for the energy decay, as well as the power laws for growth of the surface roughness and the mound width. In particular, the power index for the surface roughness and the mound width growth, created by the third order numerical scheme, is more accurate than those produced by certain second order energy stable schemes in the existing literature. ",https://doi.org/10.4208/cicp.OA-2020-0074,2011.01525v1,Yes,potent(1)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),The Application of Driver Models in the Safety Assessment of Autonomous   Vehicles: A Survey,1970,"  Driver models play a vital role in developing and verifying autonomous vehicles (AVs). Previously, they are mainly applied in traffic flow simulation to model driver behavior. With the development of AVs, driver models attract much attention again due to their potential contributions to AV safety assessment. The simulation-based testing method is an effective measure to accelerate AV testing due to its safe and efficient characteristics. Nonetheless, realistic driver models are prerequisites for valid simulation results. Additionally, an AV is assumed to be at least as safe as a careful and competent driver, which is modeled by driver models as well. Therefore, driver models are essential for AV safety assessment from the current perspective. However, no comparison or discussion of driver models is available regarding their utility to AVs in the last five years despite their necessities in the release of AVs. This motivates us to present a comprehensive survey of driver models in the paper and compare their applicability. Requirements for driver models as applied to AV safety assessment are discussed. A summary of driver models for simulation-based testing and AV benchmarks is provided. Evaluation metrics are defined to compare their strength and weakness. Finally, potential gaps in existing driver models are identified, which provide direction for future work. This study gives related researchers especially regulators an overview and helps them to define appropriate driver models for AVs. ",https://doi.org/10.1109/TIV.2023.3333796,2303.14779v2,Yes,potent(2)
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),Behavioral Authentication for Security and Safety,1970,"  The issues of both system security and safety can be dissected integrally from the perspective of behavioral \emph{appropriateness}. That is, a system is secure or safe can be judged by whether the behavior of certain agent(s) is \emph{appropriate} or not. Specifically, a so-called \emph{appropriate behavior} involves the right agent performing the right actions at the right time under certain conditions. Then, according to different levels of appropriateness and degrees of custodies, behavioral authentication can be graded into three levels, i.e., the authentication of behavioral \emph{Identity}, \emph{Conformity}, and \emph{Benignity}. In a broad sense, for the security and safety issue, behavioral authentication is not only an innovative and promising method due to its inherent advantages but also a critical and fundamental problem due to the ubiquity of behavior generation and the necessity of behavior regulation in any system. By this classification, this review provides a comprehensive examination of the background and preliminaries of behavioral authentication. It further summarizes existing research based on their respective focus areas and characteristics. The challenges confronted by current behavioral authentication methods are analyzed, and potential research directions are discussed to promote the diversified and integrated development of behavioral authentication. ",https://doi.org/10.1051/sands/2024003,2312.03429v2,Yes,"innovative(1), potent(1)"
0000-0001-7527-9702,Cheng Wang,The Hong Kong University of Science and Technology (Guangzhou),"Positivity-preserving, energy stable numerical schemes for the   Cahn-Hilliard equation with logarithmic potential",1970,"  We present and analyze finite difference numerical schemes for the Allen Cahn/Cahn-Hilliard equation with a logarithmic Flory Huggins energy potential. Both the first order and second order accurate temporal algorithms are considered. In the first order scheme, we treat the nonlinear logarithmic terms and the surface diffusion term implicitly, and update the linear expansive term and the mobility explicitly. We provide a theoretical justification that, this numerical algorithm has a unique solution such that the positivity is always preserved for the logarithmic arguments. In particular, our analysis reveals a subtle fact: the singular nature of the logarithmic term around the values of $-1$ and 1 prevents the numerical solution reaching these singular values, so that the numerical scheme is always well-defined as long as the numerical solution stays similarly bounded at the previous time step. Furthermore, an unconditional energy stability of the numerical scheme is derived, without any restriction for the time step size. The unique solvability and the positivity-preserving property for the second order scheme are proved using similar ideas, in which the singular nature of the logarithmic term plays an essential role. For both the first and second order accurate schemes, we are able to derive an optimal rate convergence analysis, which gives the full order error estimate. The case with a non-constant mobility is analyzed as well. We also describe a practical and efficient multigrid solver for the proposed numerical schemes, and present some numerical results, which demonstrate the robustness of the numerical schemes. ",Kein DOI-Link verfügbar,1712.03225v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),$Sp_{2n}(F_{q^{2}})$-Invariants In Irreducible Unipotent Representations   of $Sp_{4n}(F_{q})$,1970,"  We show that for any irreducible representation of $Sp_{4n}(F_{q})$, the subspace of all its $Sp_{2n}(F_{q^{2}})$-invariants is at most one-dimensional. In terms of Lusztig symbols, we give a complete list of irreducible unipotent representations of $Sp_{4n}(F_{q})$ which have a nonzero $Sp_{2n}(F_{q^{2}})$-invariant and, in particular, we prove that every irreducible unipotent cuspidal representation has a one-dimensional subspace of $Sp_{2n}(F_{q^{2}})$-invariants. As an application, we give an elementary proof of the fact that the unipotent cuspidal representation is defined over $Q$, which was proved by Lusztig. ",Kein DOI-Link verfügbar,1303.7024v1,Yes,potent(3)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Automated flakiness detection in quantum software bug reports,1970,"  A flaky test yields inconsistent results upon repetition, posing a significant challenge to software developers. An extensive study of their presence and characteristics has been done in classical computer software but not quantum computer software. In this paper, we outline challenges and potential solutions for the automated detection of flaky tests in bug reports of quantum software. We aim to raise awareness of flakiness in quantum software and encourage the software engineering community to work collaboratively to solve this emerging challenge. ",Kein DOI-Link verfügbar,2408.05331v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Global optimization-based dimer method for finding saddle points,1970,"  Searching saddle points on the potential energy surface is a challenging problem in the rare event. When there exist multiple saddle points, sampling different initial guesses are needed in most dimer-type methods in order to find distinct saddle points. In this paper, we present a novel global optimization-based dimer method (GOD) to efficiently search saddle points by coupling ant colony optimization (ACO) algorithm with optimization-based shrinking dimer (OSD) method. In particular, we apply OSD method as a local search algorithm for saddle points and construct a pheromone function in ACO to update the global population. By applying a two-dimensional example and a benchmark problem of seven-atom island on the (111) surface of an FCC crystal, we demonstrate that GOD shows a significant improvement in computational efficiency compared with OSD method. Our algorithm offers a new framework to open up possibilities of adopting other global optimization methods to search saddle points. ",Kein DOI-Link verfügbar,1910.05703v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),On Testing Quantum Programs,1970,"  A quantum computer (QC) can solve many computational problems more efficiently than a classic one. The field of QCs is growing: companies (such as DWave, IBM, Google, and Microsoft) are building QC offerings. We position that software engineers should look into defining a set of software engineering practices that apply to QC's software. To start this process, we give examples of challenges associated with testing such software and sketch potential solutions to some of these challenges. ",https://doi.org/10.1109/ICSE-NIER.2019.00023,1812.09261v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Inverse source scattering problem for a nonlinear Schrödinger equation,1970,"  We study an inverse source scattering problem for the Schr\""odinger equation with a quadratic nonlinearity. In general, uniqueness of inverse source problems can not be guaranteed at a fixed energy. Therefore, additional information is required for the source in order to obtain a unique solution. By adding reference point sources, we show that a general source function could be uniquely determined from boundary measurements at a fixed wavenumber. This method does not apply to inverse source problems of linear equations since it uses the nonlinearity as a tool. The proof utilizes the method of linearization to reduce the nonlinear inverse source scattering problem to the inverse potential scattering problem of the linear Schr\""odinger equation. ",Kein DOI-Link verfügbar,2303.11628v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Space-time registration-based model reduction of parameterized   one-dimensional hyperbolic PDEs,1970,"  We propose a model reduction procedure for rapid and reliable solution of parameterized hyperbolic partial differential equations. Due to the presence of parameter-dependent shock waves and contact discontinuities, these problems are extremely challenging for traditional model reduction approaches based on linear approximation spaces. The main ingredients of the proposed approach are (i) an adaptive space-time registration-based data compression procedure to align local features in a fixed reference domain, (ii) a space-time Petrov-Galerkin (minimum residual) formulation for the computation of the mapped solution, and (iii) a hyper-reduction procedure to speed up online computations. We present numerical results for a Burgers model problem and a shallow water model problem, to empirically demonstrate the potential of the method. ",Kein DOI-Link verfügbar,2004.06693v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Blended Ghost Force Correction Method for 3D Crystalline Defects,1970,"  Atomistic/continuum coupling method is a class of multiscale computational method for the efficient simulation of crystalline defects. The recently developed blended ghost force correction (BGFC) method combines the efficiency of blending methods and the accuracy of QNL type methods. BGFC method can be applied to multi-body interaction potentials and general interfaces. In this paper, we present the formulation, implementation and analysis of the BGFC method in three dimensions. In particular, we focus on the difference and connection with other blending variants, such as energy based blended quasi-continuum method (BQCE) and force based blended quasi-continuum method (BQCF). The theoretical results are justified by a few benchmark numerical experiments with point defects and microcrack in the three dimensional FCC lattice. ",Kein DOI-Link verfügbar,2006.06501v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),DeePN$^2$: A deep learning-based non-Newtonian hydrodynamic model,1970,"  A long standing problem in the modeling of non-Newtonian hydrodynamics of polymeric flows is the availability of reliable and interpretable hydrodynamic models that faithfully encode the underlying micro-scale polymer dynamics. The main complication arises from the long polymer relaxation time, the complex molecular structure and heterogeneous interaction. DeePN$^2$, a deep learning-based non-Newtonian hydrodynamic model, has been proposed and has shown some success in systematically passing the micro-scale structural mechanics information to the macro-scale hydrodynamics for suspensions with simple polymer conformation and bond potential. The model retains a multi-scaled nature by mapping the polymer configurations into a set of symmetry-preserving macro-scale features. The extended constitutive laws for these macro-scale features can be directly learned from the kinetics of their micro-scale counterparts. In this paper, we develop DeePN$^2$ using more complex micro-structural models. We show that DeePN$^2$ can faithfully capture the broadly overlooked viscoelastic differences arising from the specific molecular structural mechanics without human intervention. ",https://doi.org/10.4208/jml.220115,2112.14798v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),A discretize-then-map approach for the treatment of parameterized   geometries in model order reduction,1970,"  We present a general approach for the treatment of parameterized geometries in projection-based model order reduction. During the offline stage, given (i) a family of parameterized domains $\{ \Omega_{\mu}: \mu \in \mathcal{P} \} \subset \mathbb{R}^D$ where $\mu \in \mathcal{P} \subset \mathbb{R}^P$ denotes a vector of parameters, (ii) a parameterized mapping ${\Phi}_{\mu}$ between a reference domain $\Omega$ and the parameter-dependent domain $\Omega_{\mu}$, and (iii) a finite element triangulation of $\Omega$, we resort to an empirical quadrature procedure to select a subset of the elements of the grid. During the online stage, we first use the mapping to ""move"" the nodes of the selected elements and then we use standard element-wise residual evaluation routines to evaluate the residual and possibly its Jacobian. We discuss how to devise an online-efficient reduced-order model and we discuss the differences with the more standard ""map-then-discretize"" approach (e.g., Rozza, Huynh, Patera, ACME, 2007); in particular, we show how the discretize-then-map framework greatly simplifies the implementation of the reduced-order model. We apply our approach to a two-dimensional potential flow problem past a parameterized airfoil, and to the two-dimensional RANS simulations of the flow past the Ahmed body. ",https://doi.org/10.1016/j.cma.2021.113956,2010.13935v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Registration-based model reduction in complex two-dimensional geometries,1970,"  We present a general -- i.e., independent of the underlying equation -- registration procedure for parameterized model order reduction. Given the spatial domain $\Omega \subset \mathbb{R}^2$ and the manifold $\mathcal{M}= \{ u_{\mu} : \mu \in \mathcal{P} \}$ associated with the parameter domain $\mathcal{P} \subset \mathbb{R}^P$ and the parametric field $\mu \mapsto u_{\mu} \in L^2(\Omega)$, our approach takes as input a set of snapshots $\{ u^k \}_{k=1}^{n_{\rm train}} \subset \mathcal{M}$ and returns a parameter-dependent bijective mapping ${\Phi}: \Omega \times \mathcal{P} \to \mathbb{R}^2$: the mapping is designed to make the mapped manifold $\{ u_{\mu} \circ {\Phi}_{\mu}: \, \mu \in \mathcal{P} \}$ more amenable for linear compression methods. In this work, we extend and further analyze the registration approach proposed in [Taddei, SISC, 2020]. The contributions of the present work are twofold. First, we extend the approach to deal with annular domains by introducing a suitable transformation of the coordinate system. Second, we discuss the extension to general two-dimensional geometries: towards this end, we introduce a spectral element approximation, which relies on a partition $\{ \Omega_{q} \}_{q=1} ^{N_{\rm dd}}$ of the domain $\Omega$ such that $\Omega_1,\ldots,\Omega_{N_{\rm dd}}$ are isomorphic to the unit square. We further show that our spectral element approximation can cope with parameterized geometries. We present rigorous mathematical analysis to justify our proposal; furthermore, we present numerical results for a heat-transfer problem in an annular domain, a potential flow past a rotating symmetric airfoil, and an inviscid transonic compressible flow past a non-symmetric airfoil, to demonstrate the effectiveness of our method. ",Kein DOI-Link verfügbar,2101.10259v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis,1970,"  The neural radiance field (NeRF) has emerged as a prominent methodology for synthesizing realistic images of novel views. While neural radiance representations based on voxels or mesh individually offer distinct advantages, excelling in either rendering quality or speed, each has limitations in the other aspect. In response, we propose a pioneering hybrid representation named Vosh, seamlessly combining both voxel and mesh components in hybrid rendering for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid of NeRF, strategically with selected voxels replaced by mesh. Therefore, it excels in fast rendering scenes with simple geometry and textures through its mesh component, while simultaneously enabling high-quality rendering in intricate regions by leveraging voxel component. The flexibility of Vosh is showcased through the ability to adjust hybrid ratios, providing users the ability to control the balance between rendering quality and speed based on flexible usage. Experimental results demonstrates that our method achieves commendable trade-off between rendering quality and speed, and notably has real-time performance on mobile devices. ",Kein DOI-Link verfügbar,2403.06505v1,Yes,"commendable(1), meticulous(1), intricate(1), meticulously(1), strategically(1)"
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),On the construction of non-simple blow-up solutions for the singular   Liouville equation with a potential,1970,"  We are concerned with the existence of blowing-up solutions to the following boundary value problem $$-\Delta u= \lambda V(x) e^u-4\pi N \delta_0\;\mbox{ in } B_1,\quad u=0 \;\mbox{ on }\partial B_1,$$ where $B_1$ is the unit ball in $\mathbb R^2$ centered at the origin, $V(x)$ is a positive smooth potential, $N$ is a positive integer ($N\geq 1$). Here $\delta_0$ defines the Dirac measure with pole at $0$, and $\lambda>0$ is a small parameter. We assume that $N=1$ and, under some suitable assumptions on the derivatives of the potential $V$ at $0$, we find a solution which exhibits a non-simple blow-up profile as $\lambda\to 0^+$. ",Kein DOI-Link verfügbar,2307.15811v1,Yes,potent(2)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Audio-Visual LLM for Video Understanding,1970,"  This paper presents Audio-Visual LLM, a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality-augmented training, which involves the integration of modality-specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. This mechanism is pivotal in enabling end-to-end joint training with video data at different modalities, including visual-only, audio-only, and audio-visual formats. Moreover, we introduce a high-quality video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual LLM to adeptly process a variety of task-oriented video instructions, ranging from multi-turn conversations and audio-visual narratives to complex reasoning tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively achieves strong zero-shot results across a range of video understanding tasks. For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA, outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%, respectively. Additionally, our Audio-Visual LLM also achieves competitive performance on audio tasks (e.g., AudioCaps). ",Kein DOI-Link verfügbar,2312.06720v2,Yes,"pivotal(1), impressively(1)"
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),"Current-induced near-field radiative energy, linear-momentum, and   angular-momentum transfer",1970,"  In this paper, we study the near-field radiative energy, linear-momentum, and angular-momentum transfer from a current-biased graphene to nanoparticles. The electric current through the graphene sheet induces nonequilibrium fluctuations, causing energy and momentum transfer even in the absence of a temperature difference. The inherent spin-momentum locking of graphene surface plasmons leads to an in-plane torque perpendicular to the direction of the electric current. In the presence of a temperature difference, the energy transfer is greatly enhanced while the lateral force and torque remain within the same order. Our work explores the potential of utilizing current-biased graphene to manipulate nanoparticles. ",https://doi.org/10.1103/PhysRevB.109.075413,2312.07954v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Soft Masked Mamba Diffusion Model for CT to MRI Conversion,1970,"  Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) are the predominant modalities utilized in the field of medical imaging. Although MRI capture the complexity of anatomical structures with greater detail than CT, it entails a higher financial costs and requires longer image acquisition times. In this study, we aim to train latent diffusion model for CT to MRI conversion, replacing the commonly-used U-Net or Transformer backbone with a State-Space Model (SSM) called Mamba that operates on latent patches. First, we noted critical oversights in the scan scheme of most Mamba-based vision methods, including inadequate attention to the spatial continuity of patch tokens and the lack of consideration for their varying importance to the target task. Secondly, extending from this insight, we introduce Diffusion Mamba (DiffMa), employing soft masked to integrate Cross-Sequence Attention into Mamba and conducting selective scan in a spiral manner. Lastly, extensive experiments demonstrate impressive performance by DiffMa in medical image generation tasks, with notable advantages in input scaling efficiency over existing benchmark models. The code and models are available at https://github.com/wongzbb/DiffMa-Diffusion-Mamba ",Kein DOI-Link verfügbar,2406.15910v1,Yes,notable(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Vision Mamba: A Comprehensive Survey and Taxonomy,1970,"  State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy. ",Kein DOI-Link verfügbar,2405.04404v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Mobile Instant Video Clip Sharing: Modeling and Enhancing View   Experience,1970,"  With the rapid development of wireless networking and mobile devices, anytime and anywhere data access becomes readily available nowadays. Given the crowdsourced content capturing and sharing, the preferred content length becomes shorter and shorter, even for such multimedia data as video. A representative is Twitter's Vine service, which, mainly targeting mobile users, enables them to create ultra-short video clips and instantly post and share with their followers. In this paper, we present an initial study on this new generation of instant video clip sharing service enabled by mobile platforms and explore the potentials towards its further enhancement. We closely investigate its unique mobile interface, revealing the key differences between Vine-enabled anytime anywhere data access patterns and that of traditional counterparts. We then examine the scheduling policy to maximize the user watching experience as well as the efficiency on the monetary and energy costs. We show that the generic scheduling problem involves two subproblems, namely, pre-fetching scheduling and watch-time download scheduling, and develop effective solutions towards both of them. The superiority of our solution is demonstrated by extensive trace-driven simulations. To the best of our knowledge, this is the first work on modeling and optimizing the instant video clip sharing on mobile devices. ",Kein DOI-Link verfügbar,1412.7595v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),The Impact of Position Errors on Crowd Simulation,1970,"  In large crowd events, there is always a potential possibility that a stampede accident will occur. The accident may cause injuries or even death. Approaches for controlling crowd flows and predicting dangerous congestion spots would be a boon to on-site authorities to manage the crowd and to prevent fatal accidents. One of the most popular approaches is real-time crowd simulation based on position data from personal Global Positioning System (GPS) devices. However, the accuracy of spatial data varies for different GPS devices, and it is also affected by an environment in which an event takes place. In this paper, we would like to assess the effect of position errors on stampede prediction. We propose an Automatic Real-time dEtection of Stampedes (ARES) method to predict stampedes for large events. We implement three different stampede assessment methods in Menge framework and incorporate position errors. Our analysis suggests that the probability of simulated stampede changes significantly with the increase of the magnitude of position errors, which cannot be eliminated entirely with the help of classic techniques, such as the Kalman filter. Thus, it is our position that novel stampede assessment methods should be developed, focusing on the detection of position noise and the elimination of its effect. ",https://doi.org/10.1016/j.simpat.2018.10.010,1810.11131v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Probability Weighted Compact Feature for Domain Adaptive Retrieval,1970,"  Domain adaptive image retrieval includes single-domain retrieval and cross-domain retrieval. Most of the existing image retrieval methods only focus on single-domain retrieval, which assumes that the distributions of retrieval databases and queries are similar. However, in practical application, the discrepancies between retrieval databases often taken in ideal illumination/pose/background/camera conditions and queries usually obtained in uncontrolled conditions are very large. In this paper, considering the practical application, we focus on challenging cross-domain retrieval. To address the problem, we propose an effective method named Probability Weighted Compact Feature Learning (PWCF), which provides inter-domain correlation guidance to promote cross-domain retrieval accuracy and learns a series of compact binary codes to improve the retrieval speed. First, we derive our loss function through the Maximum A Posteriori Estimation (MAP): Bayesian Perspective (BP) induced focal-triplet loss, BP induced quantization loss and BP induced classification loss. Second, we propose a common manifold structure between domains to explore the potential correlation across domains. Considering the original feature representation is biased due to the inter-domain discrepancy, the manifold structure is difficult to be constructed. Therefore, we propose a new feature named Histogram Feature of Neighbors (HFON) from the sample statistics perspective. Extensive experiments on various benchmark databases validate that our method outperforms many state-of-the-art image retrieval methods for domain adaptive image retrieval. The source code is available at https://github.com/fuxianghuang1/PWCF ",Kein DOI-Link verfügbar,2003.03293v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Solution landscape of the Onsager model identifies non-axisymmetric   critical points,1970,"  We investigate critical points of the Onsager free-energy model on a sphere with different potential kernels, including the dipolar potential, the Maier-Saupe potential, the coupled dipolar/Maier-Saupe potential, and the Onsager potential. A uniform sampling method is implemented for the discretization of the Onsager model, and solution landscapes of the Onsager model are constructed using saddle dynamics coupled with downward/upward search algorithms. We first construct the solution landscapes with the dipolar and Maier-Saupe potentials, for which all critical points are axisymmetric. For the coupled dipolar/Maier-Saupe potential, the solution landscape shows a novel non-axisymmetric critical point, named tennis, which exists for a wide range of parameters. We further demonstrate various non-axisymmetric critical points in the Onsager model with the Onsager potential, including square, hexagon, octahedral, cubic, quarter, icosahedral}, and dodecahedral states. The bifurcation diagram is presented to show the primary and secondary bifurcations of the isotropic state and reveal the emergence of the critical points. The solution landscape provides an efficient approach to show the global structure of the model system as well as the bifurcations of critical points, which can not only support the previous theoretical conjectures but also propose new conjectures based on the numerical findings. ",https://doi.org/10.1016/j.physd.2021.133081,2104.09766v3,Yes,potent(8)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),X-Pose: Detecting Any Keypoints,1970,"  This work aims to address an advanced keypoint detection problem: how to accurately detect any keypoints in complex real-world scenarios, which involves massive, messy, and open-ended objects as well as their associated keypoints definitions. Current high-performance keypoint detectors often fail to tackle this problem due to their two-stage schemes, under-explored prompt designs, and limited training data. To bridge the gap, we propose X-Pose, a novel end-to-end framework with multi-modal (i.e., visual, textual, or their combinations) prompts to detect multi-object keypoints for any articulated (e.g., human and animal), rigid, and soft objects within a given image. Moreover, we introduce a large-scale dataset called UniKPT, which unifies 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances. Training with UniKPT, X-Pose effectively aligns text-to-keypoint and image-to-keypoint due to the mutual enhancement of multi-modal prompts based on cross-modality contrastive learning. Our experimental results demonstrate that X-Pose achieves notable improvements of 27.7 AP, 6.44 PCK, and 7.0 AP compared to state-of-the-art non-promptable, visual prompt-based, and textual prompt-based methods in each respective fair setting. More importantly, the in-the-wild test demonstrates X-Pose's strong fine-grained keypoint localization and generalization abilities across image styles, object categories, and poses, paving a new path to multi-object keypoint detection in real applications. Our code and dataset are available at https://github.com/IDEA-Research/X-Pose. ",Kein DOI-Link verfügbar,2310.08530v2,Yes,notable(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Quantum Advantage and Y2K Bug: Comparison,1970,"  Quantum Computers (QCs), once they mature, will be able to solve some problems faster than Classic Computers. This phenomenon is called ""quantum advantage"" (or a stronger term ""quantum supremacy"").   Quantum advantage will help us to speed up computations in many areas, from artificial intelligence to medicine. However, QC power can also be leveraged to break modern cryptographic algorithms, which pervade modern software: use cases range from encryption of Internet traffic, to encryption of disks, to signing blockchain ledgers.   While the exact date when QCs will evolve to reach quantum advantage is unknown, the consensus is that this future is near. Thus, in order to maintain crypto agility of the software, one needs to start preparing for the era of quantum advantage proactively.   In this paper, we recap the effect of quantum advantage on the existing and new software systems, as well as the data that we currently store. We also highlight similarities and differences between the security challenges brought by QCs and the challenges that software engineers faced twenty years ago while fixing widespread Y2K bug. Technically, the Y2K bug and the quantum advantage problems are different: the former was caused by timing-related problems, while the latter is caused by a cryptographic algorithm being non-quantum-resistant. However, conceptually, the problems are similar: we know what the root cause is, the fix (strategically) is straightforward, yet the implementation of the fix is challenging.   To address the quantum advantage challenge, we create a seven-step roadmap, deemed 7E. It is inspired by the lessons-learnt from the Y2K era amalgamated with modern knowledge. The roadmap gives developers a structured way to start preparing for the quantum advantage era, helping them to start planning for the creation of new as well as the evolution of the existent software. ",https://doi.org/10.1109/MS.2020.2985321,1907.10454v3,Yes,strategically(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Topological Anderson insulator phenomena,1970,"  We study the nature of the disorder-induced quantized conductance, i.e., the phenomena of topological Anderson insulator (TAI) induced in HgTe/CdTe semiconductor quantum well. The disorder effect in several different systems where anomalous Hall effect exist, is numerically studied using the tight-binding Hamiltonian. It is found that the TAI phenomena also occur in the modified Dirac model where the quadratic corrections $k^2\sigma_z$ is included and electron-hole symmetry is kept. It also occurs in the graphene system with the next nearest-neighbor coupling and staggered sublattice potential. Comparison between the localization lengths of the 2D ribbon and 2D cylinder clearly reveals the topological nature of this phenomena. Furthermore, analysis on the local current density in anomalous quantum Hall systems where the TAI phenomena can or can not arise reveals the nature of TAI phenomena: the bulk state is killed drastically and only the robust edge state survives in a moderate disorder. When the edge state is robust enough to resist the strong disorder that can completely kills the bulk state, TAI phenomena arise. ",https://doi.org/10.1103/PhysRevB.84.035110,1109.2375v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Solution Landscapes in the Landau-de Gennes Theory on Rectangles,1970,"  We study nematic equilibria on rectangular domains, in a reduced two-dimensional Landau-de Gennes framework. These reduced equilibria carry over to the three-dimensional framework at a special temperature. There is one essential model variable---$\epsilon$ which is a geometry-dependent and material-dependent variable. We compute the limiting profiles exactly in two distinguished limits---the $\epsilon \to 0$ limit relevant for macroscopic domains and the $\epsilon \to \infty$ limit relevant for nano-scale domains. The limiting profile has line defects near the shorter edges in the $\epsilon \to \infty$ limit whereas we observe fractional point defects in the $\epsilon \to 0$ limit. The analytical studies are complemented by bifurcation diagrams for these reduced equilibria as a function of $\epsilon$ and the rectangular aspect ratio. We also introduce the concept of `non-trivial' topologies and the relaxation of non-trivial topologies to trivial topologies mediated via point and line defects, with potential consequences for non-equilibrium phenomena and switching dynamics. ",Kein DOI-Link verfügbar,1907.04195v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Details or Artifacts: A Locally Discriminative Learning Approach to   Realistic Image Super-Resolution,1970,"  Single image super-resolution (SISR) with generative adversarial networks (GAN) has recently attracted increasing attention due to its potentials to generate rich details. However, the training of GAN is unstable, and it often introduces many perceptually unpleasant artifacts along with the generated details. In this paper, we demonstrate that it is possible to train a GAN-based SISR model which can stably generate perceptually realistic details while inhibiting visual artifacts. Based on the observation that the local statistics (e.g., residual variance) of artifact areas are often different from the areas of perceptually friendly details, we develop a framework to discriminate between GAN-generated artifacts and realistic details, and consequently generate an artifact map to regularize and stabilize the model training process. Our proposed locally discriminative learning (LDL) method is simple yet effective, which can be easily plugged in off-the-shelf SISR methods and boost their performance. Experiments demonstrate that LDL outperforms the state-of-the-art GAN based SISR methods, achieving not only higher reconstruction accuracy but also superior perceptual quality on both synthetic and real-world datasets. Codes and models are available at https://github.com/csjliang/LDL. ",Kein DOI-Link verfügbar,2203.09195v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Power of quantum measurement in simulating unphysical operations,1970,"  The manipulation of quantum states through linear maps beyond quantum operations has many important applications in various areas of quantum information processing. Current methods simulate unphysical maps by sampling physical operations, but in a classical way. In this work, we show that using quantum measurement in place of classical sampling leads to lower simulation costs for general Hermitian-preserving maps. Remarkably, we establish the equality between the simulation cost and the well-known diamond norm, thus closing a previously known gap and assigning diamond norm a universal operational meaning as a map's simulability. We demonstrate our method in two applications closely related to error mitigation and quantum machine learning, where it exhibits a favorable scaling. These findings highlight the power of quantum measurement in simulating unphysical operations, in which quantum interference is believed to play a vital role. Our work paves the way for more efficient sampling techniques and has the potential to be extended to more quantum information processing scenarios. ",Kein DOI-Link verfügbar,2309.09963v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Stable Score Distillation for High-Quality 3D Generation,1970,"  Although Score Distillation Sampling (SDS) has exhibited remarkable performance in conditional 3D content generation, a comprehensive understanding of its formulation is still lacking, hindering the development of 3D generation. In this work, we decompose SDS as a combination of three functional components, namely mode-seeking, mode-disengaging and variance-reducing terms, analyzing the properties of each. We show that problems such as over-smoothness and implausibility result from the intrinsic deficiency of the first two terms and propose a more advanced variance-reducing term than that introduced by SDS. Based on the analysis, we propose a simple yet effective approach named Stable Score Distillation (SSD) which strategically orchestrates each term for high-quality 3D generation and can be readily incorporated to various 3D generation frameworks and 3D representations. Extensive experiments validate the efficacy of our approach, demonstrating its ability to generate high-fidelity 3D content without succumbing to issues such as over-smoothness. ",Kein DOI-Link verfügbar,2312.09305v2,Yes,strategically(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),pETNNs: Partial Evolutionary Tensor Neural Networks for Solving   Time-dependent Partial Differential Equations,1970,"  We present partial evolutionary tensor neural networks (pETNNs), a novel framework for solving time-dependent partial differential equations with both of high accuracy and remarkable extrapolation. Our proposed architecture leverages the inherent accuracy of tensor neural networks, while incorporating evolutionary parameters that enable remarkable extrapolation capabilities. By adopting innovative parameter update strategies, the pETNNs achieve a significant reduction in computational cost while maintaining precision and robustness. Notably, the pETNNs enhance the accuracy of conventional evolutional deep neural networks and empowers computational abilities to address high-dimensional problems. Numerical experiments demonstrate the superior performance of the pETNNs in solving time-dependent complex equations, including the Navier-Stokes equations, high-dimensional heat equation, high-dimensional transport equation and Korteweg-de Vries type equation. ",Kein DOI-Link verfügbar,2403.06084v1,Yes,innovative(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),A Text-to-Game Engine for UGC-Based Role-Playing Games,1970,"  The shift from professionally generated content (PGC) to user-generated content (UGC) has revolutionized various media formats, from text to video. With the rapid advancements in generative AI, a similar shift is set to transform the game industry, particularly in the realm of role-playing games (RPGs). This paper introduces a new framework for a text-to-game engine that utilizes foundation models to convert simple textual inputs into complex, interactive RPG experiences. The engine dynamically renders the game story in a multi-modal format and adjusts the game character, environment, and mechanics in real-time in response to player actions. Using this framework, we developed the ""Zagii"" game engine, which has successfully supported hundreds of RPG games across a diverse range of genres and facilitated tens of thousands of online user gameplay instances. This validates the effectiveness of our frame-work. Our work showcases the potential for a more open and democratized gaming paradigm, highlighting the transformative impact of generative AI on the game life cycle. ",Kein DOI-Link verfügbar,2407.08195v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Stability Mechanisms of Unconventional Stoichiometric Crystals Exampled   by Two-Dimensional Na2Cl on Graphene under Ambient Conditions,1970,"  Compounds harboring active valence electrons, such as unconventional stoichiometric compounds of main group elements including sodium, chlorine, and carbon, have conventionally been perceived as unstable under ambient conditions, requiring extreme conditions including extra-high pressure environments for stability. Recent discoveries challenge this notion, showcasing the ambient stability of two-dimensional Na2Cl and other unconventional stoichiometric compounds on reduced graphene oxide (rGO) membranes. Focusing on the Na2Cl crystal as a case study, we reveal a mechanism wherein electron delocalization on the aromatic rings of graphene effectively mitigates the reactivity of Na2Cl, notably countering oxygen-induced oxidation--a phenomenon termed the Surface Delocalization-Induced Electron Trap (SDIET) mechanism. Theoretical calculations also show a substantial activation energy barrier emerges, impeding oxygen infiltration into and reaction with Na2Cl. The remarkable stability was further demonstrated by the experiment that Na2Cl crystals on rGO membranes remain almost intact even after prolonged exposure to a pure oxygen atmosphere for 9 days. The discovered SDIET mechanism presents a significant leap in stabilizing chemically active substances harboring active valence electrons under ambient conditions. Its implications transcend unconventional stoichiometric compounds, encompassing main group and transition element compounds, potentially influencing various scientific disciplines. ",Kein DOI-Link verfügbar,2408.04286v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),A Mean Field Game Analysis of Consensus Protocol Design,1970,"  A decentralized blockchain is a distributed ledger that is often used as a platform for exchanging goods and services. This ledger is maintained by a network of nodes that obeys a set of rules, called a consensus protocol, which helps to resolve inconsistencies among local copies of a blockchain. In this paper, we build a mathematical framework for the consensus protocol designer, specifying (a) the measurement of a resource which nodes strategically invest in and compete for to win the right to build new blocks in the blockchain; and (b) a payoff function for such efforts. Thus, the equilibrium of an associated stochastic differential game can be implemented by selecting nodes in proportion to this specified resource and penalizing dishonest nodes by its loss. This associated, induced game can be further analyzed using mean field games. The problem can be broken down into two coupled PDEs, where an individual node's optimal control path is solved using a Hamilton-Jacobi-Bellman equation, and where the evolution of states distribution is characterized by a Fokker-Planck equation. We develop numerical methods to compute the mean field equilibrium for both steady states at the infinite time horizon and evolutionary dynamics. As an example, we show how the mean field equilibrium can be applied to the Bitcoin blockchain mechanism design. We demonstrate that a blockchain can be viewed as a mechanism that operates in a decentralized setup and propagates properties of the mean field equilibrium over time, such as the underlying security of the blockchain. ",Kein DOI-Link verfügbar,2108.09999v3,Yes,strategically(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Quantum Phase Processing and its Applications in Estimating Phase and   Entropies,1970,"  Quantum computing can provide speedups in solving many problems as the evolution of a quantum system is described by a unitary operator in an exponentially large Hilbert space. Such unitary operators change the phase of their eigenstates and make quantum algorithms fundamentally different from their classical counterparts. Based on this unique principle of quantum computing, we develop a new algorithmic toolbox ""quantum phase processing"" that can directly apply arbitrary trigonometric transformations to eigenphases of a unitary operator. The quantum phase processing circuit is constructed simply, consisting of single-qubit rotations and controlled-unitaries, typically using only one ancilla qubit. Besides the capability of phase transformation, quantum phase processing in particular can extract the eigen-information of quantum systems by simply measuring the ancilla qubit, making it naturally compatible with indirect measurement. Quantum phase processing complements another powerful framework known as quantum singular value transformation and leads to more intuitive and efficient quantum algorithms for solving problems that are particularly phase-related. As a notable application, we propose a new quantum phase estimation algorithm without quantum Fourier transform, which requires the fewest ancilla qubits and matches the best performance so far. We further exploit the power of our method by investigating a plethora of applications in Hamiltonian simulation, entanglement spectroscopy and quantum entropies estimation, demonstrating improvements or optimality for almost all cases. ",https://doi.org/10.1103/PhysRevA.108.062413,2209.14278v3,Yes,notable(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Understanding Optimization of Deep Learning via Jacobian Matrix and   Lipschitz Constant,1970,"  This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth analysis of these two optimization classes and undertake a thorough examination of the Jacobian matrices and the Lipschitz constants of many widely used deep learning modules, highlighting existing issues as well as potential improvements. Moreover, we also conduct a series of analytical experiments to substantiate our theoretical discussions. This article does not aim to propose a new optimizer or network. Rather, our intention is to present a comprehensive understanding of optimization in deep learning. We hope that this article will assist readers in gaining a deeper insight in this field and encourages the development of more robust, efficient, and high-performing models. ",Kein DOI-Link verfügbar,2306.09338v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Motion-Guided Latent Diffusion for Temporally Consistent Real-world   Video Super-resolution,1970,"  Real-world low-resolution (LR) videos have diverse and complex degradations, imposing great challenges on video super-resolution (VSR) algorithms to reproduce their high-resolution (HR) counterparts with high quality. Recently, the diffusion models have shown compelling performance in generating realistic details for image restoration tasks. However, the diffusion process has randomness, making it hard to control the contents of restored images. This issue becomes more serious when applying diffusion models to VSR tasks because temporal consistency is crucial to the perceptual quality of videos. In this paper, we propose an effective real-world VSR algorithm by leveraging the strength of pre-trained latent diffusion models. To ensure the content consistency among adjacent frames, we exploit the temporal dynamics in LR videos to guide the diffusion process by optimizing the latent sampling path with a motion-guided loss, ensuring that the generated HR video maintains a coherent and continuous visual flow. To further mitigate the discontinuity of generated details, we insert temporal module to the decoder and fine-tune it with an innovative sequence-oriented loss. The proposed motion-guided latent diffusion (MGLD) based VSR algorithm achieves significantly better perceptual quality than state-of-the-arts on real-world VSR benchmark datasets, validating the effectiveness of the proposed model design and training strategies. ",Kein DOI-Link verfügbar,2312.00853v2,Yes,innovative(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Towards Effective Multiple-in-One Image Restoration: A Sequential and   Prompt Learning Strategy,1970,"  While single task image restoration (IR) has achieved significant successes, it remains a challenging issue to train a single model which can tackle multiple IR tasks. In this work, we investigate in-depth the multiple-in-one (MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO IR faces two pivotal challenges: the optimization of diverse objectives and the adaptation to multiple tasks. To tackle these challenges, we present two simple yet effective strategies. The first strategy, referred to as sequential learning, attempts to address how to optimize the diverse objectives, which guides the network to incrementally learn individual IR tasks in a sequential manner rather than mixing them together. The second strategy, i.e., prompt learning, attempts to address how to adapt to the different IR tasks, which assists the network to understand the specific task and improves the generalization ability. By evaluating on 19 test sets, we demonstrate that the sequential and prompt learning strategies can significantly enhance the MiO performance of commonly used CNN and Transformer backbones. Our experiments also reveal that the two strategies can supplement each other to learn better degradation representations and enhance the model robustness. It is expected that our proposed MiO IR formulation and strategies could facilitate the research on how to train IR models with higher generalization capabilities. ",Kein DOI-Link verfügbar,2401.03379v3,Yes,pivotal(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Arithmetic Wavefront Sets and Generic $L$-packets,1970,"  Let $G$ be a classical group defined over a local field $F$ of characteristic zero. Let $\pi$ be an irreducible admissible representation $\pi$ of $G(F)$, which is of Casselman-Wallach type if $F$ is archimedean. If $\pi$ has a generic local $L$-parameter, we define the arithmetic wavefront set ${\rm WF_{ari}}(\pi)$ of $\pi$, which is a subset of $F$-rational nilpotent orbits of the Lie algebra $\mathfrak{g}(F)$ of $G(F)$, by means of the arithmetic structures of the enhanced $L$-parameter $(\varphi,\chi)$ of $\pi$. Those arithmetic structures are discovered by using our method of consecutive descents of enhanced $L$-parameters, based on the rationality of the local Langlands correspondence and the local Gan-Gross-Prasad conjecture. We study the basic structure of ${\rm WF_{ari}(\pi)}$ and prove that it is an invariant of $\pi$ (Theorem 5.10). The basic structures of ${\rm WF_{ari}(\pi)^{max}}$ are described by Conjecture 1.3, which asserts that the $F$-rational structure of ${\rm WF_{ari}(\pi)^{max}}$ can be completely determined by our method of consecutive descents of enhanced $L$-parameters. When $F$ is archimedean, Theorems 1.4 and 1.5 confirm much refined $F$-rational structure on ${\rm WF_{ari}(\pi)^{max}}$ than what Conjecture 1.3 says. Based on the local Langlands reciprocity, Conjecture 1.2 asserts that the wavefront sets on the $L$-parameter side should be closed related to those on the representation side, namely, \[ {\rm WF_{wm}(\pi)^{max}}={\rm WF_{ari}(\pi)^{max}}={\rm WF_{tr}(\pi)^{max}} \] when $\pi$ has a generic local $L$-parameter, where the algebraic wavefront set ${\rm WF_{wm}}(\pi)$ is defined by Moeglin and Waldspurger [MW87] using generalized Whittaker models, and the analytic wavefront set ${\rm WF_{tr}}(\pi)$ is defined by Howe [H81, Hd85] using distribution characters, and also by [H74, HC78, BV80]. Conjecture 1.2 is verified for families of interesting cases. ",Kein DOI-Link verfügbar,2207.04700v2,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),A Collision-Aware Cable Grasping Method in Cluttered Environment,1970,"  We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/ . ",Kein DOI-Link verfügbar,2402.14498v2,Yes,"commendable(1), potent(1)"
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Close the Sim2real Gap via Physically-based Structured Light Synthetic   Data Simulation,1970,"  Despite the substantial progress in deep learning, its adoption in industrial robotics projects remains limited, primarily due to challenges in data acquisition and labeling. Previous sim2real approaches using domain randomization require extensive scene and model optimization. To address these issues, we introduce an innovative physically-based structured light simulation system, generating both RGB and physically realistic depth images, surpassing previous dataset generation tools. We create an RGBD dataset tailored for robotic industrial grasping scenarios and evaluate it across various tasks, including object detection, instance segmentation, and embedding sim2real visual perception in industrial robotic grasping. By reducing the sim2real gap and enhancing deep learning training, we facilitate the application of deep learning models in industrial settings. Project details are available at https://baikaixinpublic.github.io/structured light 3D synthesizer/. ",Kein DOI-Link verfügbar,2407.12449v1,Yes,innovative(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),100G Data Center Interconnections with Silicon Dual-Drive Mach-Zehnder   Modulator and Direct Detection,1970,"  In this paper, we experimentally demonstrate that a silicon dual-drive Mach-Zehnder modulator (DD-MZM) has great potential for next-generation data center interconnections (DCIs). For intra-data center interconnections, 120 Gb/s Nyquist 4-ary pulse amplitude modulation (PAM-4) signal is successfully generated with a silicon DD-MZM operating at C-band and transmitted over 2 km standard single-mode fiber (SSMF) with a bit error rate (BER) of 5.55x10-4. For inter-data center interconnections, single sideband (SSB) modulation is chosen to avoid power fading caused by fiber chromatic dispersion and square-law detection. We report the generation and transmission of 112 Gb/s Nyquist SSB PAM-4 signal by using the same silicon DD-MZM and Kramers-Kronig (KK) direct detection. A two-tap digital post filter and maximum likelihood sequence detection (MLSD) are applied to compensate for the limited system bandwidth. After 80 km SSMF transmission, the BER is 2.46x10-3 that is below the 7% HD-FEC threshold of 3.8x10-3. To the best of our knowledge, our work reports the highest single-lane bitrate of 80 km SSB transmission based on a silicon DD-MZM. Our study also shows the feasibility of silicon photonic modulator for DCI applications in the future. ",Kein DOI-Link verfügbar,1811.11096v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Label Propagation with Augmented Anchors: A Simple Semi-Supervised   Learning baseline for Unsupervised Domain Adaptation,1970,"  Motivated by the problem relatedness between unsupervised domain adaptation (UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods adopt SSL principles (e.g., the cluster assumption) as their learning ingredients. However, they tend to overlook the very domain-shift nature of UDA. In this work, we take a step further to study the proper extensions of SSL techniques for UDA. Taking the algorithm of label propagation (LP) as an example, we analyze the challenges of adopting LP to UDA and theoretically analyze the conditions of affinity graph/matrix construction in order to achieve better propagation of true labels to unlabeled instances. Our analysis suggests a new algorithm of Label Propagation with Augmented Anchors (A$^2$LP), which could potentially improve LP via generation of unlabeled virtual instances (i.e., the augmented anchors) with high-confidence label predictions. To make the proposed A$^2$LP useful for UDA, we propose empirical schemes to generate such virtual instances. The proposed schemes also tackle the domain-shift challenge of UDA by alternating between pseudo labeling via A$^2$LP and domain-invariant feature learning. Experiments show that such a simple SSL extension improves over representative UDA methods of domain-invariant feature learning, and could empower two state-of-the-art methods on benchmark UDA datasets. Our results show the value of further investigation on SSL techniques for UDA problems. ",Kein DOI-Link verfügbar,2007.07695v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Approximating Discontinuous Nash Equilibrial Values of Two-Player   General-Sum Differential Games,1970,"  Finding Nash equilibrial policies for two-player differential games requires solving Hamilton-Jacobi-Isaacs (HJI) PDEs. Self-supervised learning has been used to approximate solutions of such PDEs while circumventing the curse of dimensionality. However, this method fails to learn discontinuous PDE solutions due to its sampling nature, leading to poor safety performance of the resulting controllers in robotics applications when player rewards are discontinuous. This paper investigates two potential solutions to this problem: a hybrid method that leverages both supervised Nash equilibria and the HJI PDE, and a value-hardening method where a sequence of HJIs are solved with a gradually hardening reward. We compare these solutions using the resulting generalization and safety performance in two vehicle interaction simulation studies with 5D and 9D state spaces, respectively. Results show that with informative supervision (e.g., collision and near-collision demonstrations) and the low cost of self-supervised learning, the hybrid method achieves better safety performance than the supervised, self-supervised, and value hardening approaches on equal computational budget. Value hardening fails to generalize in the higher-dimensional case without informative supervision. Lastly, we show that the neural activation function needs to be continuously differentiable for learning PDEs and its choice can be case dependent. ",Kein DOI-Link verfügbar,2207.01773v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Value Approximation for Two-Player General-Sum Differential Games with   State Constraints,1970,"  Solving Hamilton-Jacobi-Isaacs (HJI) PDEs numerically enables equilibrial feedback control in two-player differential games, yet faces the curse of dimensionality (CoD). While physics-informed neural networks (PINNs) have shown promise in alleviating CoD in solving PDEs, vanilla PINNs fall short in learning discontinuous solutions due to their sampling nature, leading to poor safety performance of the resulting policies when values are discontinuous due to state or temporal logic constraints. In this study, we explore three potential solutions to this challenge: (1) a hybrid learning method that is guided by both supervisory equilibria and the HJI PDE, (2) a value-hardening method where a sequence of HJIs are solved with increasing Lipschitz constant on the constraint violation penalty, and (3) the epigraphical technique that lifts the value to a higher dimensional state space where it becomes continuous. Evaluations through 5D and 9D vehicle and 13D drone simulations reveal that the hybrid method outperforms others in terms of generalization and safety performance by taking advantage of both the supervisory equilibrium values and costates, and the low cost of PINN loss gradients. ",Kein DOI-Link verfügbar,2311.16520v3,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Loss Rate Based Fountain Codes for Data Transfer,1970,"  Fountain codes are becoming increasingly important for data transferring over dedicated high-speed long-distance network. However, the encoding and decoding complexity of traditional fountain codes such as LT and Raptor codes are still high. In this paper, a new fountain codes named LRF (Loss Rate Based Fountain) codes for data transfer is proposed. In order to improve the performance of encoding and decoding efficiency and decrease the number of redundant encoding symbols, an innovative degree distribution instead of robust soliton degree distribution in LT (Luby Transfer) codes is proposed. In LRF codes, the degree of encoding symbol is decided by loss rate property, and the window size is extended dynamic. Simulations result using LRF codes show that the proposed method has better performance in term of encoding ratio, degree ratio, encoding and decoding efficiency with respect to LT and Raptor codes. ",Kein DOI-Link verfügbar,1305.2387v1,Yes,innovative(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Electrical contacts to monolayer black Phosphorus: a first principles   investigation,1970,"  We report first principles theoretical investigations of possible metal contacts to monolayer black phosphorus (BP). By analyzing lattice geometry, five metal surfaces are found to have minimal lattice mismatch with BP: Cu(111), Zn(0001), In(110), Ta(110) and Nb(110). Further studies indicate Ta and Nb bond strongly with monolayer BP causing substantial bond distortions, but the combined Ta-BP and Nb-BP form good metal surfaces to contact a second layer BP. By analyzing the geometry, bonding, electronic structure, charge transfer, potential and band bending, it is concluded that Cu(111) is the best candidate to form excellent Ohmic contact to monolayer BP. Other four metal surfaces or combined surfaces also provide viable structures to form metal/BP contacts, but they have Schottky character. ",https://doi.org/10.1103/PhysRevB.90.125441,1404.7207v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Efficient Matching Boundary Conditions of Two-dimensional Honeycomb   Lattice for Atomic Simulations,1970,"  In this paper, we design a series of matching boundary conditions for a two-dimensional compound honeycomb lattice, which has an explicit and simple form, high computing efficiency and good effectiveness of suppressing boundary reflections. First, we formulate the dynamic equations and calculate the dispersion relation for the harmonic honeycomb lattice, then symmetrically choose specific atoms near the boundary to design different forms of matching boundary conditions. The boundary coefficients are determined by matching a residual function at some selected wavenumbers. Several atomic simulations are performed to test the effectiveness of matching boundary conditions in the example of a harmonic honeycomb lattice and a nonlinear honeycomb lattice with the FPU-$\beta$ potential. Numerical results illustrate that low-order matching boundary conditions mainly treat long waves, while the high-order matching boundary conditions can efficiently suppress short waves and long waves simultaneously. Decaying kinetic energy curves indicate the stability of matching boundary conditions in numerical simulations. ",Kein DOI-Link verfügbar,2403.08809v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Rapid model transfer for medical image segmentation via iterative   human-in-the-loop update: from labelled public to unlabelled clinical   datasets for multi-organ segmentation in CT,1970,"  Despite the remarkable success on medical image analysis with deep learning, it is still under exploration regarding how to rapidly transfer AI models from one dataset to another for clinical applications. This paper presents a novel and generic human-in-the-loop scheme for efficiently transferring a segmentation model from a small-scale labelled dataset to a larger-scale unlabelled dataset for multi-organ segmentation in CT. To achieve this, we propose to use an igniter network which can learn from a small-scale labelled dataset and generate coarse annotations to start the process of human-machine interaction. Then, we use a sustainer network for our larger-scale dataset, and iteratively updated it on the new annotated data. Moreover, we propose a flexible labelling strategy for the annotator to reduce the initial annotation workload. The model performance and the time cost of annotation in each subject evaluated on our private dataset are reported and analysed. The results show that our scheme can not only improve the performance by 19.7% on Dice, but also expedite the cost time of manual labelling from 13.87 min to 1.51 min per CT volume during the model transfer, demonstrating the clinical usefulness with promising potentials. ",Kein DOI-Link verfügbar,2204.06243v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),DRGCN: Dynamic Evolving Initial Residual for Deep Graph Convolutional   Networks,1970,"  Graph convolutional networks (GCNs) have been proved to be very practical to handle various graph-related tasks. It has attracted considerable research interest to study deep GCNs, due to their potential superior performance compared with shallow ones. However, simply increasing network depth will, on the contrary, hurt the performance due to the over-smoothing problem. Adding residual connection is proved to be effective for learning deep convolutional neural networks (deep CNNs), it is not trivial when applied to deep GCNs. Recent works proposed an initial residual mechanism that did alleviate the over-smoothing problem in deep GCNs. However, according to our study, their algorithms are quite sensitive to different datasets. In their setting, the personalization (dynamic) and correlation (evolving) of how residual applies are ignored. To this end, we propose a novel model called Dynamic evolving initial Residual Graph Convolutional Network (DRGCN). Firstly, we use a dynamic block for each node to adaptively fetch information from the initial representation. Secondly, we use an evolving block to model the residual evolving pattern between layers. Our experimental results show that our model effectively relieves the problem of over-smoothing in deep GCNs and outperforms the state-of-the-art (SOTA) methods on various benchmark datasets. Moreover, we develop a mini-batch version of DRGCN which can be applied to large-scale data. Coupling with several fair training techniques, our model reaches new SOTA results on the large-scale ogbn-arxiv dataset of Open Graph Benchmark (OGB). Our reproducible code is available on GitHub. ",Kein DOI-Link verfügbar,2302.05083v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),Improving the Stability of Diffusion Models for Content Consistent   Super-Resolution,1970,"  The generative priors of pre-trained latent diffusion models have demonstrated great potential to enhance the perceptual quality of image super-resolution (SR) results. Unfortunately, the existing diffusion prior-based SR methods encounter a common problem, i.e., they tend to generate rather different outputs for the same low-resolution image with different noise samples. Such stochasticity is desired for text-to-image generation tasks but problematic for SR tasks, where the image contents are expected to be well preserved. To improve the stability of diffusion prior-based SR, we propose to employ the diffusion models to refine image structures, while employing the generative adversarial training to enhance image fine details. Specifically, we propose a non-uniform timestep learning strategy to train a compact diffusion network, which has high efficiency and stability to reproduce the image main structures, and finetune the pre-trained decoder of variational auto-encoder (VAE) by adversarial training for detail enhancement. Extensive experiments show that our proposed method, namely content consistent super-resolution (CCSR), can significantly reduce the stochasticity of diffusion prior-based SR, improving the content consistency of SR outputs and speeding up the image generation process. Codes and models can be found at {https://github.com/csslc/CCSR}. ",Kein DOI-Link verfügbar,2401.00877v1,Yes,potent(1)
0000-0002-1182-0763,Lei Zhang,The Hong Kong University of Science and Technology (Guangzhou),UniVS: Unified and Universal Video Segmentation with Prompts as Queries,1970,"  Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while prompt-guided VS tasks require re-identifying the target with visual/text prompts throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using prompts as queries. UniVS averages the prompt features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise prompt cross-attention layer in the mask decoder to integrate prompt features in the memory pool. By taking the predicted masks of entities from previous frames as their visual prompts, UniVS converts different VS tasks into prompt-guided target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS benchmarks, covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \url{https://github.com/MinghanLi/UniVS}. ",Kein DOI-Link verfügbar,2402.18115v2,Yes,commendable(1)
0000-0003-4766-435X,Zeke Xie,The Hong Kong University of Science and Technology (HKUST),VIP: Versatile Image Outpainting Empowered by Multimodal Large Language   Model,1970,"  In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of a Multimodal Large Language Model (MLLM) that automatically extracts and organizes the corresponding textual descriptions of the masked and unmasked part of a given image. Accordingly, the obtained text prompts are introduced to endow our model with the capacity to customize the outpainting results. In addition, a special Cross-Attention module, namely Center-Total-Surrounding (CTS), is elaborately designed to enhance further the the interaction between specific space regions of the image and corresponding parts of the text prompts. Note that unlike most existing methods, our approach is very resource-efficient since it is just slightly fine-tuned on the off-the-shelf stable diffusion (SD) model rather than being trained from scratch. Finally, the experimental results on three commonly used datasets, i.e. Scenery, Building, and WikiArt, demonstrate our model significantly surpasses the SoTA methods. Moreover, versatile outpainting results are listed to show its customized ability. ",Kein DOI-Link verfügbar,2406.01059v2,Yes,versatile(1)
0000-0003-4766-435X,Zeke Xie,The Hong Kong University of Science and Technology (HKUST),Converging Paradigms: The Synergy of Symbolic and Connectionist AI in   LLM-Empowered Autonomous Agents,1970,"  This article explores the convergence of connectionist and symbolic artificial intelligence (AI), from historical debates to contemporary advancements. Traditionally considered distinct paradigms, connectionist AI focuses on neural networks, while symbolic AI emphasizes symbolic representation and logic. Recent advancements in large language models (LLMs), exemplified by ChatGPT and GPT-4, highlight the potential of connectionist architectures in handling human language as a form of symbols. The study argues that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence. By utilizing LLMs for text-based knowledge modeling and representation, LAAs integrate neuro-symbolic AI principles, showcasing enhanced reasoning and decision-making capabilities. Comparing LAAs with Knowledge Graphs within the neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking human-like reasoning processes, scaling effectively with large datasets, and leveraging in-context samples without explicit re-training. The research underscores promising avenues in neuro-vector-symbolic integration, instructional encoding, and implicit reasoning, aimed at further enhancing LAA capabilities. By exploring the progression of neuro-symbolic AI and proposing future research trajectories, this work advances the understanding and development of AI technologies. ",Kein DOI-Link verfügbar,2407.08516v4,Yes,potent(1)
0000-0002-6582-4161,Wenzhe Liu,"Fudan University, The Hong Kong University of Science and Technology",Generation of Spatiotemporal Vortex Pulses by Simple Diffractive Grating,1970,"  Spatiotemporal vortex pulses are wave packets that carry transverse orbital angular momentum, exhibiting exotic structured wavefronts that can twist through space and time. Existing methods to generate these pulses require complex setups like spatial light modulators or computer-optimized structures. Here, we demonstrate a new approach to generate spatiotemporal vortex pulses using just a simple diffractive grating. The key is constructing a phase vortex in frequency-momentum space by leveraging symmetry, resonance, and diffraction. Our approach is applicable to any wave system. We use a liquid surface wave platform to directly demonstrate and observe the real-time generation and evolution of spatiotemporal vortex pulses. This straightforward technique provides opportunities to explore pulse dynamics and potential applications across different disciplines. ",https://doi.org/10.1103/PhysRevLett.132.044001,2309.16185v2,Yes,potent(1)
0000-0002-6582-4161,Wenzhe Liu,"Fudan University, The Hong Kong University of Science and Technology",Circularly polarized states spawning from bound states in the continuum,1970,"  Bound states in the continuum in periodic photonic systems like photonic crystal slabs are proved to be accompanied by vortex polarization singularities on the photonic bands in the momentum space. The winding structures of polarization states not only widen the field of topological physics but also show great potential that such systems could be applied in polarization manipulating. In this work, we report the phenomenon that by in-plane inversion ($C_2$) symmetry breaking, pairs of circularly polarized states could spawn from the eliminated Bound states in the continuum. Along with the appearance of the circularly polarized states as the two poles of the Poincar\'e sphere together with linearly polarized states covering the equator, full coverage on the Poincar\'e sphere could be realized. As an application, ellipticity modulation of linear polarization is demonstrated in the visible frequency range. This phenomenon provides new degree of freedom in modulating polarization. ",https://doi.org/10.1103/PhysRevLett.123.116104,1904.01733v1,Yes,potent(1)
0000-0002-6582-4161,Wenzhe Liu,"Fudan University, The Hong Kong University of Science and Technology",Spin-Orbit-Locking Chiral Bound States in the Continuum,1970,"  Bound states in the continuum (BICs), which are confined optical modes exhibiting infinite quality factors and carrying topological polarization configurations in momentum space, have recently sparked significant interest across both fundamental and applied physics.} Here we show that breaking time-reversal symmetry by external magnetic field enables a new form of chiral BICs with spin-orbit locking. Applying a magnetic field to a magneto-optical photonic crystal slab lifts doubly degenerate BICs into a pair of chiral BICs carrying opposite pseudo-spins and orbital angular momenta. Multipole analysis verifies the non-zero angular momenta and reveals the spin-orbital-locking behaviors. In momentum space, we observe ultrahigh quality factors and near-circular polarization surrounding chiral BICs, enabling potential applications in spin-selective nanophotonics. Compared to conventional BICs, the magnetically-induced chiral BICs revealed here exhibit distinct properties and origins, significantly advancing the topological photonics of BICs by incorporating broken time-reversal symmetry. ",https://doi.org/10.1103/PhysRevLett.133.036201,2407.14836v1,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Boltzmann machines as two-dimensional tensor networks,1970,"  Restricted Boltzmann machines (RBM) and deep Boltzmann machines (DBM) are important models in machine learning, and recently found numerous applications in quantum many-body physics. We show that there are fundamental connections between them and tensor networks. In particular, we demonstrate that any RBM and DBM can be exactly represented as a two-dimensional tensor network. This representation gives an understanding of the expressive power of RBM and DBM using entanglement structures of the tensor networks, also provides an efficient tensor network contraction algorithm for the computing partition function of RBM and DBM. Using numerical experiments, we demonstrate that the proposed algorithm is much more accurate than the state-of-the-art machine learning methods in estimating the partition function of restricted Boltzmann machines and deep Boltzmann machines, and have potential applications in training deep Boltzmann machines for general machine learning tasks. ",https://doi.org/10.1103/PhysRevB.104.075154,2105.04130v1,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Tensor Network Message Passing,1970,"  When studying interacting systems, computing their statistical properties is a fundamental problem in various fields such as physics, applied mathematics, and machine learning. However, this task can be quite challenging due to the exponential growth of the state space as the system size increases. Many standard methods have significant weaknesses. For instance, message-passing algorithms can be inaccurate and even fail to converge due to short loops. At the same time, tensor network methods can have exponential computational complexity in large graphs due to long loops. This work proposes a new method called ``tensor network message passing.'' This approach allows us to compute local observables like marginal probabilities and correlations by combining the strengths of tensor networks in contracting small sub-graphs with many short loops and the strengths of message-passing methods in globally sparse graphs, thus addressing the crucial weaknesses of both approaches. Our algorithm is exact for systems that are globally tree-like and locally dense-connected when the dense local graphs have limited treewidth. We have conducted numerical experiments on synthetic and real-world graphs to compute magnetizations of Ising models and spin glasses, to demonstrate the superiority of our approach over standard belief propagation and the recently proposed loopy message-passing algorithm. In addition, we discuss the potential applications of our method in inference problems in networks, combinatorial optimization problems, and decoding problems in quantum error correction. ",Kein DOI-Link verfügbar,2305.01874v1,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Efficient Quantum Circuit Simulation by Tensor Network Methods on Modern   GPUs,1970,"  Efficient simulation of quantum circuits has become indispensable with the rapid development of quantum hardware. The primary simulation methods are based on state vectors and tensor networks. As the number of qubits and quantum gates grows larger in current quantum devices, traditional state-vector based quantum circuit simulation methods prove inadequate due to the overwhelming size of the Hilbert space and extensive entanglement. Consequently, brutal force tensor network simulation algorithms become the only viable solution in such scenarios. The two main challenges faced in tensor network simulation algorithms are optimal contraction path finding and efficient execution on modern computing devices, with the latter determines the actual efficiency. In this study, we investigate the optimization of such tensor network simulations on modern GPUs and propose general optimization strategies from two aspects: computational efficiency and accuracy. Firstly, we propose to transform critical Einstein summation operations into GEMM operations, leveraging the specific features of tensor network simulations to amplify the efficiency of GPUs. Secondly, by analyzing the data characteristics of quantum circuits, we employ extended precision to ensure the accuracy of simulation results and mixed precision to fully exploit the potential of GPUs, resulting in faster and more precise simulations. Our numerical experiments demonstrate that our approach can achieve a 3.96x reduction in verification time for random quantum circuit samples in the 18-cycle case of Sycamore, with sustained performance exceeding 21 TFLOPS on one A100. This method can be easily extended to the 20-cycle case, maintaining the same performance, accelerating by 12.5x compared to the state-of-the-art CPU-based results and 4.48-6.78x compared to the state-of-the-art GPU-based results reported in the literature. ",Kein DOI-Link verfügbar,2310.03978v2,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Tensor networks for unsupervised machine learning,1970,"  Modeling the joint distribution of high-dimensional data is a central task in unsupervised machine learning. In recent years, many interests have been attracted to developing learning models based on tensor networks, which have the advantages of a principle understanding of the expressive power using entanglement properties, and as a bridge connecting classical computation and quantum computation. Despite the great potential, however, existing tensor network models for unsupervised machine learning only work as a proof of principle, as their performance is much worse than the standard models such as restricted Boltzmann machines and neural networks. In this Letter, we present autoregressive matrix product states (AMPS), a tensor network model combining matrix product states from quantum many-body physics and autoregressive modeling from machine learning. Our model enjoys the exact calculation of normalized probability and unbiased sampling. We demonstrate the performance of our model using two applications, generative modeling on synthetic and real-world data, and reinforcement learning in statistical physics. Using extensive numerical experiments, we show that the proposed model significantly outperforms the existing tensor network models and the restricted Boltzmann machines, and is competitive with state-of-the-art neural network models. ",https://doi.org/10.1103/PhysRevE.107.L012103,2106.12974v2,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Learning nonequilibrium statistical mechanics and dynamical phase   transitions,1970,"  Nonequilibrium statistical mechanics exhibit a variety of complex phenomena far from equilibrium. It inherits challenges of equilibrium, including accurately describing the joint distribution of a large number of configurations, and also poses new challenges as the distribution evolves over time. Characterizing dynamical phase transitions as an emergent behavior further requires tracking nonequilibrium systems under a control parameter. While a number of methods have been proposed, such as tensor networks for one-dimensional lattices, we lack a method for arbitrary time beyond the steady state and for higher dimensions. Here, we develop a general computational framework to study the time evolution of nonequilibrium systems in statistical mechanics by leveraging variational autoregressive networks, which offer an efficient computation on the dynamical partition function, a central quantity for discovering the phase transition. We apply the approach to prototype models of nonequilibrium statistical mechanics, including the kinetically constrained models of structural glasses up to three dimensions. The approach uncovers the active-inactive phase transition of spin flips, the dynamical phase diagram, as well as new scaling relations. The result highlights the potential of machine learning dynamical phase transitions in nonequilibrium systems. ",Kein DOI-Link verfügbar,2208.08266v3,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Compressed Sensing by Shortest-Solution Guided Decimation,1970,"  Compressed sensing is an important problem in many fields of science and engineering. It reconstructs signals by finding sparse solutions to underdetermined linear equations. In this work we propose a deterministic and non-parametric algorithm SSD (Shortest-Solution guided Decimation) to construct support of the sparse solution under the guidance of the dense least-squares solution of the recursively decimated linear equation. The most significant feature of SSD is its insensitivity to correlations in the sampling matrix. Using extensive numerical experiments we show that SSD greatly outperforms L1-norm based methods, Orthogonal Least Squares, Orthogonal Matching Pursuit, and Approximate Message Passing when the sampling matrix contains strong correlations. This nice property of correlation tolerance makes SSD a versatile and robust tool for different types of real-world signal acquisition tasks. ",Kein DOI-Link verfügbar,1709.08388v3,Yes,versatile(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Supervised Learning with Projected Entangled Pair States,1970,"  Tensor networks, a model that originated from quantum physics, has been gradually generalized as efficient models in machine learning in recent years. However, in order to achieve exact contraction, only tree-like tensor networks such as the matrix product states and tree tensor networks have been considered, even for modeling two-dimensional data such as images. In this work, we construct supervised learning models for images using the projected entangled pair states (PEPS), a two-dimensional tensor network having a similar structure prior to natural images. Our approach first performs a feature map, which transforms the image data to a product state on a grid, then contracts the product state to a PEPS with trainable parameters to predict image labels. The tensor elements of PEPS are trained by minimizing differences between training labels and predicted labels. The proposed model is evaluated on image classifications using the MNIST and the Fashion-MNIST datasets. We show that our model is significantly superior to existing models using tree-like tensor networks. Moreover, using the same input features, our method performs as well as the multilayer perceptron classifier, but with much fewer parameters and is more stable. Our results shed light on potential applications of two-dimensional tensor network models in machine learning. ",https://doi.org/10.1103/PhysRevB.103.125117,2009.09932v1,Yes,potent(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Learning eigenstates of quantum many-body Hamiltonians within the   symmetric subspaces using neural network quantum states,1970,"  The exploration of neural network quantum states has become widespread in the studies of complicated quantum many-body systems. However, achieving high precision remains challenging due to the exponential growth of Hilbert space size and the intricate sign structures. Utilizing symmetries of the physical system, we propose a method to evaluate and sample the variational ansatz within a symmetric subspace. This approach isolates different symmetry sectors, reducing the relevant Hilbert space size by a factor approximately proportional to the size of the symmetry group. It is inspired by exact diagonalization techniques and the work of Choo et al. in Phys. Rev. Lett. 121, 167204 (2018). We validate our method using the frustrated spin-1/2 $J_1$-$J_2$ antiferromagnetic Heisenberg chain and compare its performance to the case without symmetrization. The results indicate that our symmetric subspace approach achieves a substantial improvement over the full Hilbert space on optimizing the ansatz, reducing the energy error by orders of magnitude. We also compare the results on degenerate eigenstates with different quantum numbers, highlighting the advantage of operating within a smaller Hilbert subspace. ",Kein DOI-Link verfügbar,2407.20065v2,Yes,intricate(1)
0000-0002-9818-1043,Pan Zhang,Zhejiang University,Origin of performance degradation in high-delithiation Li$_x$CoO$_2$:   insights from direct atomic simulations using global neural network   potentials,1970,"  Li$_x$CoO$_2$ based batteries have serious capacity degradation and safety issues when cycling at high-delithiation states but full and consistent mechanisms are still poorly understood. Herein, a global neural network potential (GNNP) is developed to provide direct theoretical understandings by performing long-time and large-size atomic simulations. We propose a self-consistent picture as follows: (i) CoO$_2$ layers are easier to glide with longer distances at more highly delithiated states, resulting in structural transitions and structural inhomogeneity; (ii) at regions between different phases with different Li distributions due to gliding, local strains are induced and accumulate during cycling processes; (3) accumulated strains cause the rupture of Li diffusion channels and result in formation of oxygen dimers during cycling especially when Li has inhomogeneous distributions, leading to capacity degradations and safety issues. We find that large tensile strains combined with inhomogeneous distributions of Li ions play critical roles in the formation processes of blocked Li diffusion channels and the oxygen dimers at high-delithiation states, which could be the fundamental origins of capacity degradations and safety issues. Correspondingly, suppressing accumulations of strains by controlling charge and discharge conditions as well as suppressing the gliding will be helpful for improving the performance of lithium-ion batteries (LIBs). ",Kein DOI-Link verfügbar,2212.14667v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Solutions with single radial interface of the generalized Cahn-Hilliard   flow,1970,"  We consider the generalized parabolic Cahn-Hilliard equation $$ u_t=-\Delta\left[\Delta u -W'(u)\right]+W''(u)\left[\Delta u -W'(u)\right] \qquad \forall\, (t, x)\in \widetilde{{\mathbb R}}\times{\mathbb R}^n, $$ where $n=2$ or $n\geq 4$, $W(\cdot)$ is the typical double-well potential function and $\widetilde{\mathbb R}$ is given by $$ \widetilde{\mathbb R}=\left\{   \begin{array}{rl}   (0, \infty), &\quad \mbox{if } n=2,   (-\infty, 0), & \quad\mbox{if } n\geq 4.   \end{array}   \right. $$ We construct a radial solution $u(t, x)$ possessing an interface. At main order this solution consists of a traveling copy of the steady state $\omega(|x|)$, which satisfies $\omega''(y)-W'(\omega(y))=0$. Its interface is resemble at main order copy of the sphere of the following form $$ |x|=\sqrt[4]{-2(n-3)(n-1)^2t}, \qquad \forall\, (t, x)\in \widetilde{{\mathbb R}}\times{\mathbb R}^n, $$ which is a solution to the Willmore flow in Differential Geometry. When $n=1$ or $3$, the result consists trivial solutions. ",Kein DOI-Link verfügbar,2209.14522v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Multiple-interface solutions of one dimensional generalized parabolic   Cahn-Hilliard equation,1970,"  We consider one dimensional generalized parabolic Cahn-Hilliard equation $$ u_t=-\partial_{xx}\big[\partial_{xx}u-W'(u)\big]+W''(u)\big[\partial_{xx} u -W'(u)\big], \qquad \forall\, (t,x)\in [0,+\infty)\times {\mathbb R}, $$ where the function $W(\cdot)$ is the standard double-well potential. For any given positive integer $k\geq2$, we construct a solution $u(t,x)$ with $k$ interfaces, which has the form $$ u(t,x)\approx\sum_{j=1}^k(-1)^{j+1}\omega\big(x-\gamma_j(t)\big)-\frac{1+(-1)^k}{2}\qquad \text{as}\ t\rightarrow +\infty, $$ where $\omega$ is the solution to the Allen-Cahn equation $$ \omega''-W'(\omega)=0,\quad\omega'>0\quad\mbox{in }{\mathbb R}, \quad \omega(0)=0, \quad \omega(\pm\infty)=\pm 1. $$ The interfaces are described by the functions $\gamma_j(t)$ with $j=1,\cdots,k$, which are determined by a Toda system and have the forms $$ \gamma_j(t)=\frac{1}{2\sqrt{2}}\left(j-\frac{k+1}{2}\right)\ln t +O(1). $$ The Toda system is different from the one that determine the dynamics of the multiple interfaces of solutions to one dimensional parabolic Allen-Cahn equation established by M. del Pino and K. Gkikas in {\em Proc. R. Soc. Edinb. Sect. A}, 148 (2018), 6: 1165-1199. ",Kein DOI-Link verfügbar,2303.17288v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,DeepBranchTracer: A Generally-Applicable Approach to Curvilinear   Structure Reconstruction Using Multi-Feature Learning,1970,"  Curvilinear structures, which include line-like continuous objects, are fundamental geometrical elements in image-based applications. Reconstructing these structures from images constitutes a pivotal research area in computer vision. However, the complex topology and ambiguous image evidence render this process a challenging task. In this paper, we introduce DeepBranchTracer, a novel method that learns both external image features and internal geometric characteristics to reconstruct curvilinear structures. Firstly, we formulate the curvilinear structures extraction as a geometric attribute estimation problem. Then, a curvilinear structure feature learning network is designed to extract essential branch attributes, including the image features of centerline and boundary, and the geometric features of direction and radius. Finally, utilizing a multi-feature fusion tracing strategy, our model iteratively traces the entire branch by integrating the extracted image and geometric features. We extensively evaluated our model on both 2D and 3D datasets, demonstrating its superior performance over existing segmentation and reconstruction methods in terms of accuracy and continuity. ",https://doi.org/10.1609/aaai.v38i4.28143,2402.01187v1,Yes,pivotal(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Characterising the Performance of High-Speed Data Converters for   RFSoC-based Radio Astronomy Receivers,1970,"  RF system-on-chip (RFSoC) devices provide the potential for implementing a complete radio astronomy receiver on a single board, but performance of the integrated analogue-to-digital converters is critical. We have evaluated the performance of the data converters in the Xilinx ZU28DR RFSoC, which are 12-bit, 8-fold interleaved converters with a maximum sample speed of 4.096 Giga-sample per second (GSPS). We measured the spurious-free dynamic range (SFDR), signal-to-noise and distortion (SINAD), effective number of bits (ENOB), intermodulation distortion (IMD) and cross-talk between adjacent channels over the bandwidth of 2.048 GHz. We both captured data for off-line analysis with floating-point arithmetic, and implemented a real-time integer arithmetic spectrometer on the RFSoC. The performance of the ADCs is sufficient for radio astronomy applications and close to the vendor specifications in most of the scenarios. We have carried out spectral integrations of up to 100 s and stability tests over tens of hours and find thermal noise-limited performance over these timescales. ",https://doi.org/10.1093/mnras/staa3895,2011.05691v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,An Error-Surface-Based Fractional Motion Estimation Algorithm and   Hardware Implementation for VVC,1970,"  Versatile Video Coding (VVC) introduces more coding tools to improve compression efficiency compared to its predecessor High Efficiency Video Coding (HEVC). For inter-frame coding, Fractional Motion Estimation (FME) still has a high computational effort, which limits the real-time processing capability of the video encoder. In this context, this paper proposes an error-surface-based FME algorithm and the corresponding hardware implementation. The algorithm creates an error surface constructed by the Rate-Distortion (R-D) cost of the integer motion vector (IMV) and its neighbors. This method requires no iteration and interpolation, thus reducing the area and power consumption and increasing the throughput of the hardware. The experimental results show that the corresponding BDBR loss is only 0.47% compared to VTM 16.0 in LD-P configuration. The hardware implementation was synthesized using GF 28nm process. It can support 13 different sizes of CU varying from 128x128 to 8x8. The measured throughput can reach 4K@30fps at 400MHz, with a gate count of 192k and power consumption of 12.64 mW. And the throughput can reach 8K@30fps at 631MHz when only quadtree is searched. To the best of our knowledge, this work is the first hardware architecture for VVC FME with interpolation-free strategies ",Kein DOI-Link verfügbar,2302.06167v1,Yes,versatile(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Is ChatGPT a Good Recommender? A Preliminary Study,1970,"  Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field. ",Kein DOI-Link verfügbar,2304.10149v3,Yes,potent(3)
0000-0002-0078-5982,Chao Liu,Zhejiang University,EASYFLOW: Keep Ethereum Away From Overflow,1970,"  While Ethereum smart contracts enabled a wide range of blockchain applications, they are extremely vulnerable to different forms of security attacks. Due to the fact that transactions to smart contracts commonly involve cryptocurrency transfer, any successful attacks can lead to money loss or even financial disorder. In this paper, we focus on the overflow attacks in Ethereum , mainly because they widely rooted in many smart contracts and comparatively easy to exploit. We have developed EASYFLOW , an overflow detector at Ethereum Virtual Machine level. The key insight behind EASYFLOW is a taint analysis based tracking technique to analyze the propagation of involved taints. Specifically, EASYFLOW can not only divide smart contracts into safe contracts, manifested overflows, well-protected overflows and potential overflows, but also automatically generate transactions to trigger potential overflows. In our preliminary evaluation, EASYFLOW managed to find potentially vulnerable Ethereum contracts with little runtime overhead. ",https://doi.org/10.1109/ICSE-Companion.2019.00029,1811.03814v2,Yes,potent(3)
0000-0002-0078-5982,Chao Liu,Zhejiang University,BLOCKEYE: Hunting For DeFi Attacks on Blockchain,1970,"  Decentralized finance, i.e., DeFi, has become the most popular type of application on many public blockchains (e.g., Ethereum) in recent years. Compared to the traditional finance, DeFi allows customers to flexibly participate in diverse blockchain financial services (e.g., lending, borrowing, collateralizing, exchanging etc.) via smart contracts at a relatively low cost of trust. However, the open nature of DeFi inevitably introduces a large attack surface, which is a severe threat to the security of participants funds. In this paper, we proposed BLOCKEYE, a real-time attack detection system for DeFi projects on the Ethereum blockchain. Key capabilities provided by BLOCKEYE are twofold: (1) Potentially vulnerable DeFi projects are identified based on an automatic security analysis process, which performs symbolic reasoning on the data flow of important service states, e.g., asset price, and checks whether they can be externally manipulated. (2) Then, a transaction monitor is installed offchain for a vulnerable DeFi project. Transactions sent not only to that project but other associated projects as well are collected for further security analysis. A potential attack is flagged if a violation is detected on a critical invariant configured in BLOCKEYE, e.g., Benefit is achieved within a very short time and way much bigger than the cost. We applied BLOCKEYE in several popular DeFi projects and managed to discover potential security attacks that are unreported before. A video of BLOCKEYE is available at https://youtu.be/7DjsWBLdlQU. ",Kein DOI-Link verfügbar,2103.02873v1,Yes,potent(3)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Immunofluorescence Capillary Imaging Segmentation: Cases Study,1970,"  Nonunion is one of the challenges faced by orthopedics clinics for the technical difficulties and high costs in photographing interosseous capillaries. Segmenting vessels and filling capillaries are critical in understanding the obstacles encountered in capillary growth. However, existing datasets for blood vessel segmentation mainly focus on the large blood vessels of the body, and the lack of labeled capillary image datasets greatly limits the methodological development and applications of vessel segmentation and capillary filling. Here, we present a benchmark dataset, named IFCIS-155, consisting of 155 2D capillary images with segmentation boundaries and vessel fillings annotated by biomedical experts, and 19 large-scale, high-resolution 3D capillary images. To obtain better images of interosseous capillaries, we leverage state-of-the-art immunofluorescence imaging techniques to highlight the rich vascular morphology of interosseous capillaries. We conduct comprehensive experiments to verify the effectiveness of the dataset and the benchmarking deep learning models (\eg UNet/UNet++ and the modified UNet/UNet++). Our work offers a benchmark dataset for training deep learning models for capillary image segmentation and provides a potential tool for future capillary research. The IFCIS-155 dataset and code are all publicly available at \url{https://github.com/ncclabsustech/IFCIS-55}. ",https://doi.org/10.1145/3503161.3548429,2207.06861v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,"Mira Variable Stars From LAMOST DR4 Data: Emission Features, Temperature   Types, and Candidate Selection",1970,"  Based on an extensive spectral study of a photometrically confirmed sample of Mira variables, we find a relationship between relative Balmer emission-line strength and spectral temperature of O-rich Mira stars. The $F_{\rm H\delta}/F_{\rm H\gamma}$ flux ratio increases from less than unity to five as stars cool down from M0 to M10, which is likely driven by increasing TiO absorption above the deepest shock-emitting regions. We also discuss the relationship between the equivalent widths of the Balmer emission lines and the photometric luminosity phase of our Mira sample stars. Using our 291 Mira spectra as templates for reference, 191 Mira candidates are newly identified from the LAMOST DR4 catalog. We summarize the criteria adopted to select Mira candidates based on emission-line indices and molecular absorption bands. This enlarged spectral sample of Mira variables has the potential to contribute significantly to our knowledge of the optical properties of Mira stars and will facilitate further studies of these late-type, long-period variables. ",https://doi.org/10.3847/1538-4365/aa88a9,1708.04703v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Corner Detection Based on Multi-directional Gabor Filters with   Multi-scales,1970,"  Gabor wavelet is an essential tool for image analysis and computer vision tasks. Local structure tensors with multiple scales are widely used in local feature extraction. Our research indicates that the current corner detection method based on Gabor wavelets can not effectively apply to complex scenes. In this work, the capability of the Gabor function to discriminate the intensity changes of step edges, L-shaped corners, Y-shaped or T-shaped corners, X-shaped corners, and star-shaped corners are investigated. The properties of Gabor wavelets to suppress affine image transformation are investigated and obtained. Many properties for edges and corners were discovered, which prompted us to propose a new corner extraction method. To fully use the structural information from the tuned Gabor filters, a novel multi-directional structure tensor is constructed for corner detection, and a multi-scale corner measurement function is proposed to remove false candidate corners. Furthermore, we compare the proposed method with twelve current state-of-the-art methods, which exhibit optimal performance and practical application to 3D reconstruction with good application potential. ",Kein DOI-Link verfügbar,2303.04334v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,The impact of bias row noise to photometric accuracy: case study based   on a scientific CMOS detector,1970,"  We tested a new model of CMOS detector manufactured by the Gpixel Inc, for potential space astronomical application. In laboratory, we obtain some bias images under the typical application environment. In these bias images, clear random row noise pattern is observed. The row noise also contains some characteristic spatial frequencies. We quantitatively estimated the impact of this feature to photometric measurements, by making simulated images. We compared different bias noise types under strict parameter control. The result shows the row noise will significantly deteriorate the photometric accuracy. It effectively increases the readout noise by a factor of 2 to 10. However, if it is properly removed, the image quality and photometric accuracy will be significantly improved. ",https://doi.org/10.1088/1674-4527/ad1793,2312.13539v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,The North/South Asymmetry of the Galaxy: Possible Connection to the   Vertical Phase Space Snail,1970,"  The Galaxy is found to be in disequilibrium based on recent findings of the North/South (N/S) asymmetry and the phase mixing signatures, such as a phase spiral (snail) structure in the vertical phase space ($z-V_{z}$). We show that the N/S asymmetry in a tracer population of dwarfs may be quantitatively modeled with a simple phase snail model superimposed on a smooth equilibrium background. As the phase snail intersects with the $z$ axis, the number density is enhanced, and the velocity dispersion ($\sigma_{z}$) is decreased relative to the other side of the Galactic plane. Fitting only to the observed asymmetric N/S $\sigma_{z}$ profiles, we obtain reasonable parameters for the phase space snail and the potential utilized in modeling the background, despite the complex dependence of the model on the potential parameters and the significant selection effects of the data. Both the snail shape and the N/S number density difference given by our best-fit model are consistent with previous observations. The equilibrium background implies a local dark matter density of $0.0151^{+0.0050}_{-0.0051}$ ${\rm M}_{\odot}\,{\rm pc}^{-3}$. The vertical bulk motion of our model is similar to the observation, but with a $\sim$1.2 $\rm km\,s^{-1}$ shift. Our work demonstrates the strong correlation between the phase space snail and the N/S asymmetry. Future observational constraints will facilitate more comprehensive snail models to unravel the Milky Way potential and the perturbation history encoded in the snail feature. ",https://doi.org/10.3847/1538-4357/ac86cd,2208.03667v1,Yes,potent(3)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Classifying globular clusters and applying them to estimate the mass of   the Milky Way,1970,"  We combine the kinematics of 159 globular clusters (GCs) provided by the Gaia Early Data Release 3 (EDR3) with other observational data to classify the GCs, and to estimate the mass of the Milky Way (MW). We use the age-metallicity relation, integrals of motion, action space and the GC orbits to identify the GCs as either formed in-situ (Bulge and Disk) or ex situ (via accretion). We find that $45.3\%$ have formed in situ, $38.4\%$ may be related to known merger events: Gaia-Sausage-Enceladus, the Sagittarius dwarf galaxy, the Helmi streams, the Sequoia galaxy, and the Kraken galaxy. We also further identify three new sub-structures associated with the Gaia-Sausage-Enceladus. The remaining $16.3\%$ of GCs are unrelated to the known mergers and thought to be from small accretion events. We select 46 GCs which have radii $8.0<r<37.3$ kpc and obtain the anisotropy parameter $\beta=0.315_{-0.049}^{+0.055}$, which is lower than the recent result using the sample of GCs in Gaia Data Release 2, but still in agreement with it by considering the error bar. By using the same sample, we obtain the MW mass inside the outermost GC as $M(<37.3 kpc)=0.423_{-0.02}^{+0.02}\times10^{12}M_{\odot}$, and the corresponding $M_{200}=1.11_{-0.18}^{+0.25}\times10^{12}M_{\odot}$. The estimated mass is consistent with the results in many recent studies. We also find that the estimated $\beta$ and mass depend on the selected sample of GCs. However, it is difficult to determine whether a GC fully traces the potential of the MW. ",https://doi.org/10.1088/1674-4527/ac9e91,2210.12336v2,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,PDSR: A Privacy-Preserving Diversified Service Recommendation Method on   Distributed Data,1970,"  The last decade has witnessed a tremendous growth of service computing, while efficient service recommendation methods are desired to recommend high-quality services to users. It is well known that collaborative filtering is one of the most popular methods for service recommendation based on QoS, and many existing proposals focus on improving recommendation accuracy, i.e., recommending high-quality redundant services. Nevertheless, users may have different requirements on QoS, and hence diversified recommendation has been attracting increasing attention in recent years to fulfill users' diverse demands and to explore potential services. Unfortunately, the recommendation performances relies on a large volume of data (e.g., QoS data), whereas the data may be distributed across multiple platforms. Therefore, to enable data sharing across the different platforms for diversified service recommendation, we propose a Privacy-preserving Diversified Service Recommendation (PDSR) method. Specifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH) mechanism such that privacy-preserved data sharing across different platforms is enabled to construct a service similarity graph. Based on the similarity graph, we propose a novel accuracy-diversity metric and design a $2$-approximation algorithm to select $K$ services to recommend by maximizing the accuracy-diversity measure. Extensive experiments on real datasets are conducted to verify the efficacy of our PDSR method. ",Kein DOI-Link verfügbar,2408.15688v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,North-South asymmetries in the Galactic thin disk associated with the   vertical phase spiral as seen using LAMOST-Gaia stars,1970,"  We select 1,052,469 (754,635) thin disk stars from {\it Gaia} eDR3 and LAMOST DR7 in the range of Galactocentric radius $R$ (guiding center radius $R_\mathrm{g}$) from 8 to 11\,kpc to investigate the asymmetries between the North and South of the disk midplane. More specifically we analyze the vertical velocity dispersion profiles ($\sigma_{v_{z}}(z$)) in different bins of $R$ ($R_\mathrm{g}$) and $[\mathrm{Fe/H}]$. We find troughs in the profiles of $\sigma_{v_{z}}(z)$ located in both the North ($z \sim 0.7$\,kpc) and South ($z \sim -0.5$\,kpc) of the disk at all radial and chemical bins studied. The difference between the Northern and Southern vertical velocity dispersion profiles ($\Delta\sigma_{v_{z}}(|z|)$) shows a shift between curves of different $R$ and $R_\mathrm{g}$. A similar shift exists in these NS asymmetry profiles further divided into different $[\mathrm{Fe/H}]$ ranges. The sample binned with $R_\mathrm{g}$ more clearly displays the features in the velocity dispersion profiles. The shift in the peaks of the $\Delta\sigma_{v_{z}}$ profiles and the variation in the phase spiral shape binned by metallicity indicate the variation of the vertical potential profiles and the radial metallicity gradient. The wave-like signal in NS asymmetry of $\sigma_{v_{z}}(z)$ largely originates from phase spiral; while the NS asymmetry profiles of [Fe/H] only display a weak wave-like feature near solar radius. We perform a test particle simulation to qualitatively reproduce the observed results. A quantitative explanation of the NS asymmetry in the metallicity profile needs careful consideration of the spiral shape and the perturbation model, and we leave this for future work. ",Kein DOI-Link verfügbar,2401.04688v1,Yes,potent(1)
0000-0002-0078-5982,Chao Liu,Zhejiang University,Red Clump Stars from the LAMOST data I: identification and distance,1970,"  We present a sample of about 120,000 red clump candidates selected from the LAMOST DR2 catalog based on the empirical distribution model in the effective temperature vs. surface gravity plane. Although, in general, red clump stars are considered as the standard candle, they do not exactly stay in a narrow range of absolute magnitude, but may extend to more than 1 magnitude depending on their initial mass. Consequently, conventional oversimplified distance estimations with assumption of fixed luminosity may lead to systematic bias related to the initial mass or the age, which may potentially affect the study of the evolution of the Galaxy with red clump stars. We therefore employ an isochrone-based method to estimate the absolute magnitude of red clump stars from their observed surface gravities, effective temperatures, and metallicities. We verify that the estimation well removes the systematics and provide an initial mass/age independent distance estimates with accuracy less than 10%. ",https://doi.org/10.1088/1674-4527/15/8/006,1505.04878v2,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university","FakeGPT: Fake News Generation, Explanation and Detection of Large   Language Models",1970,"  The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional natural language processing capabilities. In this study, we present a thorough exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows. Generation -- We employ four prompt methods to generate fake news samples and prove the high quality of these samples through both self-assessment and human evaluation. Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets. Detection -- We examine ChatGPT's capacity to identify fake news. We explore its detection consistency and then propose a reason-aware prompt method to improve its performance. Although our experiments demonstrate that ChatGPT shows commendable performance in detecting fake news, there is still room for its improvement. Consequently, we further probe into the potential extra information that could bolster its effectiveness in detecting fake news. ",Kein DOI-Link verfügbar,2310.05046v2,Yes,"commendable(1), notable(1), potent(1)"
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Lightweight Pyramid Networks for Image Deraining,1970,"  Existing deep convolutional neural networks have found major success in image deraining, but at the expense of an enormous number of parameters. This limits their potential application, for example in mobile devices. In this paper, we propose a lightweight pyramid of networks (LPNet) for single image deraining. Instead of designing a complex network structures, we use domain-specific knowledge to simplify the learning process. Specifically, we find that by introducing the mature Gaussian-Laplacian image pyramid decomposition technology to the neural network, the learning problem at each pyramid level is greatly simplified and can be handled by a relatively shallow network with few parameters. We adopt recursive and residual network structures to build the proposed LPNet, which has less than 8K parameters while still achieving state-of-the-art performance on rain removal. We also discuss the potential value of LPNet for other low- and high-level vision tasks. ",Kein DOI-Link verfügbar,1805.06173v1,Yes,potent(2)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Hint-dynamic Knowledge Distillation,1970,"  Knowledge Distillation (KD) transfers the knowledge from a high-capacity teacher model to promote a smaller student model. Existing efforts guide the distillation by matching their prediction logits, feature embedding, etc., while leaving how to efficiently utilize them in junction less explored. In this paper, we propose Hint-dynamic Knowledge Distillation, dubbed HKD, which excavates the knowledge from the teacher' s hints in a dynamic scheme. The guidance effect from the knowledge hints usually varies in different instances and learning stages, which motivates us to customize a specific hint-learning manner for each instance adaptively. Specifically, a meta-weight network is introduced to generate the instance-wise weight coefficients about knowledge hints in the perception of the dynamical learning progress of the student model. We further present a weight ensembling strategy to eliminate the potential bias of coefficient estimation by exploiting the historical statics. Experiments on standard benchmarks of CIFAR-100 and Tiny-ImageNet manifest that the proposed HKD well boost the effect of knowledge distillation tasks. ",Kein DOI-Link verfügbar,2211.17059v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",ObscurePrompt: Jailbreaking Large Language Models via Obscure Input,1970,"  Recently, Large Language Models (LLMs) have garnered significant attention for their exceptional natural language processing capabilities. However, concerns about their trustworthiness remain unresolved, particularly in addressing ""jailbreaking"" attacks on aligned LLMs. Previous research predominantly relies on scenarios with white-box LLMs or specific and fixed prompt templates, which are often impractical and lack broad applicability. In this paper, we introduce a straightforward and novel method, named ObscurePrompt, for jailbreaking LLMs, inspired by the observed fragile alignments in Out-of-Distribution (OOD) data. Specifically, we first formulate the decision boundary in the jailbreaking process and then explore how obscure text affects LLM's ethical decision boundary. ObscurePrompt starts with constructing a base prompt that integrates well-known jailbreaking techniques. Powerful LLMs are then utilized to obscure the original prompt through iterative transformations, aiming to bolster the attack's robustness. Comprehensive experiments show that our approach substantially improves upon previous methods in terms of attack effectiveness, maintaining efficacy against two prevalent defense mechanisms. We believe that our work can offer fresh insights for future research on enhancing LLM alignment. ",Kein DOI-Link verfügbar,2406.13662v1,Yes,fresh(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge   Aggregators?,1970,"  Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different languages, presenting challenges for further advancement. This paper introduces a method to enhance the multilingual performance of LLMs by aggregating knowledge from diverse languages. This approach incorporates a low-resource knowledge detector specific to a language, a language selection process, and mechanisms for answer replacement and integration. Our experiments demonstrate notable performance improvements, particularly in reducing language performance disparity. An ablation study confirms that each component of our method significantly contributes to these enhancements. This research highlights the inherent potential of LLMs to harmonize multilingual capabilities and offers valuable insights for further exploration. ",Kein DOI-Link verfügbar,2406.14721v1,Yes,"notable(1), potent(1)"
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Can Large Language Models Automatically Jailbreak GPT-4V?,1970,"  GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited. In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization. We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency. Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure. Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3\%. This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity. ",Kein DOI-Link verfügbar,2407.16686v2,Yes,"innovative(1), potent(1)"
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Bootstrap Segmentation Foundation Model under Distribution Shift via   Object-Centric Learning,1970,"  Foundation models have made incredible strides in achieving zero-shot or few-shot generalization, leveraging prompt engineering to mimic the problem-solving approach of human intelligence. However, when it comes to some foundation models like Segment Anything, there is still a challenge in performing well on out-of-distribution data, including camouflaged and medical images. Inconsistent prompting strategies during fine-tuning and testing further compound the issue, leading to decreased performance. Drawing inspiration from how human cognition processes new environments, we introduce SlotSAM, a method that reconstructs features from the encoder in a self-supervised manner to create object-centric representations. These representations are then integrated into the foundation model, bolstering its object-level perceptual capabilities while reducing the impact of distribution-related variables. The beauty of SlotSAM lies in its simplicity and adaptability to various tasks, making it a versatile solution that significantly enhances the generalization abilities of foundation models. Through limited parameter fine-tuning in a bootstrap manner, our approach paves the way for improved generalization in novel environments. The code is available at github.com/lytang63/SlotSAM. ",Kein DOI-Link verfügbar,2408.16310v1,Yes,versatile(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",A Closer Look at Personalization in Federated Image Classification,1970,"  Federated Learning (FL) is developed to learn a single global model across the decentralized data, while is susceptible when realizing client-specific personalization in the presence of statistical heterogeneity. However, studies focus on learning a robust global model or personalized classifiers, which yield divergence due to inconsistent objectives. This paper shows that it is possible to achieve flexible personalization after the convergence of the global model by introducing representation learning. In this paper, we first analyze and determine that non-IID data harms representation learning of the global model. Existing FL methods adhere to the scheme of jointly learning representations and classifiers, where the global model is an average of classification-based local models that are consistently subject to heterogeneity from non-IID data. As a solution, we separate representation learning from classification learning in FL and propose RepPer, an independent two-stage personalized FL framework.We first learn the client-side feature representation models that are robust to non-IID data and aggregate them into a global common representation model. After that, we achieve personalization by learning a classifier head for each client, based on the common representation obtained at the former stage. Notably, the proposed two-stage learning scheme of RepPer can be potentially used for lightweight edge computing that involves devices with constrained computation power.Experiments on various datasets (CIFAR-10/100, CINIC-10) and heterogeneous data setup show that RepPer outperforms alternatives in flexibility and personalization on non-IID data. ",Kein DOI-Link verfügbar,2204.11841v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",A Deep Information Sharing Network for Multi-contrast Compressed Sensing   MRI Reconstruction,1970,"  In multi-contrast magnetic resonance imaging (MRI), compressed sensing theory can accelerate imaging by sampling fewer measurements within each contrast. The conventional optimization-based models suffer several limitations: strict assumption of shared sparse support, time-consuming optimization and ""shallow"" models with difficulties in encoding the rich patterns hiding in massive MRI data. In this paper, we propose the first deep learning model for multi-contrast MRI reconstruction. We achieve information sharing through feature sharing units, which significantly reduces the number of parameters. The feature sharing unit is combined with a data fidelity unit to comprise an inference block. These inference blocks are cascaded with dense connections, which allows for information transmission across different depths of the network efficiently. Our extensive experiments on various multi-contrast MRI datasets show that proposed model outperforms both state-of-the-art single-contrast and multi-contrast MRI methods in accuracy and efficiency. We show the improved reconstruction quality can bring great benefits for the later medical image analysis stage. Furthermore, the robustness of the proposed model to the non-registration environment shows its potential in real MRI applications. ",https://doi.org/10.1109/TIP.2019.2925288,1804.03596v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",An Adversarial Learning Approach to Medical Image Synthesis for Lesion   Detection,1970,"  The identification of lesion within medical image data is necessary for diagnosis, treatment and prognosis. Segmentation and classification approaches are mainly based on supervised learning with well-paired image-level or voxel-level labels. However, labeling the lesion in medical images is laborious requiring highly specialized knowledge. We propose a medical image synthesis model named abnormal-to-normal translation generative adversarial network (ANT-GAN) to generate a normal-looking medical image based on its abnormal-looking counterpart without the need for paired training data. Unlike typical GANs, whose aim is to generate realistic samples with variations, our more restrictive model aims at producing a normal-looking image corresponding to one containing lesions, and thus requires a special design. Being able to provide a ""normal"" counterpart to a medical image can provide useful side information for medical imaging tasks like lesion segmentation or classification validated by our experiments. In the other aspect, the ANT-GAN model is also capable of producing highly realistic lesion-containing image corresponding to the healthy one, which shows the potential in data augmentation verified in our experiments. ",Kein DOI-Link verfügbar,1810.10850v2,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Urban Surface Reconstruction in SAR Tomography by Graph-Cuts,1970,"  SAR (Synthetic Aperture Radar) tomography reconstructs 3-D volumes from stacks of SAR images. High-resolution satellites such as TerraSAR-X provide images that can be combined to produce 3-D models. In urban areas, sparsity priors are generally enforced during the tomographic inversion process in order to retrieve the location of scatterers seen within a given radar resolution cell. However, such priors often miss parts of the urban surfaces. Those missing parts are typically regions of flat areas such as ground or rooftops. This paper introduces a surface segmentation algorithm based on the computation of the optimal cut in a flow network. This segmentation process can be included within the 3-D reconstruction framework in order to improve the recovery of urban surfaces. Illustrations on a TerraSAR-X tomographic dataset demonstrate the potential of the approach to produce a 3-D model of urban surfaces such as ground, fa\c{c}ades and rooftops. ",https://doi.org/10.1016/j.cviu.2019.07.011,2103.07202v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Quantifying AI Psychology: A Psychometrics Benchmark for Large Language   Models,1970,"  Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences. ",Kein DOI-Link verfügbar,2406.17675v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",SRCD: Semantic Reasoning with Compound Domains for Single-Domain   Generalized Object Detection,1970,"  This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main components, namely, the texture-based self-augmentation (TBSA) module, and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by a light-yet-efficient self-augmentation. Moreover, LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures. Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD. ",Kein DOI-Link verfügbar,2307.01750v2,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market,1970,"  In recent years, great advances in pre-trained language models (PLMs) have sparked considerable research focus and achieved promising performance on the approach of dense passage retrieval, which aims at retrieving relative passages from massive corpus with given questions. However, most of existing datasets mainly benchmark the models with factoid queries of general commonsense, while specialised fields such as finance and economics remain unexplored due to the deficiency of large-scale and high-quality datasets with expert annotations. In this work, we propose a new task, policy retrieval, by introducing the Chinese Stock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages labeled by experienced experts with relevant articles from 10k+ entries in our collected Chinese policy corpus. Experiments on lexical, embedding and fine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet also suggests ample potential for improvement. Our best performing baseline achieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on dev set. ",Kein DOI-Link verfügbar,2309.04389v2,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Activate and Reject: Towards Safe Domain Generalization under Category   Shift,1970,"  Albeit the notable performance on in-domain test points, it is non-trivial for deep neural networks to attain satisfactory accuracy when deploying in the open world, where novel domains and object classes often occur. In this paper, we study a practical problem of Domain Generalization under Category Shift (DGCS), which aims to simultaneously detect unknown-class samples and classify known-class samples in the target domains. Compared to prior DG works, we face two new challenges: 1) how to learn the concept of ``unknown'' during training with only source known-class samples, and 2) how to adapt the source-trained model to unseen environments for safe model deployment. To this end, we propose a novel Activate and Reject (ART) framework to reshape the model's decision boundary to accommodate unknown classes and conduct post hoc modification to further discriminate known and unknown classes using unlabeled test data. Specifically, during training, we promote the response to the unknown by optimizing the unknown probability and then smoothing the overall output to mitigate the overconfidence issue. At test time, we introduce a step-wise online adaptation method that predicts the label by virtue of the cross-domain nearest neighbor and class prototype information without updating the network's parameters or using threshold-based mechanisms. Experiments reveal that ART consistently improves the generalization capability of deep networks on different vision tasks. For image classification, ART improves the H-score by 6.1% on average compared to the previous best method. For object detection and semantic segmentation, we establish new benchmarks and achieve competitive performance. ",Kein DOI-Link verfügbar,2310.04724v1,Yes,notable(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",The Best of Both Worlds: Toward an Honest and Helpful Large Language   Model,1970,"  Large Language Models (LLMs) have achieved remarkable success across various industries due to their exceptional generative capabilities. However, for safe and effective real-world deployments, ensuring honesty and helpfulness is critical. This paper addresses the question: Can we prioritize the helpfulness of LLMs while preserving their honesty? To begin with, we establish exhaustive principles aimed at guaranteeing the honesty of LLM. Additionally, we introduce a novel dataset, referred to as HoneSet, comprising 930 queries spanning six categories meticulously crafted to assess an LLM's capacity for maintaining honesty. Subsequently, we present two approaches to augmenting honesty and helpfulness in LLMs: a training-free enhancement and a fine-tuning-based improvement. The training-free approach, which is based on curiosity-driven prompting, empowers LLMs to articulate internal confusion and uncertainty regarding queries, thereby optimizing their responses. Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness. Experiments conducted on nine prominent LLMs demonstrate a significant improvement in alignment with honesty across all models through the implementation of our proposed enhancements. Particularly noteworthy is the 65.3% enhancement observed in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as measured by the H$^{2}$ (honest and helpful) assessment. We believe that our work can pave the way for developing more trustworthy LLMs for real-world applications. ",Kein DOI-Link verfügbar,2406.00380v2,Yes,"meticulous(1), noteworthy(1), meticulously(1)"
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",Unity in Diversity: Multi-expert Knowledge Confrontation and   Collaboration for Generalizable Vehicle Re-identification,1970,"  Generalizable vehicle re-identification (ReID) aims to enable the well-trained model in diverse source domains to broadly adapt to unknown target domains without additional fine-tuning or retraining. However, it still faces the challenges of domain shift problem and has difficulty accurately generalizing to unknown target domains. This limitation occurs because the model relies heavily on primary domain-invariant features in the training data and pays less attention to potentially valuable secondary features. To solve this complex and common problem, this paper proposes the two-stage Multi-expert Knowledge Confrontation and Collaboration (MiKeCoCo) method, which incorporates multiple experts with unique perspectives into Contrastive Language-Image Pretraining (CLIP) and fully leverages high-level semantic knowledge for comprehensive feature representation. Specifically, we propose to construct the learnable prompt set of all specific-perspective experts by adversarial learning in the latent space of visual features during the first stage of training. The learned prompt set with high-level semantics is then utilized to guide representation learning of the multi-level features for final knowledge fusion in the next stage. In this process of knowledge fusion, although multiple experts employ different assessment ways to examine the same vehicle, their common goal is to confirm the vehicle's true identity. Their collective decision can ensure the accuracy and consistency of the evaluation results. Furthermore, we design different image inputs for two-stage training, which include image component separation and diversity enhancement in order to extract the ID-related prompt representation and to obtain feature representation highlighted by all experts, respectively. Extensive experimental results demonstrate that our method achieves state-of-the-art recognition performance. ",Kein DOI-Link verfügbar,2407.07351v1,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents,1970,"  Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. The code and dataset are publicly available at our project homepage: https://gui-world.github.io/. ",Kein DOI-Link verfügbar,2406.10819v1,Yes,"meticulous(1), meticulously(1)"
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be   Detected?,1970,"  With the rapid development and widespread application of Large Language Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly common, bringing with it potential risks, especially in terms of quality and integrity in fields like news, education, and science. Current research mainly focuses on purely MGT detection without adequately addressing mixed scenarios, including AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge, we define mixtext, a form of mixed text involving both AI and human-generated content. Then, we introduce MixSet, the first dataset dedicated to studying these mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to assess the efficacy of prevalent MGT detectors in handling mixtext situations, evaluating their performance in terms of effectiveness, robustness, and generalization. Our findings reveal that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixtext, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet. ",Kein DOI-Link verfügbar,2401.05952v2,Yes,potent(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",UniGen: A Unified Framework for Textual Dataset Generation Using Large   Language Models,1970,"  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents UniGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. UniGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, UniGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by UniGen, and each module within UniGen plays a critical role in this enhancement. Additionally, UniGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that UniGen effectively supports dynamic and evolving benchmarking, and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills. ",Kein DOI-Link verfügbar,2406.18966v3,Yes,innovative(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university",MetaTool Benchmark for Large Language Models: Deciding Whether to Use   Tools and Which to Use,1970,"  Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to. Our code is in https://github.com/HowieHwong/MetaTool. ",Kein DOI-Link verfügbar,2310.03128v5,Yes,intricate(1)
0000-0001-6923-9325,Yue Huang,"Zhejiang University, Zhejiang university","Sora: A Review on Background, Technology, Limitations, and Opportunities   of Large Vision Models",1970,"  Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this ""world simulator"". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation. ",Kein DOI-Link verfügbar,2402.17177v3,Yes,potent(2)
0000-0002-5569-5650,Jin Xia,"Zhejiang University, Zhejiang university",Asynchronous Interaction Aggregation for Action Detection,1970,"  Understanding interaction is an essential part of video action detection. We propose the Asynchronous Interaction Aggregation network (AIA) that leverages different interactions to boost action detection. There are two key designs in it: one is the Interaction Aggregation structure (IA) adopting a uniform paradigm to model and integrate multiple types of interaction; the other is the Asynchronous Memory Update algorithm (AMU) that enables us to achieve better performance by modeling very long-term interaction dynamically without huge computation cost. We provide empirical evidence to show that our network can gain notable accuracy from the integrative interactions and is easy to train end-to-end. Our method reports the new state-of-the-art performance on AVA dataset, with 3.7 mAP gain (12.6% relative improvement) on validation split comparing to our strong baseline. The results on dataset UCF101-24 and EPIC-Kitchens further illustrate the effectiveness of our approach. Source code will be made public at: https://github.com/MVIG-SJTU/AlphAction . ",Kein DOI-Link verfügbar,2004.07485v1,Yes,notable(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Velocity renormalization of nodal quasiparticles in d-wave   superconductors,1970,"  Gapless nodal quasiparticles emerge at a low-energy regime of high-$T_c$ cuprate superconductors due to the $d_{x^2 - y^2}$ gap symmetry. We study the unusual renormalizations of the Fermi velocity $v_F$ and gap velocity $v_{\Delta}$ of these quasiparticles close to various quantum critical points in a superconducting dome. Special attention is paid to the behavior of the velocity ratio, $v_{\Delta}/v_F$, since it determines a number of observable quantities. We perform a renormalization-group analysis and show that the velocity ratio may vanish, approach unity, or diverge at different quantum critical points. The corresponding superfluid densities and critical temperatures are suppressed, slightly increased, or significantly enhanced. The effects of three types of static disorders, namely, random mass, random gauge potential, and random chemical potential, on the stability of the system are also addressed. An analogous analysis reveals that both random mass and random gauge potential are irrelevant. This implies that these fixed points of the velocity ratio are stable, and hence observable effects ignited by them are unchanged. However, the random chemical potential is marginal. As a result, these fixed points are broken, and thus, the instabilities of quantum phase transitions are triggered. ",https://doi.org/10.1103/PhysRevB.87.054511,1302.6137v1,Yes,potent(4)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Two-loop disorder effects on the nematic quantum criticality in $d$-wave   superconductors,1970,"  The gapless nodal fermions exhibit non-Fermi liquid behaviors at the nematic quantum critical point that is supposed to exist in some $d$-wave cuprate superconductors. This non-Fermi liquid state may be turned into a disorder-dominated diffusive metal if the fermions also couple to a disordered potential that generates a relevant perturbation in the sense of renormalization group theory. It is therefore necessary to examine whether a specific disorder is relevant or not. We study the interplay between critical nematic fluctuation and random chemical potential by performing renormalization group analysis. The parameter that characterizes the strength of random chemical potential is marginal at the one-loop level, but becomes marginally relevant after including the two-loop corrections. Thus even weak random chemical potential leads to diffusive motion of nodal fermions and the significantly critical behaviors of physical implications, since the strength flows eventually to large values at low energies. ",https://doi.org/10.1016/j.physleta.2015.05.028,1505.06533v1,Yes,potent(4)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Quantum Corrections to Gravitational Potential of Scalarized Neutron   Star Binary,1970,"  We investigate the long-distance, low-energy, leading quantum corrections to gravitational potential for scalarized neutron star (NS) binary systems, by treating general relativity as an effective field theory. We neglect the extended scales of two star components and treat them as heavy point particles, which gravitationally interact with each other via the exchanges of both gravitons and scalar particles, because of the settled scalar configurations inside the stars. Accordingly, the gravitational potential includes both Newtonian potential and scalar-modified Newtonian-like part. We, in the non-relativistic limit, calculate the non-analytic corrections to the modified gravitational potential directly from the sum of all exchanges of both gravitons and scalar particles to one-loop order. The appropriate vertex rules are extracted from the effective Lagrangian. Our calculations demonstrate that either the graviton exchanges or the exchanges of scalar particles contribute to both classical relativistic corrections and quantum corrections to the gravitational potential of the scalarized NS binaries. ",https://doi.org/10.1140/epjc/s10052-019-7052-5,1909.01047v1,Yes,potent(5)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Physical Environment of Accreting Neutron Stars,1970,"  Neutron stars (NSs) powered by accretion, which are known as accretion-powered NSs, always locate in binary systems and manifest themselves as X-ray sources. Physical process during accreting material from their companions is a challenging and appealing topic, because of the strong magnetic field of NSs. In this article, we review the physical process of accretion onto magnetized NS in X-ray binary systems. We, firstly, give an introduction to accretion-powered NSs and review the accretion mechanism in X-ray binaries. This review will be mostly focused on accretion-induced evolution of NSs. Finally, we extend the accretion to cosmic scheme and discuss that the NS accretion can be a potential promising tool to constrain dark matter candidates and investigate the nature of dark energy. ",https://doi.org/10.1155/2016/3424565,1909.01057v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Incommensurate magnetic states induced by ordering competition in   $\mathrm{Ba_{1-x}Na_xFe_2As_2}$,1970,"  Quantum criticality nearby a certain magnetic phase transition beneath the superconducting dome of $\mathrm{Ba_{1-x}Na_xFe_2As_2}$ is attentively studied by virtue of a phenomenological theory in conjunction with renormalization group approach. We report that ordering competition between magnetic and superconducting fluctuations is capable of coaxing incommensurate (IC) magnetic states to experience distinct fates depending upon their spin configurations. The $C_2$-symmetry IC magnetic stripe with perpendicular magnetic helix dominates over other $C_2$-symmetry magnetic competitors and hints at a potential candidate for the unknown $C_2$-symmetry magnetic state. Amongst $C_4$-symmetry IC magnetic phases, IC charge spin density wave is substantiated to be superior, shedding light on the significant intertwining of charge and spin degrees of freedom. Meanwhile, ferocious fluctuations render a sharp fall of superfluid density alongside dip of critical temperature as well as intriguing behavior of London penetration depth. ",https://doi.org/10.1088/1361-6668/ac9a86,2007.14981v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Gravitational Higgs mechanism in inspiraling scalarized NS-WD binary,1970,"  We investigate the gravitational Higgs mechanism in the inspiraling scalarized neutron star - white dwarf (NS-WD) binaries, whose dynamics are described by the scalar-tensor theory. Because of the difference in binding energy of NS and WD, the orbital decay of scalarized NS-WD system actually sources an emission of dipolar gravitational scalar radiation, in addition to the tensor gravitational waves, which breaks the Lorentz invariance constructed in the framework of general relativity. The resulted gravitational scalar radiation field obtains a scalar-energy-density-dependent effective mass, arising from a gravitational scalar potential that consists of a monotonically decreasing self-interactions of gravitational scalar field and an increasing exponential coupling between the scalar field and the NS/WD matter. Owing to a thin-ring-orbit effect, the gravitational interactions encoded by the massive scalar field is screened in the region of binary orbit, with high density of stars' scalar energy, which gives us the estimation for scalar masses of about $10^{-21} eV/c^2$ and leads to a Yulkawa-like correction to the Newtonian potential of the binary system. We demonstrate that the radiated gravitational tensor waves, propagating in the Yukawa type of potential, gain a scalar-background-dependent mass term of the order of $\sim 10^{-23} eV/c^2$. ",https://doi.org/10.4236/ijaa.2017.73016,1612.06102v3,Yes,potent(3)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Graviton mass generation in in-spiraling DNS,1970,"  We point out that a spontaneous scalarized inspiring double neutron star (DNS) system can provide us a natural laboratory to investigate the generation mechanism of masses for gravitons. Because of the appearance of a gravitational scalar background field, with small fluctuations, converged by iterative interplay of the mass dimensional external scalar fields, the binary system suffers from a spontaneous Lorentz symmetry breaking. The two scalarized NSs dip in a Higgs-like gravitational scalar potential, where the massless scalar background fluctuation field plays the role of Higgs field. Consequently, the gravitational scalar background field becomes massive. The radiated gravitons, propagating in a Yukawa-corrected potential, acquire a scalar-background-dependent mass term, in a massive-scalar-field-mediated way. We demonstrate that the mass of gravitons depends on intrinsic properties of the sources, which is not a certain value. The background-dependent masses for gravitons from scalarized orbital shrinking DNS is variable with the compactness of two components, as well as the separation of the binary. We get the effective masses for gravitons radiated from 8 detected DNS binaries with more precise mass measurements in our galaxy, whose values appear to be of the order of $10^{-23} ev/c^2$. It is found that more massive gravitons radiate from more closer DNS system, consisting with the higher-frequency gravitational waves from closer binaries. ",https://doi.org/10.21474/IJAR01/5808,1612.06104v2,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Massive scalar counterpart of gravitational waves in scalarized neutron   star binaries,1970,"  In analogy with spontaneous magnetization of ferromagnets below the Curie temperature, a neutron star (NS), with a compactness above a certain critical value, may undergo spontaneous scalarization and exhibit an interior nontrivial scalar configuration. Consequently, the exterior space-time is changed, and an external scalar field appears, which subsequently triggers a scalarization of its companion. The dynamical interplay produces a gravitational scalar counterpart of tensor gravitational waves. In this paper, we resort to scalar-tensor theory and demonstrate that the gravitational scalar counterpart from double neutron star (DNS) and neutron star-white dwarf (NS-WD) become massive. We report that (i) a gravitational scalar background field, arising from convergence of external scalar fields, plays the role of gravitational scalar counterpart in scalarized DNS binary, and the appearance of a mass-dimensional constant in Higgs-like gravitational scalar potential is responsible for a massive gravitational scalar counterpart with mass of order of Planck scale; (ii) a dipolar gravitational scalar radiated field, resulting from different binding energy of NS and WD, plays the role of gravitational scalar counterpart in scalarized orbital shrinking NS-WDs, which oscillates around a local and scalar-energy-density dependent minimum of the gravitational scalar potential and gains a mass of the order of about $10^{-21} ev/c^2$. ",https://doi.org/10.1140/epjc/s10052-017-5214-x,1909.01045v1,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Inspiraling Corrugation-Induced Quantum Effects on Neutron Star Binary   Plane,1970,"  We use the path-integral formula and investigate some dynamical quantum effects induced by the inspiraling lateral corrugation of orbital plane in gravitationally bound neutron star (NS) binaries, with orbital separation of $10^9$ m. Based on Dewitt's approach, we calculate the gravitational Casimir energy cost of the binary plane, which consists of statically gravitational effects and deformation-induced effects. It is found that the static effects include a term coming from the self-gravity of the orbital plane and the contribution of Newtonian gravitational potential of the binary system. While the deformation-induced effect also results from two parts, i.e. the instability of orbital binding energy, scaling as $\frac{1}{(R-r)^2}$, and the dynamically Casimir energy cost of the orbital binding energy, decaying as $\frac{1}{(R-r)^4}$. The dynamically gravitational Casimir phenomena and the corresponding energy cost modify the spiral-in orbital motion of the binary and thus the frequency of released gravitational waves (GWs). We consider the mechanical response of two NS components and qualitatively study the corrections to the orbital motion of the system and the GW frequencies. It is found that the dynamical Casimir effects exert a dissipative force on the binary plane, depending on the frequency of GWs. The resultant dissipation may enhance with the decaying separation and increasing GW frequencies, which subsequently accelerates the orbital decay of the binary. However, the dissipation rate just has an order of $10^{-70}$ eV/s. So the corrections to the dynamics of NS binaries are very marginal, by considering the wide separation, the cosmological coalescence time, and low-frequency GWs of the system. ",https://doi.org/10.1016/j.physletb.2022.136980,2111.07230v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",A Guide to Global Quantum Key Distribution Networks,1970,"  We describe systems and methods for the deployment of global quantum key distribution (QKD) networks covering transoceanic, long-haul, metro, and access segments of the network. A comparative study of the state-of-the-art QKD technologies is carried out, including both terrestrial QKD via optical fibers and free-space optics, as well as spaceborne solutions via satellites. We compare the pros and cons of various existing QKD technologies, including channel loss, potential interference, distance, connection topology, deployment cost and requirements, as well as application scenarios. Technical selection criteria and deployment requirements are developed for various different QKD solutions in each segment of networks. For example, optical fiber-based QKD is suitable for access networks due to its limited distance and compatibility with point-to-multipoint (P2MP) topology; with the help of trusted relays, it can be extended to long-haul and metro networks. Spaceborne QKD on the other hand, has much smaller channel loss and extended transmission distance, which can be used for transoceanic and long-haul networks exploiting satellite-based trusted relays. ",Kein DOI-Link verfügbar,2012.14396v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Exploring Text-based Realistic Building Facades Editing Applicaiton,1970,"  This paper explores the utilization of diffusion models and textual guidance for achieving localized editing of building facades, addressing the escalating demand for sophisticated editing methodologies in architectural design and urban planning. Leveraging the robust generative capabilities of diffusion models, this study presents a promising avenue for realistically synthesizing and modifying architectural facades. Through iterative diffusion and text descriptions, these models adeptly capture both the intricate global and local structures inherent in architectural facades, thus effectively navigating the complexity of such designs. Additionally, the paper examines the expansive potential of diffusion models in various facets, including the generation of novel facade designs, the enhancement of existing facades, and the realization of personalized customization. Despite their promise, diffusion models encounter obstacles such as computational resource constraints and data imbalances. To address these challenges, the study introduces the innovative Blended Latent Diffusion method for architectural facade editing, accompanied by a comprehensive visual analysis of its viability and efficacy. Through these endeavors, we aims to propel forward the field of architectural facade editing, contributing to its advancement and practical application. ",Kein DOI-Link verfügbar,2405.02967v1,Yes,"innovative(1), intricate(1), potent(1)"
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university","Dendrite Net: A White-Box Module for Classification, Regression, and   System Identification",1970,"  The simulation of biological dendrite computations is vital for the development of artificial intelligence (AI). This paper presents a basic machine learning algorithm, named Dendrite Net or DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's main concept is that the algorithm can recognize this class after learning, if the output's logical expression contains the corresponding class's logical relationship among inputs (and$\backslash$or$\backslash$not). Experiments and main results: DD, a white-box machine learning algorithm, showed excellent system identification performance for the black-box system. Secondly, it was verified by nine real-world applications that DD brought better generalization capability relative to MLP architecture that imitated neurons' cell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST datasets, it was verified that DD showed higher testing accuracy under greater training loss than Cell body Net for classification. The number of modules can effectively adjust DD's logical expression capacity, which avoids over-fitting and makes it easy to get a model with outstanding generalization capability. Finally, repeated experiments in MATLAB and PyTorch (Python) demonstrated that DD was faster than Cell body Net both in epoch and forward-propagation. The main contribution of this paper is the basic machine learning algorithm (DD) with a white-box attribute, controllable precision for better generalization capability, and lower computational complexity. Not only can DD be used for generalized engineering, but DD has vast development potential as a module for deep learning. DD code is available at GitHub: https://github.com/liugang1234567/Gang-neuron . ",Kein DOI-Link verfügbar,2004.03955v6,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance,1970,"  Automatic segmentation of medical images is crucial in modern clinical workflows. The Segment Anything Model (SAM) has emerged as a versatile tool for image segmentation without specific domain training, but it requires human prompts and may have limitations in specific domains. Traditional models like nnUNet perform automatic segmentation during inference and are effective in specific domains but need extensive domain-specific training. To combine the strengths of foundational and domain-specific models, we propose nnSAM, integrating SAM's robust feature extraction with nnUNet's automatic configuration to enhance segmentation accuracy on small datasets. Our nnSAM model optimizes two main approaches: leveraging SAM's feature extraction and nnUNet's domain-specific adaptation, and incorporating a boundary shape supervision loss function based on level set functions and curvature calculations to learn anatomical shape priors from limited data. We evaluated nnSAM on four segmentation tasks: brain white matter, liver, lung, and heart segmentation. Our method outperformed others, achieving the highest DICE score of 82.77% and the lowest ASD of 1.14 mm in brain white matter segmentation with 20 training samples, compared to nnUNet's DICE score of 79.25% and ASD of 1.36 mm. A sample size study highlighted nnSAM's advantage with fewer training samples. Our results demonstrate significant improvements in segmentation performance with nnSAM, showcasing its potential for small-sample learning in medical image segmentation. ",Kein DOI-Link verfügbar,2309.16967v3,Yes,"versatile(1), potent(1)"
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Very rare events for diffusion processes in short time,1970,"  We study the large deviation estimates for the short time asymptotic behavior of a strongly degenerate diffusion process. Assuming a nilpotent structure of the Lie algebra generated by the driving vector fields, we obtain a graded large deviation principle and prove the existence of those ""very rare events"". In particular the first grade coincides with the classical Large Deviation Principle. ",Kein DOI-Link verfügbar,1901.10025v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Language-guided Few-shot Semantic Segmentation,1970,"  Few-shot learning is a promising way for reducing the label cost in new categories adaptation with the guidance of a small, well labeled support set. But for few-shot semantic segmentation, the pixel-level annotations of support images are still expensive. In this paper, we propose an innovative solution to tackle the challenge of few-shot semantic segmentation using only language information, i.e.image-level text labels. Our approach involves a vision-language-driven mask distillation scheme, which contains a vision-language pretraining (VLP) model and a mask refiner, to generate high quality pseudo-semantic masks from text prompts. We additionally introduce a distributed prototype supervision method and complementary correlation matching module to guide the model in digging precise semantic relations among support and query images. The experiments on two benchmark datasets demonstrate that our method establishes a new baseline for language-guided few-shot semantic segmentation and achieves competitive results to recent vision-guided methods. ",Kein DOI-Link verfügbar,2311.13865v1,Yes,innovative(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Moiré Engineering and Topological Flat Bands in Twisted Orbital-Active   Bilayers,1970,"  Topological flat bands at the Fermi level offer a promising platform to study a variety of intriguing correlated phase of matter. Here we present band engineering in the twisted orbital-active bilayers with spin-orbit coupling. The symmetry constraints on the interlayer coupling that determines the effective potential for low-energy physics of moir\'e electrons are exhaustively derived for two-dimensional point groups. We find the line graph or biparticle sublattice of moir\'e pattern emerge with a minimal $C_3$ symmetry, which exhibit isolated electronic flat bands with nontrivial topology. The band flatness is insensitive to the twist angle since they come from the interference effect. Armed with this guiding principle, we predict that twisted bilayers of 2H-PbS$_2$ and CdS realize the salient physics to engineer two-dimensional topological quantum phases. At small twist angles, PbS$_2$ heterostructures give rise to an emergent moir\'e Kagom\'e lattice, while CdS heterostructures lead to an emergent moir\'e honeycomb lattice, and both of them host moir\'e quantum spin Hall insulators with almost flat topological bands. We further study superconductivity of these two systems with local attractive interactions. The superfluid weight and Berezinskii-Kosterlitz-Thouless temperature are determined by multiband processes and quantum geometry of the band in the flat-band limit when the pairing potential exceeds the band width. Our results demonstrate twisted bilayers with multi-orbitals as a promising tunable platform to realize correlated topological phases. ",Kein DOI-Link verfügbar,2209.06524v1,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Discrete Wilson Lines in N=1 D=4 Type IIB Orientifolds: A Systematic   Exploration for $\IZ_6$ Orientifold,1970,"  We develop techniques to construct general discrete Wilson lines in four-dimensional N=1 Type IIB orientifolds, their T-dual realization corresponds to branes positioned at the orbifold fixed points. The explicit order two and three Wilson lines along with their tadpole consistency conditions are given for D=4 N=1 Z_6 Type IIB orientifold. The systematic search for all models with general order three Wilson lines leads to a small class of inequivalent models. There are only two inequivalent classes of a potentially phenomenologically interesting model that has a possible SU(3)_{color} x SU(2)_L x SU(2)_R x U(1)_{B-L} gauge structure, arising from a set of branes located at the Z_6 orbifold fixed point. We calculate the spectrum and Yukawa couplings for this model. On the other hand, introduction of anti-branes allows for models with three families and realistic gauge group assignment, arising from branes located at the Z_3 orbifold fixed points. ",https://doi.org/10.1016/S0550-3213(00)00669-6,hep-th/0010091v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Magnetic moiré surface states and flat chern band in topological   insulators,1970,"  We theoretically study the effect of magnetic moir\'e superlattice on the topological surface states by introducing a continuum model of Dirac electrons with a single Dirac cone moving in the time-reversal symmetry breaking periodic pontential. The Zeeman-type moir\'e potentials generically gap out the moir\'e surface Dirac cones and give rise to isolated flat Chern minibands with Chern number $\pm1$. This result provides a promising platform for realizing the time-reversal breaking correlated topological phases. In a $C_6$ periodic potential, when the scalar $U_0$ and Zeeman $\Delta_1$ moir\'e potential strengths are equal to each other, we find that energetically the first three bands of $\Gamma$-valley moir\'e surface electrons are non-degenerate and realize i) an $s$-orbital model on a honeycomb lattice, ii) a degenerate $p_x,p_y$-orbitals model on a honeycomb lattice, and iii) a hybridized $sd^2$-orbital model on a kagome lattice, where moir\'e surface Dirac cones in these bands emerge. When $U_0\neq\Delta_1$, the difference between the two moir\'e potential serves as an effective spin-orbit coupling and opens a topological gap in the emergent moir\'e surface Dirac cones. ",https://doi.org/10.1103/PhysRevB.106.035114,2106.01630v1,Yes,potent(4)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Large-Gap Quantum Anomalous Hall Insulators in $A$Ti$X$ Class,1970,"  We theoretically propose that the monolayer $A$Ti$X$ family (KTiSb, KTiBi, RbTiSb, SrTiSn) are potential candidates for large-gap quantum anomalous Hall insulators with high Chern number $\mathcal{C}=2$. Both of the topology and magnetism in these materials are from $3d$-orbitals of Ti. We construct the tight-binding model with symmetry analysis to reveal the origin of topology. Remarkably, quite different from the conventional $s$-$d$ band inversion, here the topological band inversion within $3d$ orbitals is due to the crystal field and electron hopping, while spin-orbit coupling only trivially gaps out the Dirac cone at Fermi level. The general physics from the $3d$ orbitals here applies to a large class of transition metal compounds with the space group $P4/nmm$ or $P$-$42m$ and their subgroups. ",https://doi.org/10.1103/PhysRevB.108.165122,2211.08938v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Effects of Heavy States on the Effective N=1 Supersymmetric Action,1970,"  Using the power of superspace formalism, we investigate the decoupling effects of heavy states in N=1 supersymmetric field theory. We find that ""mixed"" couplings in the superpotential between the heavy and light fields contribute to the effective superpotential at the leading order, and also contribute to the effective K\""{a}hler potential (in the next to leading order). Mixed couplings in the K\""{a}hler potential always contribute to the effective K\""{a}hler potential at the leading order. Several examples are presented which illustrate the effects explicitly. ",https://doi.org/10.1016/S0550-3213(98)00690-7,hep-ph/9807321v1,Yes,potent(5)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Absolutely Continuous Spectrum of Multifrequency Quasiperiodic   Schrödinger operator,1970,"  In this paper, we prove that for any $d$-frequency analytic quasiperiodic Schr\""odinger operator, if the frequency is weak Liouvillean, and the potential is small enough, then the corresponding operator has absolutely continuous spectrum. Moreover, in the case $d=2$, we even establish the existence of ac spectrum under small potential and some super-Liouvillean frequency, and this result is optimal due to a recent counterexample of Avila and Jitomirskaya. ",Kein DOI-Link verfügbar,2004.04409v1,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Absolute continuity of the integrated density of states in the localized   regime,1970,"  We establish the absolute continuity of the integrated density of states (IDS) for quasi-periodic Schr\""odinger operators with a large trigonometric potential and Diophantine frequency. This partially solves Eliasson's open problem in 2002. Furthermore, this result can be extended to a class of quasi-periodic long-range operators on $\ell^2(\Z^d)$. Our proof is based on stratified quantitative almost reducibility results of dual cocycles. Specifically, we prove that a generic analytic one-parameter family of cocycles, sufficiently close to constant coefficients, is reducible except for a zero Hausdorff dimension set of parameters. This result affirms Eliasson's conjecture in 2017. ",Kein DOI-Link verfügbar,2305.00457v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Object Proposal with Kernelized Partial Ranking,1970,"  Object proposals are an ensemble of bounding boxes with high potential to contain objects. In order to determine a small set of proposals with a high recall, a common scheme is extracting multiple features followed by a ranking algorithm which however, incurs two major challenges: {\bf 1)} The ranking model often imposes pairwise constraints between each proposal, rendering the problem away from an efficient training/testing phase; {\bf 2)} Linear kernels are utilized due to the computational and memory bottleneck of training a kernelized model.   In this paper, we remedy these two issues by suggesting a {\em kernelized partial ranking model}. In particular, we demonstrate that {\bf i)} our partial ranking model reduces the number of constraints from $O(n^2)$ to $O(nk)$ where $n$ is the number of all potential proposals for an image but we are only interested in top-$k$ of them that has the largest overlap with the ground truth; {\bf ii)} we permit non-linear kernels in our model which is often superior to the linear classifier in terms of accuracy. For the sake of mitigating the computational and memory issues, we introduce a consistent weighted sampling~(CWS) paradigm that approximates the non-linear kernel as well as facilitates an efficient learning. In fact, as we will show, training a linear CWS model amounts to learning a kernelized model. Extensive experiments demonstrate that equipped with the non-linear kernel and the partial ranking algorithm, recall at top-$k$ proposals can be substantially improved. ",https://doi.org/10.1016/j.patcog.2017.03.022,1502.01526v3,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Fermion-fermion interaction driven instability and criticality of   quadratic band crossing systems with the breaking of time-reversal symmetry,1970,"  We carefully study how the fermion-fermion interactions affect the low-energy states of a two-dimensional spin-$1/2$ fermionic system on the kagom\'{e} lattice with a quadratic band crossing point. With the help of the renormalization group approach, we can treat all kinds of fermionic interactions on the the same footing and then establish the coupled energy-dependent flows of fermionic interaction parameters via collecting one-loop corrections, from which a number of interesting results are extracted in the low-energy regime. At first, various sorts of fermion-fermion interactions furiously compete with each other and are inevitably attracted by certain fixed point in the parameter space, which clusters into three qualitatively distinct regions relying heavily upon the structure parameters of materials. In addition, we notice that an instability accompanied by some symmetry breaking is triggered around different sorts of fixed points. Computing and comparing susceptibilities of twelve potential candidates indicates that charge density wave always dominates over all other instabilities. Incidently, there exist several subleading ones including the $x$-current, bond density, and chiral plus s-wave superconductors. Finally, we realize that strong fluctuations nearby the leading instability prefer to suppress density of states and specific heat as well compressibility of quasiparticles in the lowest-energy limit. ",https://doi.org/10.1016/j.nuclphysb.2021.115371,2010.10963v6,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Physics-Assisted Reduced-Order Modeling for Identifying Dominant   Features of Transonic Buffet,1970,"  Transonic buffet is a flow instability phenomenon that arises from the interaction between the shock wave and the separated boundary layer. This flow phenomenon is considered to be highly detrimental during flight and poses a significant risk to the structural strength and fatigue life of aircraft. Up to now, there has been a lack of an accurate, efficient, and intuitive metric to predict buffet and impose a feasible constraint on aerodynamic design. In this paper, a Physics-Assisted Variational Autoencoder (PAVAE) is proposed to identify dominant features of transonic buffet, which combines unsupervised reduced-order modeling with additional physical information embedded via a buffet classifier. Specifically, four models with various weights adjusting the contribution of the classifier are trained, so as to investigate the impact of buffet information on the latent space. Statistical results reveal that buffet state can be determined exactly with just one latent space when a proper weight of classifier is chosen. The dominant latent space further reveals a strong relevance with the key flow features located in the boundary layers downstream of shock. Based on this identification, the displacement thickness at 80% chordwise location is proposed as a metric for buffet prediction. This metric achieves an accuracy of 98.5% in buffet state classification, which is more reliable than the existing separation metric used in design. The proposed method integrates the benefits of feature extraction, flow reconstruction, and buffet prediction into a unified framework, demonstrating its potential in low-dimensional representations of high-dimensional flow data and interpreting the ""black box"" neural network. ",https://doi.org/10.1063/5.0152127,2305.13644v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Critical behavior around the fixed points driven by fermion-fermion   interactions and disorders in the nodal-line superconductors,1970,"  We systematically investigate the intricate interplay between short-range fermion-fermion interactions and disorder scatterings beneath the superconducting dome of noncentrosymmetric nodal-line superconductors. Employing the renormalization group that unbiasedly treats all kinds of potential degrees of freedom, we establish energy-dependent coupled flows for all associated interaction parameters. Decoding the low-energy information from these coupled evolutions leads to the emergence of several intriguing behavior in the low-energy regime. At first, we identify eight distinct types of fixed points, which are determined by the competition of all interaction parameters and dictate the low-energy properties. Next, we carefully examine and unveil distinct fates of physical implications as approaching such fixed points. The density of states of quasiparticles displays a linear dependence on frequency around the first fixed point, while other fixed points present diverse frequency-dependent behavior. Compressibility and specific heat exhibit unique trends around different fixed points, with the emergence of non-Fermi-liquid behavior nearby the fifth fixed point. Furthermore, after evaluating the susceptibilities of the potential states, we find that a certain phase transition below the critical temperature can be induced when the system approaches the fifth fixed point, transitioning from the nodal-line superconducting state to another superconducting state. This research would enhance our understanding of the unique behavior in the low-energy regime of nodal-line superconductors. ",https://doi.org/10.1140/epjp/s13360-024-05388-5,2402.02040v2,Yes,"intricate(1), potent(2)"
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Fermion-fermion interaction driven phase transitions in the rhombohedral   trilayer graphene,1970,"  The effects of short-range fermion-fermion interactions on the low-energy properties of the rhombohedral trilayer graphene are comprehensively investigated by virtue of the momentum-shell renormalization group method. We take into account all one-loop corrections and establish the energy-dependent coupled evolutions of independent fermionic couplings that carry the physical information stemming from the interplay of various fermion-fermion interactions. With the help of the detailed numerical analysis, we notice that the ferocious competition among all fermion-fermion interactions can drive fermionic couplings to four distinct kinds of fixed points, dubbed $\textrm{FP}_{1}$, $\textrm{FP}_{2}$, $\textrm{FP}_{3}$ and $\textrm{FP}_{4}$, in the interaction-parameter space. Such fixed points principally dictate the fate of the system in the low-energy regime, which are always associated with some instabilities with specific symmetry breakings and thus accompanied by certain phase transitions. In order to judge the favorable states arising from the potential phase transitions, we bring out a number of fermion-bilinear source terms to characterize the underlying candidate states. By comparing their related susceptibilities, it is determined that the dominant states correspond to a spin-singlet superconductivity, a spin-triplet pair-density-wave, and a spin-triplet superconductivity for approaching the fixed points $\textrm{FP}_{1,3}$, $\textrm{FP}_{2}$, and $\textrm{FP}_{4}$, respectively. These results would be helpful to further reveal the low-energy properties of the rhombohedral trilayer graphene and analogous materials. ",Kein DOI-Link verfügbar,2406.01877v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Units and Numerical Values of the Effective Couplings in Perturbative   Heterotic String Vacua,1970,"  We determine the units and numerical values for a class of couplings in the effective theory of perturbative heterotic string vacua, with the emphasis on the correct translation between the canonical gauge coupling g and Planck scale M_Planck ~ 1.2 x 10^19 GeV as used in the effective theory description and the string coupling g_string and string tension alpha' as used in the S-matrix amplitude calculation. In particular, we determine the effective couplings in the superpotential and revisit the Fayet-Iliopoulos (FI) term in a class of models with an anomalous U(1). We derive the values of the effective Yukawa couplings (at the third and fourth order) after the restabilization of vacuum along a particular F- and D-flat direction and show that they are comparable in magnitude. The result corrects results quoted in the literature, and may have implications for the string derived phenomenology, e.g., that of fermion textures. ",https://doi.org/10.1103/PhysRevD.59.107901,hep-ph/9808321v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",A low-energy solution to the mu-problem in gauge mediation,1970,"  In the gauge-mediation framework the soft supersymmetry breaking mass parameters of the supersymmetric standard model are induced by the gauge interactions of some messenger fields. The parameters exhibit flavor universality which is dictated by the gauge interactions and which efficiently eliminates new dangerous contributions to flavor changing neutral currents. However, the Higgs potential in this framework typically contains an unacceptable hierarchy between its dimensionful parameters (the $\mu$-problem of gauge mediation). We show that the problem can be resolved if the Higgs potential arises dynamically once an intermediate U(1)' sector is integrated out rather than arising radiatively from some Yukawa interactions at the messenger scale. As an added benefit, such models may naturally avoid new contribution to CP violating amplitudes. The proposed framework is described, explicit examples are given and its phenomenology is explored. The $\mu$ problem is resolved in this case by the low-energy U(1)' dynamics which could be tested in future collider experiments. ",https://doi.org/10.1103/PhysRevD.60.115005,hep-ph/9905252v2,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Three Family Type IIB Orientifold String Vacua with Non-Abelian Wilson   Lines,1970,"  We address the implementation of non-Abelian Wilson lines in D=4 N=1 Type IIB orientifold constructions. We present an explicit three-family example with the gauge group (U(4)xU(2)xSU(2)xSU(2))^2x(U(6)xSp(4))^2 and give the particle spectrum and the trilinear superpotential. Emphasizing the new subtleties associated with the introduction of non-Abelian Wilson lines, we show that the Abelian gauge anomalies are cancelled by the Green-Schwarz-type mechanism, and calculate the Fayet-Iliopoulos terms and gauge coupling corrections. The analysis thus sets a stage for further investigations of the phenomenological implications of this model. ",https://doi.org/10.1088/1126-6708/2000/04/004,hep-th/9911021v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Dynamical Supersymmetry Breaking in Standard-like Models with   Intersecting D6-branes,1970,"  We address dynamical supersymmetry breaking within a N=1 supersymmetric Standard-like Model based on a Z_2 x Z_2 Type IIA orientifold with intersecting D6-branes. The model possesses an additional, confining gauge sector with the USp(2)_A x USp(2)_B x USp(4) gauge group, where the gaugino condensation mechanism allows for the breaking of supersymmetry and stabilizes moduli. We derive the leading contribution to the non-perturbative effective superpotential and determine numerically the minima of the supergravity potential. These minima break supersymmetry and fix two undetermined moduli, which in turn completely specify the gauge couplings at the string scale. For this specific construction the minima have a negative cosmological constant. We expect that for other supersymmetric Standard-like models with intersecting D6-branes, which also possess confining gauge sectors, the supersymmetry breaking mechanism would have qualitatively similar features. ",https://doi.org/10.1103/PhysRevD.68.046002,hep-th/0303208v1,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Dark Energy Model with Spinor Matter and Its Quintom Scenario,1970,"  A class of dynamical dark energy models, dubbed Spinor Quintom, can be constructed by a spinor field $\psi$ with a nontraditional potential. We find that, if choosing suitable potential, this model is able to allow the equation-of-state to cross the cosmological constant boundary without introducing any ghost fields. In a further investigation, we show that this model is able to mimic a perfect fluid of Chaplygin gas with $p=-c/\rho$ during the evolution, and also realizes the Quintom scenario with its equation-of-state across -1. ",https://doi.org/10.1088/0264-9381/25/16/165014,0806.3890v2,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Magnetic Silicon Fullerene,1970,"  A metal-encapsulating silicon fullerene, Eu@Si20, has been predicted by density functional theory to be by far the most stable fullerene-like silicon structure. The Eu@Si20 structure is a dodecahedron with D2h symmetry in which the europium atom occupies the center site. The calculated results show that the europium atom has a large magnetic moment of nearly 7.0 Bohr magnetons. In addition, it was found that a stable ""pearl necklace"" nanowire, constructed by concatenating a series of Eu@Si20 units, with the central europium atom, retains the high spin moment. The magnetic structure of the nanowire indicates potential applications in the fields of spintronics and high-density magnetic storage. ",https://doi.org/10.1039/B923865D,0908.1494v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Disorder effects at a nematic quantum critical point in d-wave cuprate   superconductor,1970,"  A d-wave high temperature cuprate superconductor exhibits a nematic ordering transition at zero temperature. Near the quantum critical point, the coupling between gapless nodal quasiparticles and nematic order parameter fluctuation can result in unusual behaviors, such as extreme anisotropy of fermion velocities. We study the disorder effect on the nematic quantum critical behavior and especially on the flow of fermion velocities. The disorders that couple to nodal quasiparticles are divided into three types: random mass, random gauge field, and random chemical potential. A renormalization group analysis shows that random mass and random gauge field are both irrelevant and thus do not change the fixed point of extreme velocity anisotropy. However, the marginal interaction due to random chemical potential destroys this fixed point and makes the nematic phase transition unstable. ",https://doi.org/10.1103/PhysRevB.83.214503,1101.1551v3,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Topological insulators for high performance terahertz to infrared   applications,1970,"  Topological insulators in the Bi2Se3 family have an energy gap in the bulk and a gapless surface state consisting of a single Dirac cone. Low frequency optical absorption due to the surface state is universally determined by the fine structure constant. When the thickness of these three dimensional topological insulators is reduced, they become quasi-two dimensional insulators with enhanced absorbance. The two dimensional insulators can be topologically trivial or non-trivial depending on the thickness, and we predict that the optical absorption is larger for topological non-trivial case compared with the trivial case. Since the three dimensional topological insulator surface state is intrinsically gapless, we propose its potential application in wide bandwidth, high performance photo-detection covering a broad spectrum ranging from terahertz to infrared. The performance of photodetection can be dramatically enhanced when the thickness is reduced to several quintuple layers, with a widely tunable band gap depending on the thickness. ",https://doi.org/10.1103/PhysRevB.82.245107,1101.3583v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Unconventional behavior of Dirac fermions in three-dimensional gauge   theory,1970,"  We study the unconventional behavior of massless Dirac fermions due to interaction with a U(1) gauge field in two spatial dimensions. At zero chemical potential, the longitudinal and transverse components of gauge interaction are both long-ranged. There is no fermion velocity renormalization since the system respects Lorentz invariance. At finite chemical potential, the Lorentz invariance is explicitly broken by the finite Fermi surface. The longitudinal gauge interaction is statically screened and becomes unimportant, whereas the transverse gauge interaction remains long-ranged and leads to singular renormalization of fermion velocity. The anomalous dimension of fermion velocity is calculated by means of the renormalization group method. We then examine the influence of singular velocity renormalization on several physical quantities, and show that they exhibit different behavior at zero and finite chemical potential. ",https://doi.org/10.1103/PhysRevD.85.105010,1202.3109v3,Yes,potent(3)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",A new strategy for directly calculating the minimum eigenvector of   matrices without diagonalization,1970,"  The diagonalization of matrices may be the top priority in the application of modern physics. In this paper, we numerically demonstrate that, for real symmetric random matrices with non-positive off-diagonal elements, a universal scaling between the eigenvector and matrix elements exists. Namely, each element of the eigenvector of ground states linearly correlates with the sum of matrix elements in the corresponding row. Although the conclusion is obtained based on the random matrices, the linear relationship still keeps for regular matrices, in which off-diagonal elements are non-positive. The relationship implies a straightforward method to directly calculate the eigenvector of ground states for a kind of matrices. The test on both Hubbard and Ising models shows that, this new method works excellently. ",https://doi.org/10.1038/s41598-020-60103-5,1901.10626v3,Yes,excellently(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Performance evaluation of an integrated photonic convolutional neural   network based on delay buffering and wavelength division multiplexing,1970,"  Photonic technologies have shown a promising way to build high-speed and high-energy-efficiency neural network accelerators. In previously presented photonic neural networks, architectures are mainly designed for fully-connected layers. When convolutional layers are executed in such neural networks, the large-scale electrooptic modulation array heavily increases the energy dissipation on chip. To increase the energy efficiency, here we show an integrated photonic architecture specifically for convolutional layer calculations. Optical delay lines replace electronics to execute data manipulations on optical chip, reducing the scale of electro-optic modulation array. Consequently, the energy dissipation of these parts is mitigated. Powered by wavelength division multiplexing, the footprint of delay lines is significantly reduced compared with previous art, thus being practical to fabricate. We evaluate the potential performance of the proposed architecture with respect to component flaws in practical fabrications. According to the results, with well-controlled system insertion loss, energy efficiency of the proposed architecture would surpass previously presented works and the state-of-art electronic processors. We anticipate the proposed architecture is beneficial for future fast and energy-efficient convolutional neural network accelerators. ",Kein DOI-Link verfügbar,1910.12635v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Dissipative Edge Transport in Disordered Axion Insulator Films,1970,"  We investigate the role of disorder in the edge transport of axion insulator films. We predict by first-principles calculations that even-number-layer MnBi$_2$Te$_4$ have gapped helical edge states. The random potential will dramatically modify the edge spectral function to become gapless. However, such gapless helical state here is fundamentally different from that in quantum spin Hall insulator or topological Anderson insulator. We further study the edge transport in this system by Landauer-B\""{u}ttiker formalism, and find such gapless edge state is dissipative and not immune to backscattering, which would explain the dissipative nonlocal transport in the axion insulator state observed in six septuple layer MnBi$_2$Te$_4$ experimentally. Several transport experiments are proposed to verify our theory on the dissipative helical edge channels. In particular, the longitudinal resistance can be greatly reduced by adding an extra floating probe even if it is not used. These results will facilitate the observsation of long-sought topological magnetoelectric effect in axion insulators. ",https://doi.org/10.1103/PhysRevB.108.245116,2109.06178v1,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Exploring Active Learning in Meta-Learning: Enhancing Context Set   Labeling,1970,"  Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets. ",Kein DOI-Link verfügbar,2311.02879v3,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Predicting Many Properties of Crystals by a Single Deep Learning Model,1970,"  The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability. Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information. The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more. CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties. Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information. This underscores the intricate nature of topological and superconducting properties. By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model. ",Kein DOI-Link verfügbar,2405.18944v1,Yes,intricate(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Confinement induced by fermion damping in three-dimensional QED,1970,"  The three-dimensional non-compact QED is known to exhibit weak confinement when fermions acquire a finite mass via the mechanism of dynamical chiral symmetry breaking. In this paper, we study the effect of fermion damping caused by elastic scattering on the classical potential between fermions. By calculating the vacuum polarization function that incorporates the fermion damping effect, we show that fermion damping can induce a weak confinement even when the fermions are massless and the chiral symmetry is not broken. ",https://doi.org/10.1103/PhysRevD.82.067701,1008.0736v2,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Cooper instability generated by attractive fermion-fermion interaction   in the two-dimensional semi-Dirac semimetals,1970,"  Cooper instability associated with superconductivity in the two-dimensional semi-Dirac semimetals is attentively studied in the presence of attractive Cooper-pairing interaction, which is the projection of an attractive fermion-fermion interaction. Performing the standard renormalization group analysis shows that the Cooper theorem is violated at zero chemical potential but instead Cooper instability can be generated only if the absolute strength of fermion-fermion coupling exceeds certain critical value and transfer momentum is restricted to a confined region, which is determined by the initial conditions. Rather, the Cooper theorem would be instantly restored once a finite chemical potential is introduced and thus a chemical potential-tuned phase transition is expected. Additionally, we briefly examine the effects of impurity scatterings on the Cooper instability at zero chemical potential, which in principle are harmful to Cooper instability although they can enhance the density of states of systems. Furthermore, the influence of competition between a finite chemical potential and impurities upon the Cooper instability is also simply investigated. These results are expected to provide instructive clues for exploring unconventional superconductors in the kinds of semimetals. ",https://doi.org/10.1088/1361-648X/ab142d,1806.03410v6,Yes,potent(5)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Singular low-energy states of tilted Dirac semimetals induced by the   fermion-fermion interactions,1970,"  We attentively investigate the effects of short-range fermion-fermion interactions on the low-energy properties of both two-dimensional type-I and type-II tilted Dirac semimetals by means of the renormalization group framework. Practicing the standard renormalization group procedures via taking into account all one-loop corrections gives rise to the coupled energy-dependent evolutions of all interaction parameters, which are adopted to carefully examine whether and how the fermion-fermion interactions influence the low-energy physical behaviors of tilted Dirac fermions. After carrying out the detailed analysis of coupled flows, we figure out the tilting parameter dictates the low-energy states of tilted Dirac fermions in conjunction with starting values of fermion-fermion couplings. With proper variations of these two kinds of parameters, the tilted Dirac fermions can either flow towards the Gaussian fixed point or undergo certain instability that is conventionally accompanied by a phase transition in the low-energy regime. In addition, all potential instabilities can be clustered into five distinct classes owing to the competitions between the tilting parameter and initial fermionic interactions. Moreover, the dominant phases accompanied by the instabilities are determined via computing and comparing the susceptibilities of eight potential phases. ",https://doi.org/10.1140/epjb/e2019-100373-9,1907.13341v3,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo   Embeddings,1970,"  Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs. ",Kein DOI-Link verfügbar,2308.09012v1,Yes,notable(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Effects of the interplay between fermionic interactions and disorders in   the nodal-line superconductors,1970,"  We study the interplay between fermion-fermion interactions and disorder scatterings beneath the superconducting dome of noncentrosymmetric nodal-line superconductors. With the application of renormalization group, several interesting low-energy behaviors are extracted from the coupled equations of all interaction parameters. At the clean limit, fermion-fermion interactions decrease with lowering the energy scales but conversely fermion velocities climb up and approach certain saturated values. This yields a slight decrease or increase of the anisotropy of fermion velocities depending upon their initial ratio. After bringing out four kinds of disorders designated by the random charge ($\Delta_{1}$), random mass ($\Delta_{2}$), random axial chemical potential ($\Delta_{3}$), and spin-orbit scatterers ($\Delta_{4}$) based on their own unique features, we begin with presenting the distinct low-energy fates of these disorders. For the presence of sole disorder, its strength becomes either relevant ($\Delta_{1,4}$) or irrelevant($\Delta_{2,3}$) in the low-energy regime. However, the competition for multiple sorts of disorders is capable of qualitatively reshaping the low-energy properties of disorders $\Delta_{2,3,4}$. Besides, it can generate an initially absent disorder as long as two of $\Delta_{1,2,3}$ are present. In addition, the fermion-fermion couplings are insensitive to the presence of $\Delta_4$ but rather substantially modified by $\Delta_1$, $\Delta_2$, or $\Delta_3$, and evolve towards zero or certain finite non-zero values under the coexistence of distinct disorders. Furthermore, the fermion velocities flow towards certain finite saturated value for the only presence of $\Delta_{2,3}$ and vanish for all other situations. As to their ratio, it acquires a little increase once the disorder is subordinate to fermionic interactions, otherwise keeps some fixed constant. ",Kein DOI-Link verfügbar,2212.02356v3,Yes,potent(1)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",Edge State Induced Andreev Oscillation in Quantum Anomalous Hall   Insulator-Superconductor Junctions,1970,"  We study the quantum Andreev oscillation induced by interference of the edge chiral Majorana fermions in junctions made of quantum anomalous Hall (QAH) insulators and superconductors (SCs). We show two chiral Majorana fermions on a QAH edge with SC proximity generically have a momentum difference $\Delta k$, which depends on the chemical potentials of both the QAH insulator and the SC. Due to the spatial interference induced by $\Delta k$, the longitudinal conductance of QAH-SC junctions oscillates with respect to the edge lengths and the chemical potentials, which can be probed via charge transport. Furthermore, we show the dynamical SC phase fluctuation will give rise to a geometrical correction to the longitudinal conductance of the junctions. ",https://doi.org/10.1103/PhysRevB.93.161401,1512.05856v2,Yes,potent(2)
0000-0002-6394-5357,Jing Wang,"Zhejiang University, Zhejiang university",The Fokas-Lenells equations: Bilinear approach,1970,"  In this paper, the Fokas-Lenells equations are investigated via bilinear approach. We bilinearize the unreduced Fokas-Lenells system, derive double Wronskian solutions, and then, by means of a reduction technique we obtain variety of solutions of the reduced equations. This enables us to have a full profile of solutions of the classical and nonlocal Fokas-Lenells equations. Some obtained solutions are illustrated based on asymptotic analysis. As a notable new result, we obtain solutions to the Fokas-Lenells equation, which are related to real discrete eigenvalues and not reported before in the analytic approaches. These solutions behave like (multi-)periodic waves or solitary waves with algebraic decay. In addition, we also obtain solutions to the two-dimensional massive Thirring model from those of the Fokas-Lenells equation. ",https://doi.org/10.1111/sapm.12454,2104.04938v3,Yes,notable(1)
0000-0002-2624-9125,Fang Tao,"Zhejiang University, Zhejiang University Hangzhou",Unusually low thermal conductivity of atomically thin 2D tellurium,1970,"  Tellurium is a high-performance thermoelectric material due to its superior electronic transport and low lattice thermal conductivity ($\kappa_L$). Here, we report the ultralow $\kappa_L$ in the monolayer tellurium, i.e., tellurene, which has been successfully synthesized in recent experiments. We find tellurene has a compellingly low room temperature $\kappa_L$ of 2.16 and 4.08 W m$^{-1}$ K$^{-1}$ along the armchair and zigzag directions, respectively, which is lower than any reported values for other 2D materials. We attribute this unusually low $\kappa_L$ to the soft acoustic modes, extremely low-energy optical modes and the strong scattering among optical-acoustic phonons, which place tellurene as a potential novel thermoelectric material. Finally, we disclose that $\kappa_L$ is proportional to the largest acoustic phonon frequency ($\omega_{D}^{a}$) and the lowest optical phonon frequency at $\Gamma$ point ($\omega_{\Gamma}^{o}$) in 2D materials, which reflect both harmonic and anharmonic thermal properties respectively. ",https://doi.org/10.1039/C8NR01649F,1802.00722v1,Yes,"potent(1), compellingly(1)"
0000-0001-8811-5010,Zheng Ye,"Zhejiang University, Zhejiang University City College, Zhejiang University of Technology",Towards Imperceptible Document Manipulations against Neural Ranking   Models,1970,"  Adversarial attacks have gained traction in order to identify potential vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce grammatical errors, nonsensical expressions, or incoherent text fragments, which can be easily detected. Additionally, current methods rely heavily on the use of a well-imitated surrogate NRM to guarantee the attack effect, which makes them difficult to use in practice. To address these issues, we propose a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM instructs a well-established generative language model, such as BART, to generate connection sentences without introducing easy-to-detect errors, and employs a separate position-wise merging strategy to balance relevance and coherence of the perturbed text. Experimental results on the popular MS MARCO benchmark demonstrate that IDEM can outperform strong baselines while preserving fluency and correctness of the target documents as evidenced by automatic and human evaluations. Furthermore, the separation of adversarial text generation from the surrogate NRM makes IDEM more robust and less affected by the quality of the surrogate NRM. ",Kein DOI-Link verfügbar,2305.01860v1,Yes,potent(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Optimal Perfect Distinguishability between Unitaries and Quantum   Operations,1970,"  We study optimal perfect distinguishability between a unitary and a general quantum operation. In 2-dimensional case we provide a simple sufficient and necessary condition for sequential perfect distinguishability and an analytical formula of optimal query time. We extend the sequential condition to general d-dimensional case. Meanwhile, we provide an upper bound and a lower bound for optimal sequential query time. In the process a new iterative method is given, the most notable innovation of which is its independence to auxiliary systems or entanglement. Following the idea, we further obtain an upper bound and a lower bound of (entanglement-assisted) q-maximal fidelities between a unitary and a quantum operation. Thus by the recursion in [1] an upper bound and a lower bound for optimal general perfect discrimination are achieved. Finally our lower bound result can be extended to the case of arbitrary two quantum operations. ",Kein DOI-Link verfügbar,1010.2298v1,Yes,notable(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Regularized Non-monotone Submodular Maximization,1970,"  In this paper, we present a thorough study of maximizing a regularized non-monotone submodular function subject to various constraints, i.e., $\max \{ g(A) - \ell(A) : A \in \mathcal{F} \}$, where $g \colon 2^\Omega \to \mathbb{R}_+$ is a non-monotone submodular function, $\ell \colon 2^\Omega \to \mathbb{R}_+$ is a normalized modular function and $\mathcal{F}$ is the constraint set. Though the objective function $f := g - \ell$ is still submodular, the fact that $f$ could potentially take on negative values prevents the existing methods for submodular maximization from providing a constant approximation ratio for the regularized submodular maximization problem. To overcome the obstacle, we propose several algorithms which can provide a relatively weak approximation guarantee for maximizing regularized non-monotone submodular functions. More specifically, we propose a continuous greedy algorithm for the relaxation of maximizing $g - \ell$ subject to a matroid constraint. Then, the pipage rounding procedure can produce an integral solution $S$ such that $\mathbb{E} [g(S) - \ell(S)] \geq e^{-1}g(OPT) - \ell(OPT) - O(\epsilon)$. Moreover, we present a much faster algorithm for maximizing $g - \ell$ subject to a cardinality constraint, which can output a solution $S$ with $\mathbb{E} [g(S) - \ell(S)] \geq (e^{-1} - \epsilon) g(OPT) - \ell(OPT)$ using $O(\frac{n}{\epsilon^2} \ln \frac 1\epsilon)$ value oracle queries. We also consider the unconstrained maximization problem and give an algorithm which can return a solution $S$ with $\mathbb{E} [g(S) - \ell(S)] \geq e^{-1} g(OPT) - \ell(OPT)$ using $O(n)$ value oracle queries. ",Kein DOI-Link verfügbar,2103.10008v1,Yes,potent(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Score Regularized Policy Optimization through Diffusion Behavior,1970,"  Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance. ",Kein DOI-Link verfügbar,2310.07297v3,Yes,potent(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",ChatGPT Informed Graph Neural Network for Stock Movement Prediction,1970,"  ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and underscores its promising implications for the financial sector. ",https://doi.org/10.2139/ssrn.4464002,2306.03763v4,Yes,potent(2)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation   in the Wild,1970,"  Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin. ",Kein DOI-Link verfügbar,2004.03737v1,Yes,potent(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with   Variational Score Distillation,1970,"  Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/ ",Kein DOI-Link verfügbar,2305.16213v2,Yes,"meticulous(1), meticulously(1)"
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Learning to Rank Onset-Occurring-Offset Representations for   Micro-Expression Recognition,1970,"  This paper focuses on the research of micro-expression recognition (MER) and proposes a flexible and reliable deep learning method called learning to rank onset-occurring-offset representations (LTR3O). The LTR3O method introduces a dynamic and reduced-size sequence structure known as 3O, which consists of onset, occurring, and offset frames, for representing micro-expressions (MEs). This structure facilitates the subsequent learning of ME-discriminative features. A noteworthy advantage of the 3O structure is its flexibility, as the occurring frame is randomly extracted from the original ME sequence without the need for accurate frame spotting methods. Based on the 3O structures, LTR3O generates multiple 3O representation candidates for each ME sample and incorporates well-designed modules to measure and calibrate their emotional expressiveness. This calibration process ensures that the distribution of these candidates aligns with that of macro-expressions (MaMs) over time. Consequently, the visibility of MEs can be implicitly enhanced, facilitating the reliable learning of more discriminative features for MER. Extensive experiments were conducted to evaluate the performance of LTR3O using three widely-used ME databases: CASME II, SMIC, and SAMM. The experimental results demonstrate the effectiveness and superior performance of LTR3O, particularly in terms of its flexibility and reliability, when compared to recent state-of-the-art MER methods. ",Kein DOI-Link verfügbar,2310.04664v1,Yes,noteworthy(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Layer-Adapted Implicit Distribution Alignment Networks for Cross-Corpus   Speech Emotion Recognition,1970,"  In this paper, we propose a new unsupervised domain adaptation (DA) method called layer-adapted implicit distribution alignment networks (LIDAN) to address the challenge of cross-corpus speech emotion recognition (SER). LIDAN extends our previous ICASSP work, deep implicit distribution alignment networks (DIDAN), whose key contribution lies in the introduction of a novel regularization term called implicit distribution alignment (IDA). This term allows DIDAN trained on source (training) speech samples to remain applicable to predicting emotion labels for target (testing) speech samples, regardless of corpus variance in cross-corpus SER. To further enhance this method, we extend IDA to layer-adapted IDA (LIDA), resulting in LIDAN. This layer-adpated extention consists of three modified IDA terms that consider emotion labels at different levels of granularity. These terms are strategically arranged within different fully connected layers in LIDAN, aligning with the increasing emotion-discriminative abilities with respect to the layer depth. This arrangement enables LIDAN to more effectively learn emotion-discriminative and corpus-invariant features for SER across various corpora compared to DIDAN. It is also worthy to mention that unlike most existing methods that rely on estimating statistical moments to describe pre-assumed explicit distributions, both IDA and LIDA take a different approach. They utilize an idea of target sample reconstruction to directly bridge the feature distribution gap without making assumptions about their distribution type. As a result, DIDAN and LIDAN can be viewed as implicit cross-corpus SER methods. To evaluate LIDAN, we conducted extensive cross-corpus SER experiments on EmoDB, eNTERFACE, and CASIA corpora. The experimental results demonstrate that LIDAN surpasses recent state-of-the-art explicit unsupervised DA methods in tackling cross-corpus SER tasks. ",Kein DOI-Link verfügbar,2310.03992v1,Yes,strategically(1)
0000-0002-0005-9892,Cheng Lu,"Zhejiang University, Zhejiang University of Technology",Towards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach,1970,"  Cross-corpus speech emotion recognition (SER) poses a challenge due to feature distribution mismatch, potentially degrading the performance of established SER methods. In this paper, we tackle this challenge by proposing a novel transfer subspace learning method called acoustic knowledgeguided transfer linear regression (AKTLR). Unlike existing approaches, which often overlook domain-specific knowledge related to SER and simply treat cross-corpus SER as a generic transfer learning task, our AKTLR method is built upon a well-designed acoustic knowledge-guided dual sparsity constraint mechanism. This mechanism emphasizes the potential of minimalistic acoustic parameter feature sets to alleviate classifier overadaptation, which is empirically validated acoustic knowledge in SER, enabling superior generalization in cross-corpus SER tasks compared to using large feature sets. Through this mechanism, we extend a simple transfer linear regression model to AKTLR. This extension harnesses its full capability to seek emotiondiscriminative and corpus-invariant features from established acoustic parameter feature sets used for describing speech signals across two scales: contributive acoustic parameter groups and constituent elements within each contributive group. Our proposed method is evaluated through extensive cross-corpus SER experiments on three widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA. The results confirm the effectiveness and superior performance of our method, outperforming recent state-of-the-art transfer subspace learning and deep transfer learning-based cross-corpus SER methods. Furthermore, our work provides experimental evidence supporting the feasibility and superiority of incorporating domain-specific knowledge into the transfer learning model to address cross-corpus SER tasks. ",Kein DOI-Link verfügbar,2312.06466v1,Yes,potent(2)
0009-0004-5233-5328,Yujie Sun,"Zhejiang University, Zhejiang University of Technology",Layer-dependent Raman spectroscopy and electronic applications of   wide-bandgap 2D semiconductor \b{eta}-ZrNCl,1970,"  In recent years, two-dimensional (2D) layered semiconductors have received much attention for their potential in next-generation electronics and optoelectronics. Wide-bandgap 2D semiconductors are especially important in blue and ultraviolet wavelength region, while there are very few 2D materials in this region. Here, monolayer \b{eta}-type zirconium nitride chloride (\b{eta}-ZrNCl) is isolated for the first time, which is an air-stable layered material with a bandgap of ~3.0 eV in bulk. Systematical investigation of layer-dependent Raman scattering of ZrNCl from monolayer, bilayer, to bulk reveals a blue shift of its out-of-plane A1g peak at ~189 cm-1. Importantly, this A1g peak is absent in monolayer, suggesting that it is a fingerprint to quickly identify monolayer and for the thickness determination of 2D ZrNCl. The back-gate field-effect transistor based on few-layer ZrNCl shows a high on/off ratio of 108. These results suggest the potential of 2D \b{eta}-ZrNCl for electronic applications. ",Kein DOI-Link verfügbar,2202.07319v1,Yes,potent(2)
0009-0004-5233-5328,Yujie Sun,"Zhejiang University, Zhejiang University of Technology",Orbital-selective effect of spin reorientation on the Dirac fermions in   a three-dimensional kagome ferromagnet Fe$_3$Ge,1970,"  Kagome magnets provide a fascinating platform for the realization of correlated topological quantum phases under various magnetic ground states. However, the intricate effect of the magnetic spin configurations on the characteristic electronic structure directly from the kagome lattice layer remains still elusive. Here, utilizing angle-resolved photoemission spectroscopy and density functional theory calculations, we report the spectroscopic evidence for the spin-reorientation effect of a kagome ferromagnet Fe$_3$Ge, which is composed only of the kagome planes. There are two kinds of kagome-derived Dirac fermions due to the structural three-dimensionality -- one is less dispersive ($k_z$ $\sim$ 0) and the other disperses linearly ($k_z$ $\sim$ $\pi$). As the Fe moments cant from the $c$ axis into the $ab$ plane upon cooling, the Dirac fermion in $k_z$ $\sim$ 0 plane with a mixture of the Fe-$3d_{xy}$ and Fe-$3d_{x^2-y^2}$ components evolves from gapped into nearly gapless, while the Dirac cone in $k_z$ $\sim$ $\pi$ plane mainly of the Fe-$3d_{x^2-y^2}$ orbital character remains intact, suggesting that the effect of spin reorientation on the Dirac fermions has an orbital selectivity. Our unambiguous observations provide a feasible route to design and manipulate the mass of Dirac fermions for realizing the novel quantum phases. We also perform comparative studies between the non-charge-ordered Fe$_3$Ge and its sibling compound FeGe, a newly established charge-density-wave kagome magnet, the results suggest that the orbital-selective van Hove singularities near the Fermi level play an indispensable part in driving the charge order on a magnetic kagome lattice. ",Kein DOI-Link verfügbar,2309.06399v1,Yes,intricate(1)
0009-0004-5233-5328,Yujie Sun,"Zhejiang University, Zhejiang University of Technology",Gigantic-oxidative atomically layered epitaxy for designed complex   oxides,1970,"  In designing material functionality within the intricate realm of transition metal oxides, lattice structure and d-orbital occupancy are two principal determinants of the correlated physical properties, such as superconductivity. However, the modulation of these two factors is inherently limited by the need to balance thermodynamic stability, kinetic mobility, and synthesis precision, particularly for oxidation-demanding phases. We introduce a methodology, namely the gigantic-oxidative atomically layered epitaxy (GOAL-Epitaxy), enhancing oxidation power 3-4 orders of magnitude beyond oxide molecular beam epitaxy (OMBE) and pulsed laser deposition (PLD), while ensuring atomic-layer-by-layer growth of designed complex structures. Consequently, thermodynamic stability is markedly augmented at elevated temperatures, improving growth kinetics. We demonstrate the accurate synthesis of complex nickelates and cuprates, especially an artificially designed structure as a parent of high-temperature superconductivity, in which alternating single and double NiO2 layers possess distinct nominal d-orbital occupancy. The GOAL-Epitaxy enables material discovery within the vastly broadened growth parameter space. ",Kein DOI-Link verfügbar,2406.16520v1,Yes,intricate(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",Exploiting Constructive Interference for Backscatter Communication   Systems,1970,"  Backscatter communication (BackCom), one of the core technologies to realize zero-power communication, is expected to be a pivotal paradigm for the next generation of the Internet of Things (IoT). However, the ""strong"" direct link (DL) interference (DLI) is traditionally assumed to be harmful, and generally drowns out the ""weak"" backscattered signals accordingly, thus deteriorating the performance of BackCom. In contrast to the previous efforts to eliminate the DLI, in this paper, we exploit the constructive interference (CI), in which the DLI contributes to the backscattered signal. To be specific, our objective is to maximize the received signal power by jointly optimizing the receive beamforming vectors and tag selection factors, which is, however, non-convex and difficult to solve due to constraints on the Kullback-Leibler (KL) divergence. In order to solve this problem, we first decompose the original problem, and then propose two algorithms to solve the sub-problem with beamforming design via a change of variables and semi-definite programming (SDP) and a greedy algorithm to solve the sub-problem with tag selection. In order to gain insight into the CI, we consider a special case with the single-antenna reader to reveal the channel angle between the backscattering link (BL) and the DL, in which the DLI will become constructive. Simulation results show that significant performance gain can always be achieved in the proposed algorithms compared with the traditional algorithms without the DL in terms of the strength of the received signal. The derived constructive channel angle for the BackCom system with the single-antenna reader is also confirmed by simulation results. ",https://doi.org/10.1109/TCOMM.2023.3277519,2205.10741v2,Yes,pivotal(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",Unified Conversational Models with System-Initiated Transitions between   Chit-Chat and Task-Oriented Dialogues,1970,"  Spoken dialogue systems (SDSs) have been separately developed under two different categories, task-oriented and chit-chat. The former focuses on achieving functional goals and the latter aims at creating engaging social conversations without special goals. Creating a unified conversational model that can engage in both chit-chat and task-oriented dialogue is a promising research topic in recent years. However, the potential ``initiative'' that occurs when there is a change between dialogue modes in one dialogue has rarely been explored. In this work, we investigate two kinds of dialogue scenarios, one starts from chit-chat implicitly involving task-related topics and finally switching to task-oriented requests; the other starts from task-oriented interaction and eventually changes to casual chat after all requested information is provided. We contribute two efficient prompt models which can proactively generate a transition sentence to trigger system-initiated transitions in a unified dialogue model. One is a discrete prompt model trained with two discrete tokens, the other one is a continuous prompt model using continuous prompt embeddings automatically generated by a classifier. We furthermore show that the continuous prompt model can also be used to guide the proactive transitions between particular domains in a multi-domain task-oriented setting. ",Kein DOI-Link verfügbar,2307.01664v1,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues   with Transition Info Extractor and Transition Sentence Generator,1970,"  In this work, we study dialogue scenarios that start from chit-chat but eventually switch to task-related services, and investigate how a unified dialogue model, which can engage in both chit-chat and task-oriented dialogues, takes the initiative during the dialogue mode transition from chit-chat to task-oriented in a coherent and cooperative manner. We firstly build a {transition info extractor} (TIE) that keeps track of the preceding chit-chat interaction and detects the potential user intention to switch to a task-oriented service. Meanwhile, in the unified model, a {transition sentence generator} (TSG) is extended through efficient Adapter tuning and transition prompt learning. When the TIE successfully finds task-related information from the preceding chit-chat, such as a transition domain, then the TSG is activated automatically in the unified model to initiate this transition by generating a transition sentence under the guidance of transition information extracted by TIE. The experimental results show promising performance regarding the proactive transitions. We achieve an additional large improvement on TIE model by utilizing Conditional Random Fields (CRF). The TSG can flexibly generate transition sentences while maintaining the unified capabilities of normal chit-chat and task-oriented response generation. ",Kein DOI-Link verfügbar,2308.03098v1,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",MMKG: Multi-Modal Knowledge Graphs,1970,"  We present MMKG, a collection of three knowledge graphs that contain both numerical features and (links to) images for all entities as well as entity alignments between pairs of KGs. Therefore, multi-relational link prediction and entity matching communities can benefit from this resource. We believe this data set has the potential to facilitate the development of novel multi-modal learning approaches for knowledge graphs.We validate the utility ofMMKG in the sameAs link prediction task with an extensive set of experiments. These experiments show that the task at hand benefits from learning of multiple feature types. ",Kein DOI-Link verfügbar,1903.05485v1,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",PropertyGPT: LLM-driven Formal Verification of Smart Contracts through   Retrieval-Augmented Property Generation,1970,"  With recent advances in large language models (LLMs), this paper explores the potential of leveraging state-of-the-art LLMs, such as GPT-4, to transfer existing human-written properties (e.g., those from Certora auditing reports) and automatically generate customized properties for unknown code. To this end, we embed existing properties into a vector database and retrieve a reference property for LLM-based in-context learning to generate a new prop- erty for a given code. While this basic process is relatively straight- forward, ensuring that the generated properties are (i) compilable, (ii) appropriate, and (iii) runtime-verifiable presents challenges. To address (i), we use the compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. For (ii), we consider multiple dimensions of similarity to rank the properties and employ a weighted algorithm to identify the top-K properties as the final result. For (iii), we design a dedicated prover to formally verify the correctness of the generated prop- erties. We have implemented these strategies into a novel system called PropertyGPT, with 623 human-written properties collected from 23 Certora projects. Our experiments show that PropertyGPT can generate comprehensive and high-quality properties, achieving an 80% recall compared to the ground truth. It successfully detected 26 CVEs/attack incidents out of 37 tested and also uncovered 12 zero-day vulnerabilities, resulting in $8,256 bug bounty rewards. ",Kein DOI-Link verfügbar,2405.02580v1,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",Empowering Few-Shot Relation Extraction with The Integration of   Traditional RE Methods and Large Language Models,1970,"  Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE) that utilizes limited training instances, appeals to more researchers in Natural Language Processing (NLP) due to its capability to extract textual information in extremely low-resource scenarios. The primary methodologies employed for FSRE have been fine-tuning or prompt tuning techniques based on Pre-trained Language Models (PLMs). Recently, the emergence of Large Language Models (LLMs) has prompted numerous researchers to explore FSRE through In-Context Learning (ICL). However, there are substantial limitations associated with methods based on either traditional RE models or LLMs. Traditional RE models are hampered by a lack of necessary prior knowledge, while LLMs fall short in their task-specific capabilities for RE. To address these shortcomings, we propose a Dual-System Augmented Relation Extractor (DSARE), which synergistically combines traditional RE models with LLMs. Specifically, DSARE innovatively injects the prior knowledge of LLMs into traditional RE models, and conversely enhances LLMs' task-specific aptitude for RE through relation extraction augmentation. Moreover, an Integrated Prediction module is employed to jointly consider these two respective predictions and derive the final results. Extensive experiments demonstrate the efficacy of our proposed method. ",Kein DOI-Link verfügbar,2407.08967v1,Yes,"innovative(1), innovatively(1)"
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",Learn while Unlearn: An Iterative Unlearning Framework for Generative   Language Models,1970,"  Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive information leakage. In response, regulatory measures like the EU General Data Protection Regulation (GDPR) have driven the exploration of Machine Unlearning techniques, which aim to enable models to selectively forget certain data entries. While early approaches focused on pre-processing methods, recent research has shifted towards training-based machine unlearning methods. However, many existing methods require access to original training data, posing challenges in scenarios where such data is unavailable. Besides, directly facilitating unlearning may undermine the language model's general expressive ability. To this end, in this paper, we introduce the Iterative Contrastive Unlearning (ICU) framework, which addresses these challenges by incorporating three key components. We propose a Knowledge Unlearning Induction module for unlearning specific target sequences and a Contrastive Learning Enhancement module to prevent degrading in generation capacity. Additionally, an Iterative Unlearning Refinement module is integrated to make the process more adaptive to each target sample respectively. Experimental results demonstrate the efficacy of ICU in maintaining performance while efficiently unlearning sensitive information, offering a promising avenue for privacy-conscious machine learning applications. ",Kein DOI-Link verfügbar,2407.20271v1,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",FedJudge: Federated Legal Large Language Model,1970,"  Large Language Models (LLMs) have gained prominence in the field of Legal Intelligence, offering potential applications in assisting legal professionals and laymen. However, the centralized training of these Legal LLMs raises data privacy concerns, as legal data is distributed among various institutions containing sensitive individual information. This paper addresses this challenge by exploring the integration of Legal LLMs with Federated Learning (FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on devices or clients, and their parameters are aggregated and distributed on a central server, ensuring data privacy without directly sharing raw data. However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods. To this end, in this paper, we propose the first Federated Legal Large Language Model (FedJudge) framework, which fine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge utilizes parameter-efficient fine-tuning methods to update only a few additional parameters during the FL training. Besides, we explore the continual learning methods to preserve the global model's important parameters when training local clients to mitigate the problem of data shifts. Extensive experimental results on three real-world datasets clearly validate the effectiveness of FedJudge. Code is released at https://github.com/yuelinan/FedJudge. ",Kein DOI-Link verfügbar,2309.08173v3,Yes,potent(1)
0000-0002-6935-4208,Ye Liu,"Zhejiang University, Zhejiang University of Technology",Point cloud ridge-valley feature enhancement based on position and   normal guidance,1970,"  Ridge-valley features are important elements of point clouds, as they contain rich surface information. To recognize these features from point clouds, this paper introduces an extreme point distance (EPD) criterion with scale independence. Compared with traditional methods, the EPD greatly reduces the number of potential feature points and improves the robustness of multiscale feature point recognition. On this basis, a feature enhancement algorithm based on user priori guidance is proposed that adjusts the coordinates of the feature area by solving an objective equation containing the expected position and normal constraints. Since the expected normal can be expressed as a function of neighborhood point coordinates, the above objective equation can be converted into linear sparse equations with enhanced feature positions as variables, and thus, the closed solution can be obtained. In addition, a parameterization method for scattered point clouds based on feature line guidance is proposed, which reduces the number of unknowns by 2/3 and eliminates lateral sliding in the direction perpendicular to feature lines. Finally, the application of the algorithm in multiscale ridge-valley feature recognition, freeform surface feature enhancement and computer-aided design (CAD) workpiece sharp feature restoration verifies its effectiveness. ",Kein DOI-Link verfügbar,1910.04942v1,Yes,potent(1)
0000-0001-6843-4305,Shanshan Liu,"Zhejiang University, Zhejiang University City College",Concurrent Classifier Error Detection (CCED) in Large Scale Machine   Learning Systems,1970,"  The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is trained to detect errors. The proposed CCED scheme has been implemented and evaluated on two widely used large-scale ML models: Contrastive Language Image Pretraining (CLIP) used for image classification and Bidirectional Encoder Representations from Transformers (BERT) used for natural language applications. The results show that more than 95 percent of the errors are detected when using a simple Random Forest classifier that is order of magnitude simpler than CLIP or BERT. These results illustrate the potential of CCED to implement error detection in large-scale ML models. ",Kein DOI-Link verfügbar,2306.01820v1,Yes,potent(2)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Enhancing Machine Translation through Advanced In-Context Learning: A   Methodological Strategy for GPT-4 Improvement,1970,"  The challenge of improving translation accuracy in GPT-4 is being addressed by harnessing a method known as in-context learning. This paper introduces a strategic approach to utilize in-context learning specifically for machine translation, aiming to significantly boost accuracy. The crux of this method lies in the judicious selection of demonstrations that are most effective for in-context learning. By selecting these examples carefully, GPT-4 can utilize them to achieve remarkably accurate machine translations, eliminating the need for task-specific fine-tuning. This technique is anchored in the semantic similarities between the user's prompt and the chosen dataset. Sentences from this dataset, carefully picked for their relevance and clarity, serve as potent demonstrations for in-context learning. This approach not only enhances translation accuracy but also enriches the understanding of nuanced linguistic structures. It represents a significant step forward in machine learning, leveraging the inherent capabilities of GPT-4 to provide translations that are not only accurate but also contextually rich and linguistically sophisticated. This method demonstrates the potential of in-context learning in overcoming language barriers, opening new avenues for cross-cultural communication and global collaboration. ",Kein DOI-Link verfügbar,2311.10765v1,Yes,potent(2)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",GCDT: A Global Context Enhanced Deep Transition Architecture for   Sequence Labeling,1970,"  Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking. ",Kein DOI-Link verfügbar,1906.02437v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",A Quality-based Syntactic Template Retriever for   Syntactically-controlled Paraphrase Generation,1970,"  Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to generate qualified paraphrases. To escape this dilemma, we propose a novel Quality-based Syntactic Template Retriever (QSTR) to retrieve templates based on the quality of the to-be-generated paraphrases. Furthermore, for situations requiring multiple paraphrases for each source sentence, we design a Diverse Templates Search (DTS) algorithm, which can enhance the diversity between paraphrases without sacrificing quality. Experiments demonstrate that QSTR can significantly surpass existing retrieval methods in generating high-quality paraphrases and even perform comparably with human-annotated templates in terms of reference-free metrics. Additionally, human evaluation and the performance on downstream tasks using our generated paraphrases for data augmentation showcase the potential of our QSTR and DTS algorithm in practical scenarios. ",Kein DOI-Link verfügbar,2310.13262v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Get the Ball Rolling: Alerting Autonomous Robots When to Help to Close   the Healthcare Loop,1970,"  To facilitate the advancement of research in healthcare robots without human intervention or commands, we introduce the Autonomous Helping Challenge, along with a crowd-sourcing large-scale dataset. The goal is to create healthcare robots that possess the ability to determine when assistance is necessary, generate useful sub-tasks to aid in planning, carry out these plans through a physical robot, and receive feedback from the environment in order to generate new tasks and continue the process. Besides the general challenge in open-ended scenarios, Autonomous Helping focuses on three specific challenges: autonomous task generation, the gap between the current scene and static commonsense, and the gap between language instruction and the real world. Additionally, we propose Helpy, a potential approach to close the healthcare loop in the learning-free setting. ",Kein DOI-Link verfügbar,2311.02602v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",CM-Net: A Novel Collaborative Memory Network for Spoken Language   Understanding,1970,"  Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize co-occurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community. ",Kein DOI-Link verfügbar,1909.06937v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Confidence-Aware Scheduled Sampling for Neural Machine Translation,1970,"  Scheduled sampling is an effective method to alleviate the exposure bias problem of neural machine translation. It simulates the inference scene by randomly replacing ground-truth target input tokens with predicted ones during training. Despite its success, its critical schedule strategies are merely based on training steps, ignoring the real-time model competence, which limits its potential performance and convergence speed. To address this issue, we propose confidence-aware scheduled sampling. Specifically, we quantify real-time model competence by the confidence of model predictions, based on which we design fine-grained schedule strategies. In this way, the model is exactly exposed to predicted tokens for high-confidence positions and still ground-truth tokens for low-confidence positions. Moreover, we observe vanilla scheduled sampling suffers from degenerating into the original teacher forcing mode since most predicted tokens are the same as ground-truth tokens. Therefore, under the above confidence-aware strategy, we further expose more noisy tokens (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. We evaluate our approach on the Transformer and conduct experiments on large-scale WMT 2014 English-German, WMT 2014 English-French, and WMT 2019 Chinese-English. Results show that our approach significantly outperforms the Transformer and vanilla scheduled sampling on both translation quality and convergence speed. ",Kein DOI-Link verfügbar,2107.10427v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",MSCTD: A Multimodal Sentiment Chat Translation Dataset,1970,"  Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations. In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more accurate translations with the help of the associated dialogue history and visual context. To this end, we firstly construct a Multimodal Sentiment Chat Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs in 14,762 bilingual dialogues and 30,370 English-German utterance pairs in 3,079 bilingual dialogues. Each utterance pair, corresponding to the visual context that reflects the current conversational scene, is annotated with a sentiment label. Then, we benchmark the task by establishing multiple baseline systems that incorporate multimodal and sentiment features for MCT. Preliminary experiments on four language directions (English-Chinese and English-German) verify the potential of contextual and multimodal information fusion and the positive impact of sentiment on the MCT task. Additionally, as a by-product of the MSCTD, it also provides two new benchmarks on multimodal dialogue sentiment analysis. Our work can facilitate research on both multimodal chat translation and multimodal dialogue sentiment analysis. ",Kein DOI-Link verfügbar,2202.13645v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Unified Model Learning for Various Neural Machine Translation,1970,"  Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific models with significantly reduced model deployment costs. Furthermore, UMLNMT can achieve competitive or better performance than state-of-the-art dataset-specific methods. Human evaluation and in-depth analysis also demonstrate the superiority of our approach on generating diverse and high-quality translations. Additionally, we provide a new genre translation dataset about famous aphorisms with 186k Chinese->English sentence pairs. ",Kein DOI-Link verfügbar,2305.02777v2,Yes,versatile(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Generating Authentic Adversarial Examples beyond Meaning-preserving with   Doubly Round-trip Translation,1970,"  Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through the RTT can be related to either. To remedy this problem, we propose a new criterion for NMT adversarial examples based on the Doubly Round-Trip Translation (DRTT). Specifically, apart from the source-target-source RTT, we also consider the target-source-target one, which is utilized to pick out the authentic adversarial examples for the target NMT model. Additionally, to enhance the robustness of the NMT model, we introduce the masked language models to construct bilingual adversarial pairs based on DRTT, which are used to train the NMT model directly. Extensive experiments on both the clean and noisy test sets (including the artificial and natural noise) show that our approach substantially improves the robustness of NMT models. ",Kein DOI-Link verfügbar,2204.08689v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",A Tampering Risk of Fiber-Based Frequency Synchronization Networks and   Its Countermeasures,1970,"  Fiber optic networks are used worldwide and have been regarded as excellent media for transmitting time-frequency (TF) signals. In the past decades, fiber-based TF synchronization techniques have been extensively studied. Instruments based on these techniques have been successfully applied. With the increasing application of TF synchronization instruments, their security has become an important issue. Unfortunately, the security risks of fiber-based frequency synchronization (FbFS) instruments have been overlooked. This paper proposes a frequency tampering method called ""frequency lens"". On a 200 km fiber link, we demonstrate a frequency tampering scenario using a frequency lens-enabled frequency tampering module (FTM). On the user side, the frequency value of the recovered 100 MHz signal can be stealthily altered within a range of 100 MHz-100 Hz to 100 MHz+100 Hz, while the frequency dissemination stability of the system remains normal. Related to this tampering risk, potential hazards in three different application scenarios, which rely on precise frequency references, are analyzed. Two countermeasures are also proposed to solve this tampering risk. ",Kein DOI-Link verfügbar,2402.18184v1,Yes,potent(1)
0000-0002-3264-8202,Yufeng Chen,"Zhejiang University, Zhejiang University of Technology",Towards Understanding and Improving Knowledge Distillation for Neural   Machine Translation,1970,"  Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \textbf{E}nhanced \textbf{K}nowledge \textbf{D}istillation (TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the learning of the top-1 information from the teacher. Additionally, we develop an iterative KD procedure to infuse more additional knowledge by distilling on the data without ground-truth targets. Experiments on WMT'14 English-German, WMT'14 English-French and WMT'16 English-Romanian demonstrate that our method can respectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU scores and significantly outperform the vanilla word-level KD baseline. Besides, our method shows higher generalizability on different teacher-student capacity gaps than existing KD techniques. ",Kein DOI-Link verfügbar,2305.08096v2,Yes,potent(2)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Pressure-driven 4$f$ localized-itinerant crossover in heavy fermion   compound CeIn$_{3}$: A first-principles many-body perspective,1970,"  The localized-itinerant nature of Ce-4$f$ valence electrons in heavy fermion compound CeIn$_{3}$ under pressure is studied thoroughly by means of the combination of density functional theory and single-site dynamical mean-field theory. The detailed evolutions of electronic structures of CeIn$_{3}$, including total and partial density of states, momentum-resolved spectral functions, and valence state histograms etc., are calculated in a wide pressure range where the corresponding volume compression $V/V_0 \in [0.6,1.0]$ (here $V_0$ is the experimental crystal volume) at $T \cong 116$ K. Upon increasing pressure, two strong peaks associated with the Ce-$4f$ states emerge near the Fermi level, and the $c$-$f$ hybridization and valence state fluctuation are enhanced remarkably. Moreover, the kinetic and potential energies raise, while the occupancy, total angular momentum, and low-energy scattering rate of the Ce-$4f$ electrons decline with respect to pressure. All the physical observables considered here exhibit prominent kinks or fluctuations in $V/V_0 \in [0.80,0.90]$, which are probably the desired fingerprints for the Ce-4$f$ localized-itinerant crossover. ",https://doi.org/10.1103/PhysRevB.94.075132,1606.03367v2,Yes,potent(1)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Combined Semilocal Exchange Potential with Dynamical Mean-Field Theory,1970,"  The modern semilocal exchange potential is an accurate and efficient approximation to the exact exchange potential of density functional theory. We tried to combine it with the dynamical mean-field theory to derive a new first-principles many-body approach for studying correlated electronic materials. As a paradigm, this approach was employed to investigate the electronic structures and optical properties of strongly correlated ionic insulator YbS. Compared to the standard density functional theory plus dynamical mean-field theory which surprisingly failed to give an insulating solution, the new approach correctly captured all of the important characteristics of YbS. Not only an energy gap between a fully occupied Yb-4$f$ state and an unoccupied conduction band, but also an absence of Drude peak in the optical conductivity $\sigma(\omega)$ were successfully reproduced. ",Kein DOI-Link verfügbar,1609.02482v1,Yes,potent(2)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Stochastic pole expansion method,1970,"  In this paper, we propose a new analytic continuation method to extract real frequency spectral functions from imaginary frequency Green's functions of quantum many-body systems. This method is based on the pole representation of Matsubara Green's function and a stochastic sampling procedure is utilized to optimize the amplitudes and locations of poles. In order to capture narrow peaks and sharp band edges in the spectral functions, a constrained sampling algorithm and a self-adaptive sampling algorithm are developed. To demonstrate the usefulness and performance of the new method, we at first apply it to study the spectral functions of representative fermionic and bosonic correlators. Then we employ this method to tackle the analytic continuation problems of matrix-valued Green's functions. The synthetic Green's functions, as well as realistic correlation functions from finite temperature quantum many-body calculations, are used as input. The benchmark results demonstrate that this method is capable of reproducing most of the key characteristics in the spectral functions. The sharp, smooth, and multi-peak features in both low-frequency and high-frequency regions of spectral functions could be accurately resolved, which overcomes one of the main limitations of the traditional maximum entropy method. More importantly, it exhibits excellent robustness with respect to noisy and incomplete input data. The causality of spectral function is always satisfied even in the presence of sizable noises. As a byproduct, this method could derive a fitting formula for the Matsubara data, which provides a compact approximation to the many-body Green's functions. Hence, we expect that this new method could become a pivotal workhorse for numerically analytic continuation and be broadly useful in many applications. ",https://doi.org/10.1103/PhysRevB.108.235143,2307.11324v1,Yes,pivotal(1)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology","Novel correlated 5f electronic states in cubic AnSn3 (An=U, Np, Pu)   intermetallics",1970,"  The intricate interplay between itinerant-localized 5f states and strongly correlated electronic states have been systematically investigated in isostructural actinide compounds AnSn3 (An=U, Np, Pu) by using a combination of the density functional theory and the embedded dynamical mean-field approach. The obvious narrow flat 5f electronic band with remarkable spectral weight emerges in the vicinity of Fermi level for three compounds. Subsequently the significant hybridization between 5f states and conduction bands opens evident gaps together with conspicuous valence state fluctuations jointly indicating the partially itinerant 5f electrons. Especially, prominent quasiparticle multiplets only appear in PuSn3 due to the sizable valence state fluctuations and multiple competing atomic eigenstates. Therefore itinerant 5f states tend to involve in active chemical bonding, restraining the formation of local magnetic moment of actinide atoms, which partly elucidates the underlying mechanism of paramagnetic USn3 and PuSn3, as well as itinerant-electron antiferromagnetic NpSn3. Correspondingly, the 5f electronic correlation strength expressed in band renormalization and electron effective masse intertwines with itinerant-localized 5f states. Consequently, detail electronic structure of 5f states dependence on actinide series shall gain deep insight into our understanding of AnSn3 (An=U, Np, Pu) intermetallics and promote ongoing research. ",https://doi.org/10.1103/PhysRevB.108.165109,2305.12195v1,Yes,intricate(1)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Quasiparticle multiplets and 5f electronic correlation in prototypical   plutonium borides,1970,"  To elucidate the localized-itinerant dual nature and orbital dependent correlations of Pu-5f valence electrons in plutonium borides (PuBx, x=1, 2, 6, 12), the electronic structures are throughout investigated by using the combination of density functional theory and single-site dynamical mean-field method. We not only reproduce the correlated topological insulator of PuB6, but also predict the metallicity in PuBx (x=1, 2, 12). It is found that the momentum-resolved spectral functions, density of states, hybridization functions all indicate partially itinerant 5f states in PuBx (x=1, 2, 6, 12). Especially, quasiparticle multiplets induced noteworthy valence state fluctuations implying the mixed-valence behavior of plutonium borides. Moreover, the itinerant degree of freedom for 5f electrons in PuBx (x=1, 2, 12) is tuned by hybridization strength between 5f states and conduction bands, which is affected by atomic distance between Pu and B atoms. Lastly, 5f electronic correlations encoded in the electron self-energy functions demonstrate moderate 5f electronic correlations in PuB6 and orbital selective 5f electronic correlations in PuBx (x=1, 2, 12). Consequently, the understanding of electronic structure and related crystal structure stability shall shed light on exploring novel 5f electrons states and ongoing experiment research. ",https://doi.org/10.1088/1361-648X/ac5b01,2201.05155v2,Yes,noteworthy(1)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Natural Orbital-Based Lanczos Method for Anderson Impurity Models,1970,"  We implement the Lanczos algorithm on natural orbital basis to solve the zero-temperature Green's function of Anderson impurity models, following the work of Y. Lu, M. H\""{o}ppner, O. Gunnarsson, and M. W. Haverkort, Phys. Rev. B {\bf 90} (2014) 085102. We present the technical details, generalize the algorithm to the cases of particle-hole asymmetry, with local magnetic field, and of two impurities. The results are benchmarked with conventional Lanczos, quantum Monte Carlo, and numerical renormalization group methods, demonstrating its potential as a powerful impurity solver for the dynamical mean-field theory. ",https://doi.org/10.1016/j.cpc.2018.09.002,1802.02340v2,Yes,potent(1)
0009-0006-1921-1686,Li Huang,"Zhejiang University, Zhejiang University of Technology",Transition metals doped CuAlSe2 for promising intermediate band   materials,1970,"  Introducing an isolated intermediate band (IB) into a wide band gap semiconductor can potentially improve the optical absorption of the material beyond the Shockley-Queisser limitation for solar cells. Here, we present a systematic study of the thermodynamic stability, electronic structures, and optical properties of transition metals (M = Ti, V, and Fe) doped CuAlSe2 for potential IB thin film solar cells, by adopting the first-principles calculation based on the hybrid functional method. We found from chemical potential analysis that for all dopants considered, the stable doped phase only exits when the Al atom is substituted. More importantly, with this substitution, the IB feature is determined by $3d$ electronic nature of M^{3+} ion, and the electronic configuration of 3d^1 can drive a optimum IB that possesses half-filled character and suitable subbandgap from valence band or conduction band. We further show that Ti-doped CuAlSe2 is the more promising candidate for IB materials since the resulted IB in it is half filled and extra absorption peaks occurs in the optical spectrum accompanied with a largely enhanced light absorption intensity. The result offers a understanding for IB induced by transition metals into CuAlSe2 and is significant to fabricate the related IB materials. ",https://doi.org/10.1088/2053-1591/3/4/045905,1512.00972v1,Yes,potent(3)
0000-0003-2523-5810,Jie Lei,"Zhejiang University, Zhejiang University of Technology",Guided Hybrid Quantization for Object detection in Multimodal Remote   Sensing Imagery via One-to-one Self-teaching,1970,"  Considering the computation complexity, we propose a Guided Hybrid Quantization with One-to-one Self-Teaching (GHOST}) framework. More concretely, we first design a structure called guided quantization self-distillation (GQSD), which is an innovative idea for realizing lightweight through the synergy of quantization and distillation. The training process of the quantization model is guided by its full-precision model, which is time-saving and cost-saving without preparing a huge pre-trained model in advance. Second, we put forward a hybrid quantization (HQ) module to obtain the optimal bit width automatically under a constrained condition where a threshold for distribution distance between the center and samples is applied in the weight value search space. Third, in order to improve information transformation, we propose a one-to-one self-teaching (OST) module to give the student network a ability of self-judgment. A switch control machine (SCM) builds a bridge between the student network and teacher network in the same location to help the teacher to reduce wrong guidance and impart vital knowledge to the student. This distillation method allows a model to learn from itself and gain substantial improvement without any additional supervision. Extensive experiments on a multimodal dataset (VEDAI) and single-modality datasets (DOTA, NWPU, and DIOR) show that object detection based on GHOST outperforms the existing detectors. The tiny parameters (<9.7 MB) and Bit-Operations (BOPs) (<2158 G) compared with any remote sensing-based, lightweight or distillation-based algorithms demonstrate the superiority in the lightweight design domain. Our code and model will be released at https://github.com/icey-zhang/GHOST. ",https://doi.org/10.1109/TGRS.2023.3293147,2301.00131v1,Yes,innovative(1)
0000-0003-2523-5810,Jie Lei,"Zhejiang University, Zhejiang University of Technology",AFPN: Asymptotic Feature Pyramid Network for Object Detection,1970,"  Multi-scale features are of great importance in encoding objects with scale variance in object detection tasks. A common strategy for multi-scale feature extraction is adopting the classic top-down and bottom-up feature pyramid networks. However, these approaches suffer from the loss or degradation of feature information, impairing the fusion effect of non-adjacent levels. This paper proposes an asymptotic feature pyramid network (AFPN) to support direct interaction at non-adjacent levels. AFPN is initiated by fusing two adjacent low-level features and asymptotically incorporates higher-level features into the fusion process. In this way, the larger semantic gap between non-adjacent levels can be avoided. Given the potential for multi-object information conflicts to arise during feature fusion at each spatial location, adaptive spatial fusion operation is further utilized to mitigate these inconsistencies. We incorporate the proposed AFPN into both two-stage and one-stage object detection frameworks and evaluate with the MS-COCO 2017 validation and test datasets. Experimental evaluation shows that our method achieves more competitive results than other state-of-the-art feature pyramid networks. The code is available at \href{https://github.com/gyyang23/AFPN}{https://github.com/gyyang23/AFPN}. ",Kein DOI-Link verfügbar,2306.15988v2,Yes,potent(1)
0000-0003-2523-5810,Jie Lei,"Zhejiang University, Zhejiang University of Technology",Multimodal Informative ViT: Information Aggregation and Distribution for   Hyperspectral and LiDAR Classification,1970,"  In multimodal land cover classification (MLCC), a common challenge is the redundancy in data distribution, where irrelevant information from multiple modalities can hinder the effective integration of their unique features. To tackle this, we introduce the Multimodal Informative Vit (MIVit), a system with an innovative information aggregate-distributing mechanism. This approach redefines redundancy levels and integrates performance-aware elements into the fused representation, facilitating the learning of semantics in both forward and backward directions. MIVit stands out by significantly reducing redundancy in the empirical distribution of each modality's separate and fused features. It employs oriented attention fusion (OAF) for extracting shallow local features across modalities in horizontal and vertical dimensions, and a Transformer feature extractor for extracting deep global features through long-range attention. We also propose an information aggregation constraint (IAC) based on mutual information, designed to remove redundant information and preserve complementary information within embedded features. Additionally, the information distribution flow (IDF) in MIVit enhances performance-awareness by distributing global classification information across different modalities' feature maps. This architecture also addresses missing modality challenges with lightweight independent modality classifiers, reducing the computational load typically associated with Transformers. Our results show that MIVit's bidirectional aggregate-distributing mechanism between modalities is highly effective, achieving an average overall accuracy of 95.56% across three multimodal datasets. This performance surpasses current state-of-the-art methods in MLCC. The code for MIVit is accessible at https://github.com/icey-zhang/MIViT. ",Kein DOI-Link verfügbar,2401.03179v2,Yes,innovative(1)
0000-0003-2523-5810,Jie Lei,"Zhejiang University, Zhejiang University of Technology",Distribution-aware Interactive Attention Network and Large-scale Cloud   Recognition Benchmark on FY-4A Satellite Image,1970,"  Accurate cloud recognition and warning are crucial for various applications, including in-flight support, weather forecasting, and climate research. However, recent deep learning algorithms have predominantly focused on detecting cloud regions in satellite imagery, with insufficient attention to the specificity required for accurate cloud recognition. This limitation inspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes nine distinct cloud categories and uses precise domain adaptation methods to align 70,419 image-label pairs in terms of projection, temporal resolution, and spatial resolution, thereby facilitating the training of supervised deep learning networks. Given the complexity and diversity of cloud formations, we have thoroughly analyzed the challenges inherent to cloud recognition tasks, examining the intricate characteristics and distribution of the data. To effectively address these challenges, we designed a Distribution-aware Interactive-Attention Network (DIAnet), which preserves pixel-level details through a high-resolution branch and a parallel multi-resolution cross-branch. We also integrated a distribution-aware loss (DAL) to mitigate the imbalance across cloud categories. An Interactive Attention Module (IAM) further enhances the robustness of feature extraction combined with spatial and channel information. Empirical evaluations on the FYH dataset demonstrate that our method outperforms other cloud recognition networks, achieving superior performance in terms of mean Intersection over Union (mIoU). The code for implementing DIAnet is available at https://github.com/icey-zhang/DIAnet. ",Kein DOI-Link verfügbar,2401.03182v1,Yes,intricate(1)
0000-0003-2523-5810,Jie Lei,"Zhejiang University, Zhejiang University of Technology",Low-Power Audio Keyword Spotting using Tsetlin Machines,1970,"  The emergence of Artificial Intelligence (AI) driven Keyword Spotting (KWS) technologies has revolutionized human to machine interaction. Yet, the challenge of end-to-end energy efficiency, memory footprint and system complexity of current Neural Network (NN) powered AI-KWS pipelines has remained ever present. This paper evaluates KWS utilizing a learning automata powered machine learning algorithm called the Tsetlin Machine (TM). Through significant reduction in parameter requirements and choosing logic over arithmetic based processing, the TM offers new opportunities for low-power KWS while maintaining high learning efficacy. In this paper we explore a TM based keyword spotting (KWS) pipeline to demonstrate low complexity with faster rate of convergence compared to NNs. Further, we investigate the scalability with increasing keywords and explore the potential for enabling low-power on-chip KWS. ",Kein DOI-Link verfügbar,2101.11336v1,Yes,potent(1)
0000-0002-5731-062X,Rongrong Zhu,"Zhejiang University, Zhejiang University City College",Low-intensity pulsed ultrasound promotes mesenchymal stem cell   transplantation-based articular cartilage regeneration via inhibiting the TNF   signaling pathway,1970,"  Background: Mesenchymal stem cell (MSC) transplantation therapy is highly investigated for the regenerative repair of cartilage defects. Low-intensity pulsed ultrasound (LIPUS) has the potential to promote chondrogenic differentiation of MSCs. However, its underlying mechanism remains unclear. Here, we investigated the promoting effects and mechanisms underlying LIPUS stimulation on the chondrogenic differentiation of human umbilical cord mesenchymal stem cells (hUC-MSCs) and further evaluated its regenerative application value in articular cartilage defects in rats.   Methods: LIPUS was applied to stimulate cultured hUC-MSCs and C28/I2 cells in vitro. Immunofluorescence staining, qPCR analysis, and transcriptome sequencing were used to detect mature cartilage-related markers of gene and protein expression for a comprehensive evaluation of differentiation. Injured articular cartilage rat models were established for further hUC-MSC transplantation and LIPUS stimulation in vivo. Histopathology and H&E staining were used to evaluate the repair effects of the injured articular cartilage with LIPUS stimulation.   Results: The results showed that LIPUS stimulation with specific parameters effectively promoted the expression of mature cartilage-related genes and proteins, inhibited TNF-{\alpha} gene expression in hUC-MSCs, and exhibited anti-inflammation in C28/I2 cells. In addition, the articular cartilage defects of rats were significantly repaired after hUC-MSC transplantation and LIPUS stimulation.   Conclusions: Taken together, LIPUS stimulation could realize articular cartilage regeneration based on hUC-MSC transplantation due to the inhibition of the TNF signaling pathway, which is of clinical value for the relief of osteoarthritis. ",https://doi.org/10.1186/s13287-023-03296-6,2112.13774v2,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Ten computational challenges in human virome studies,1970,"  In recent years, substantial advancements have been achieved in understanding the diversity of the human virome and its intricate roles in human health and diseases. Despite this progress, the field of human virome research remains nascent, primarily hindered by the absence of effective methods, particularly in the domain of computational tools. This perspective systematically outlines ten computational challenges spanning various phases of virome studies, ranging from virus identification, sequencing quality evaluation, genome assembly, annotation of viral taxonomy, genome and proteins, inference of biological properties, applications of the virome in disease diagnosis, interactions with other microbes, and associations with human diseases. The resolution of these challenges necessitates ongoing collaboration among computational scientists, virologists, and multidisciplinary experts. In essence, this perspective serves as a comprehensive guide for directing computational efforts in human virome studies, aiming to significantly propel the field forward. ",Kein DOI-Link verfügbar,2402.15186v1,Yes,intricate(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",The Laplacian in RL: Learning Representations with Efficient   Approximations,1970,"  The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent. ",Kein DOI-Link verfügbar,1810.04586v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Generating and Reweighting Dense Contrastive Patterns for Unsupervised   Anomaly Detection,1970,"  Recent unsupervised anomaly detection methods often rely on feature extractors pretrained with auxiliary datasets or on well-crafted anomaly-simulated samples. However, this might limit their adaptability to an increasing set of anomaly detection tasks due to the priors in the selection of auxiliary datasets or the strategy of anomaly simulation. To tackle this challenge, we first introduce a prior-less anomaly generation paradigm and subsequently develop an innovative unsupervised anomaly detection framework named GRAD, grounded in this paradigm. GRAD comprises three essential components: (1) a diffusion model (PatchDiff) to generate contrastive patterns by preserving the local structures while disregarding the global structures present in normal images, (2) a self-supervised reweighting mechanism to handle the challenge of long-tailed and unlabeled contrastive patterns generated by PatchDiff, and (3) a lightweight patch-level detector to efficiently distinguish the normal patterns and reweighted contrastive patterns. The generation results of PatchDiff effectively expose various types of anomaly patterns, e.g. structural and logical anomaly patterns. In addition, extensive experiments on both MVTec AD and MVTec LOCO datasets also support the aforementioned observation and demonstrate that GRAD achieves competitive anomaly detection accuracy and superior inference speed. ",Kein DOI-Link verfügbar,2312.15911v1,Yes,innovative(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Log Parsing with Self-Generated In-Context Learning and Self-Correction,1970,"  Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin. ",Kein DOI-Link verfügbar,2406.03376v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Enhanced generative adversarial network for 3D brain MRI   super-resolution,1970,"  Single image super-resolution (SISR) reconstruction for magnetic resonance imaging (MRI) has generated significant interest because of its potential to not only speed up imaging but to improve quantitative processing and analysis of available image data. Generative Adversarial Networks (GAN) have proven to perform well in recovering image texture detail, and many variants have therefore been proposed for SISR. In this work, we develop an enhancement to tackle GAN-based 3D SISR by introducing a new residual-in-residual dense block (RRDG) generator that is both memory efficient and achieves state-of-the-art performance in terms of PSNR (Peak Signal to Noise Ratio), SSIM (Structural Similarity) and NRMSE (Normalized Root Mean Squared Error) metrics. We also introduce a patch GAN discriminator with improved convergence behavior to better model brain image texture. We proposed a novel the anatomical fidelity evaluation of the results using a pre-trained brain parcellation network. Finally, these developments are combined through a simple and efficient method to balance etween image and texture quality in the final output. ",Kein DOI-Link verfügbar,1907.04835v2,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Self-guided Few-shot Semantic Segmentation for Remote Sensing Imagery   Based on Large Vision Models,1970,"  The Segment Anything Model (SAM) exhibits remarkable versatility and zero-shot learning abilities, owing largely to its extensive training data (SA-1B). Recognizing SAM's dependency on manual guidance given its category-agnostic nature, we identified unexplored potential within few-shot semantic segmentation tasks for remote sensing imagery. This research introduces a structured framework designed for the automation of few-shot semantic segmentation. It utilizes the SAM model and facilitates a more efficient generation of semantically discernible segmentation outcomes. Central to our methodology is a novel automatic prompt learning approach, leveraging prior guided masks to produce coarse pixel-wise prompts for SAM. Extensive experiments on the DLRSD datasets underline the superiority of our approach, outperforming other available few-shot methodologies. ",Kein DOI-Link verfügbar,2311.13200v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Jet Tagging with More-Interaction Particle Transformer,1970,"  In this study, we introduce the More-Interaction Particle Transformer (MIParT), a novel deep learning neural network designed for jet tagging. This framework incorporates our own design, the More-Interaction Attention (MIA) mechanism, which increases the dimensionality of particle interaction embeddings. We tested MIParT using the top tagging and quark-gluon datasets. Our results show that MIParT not only matches the accuracy and AUC of LorentzNet and a series of Lorentz-equivariant methods, but also significantly outperforms the ParT model in background rejection. Specifically, it improves background rejection by approximately 25% at a 30% signal efficiency on the top tagging dataset and by 3% on the quark-gluon dataset. Additionally, MIParT requires only 30% of the parameters and 53% of the computational complexity needed by ParT, proving that high performance can be achieved with reduced model complexity. For very large datasets, we double the dimension of particle embeddings, referring to this variant as MIParT-Large (MIParT-L). We find that MIParT-L can further capitalize on the knowledge from large datasets. From a model pre-trained on the 100M JetClass dataset, the background rejection performance of the fine-tuned MIParT-L improved by 39% on the top tagging dataset and by 6% on the quark-gluon dataset, surpassing that of the fine-tuned ParT. Specifically, the background rejection of fine-tuned MIParT-L improved by an additional 2% compared to the fine-tuned ParT. The results suggest that MIParT has the potential to advance efficiency benchmarks for jet tagging and event identification in particle physics. ",Kein DOI-Link verfügbar,2407.08682v2,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Evaluating Task-based Effectiveness of MLLMs on Charts,1970,"  In this paper, we explore a forward-thinking question: Is GPT-4V effective at low-level data analysis tasks on charts? To this end, we first curate a large-scale dataset, named ChartInsights, consisting of 89,388 quartets (chart, task, question, answer) and covering 10 widely-used low-level data analysis tasks on 7 chart types. Firstly, we conduct systematic evaluations to understand the capabilities and limitations of 18 advanced MLLMs, which include 12 open-source models and 6 closed-source models. Starting with a standard textual prompt approach, the average accuracy rate across the 18 MLLMs is 36.17%. Among all the models, GPT-4V achieves the highest accuracy, reaching 56.13%. To understand the limitations of multimodal large models in low-level data analysis tasks, we have designed various experiments to conduct an in-depth test of capabilities of GPT-4V. We further investigate how visual modifications to charts, such as altering visual elements (e.g. changing color schemes) and introducing perturbations (e.g. adding image noise), affect performance of GPT-4V. Secondly, we present 12 experimental findings. These findings suggest potential of GPT-4V to revolutionize interaction with charts and uncover the gap between human analytic needs and capabilities of GPT-4V. Thirdly, we propose a novel textual prompt strategy, named Chain-of-Charts, tailored for low-level analysis tasks, which boosts model performance by 24.36%, resulting in an accuracy of 80.49%. Furthermore, by incorporating a visual prompt strategy that directs attention of GPT-4V to question-relevant visual elements, we further improve accuracy to 83.83%. Our study not only sheds light on the capabilities and limitations of GPT-4V in low-level data analysis tasks but also offers valuable insights for future research. ",Kein DOI-Link verfügbar,2405.07001v2,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein   Interactions,1970,"  Motivation: Exploring drug-protein interactions (DPIs) work as a pivotal step in drug discovery. The fast expansion of available biological data enables computational methods effectively assist in experimental methods. Among them, deep learning methods extract features only from basic characteristics, such as protein sequences, molecule structures. Others achieve significant improvement by learning from not only sequences/molecules but the protein-protein and drug-drug associations (PPAs and DDAs). The PPAs and DDAs are generally obtained by using computational methods. However, existing computational methods have some limitations, resulting in low-quality PPAs and DDAs that hamper the prediction performance. Therefore, we hope to develop a novel supervised learning method to learn the PPAs and DDAs effectively and thereby improve the prediction performance of the specific task of DPI. Results: In this research, we propose a novel deep learning framework, namely BridgeDPI. BridgeDPI introduces a class of nodes named hyper-nodes, which bridge different proteins/drugs to work as PPAs and DDAs. The hyper-nodes can be supervised learned for the specific task of DPI since the whole process is an end-to-end learning. Consequently, such a model would improve prediction performance of DPI. In three real-world datasets, we further demonstrate that BridgeDPI outperforms state-of-the-art methods. Moreover, ablation studies verify the effectiveness of the hyper-nodes. Last, in an independent verification, BridgeDPI explores the candidate bindings among COVID-19's proteins and various antiviral drugs. And the predictive results accord with the statement of the World Health Organization and Food and Drug Administration, showing the validity and reliability of BridgeDPI. ",Kein DOI-Link verfügbar,2101.12547v1,Yes,pivotal(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Learning to Combat Compounding-Error in Model-Based Reinforcement   Learning,1970,"  Despite its potential to improve sample complexity versus model-free approaches, model-based reinforcement learning can fail catastrophically if the model is inaccurate. An algorithm should ideally be able to trust an imperfect model over a reasonably long planning horizon, and only rely on model-free updates when the model errors get infeasibly large. In this paper, we investigate techniques for choosing the planning horizon on a state-dependent basis, where a state's planning horizon is determined by the maximum cumulative model error around that state. We demonstrate that these state-dependent model errors can be learned with Temporal Difference methods, based on a novel approach of temporally decomposing the cumulative model errors. Experimental results show that the proposed method can successfully adapt the planning horizon to account for state-dependent model accuracy, significantly improving the efficiency of policy learning compared to model-based and model-free baselines. ",Kein DOI-Link verfügbar,1912.11206v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Unsupervised Learning of Multi-level Structures for Anomaly Detection,1970,"  The main difficulty in high-dimensional anomaly detection tasks is the lack of anomalous data for training. And simply collecting anomalous data from the real world, common distributions, or the boundary of normal data manifold may face the problem of missing anomaly modes. This paper first introduces a novel method to generate anomalous data by breaking up global structures while preserving local structures of normal data at multiple levels. It can efficiently expose local abnormal structures of various levels. To fully exploit the exposed multi-level abnormal structures, we propose to train multiple level-specific patch-based detectors with contrastive losses. Each detector learns to detect local abnormal structures of corresponding level at all locations and outputs patchwise anomaly scores. By aggregating the outputs of all level-specific detectors, we obtain a model that can detect all potential anomalies. The effectiveness is evaluated on MNIST, CIFAR10, and ImageNet10 dataset, where the results surpass the accuracy of state-of-the-art methods. Qualitative experiments demonstrate our model is robust that it unbiasedly detects all anomaly modes. ",Kein DOI-Link verfügbar,2104.12102v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",COVIDx CXR-4: An Expanded Multi-Institutional Open-Source Benchmark   Dataset for Chest X-ray Image-Based Computer-Aided COVID-19 Diagnostics,1970,"  The global ramifications of the COVID-19 pandemic remain significant, exerting persistent pressure on nations even three years after its initial outbreak. Deep learning models have shown promise in improving COVID-19 diagnostics but require diverse and larger-scale datasets to improve performance. In this paper, we introduce COVIDx CXR-4, an expanded multi-institutional open-source benchmark dataset for chest X-ray image-based computer-aided COVID-19 diagnostics. COVIDx CXR-4 expands significantly on the previous COVIDx CXR-3 dataset by increasing the total patient cohort size by greater than 2.66 times, resulting in 84,818 images from 45,342 patients across multiple institutions. We provide extensive analysis on the diversity of the patient demographic, imaging metadata, and disease distributions to highlight potential dataset biases. To the best of the authors' knowledge, COVIDx CXR-4 is the largest and most diverse open-source COVID-19 CXR dataset and is made publicly available as part of an open initiative to advance research to aid clinicians against the COVID-19 disease. ",Kein DOI-Link verfügbar,2311.17677v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",CoCA: Fusing Position Embedding with Collinear Constrained Attention in   Transformers for Long Context Window Extending,1970,"  Self-attention and position embedding are two key modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work. To address this issue, we propose a novel attention mechanism, CoCA (Collinear Constrained Attention). Specifically, we enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE and self-attention. While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models. Extensive experiments show that CoCA performs extraordinarily well in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can seamlessly extend the context window up to 32K (60$\times$), without any fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve extrapolation up to 32K within only 2K training length. Our code is publicly available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention ",Kein DOI-Link verfügbar,2309.08646v3,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Measure-Observe-Remeasure: An Interactive Paradigm for   Differentially-Private Exploratory Analysis,1970,"  Differential privacy (DP) has the potential to enable privacy-preserving analysis on sensitive data, but requires analysts to judiciously spend a limited ``privacy loss budget'' $\epsilon$ across queries. Analysts conducting exploratory analyses do not, however, know all queries in advance and seldom have DP expertise. Thus, they are limited in their ability to specify $\epsilon$ allotments across queries prior to an analysis. To support analysts in spending $\epsilon$ efficiently, we propose a new interactive analysis paradigm, Measure-Observe-Remeasure, where analysts ``measure'' the database with a limited amount of $\epsilon$, observe estimates and their errors, and remeasure with more $\epsilon$ as needed.   We instantiate the paradigm in an interactive visualization interface which allows analysts to spend increasing amounts of $\epsilon$ under a total budget. To observe how analysts interact with the Measure-Observe-Remeasure paradigm via the interface, we conduct a user study that compares the utility of $\epsilon$ allocations and findings from sensitive data participants make to the allocations and findings expected of a rational agent who faces the same decision task. We find that participants are able to use the workflow relatively successfully, including using budget allocation strategies that maximize over half of the available utility stemming from $\epsilon$ allocation. Their loss in performance relative to a rational agent appears to be driven more by their inability to access information and report it than to allocate $\epsilon$. ",https://doi.org/10.1109/SP54263.2024.00182,2406.01964v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",NutritionVerse-Real: An Open Access Manually Collected 2D Food Scene   Dataset for Dietary Intake Estimation,1970,"  Dietary intake estimation plays a crucial role in understanding the nutritional habits of individuals and populations, aiding in the prevention and management of diet-related health issues. Accurate estimation requires comprehensive datasets of food scenes, including images, segmentation masks, and accompanying dietary intake metadata. In this paper, we introduce NutritionVerse-Real, an open access manually collected 2D food scene dataset for dietary intake estimation with 889 images of 251 distinct dishes and 45 unique food types. The NutritionVerse-Real dataset was created by manually collecting images of food scenes in real life, measuring the weight of every ingredient and computing the associated dietary content of each dish using the ingredient weights and nutritional information from the food packaging or the Canada Nutrient File. Segmentation masks were then generated through human labelling of the images. We provide further analysis on the data diversity to highlight potential biases when using this data to develop models for dietary intake estimation. NutritionVerse-Real is publicly available at https://www.kaggle.com/datasets/nutritionverse/nutritionverse-real as part of an open initiative to accelerate machine learning for dietary sensing. ",Kein DOI-Link verfügbar,2401.08598v1,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",Generating Synthetic X-ray Images of a Person from the Surface Geometry,1970,"  We present a novel framework that learns to predict human anatomy from body surface. Specifically, our approach generates a synthetic X-ray image of a person only from the person's surface geometry. Furthermore, the synthetic X-ray image is parametrized and can be manipulated by adjusting a set of body markers which are also generated during the X-ray image prediction. With the proposed framework, multiple synthetic X-ray images can easily be generated by varying surface geometry. By perturbing the parameters, several additional synthetic X-ray images can be generated from the same surface geometry. As a result, our approach offers a potential to overcome the training data barrier in the medical domain. This capability is achieved by learning a pair of networks - one learns to generate the full image from the partial image and a set of parameters, and the other learns to estimate the parameters given the full image. During training, the two networks are trained iteratively such that they would converge to a solution where the predicted parameters and the full image are consistent with each other. In addition to medical data enrichment, our framework can also be used for image completion as well as anomaly detection. ",Kein DOI-Link verfügbar,1805.00553v2,Yes,potent(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",A Survey of AIOps for Failure Management in the Era of Large Language   Models,1970,"  As software systems grow increasingly intricate, Artificial Intelligence for IT Operations (AIOps) methods have been widely used in software system failure management to ensure the high availability and reliability of large-scale distributed software systems. However, these methods still face several challenges, such as lack of cross-platform generality and cross-task flexibility. Fortunately, recent advancements in large language models (LLMs) can significantly address these challenges, and many approaches have already been proposed to explore this field. However, there is currently no comprehensive survey that discusses the differences between LLM-based AIOps and traditional AIOps methods. Therefore, this paper presents a comprehensive survey of AIOps technology for failure management in the LLM era. It includes a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps. Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain, aiming to further its development and application. ",Kein DOI-Link verfügbar,2406.11213v4,Yes,intricate(1)
0000-0002-3238-7495,Yifan Wu,"Zhejiang University, Zhejiang University of Technology",A Concept-based Interpretable Model for the Diagnosis of Choroid   Neoplasias using Multimodal Data,1970,"  Diagnosing rare diseases presents a common challenge in clinical practice, necessitating the expertise of specialists for accurate identification. The advent of machine learning offers a promising solution, while the development of such technologies is hindered by the scarcity of data on rare conditions and the demand for models that are both interpretable and trustworthy in a clinical context. Interpretable AI, with its capacity for human-readable outputs, can facilitate validation by clinicians and contribute to medical education. In the current work, we focus on choroid neoplasias, the most prevalent form of eye cancer in adults, albeit rare with 5.1 per million. We built the so-far largest dataset consisting of 750 patients, incorporating three distinct imaging modalities collected from 2004 to 2022. Our work introduces a concept-based interpretable model that distinguishes between three types of choroidal tumors, integrating insights from domain experts via radiological reports. Remarkably, this model not only achieves an F1 score of 0.91, rivaling that of black-box models, but also boosts the diagnostic accuracy of junior doctors by 42%. This study highlights the significant potential of interpretable machine learning in improving the diagnosis of rare diseases, laying a groundwork for future breakthroughs in medical AI that could tackle a wider array of complex health scenarios. ",Kein DOI-Link verfügbar,2403.05606v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Generalized Low-Rank Optimization for Topological Cooperation in   Ultra-Dense Networks,1970,"  Network densification is a natural way to support dense mobile applications under stringent requirements, such as ultra-low latency, ultra-high data rate, and massive connecting devices. Severe interference in ultra-dense networks poses a key bottleneck. Sharing channel state information (CSI) and messages across transmitters can potentially alleviate interferences and improve system performance. Most existing works on interference coordination require significant CSI signaling overhead and are impractical in ultra-dense networks. This paper investigate topological cooperation to manage interferences in message sharing based only on network connectivity information. In particular, we propose a generalized low-rank optimization approach to maximize achievable degrees-of-freedom (DoFs). To tackle the challenges of poor structure and non-convex rank function, we develop Riemannian optimization algorithms to solve a sequence of complex fixed rank subproblems through a rank growth strategy. By exploiting the non-compact Stiefel manifold formed by the set of complex full column rank matrices, we develop Riemannian optimization algorithms to solve the complex fixed-rank optimization problem by applying the semidefinite lifting technique and Burer-Monteiro factorization approach. Numerical results demonstrate the computational efficiency and higher DoFs achieved by the proposed algorithms. ",Kein DOI-Link verfügbar,1809.08537v2,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Dynamics of threshold solutions for energy critical NLS with inverse   square potential,1970,"  We consider the focusing energy critical NLS with inverse square potential in dimension $d= 3, 4, 5$ with the details given in $d=3$ and remarks on results in other dimensions. Solutions on the energy surface of the ground state are characterized. We prove that solutions with kinetic energy less than that of the ground state must scatter to zero or belong to the stable/unstable manifolds of the ground state. In the latter case they converge to the ground state exponentially in the energy space as $t\to \infty$ or $t\to -\infty$. (In 3-dim without radial assumption, this holds under the compactness assumption of non-scattering solutions on the energy surface.) When the kinetic energy is greater than that of the ground state, we show that all radial $H^1$ solutions blow up in finite time, with the only two exceptions in the case of 5-dim which belong to the stable/unstable manifold of the ground state. The proof relies on the detailed spectral analysis, local invariant manifold theory, and a global Virial analysis. ",Kein DOI-Link verfügbar,2006.04321v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Predicting patient risk of readmission with frailty models in the   Department of Veteran Affairs,1970,"  Reducing potentially preventable readmissions has been identified as an important issue for decreasing Medicare costs and improving quality of care provided by hospitals. Based on previous research by medical professionals, preventable readmissions are caused by such factors as flawed patient discharging process, inadequate follow-ups after discharging, and noncompliance of patients on discharging and follow up instructions. It is also found that the risk of preventable readmission also may relate to some patient's characteristics, such as age, health condition, diagnosis, and even treatment specialty. In this study, using both general demographic information and individual past history of readmission records, we develop a risk prediction model based on hierarchical nonlinear mixed effect framework to extract significant prognostic factors associated with patient risk of 30-day readmission. The effectiveness of our proposed approach is validated based on a real dataset from four VA facilities in the State of Michigan. Simultaneously explaining both patient and population based variations of readmission process, such an accurate model can be used to recognize patients with high likelihood of discharging non-compliances, and then targeted post-care actions can be designed to reduce further rehospitalization. ",Kein DOI-Link verfügbar,1403.1210v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Robust Beamforming for Downlink Multi-Cell Systems: A Bilevel   Optimization Perspective,1970,"  Utilization of inter-base station cooperation for information processing has shown great potential in enhancing the overall quality of communication services (QoS) in wireless communication networks. Nevertheless, such cooperations require the knowledge of channel state information (CSI) at base stations (BSs), which is assumed to be perfectly known. However, CSI errors are inevitable in practice which necessitates beamforming techniques that can achieve robust performance in the presence of channel estimation errors. Existing approaches relax the robust beamforming design problems into semidefinite programming (SDP), which can only achieve a solution that is far from being optimal. To this end, this paper views robust beamforming design problems from a bilevel optimization perspective. In particular, we focus on maximizing the worst-case weighted sum-rate (WSR) in the downlink multi-cell multi-user multiple-input single-output (MISO) system considering bounded CSI errors. We first reformulate this problem into a bilevel optimization problem and then develop an efficient algorithm based on the cutting plane method. A distributed optimization algorithm has also been developed to facilitate the parallel processing in practical settings. Numerical results are provided to confirm the effectiveness of the proposed algorithm in terms of performance and complexity, particularly in the presence of CSI uncertainties. ",Kein DOI-Link verfügbar,2401.11409v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",A predictive analytics approach to reducing avoidable hospital   readmission,1970,"  Hospital readmission has become a critical metric of quality and cost of healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20% of patients who are readmitted within 30 days of discharge. Although several interventions such as transition care management and discharge reengineering have been practiced in recent years, the effectiveness and sustainability depends on how well they can identify and target patients at high risk of rehospitalization. Based on the literature, most current risk prediction models fail to reach an acceptable accuracy level; none of them considers patient's history of readmission and impacts of patient attribute changes over time; and they often do not discriminate between planned and unnecessary readmissions. Tackling such drawbacks, we develop a new readmission metric based on administrative data that can identify potentially avoidable readmissions from all other types of readmission. We further propose a tree based classification method to estimate the predicted probability of readmission that can directly incorporate patient's history of readmission and risk factors changes over time. The proposed methods are validated with 2011-12 Veterans Health Administration data from inpatients hospitalized for heart failure, acute myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in the State of Michigan. Results shows improved discrimination power compared to the literature (c-statistics>80%) and good calibration. ",Kein DOI-Link verfügbar,1402.5991v2,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Cross sections for inelastic meson-meson scattering,1970,"  We study two kinds of inelastic meson-meson scattering. The first kind is inelastic 2-to-2 meson-meson scattering that is governed by quark interchange as well as quark-antiquark annihilation and creation. Cross-section formulas are provided to get unpolarized cross sections for $\pi K \to \rho K^\ast$ for $I=1/2$, $\pi K^\ast \to \rho K$ for $I=1/2$, $\pi K^\ast \to \rho K^\ast$ for $I=1/2$, and $\rho K \to \rho K^\ast$ for $I=1/2$. Near threshold, quark interchange dominates the reactions near the critical temperature. The second kind is 2-to-1 meson-meson scattering with the process that a quark in an initial meson and an antiquark in another initial meson annihilate into a gluon and subsequently the gluon is absorbed by the other antiquark or quark. The transition potential for the process is derived. Four Feynman diagrams at tree level contribute to the 2-to-1 meson-meson scattering. Starting from the $S$-matrix element, the isospin-averaged unpolarized cross section with transition amplitudes is derived. The cross sections for $\pi \pi \to \rho$ and $\pi K \to K^*$ decrease with increasing temperature. ",https://doi.org/10.1103/PhysRevD.96.114025,1708.03062v2,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",AI Empowered Net-RCA for 6G,1970,"  6G is envisioned to offer higher data rate, improved reliability, ubiquitous AI services, and support massive scale of connected devices. As a consequence, 6G will be much more complex than its predecessors. The growth of the system scale and complexity as well as the coexistence with the legacy networks and the diversified service requirements will inevitably incur huge maintenance cost and efforts for future 6G networks. Network Root Cause Analysis (Net-RCA) plays a critical role in identifying root causes of network faults. In this article, we first give an introduction about the envisioned 6G networks. Next, we discuss the challenges and potential solutions of 6G network operation and management, and comprehensively survey existing RCA methods. Then we propose an artificial intelligence (AI)-empowered Net-RCA framework for 6G. Performance comparisons on both synthetic and real-world network data are carried out to demonstrate that the proposed method outperforms the existing method considerably. ",Kein DOI-Link verfügbar,2212.00331v2,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Structure design and coordinated motion analysis of bionic crocodile   robot,1970,"  Crocodiles, known as one of the oldest and most resilient species on Earth, have demonstrated remarkable locomotor abilities both on land and in water, evolving over millennia to adapt to diverse environments. In this paper, we draw inspiration from crocodiles and introduce a highly biomimetic crocodile robot equipped with multiple degrees of freedom and articulated trunk joints. This design is based on a comprehensive analysis of the structural and motion characteristics observed in real crocodiles. The bionic crocodile robot has the problem of limb-torso incoordination during movement, in order to solve this problem, we apply the D-H method for both forward and inverse kinematics analysis of the robot's legs and spine. Through a series of simulation experiments, we investigate the robot's stability of motion, fault tolerance, and adaptability to the environment in two motor pattern: with and without the involvement of the spine and tail in its movements. Experiment results demonstrate that the bionic crocodile robot exhibits superior motion performance when the spine and tail cooperate with the extremities. This research not only showcases the potential of biomimicry in robotics but also underscores the significance of understanding how nature's designs can inform and enhance our technological innovations. ",Kein DOI-Link verfügbar,2311.01764v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Federated Machine Learning for Intelligent IoT via Reconfigurable   Intelligent Surface,1970,"  Intelligent Internet-of-Things (IoT) will be transformative with the advancement of artificial intelligence and high-dimensional data analysis, shifting from ""connected things"" to ""connected intelligence"". This shall unleash the full potential of intelligent IoT in a plethora of exciting applications, such as self-driving cars, unmanned aerial vehicles, healthcare, robotics, and supply chain finance. These applications drive the need of developing revolutionary computation, communication and artificial intelligence technologies that can make low-latency decisions with massive real-time data. To this end, federated machine learning, as a disruptive technology, is emerged to distill intelligence from the data at network edge, while guaranteeing device privacy and data security. However, the limited communication bandwidth is a key bottleneck of model aggregation for federated machine learning over radio channels. In this article, we shall develop an over-the-air computation based communication-efficient federated machine learning framework for intelligent IoT networks via exploiting the waveform superposition property of a multi-access channel. Reconfigurable intelligent surface is further leveraged to reduce the model aggregation error via enhancing the signal strength by reconfiguring the wireless propagation environments. ",Kein DOI-Link verfügbar,2004.05843v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Design and trajectory tracking control of CuRobot: A Cubic Reversible   Robot,1970,"  In field environments, numerous robots necessitate manual intervention for restoration of functionality post a turnover, resulting in diminished operational efficiency. This study presents an innovative design solution for a reversible omnidirectional mobile robot denoted as CuRobot, featuring a cube structure, thereby facilitating uninterrupted omnidirectional movement even in the event of flipping. The incorporation of eight conical wheels at the cube vertices ensures consistent omnidirectional motion no matter which face of the cube contacts the ground. Additionally, a kinematic model is formulated for CuRobot, accompanied by the development of a trajectory tracking controller utilizing model predictive control. Through simulation experiments, the correlation between trajectory tracking accuracy and the robot's motion direction is examined. Furthermore, the robot's proficiency in omnidirectional mobility and sustained movement post-flipping is substantiated via both simulation and prototype experiments. This design reduces the inefficiencies associated with manual intervention, thereby increasing the operational robustness of robots in field environments. ",Kein DOI-Link verfügbar,2311.16809v1,Yes,innovative(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Communication-Efficient Edge AI: Algorithms and Systems,1970,"  Artificial intelligence (AI) has achieved remarkable breakthroughs in a wide range of fields, ranging from speech processing, image classification to drug discovery. This is driven by the explosive growth of data, advances in machine learning (especially deep learning), and easy access to vastly powerful computing resources. Particularly, the wide scale deployment of edge devices (e.g., IoT devices) generates an unprecedented scale of data, which provides the opportunity to derive accurate models and develop various intelligent applications at the network edge. However, such enormous data cannot all be sent from end devices to the cloud for processing, due to the varying channel quality, traffic congestion and/or privacy concerns. By pushing inference and training processes of AI models to edge nodes, edge AI has emerged as a promising alternative. AI at the edge requires close cooperation among edge devices, such as smart phones and smart vehicles, and edge servers at the wireless access points and base stations, which however result in heavy communication overheads. In this paper, we present a comprehensive survey of the recent developments in various techniques for overcoming these communication challenges. Specifically, we first identify key communication challenges in edge AI systems. We then introduce communication-efficient techniques, from both algorithmic and system perspectives for training and inference tasks at the network edge. Potential future research directions are also highlighted. ",Kein DOI-Link verfügbar,2002.09668v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Achieve Sustainable Ultra-Dense Heterogeneous Networks for 5G,1970,"  Due to the exponentially increased demands of mobile data traffic, e.g., a 1000-fold increase in traffic demand from 4G to 5G, network densification is considered as a key mechanism in the evolution of cellular networks, and ultra-dense heterogeneous network (UDHN) is a promising technique to meet the requirements of explosive data traffic in 5G networks. In the UDHN, base station is brought closer and closer to users through densely deploying small cells, which would result in extremely high spectral efficiency and energy efficiency. In this article, we first present a potential network architecture for the UDHN, and then propose a generalized orthogonal/non-orthogonal random access scheme to improve the network efficiency while reducing the signaling overhead. Simulation results demonstrate the effectiveness of the proposed scheme. Finally, we present some of the key challenges of the UDHN. ",Kein DOI-Link verfügbar,1711.05044v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",UTBoost: A Tree-boosting based System for Uplift Modeling,1970,"  Uplift modeling refers to the set of machine learning techniques that a manager may use to estimate customer uplift, that is, the net effect of an action on some customer outcome. By identifying the subset of customers for whom a treatment will have the greatest effect, uplift models assist decision-makers in optimizing resource allocations and maximizing overall returns. Accurately estimating customer uplift poses practical challenges, as it requires assessing the difference between two mutually exclusive outcomes for each individual. In this paper, we propose two innovative adaptations of the well-established Gradient Boosting Decision Trees (GBDT) algorithm, which learn the causal effect in a sequential way and overcome the counter-factual nature. Both approaches innovate existing techniques in terms of ensemble learning method and learning objectives, respectively. Experiments on large-scale datasets demonstrate the usefulness of the proposed methods, which often yielding remarkable improvements over base models. To facilitate the application, we develop the UTBoost, an end-to-end tree boosting system specifically designed for uplift modeling. The package is open source and has been optimized for training speed to meet the needs of real industrial applications. ",Kein DOI-Link verfügbar,2312.02573v1,Yes,innovative(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",MolFM: A Multimodal Molecular Foundation Model,1970,"  Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures local and global molecular knowledge by minimizing the distance in the feature space between different modalities of the same molecule, as well as molecules sharing similar structures or functions. MolFM achieves state-of-the-art performance on various downstream tasks. On cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04% absolute gains under the zero-shot and fine-tuning settings, respectively. Furthermore, qualitative analysis showcases MolFM's implicit ability to provide grounding from molecular substructures and knowledge graphs. Code and models are available on https://github.com/BioFM/OpenBioMed. ",Kein DOI-Link verfügbar,2307.09484v2,Yes,intricate(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Fed-EINI: An Efficient and Interpretable Inference Framework for   Decision Tree Ensembles in Federated Learning,1970,"  The increasing concerns about data privacy and security drive an emerging field of studying privacy-preserving machine learning from isolated data sources, i.e., federated learning. A class of federated learning, vertical federated learning, where different parties hold different features for common users, has a great potential of driving a great variety of business cooperation among enterprises in many fields. In machine learning, decision tree ensembles such as gradient boosting decision trees (GBDT) and random forest are widely applied powerful models with high interpretability and modeling efficiency. However, stateof-art vertical federated learning frameworks adapt anonymous features to avoid possible data breaches, makes the interpretability of the model compromised. To address this issue in the inference process, in this paper, we firstly make a problem analysis about the necessity of disclosure meanings of feature to Guest Party in vertical federated learning. Then we find the prediction result of a tree could be expressed as the intersection of results of sub-models of the tree held by all parties. With this key observation, we protect data privacy and allow the disclosure of feature meaning by concealing decision paths and adapt a communication-efficient secure computation method for inference outputs. The advantages of Fed-EINI will be demonstrated through both theoretical analysis and extensive numerical results. We improve the interpretability of the model by disclosing the meaning of features while ensuring efficiency and accuracy. ",Kein DOI-Link verfügbar,2105.09540v11,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",BioMedGPT: Open Multimodal Generative Pre-trained Transformer for   BioMedicine,1970,"  Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \url{https://github.com/PharMolix/OpenBioMed}. ",Kein DOI-Link verfügbar,2308.09442v2,Yes,"meticulous(1), meticulously(1)"
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Towards Unified AI Drug Discovery with Multiple Knowledge Modalities,1970,"  In recent years, AI models that mine intrinsic patterns from molecular structures and protein sequences have shown promise in accelerating drug discovery. However, these methods partly lag behind real-world pharmaceutical approaches of human experts that additionally grasp structured knowledge from knowledge bases and unstructured knowledge from biomedical literature. To bridge this gap, we propose KEDD, a unified, end-to-end, and multimodal deep learning framework that optimally incorporates both structured and unstructured knowledge for vast AI drug discovery tasks. The framework first extracts underlying characteristics from heterogeneous inputs, and then applies multimodal fusion for accurate prediction. To mitigate the problem of missing modalities, we leverage multi-head sparse attention and a modality masking mechanism to extract relevant information robustly. Benefiting from integrated knowledge, our framework achieves a deeper understanding of molecule entities, brings significant improvements over state-of-the-art methods on a wide range of tasks and benchmarks, and reveals its promising potential in assisting real-world drug discovery. ",Kein DOI-Link verfügbar,2305.01523v2,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Learning Multi-view Molecular Representations with Structured and   Unstructured Knowledge,1970,"  Capturing molecular knowledge with representation learning approaches holds significant potential in vast scientific fields such as chemistry and life science. An effective and generalizable molecular representation is expected to capture the consensus and complementary molecular expertise from diverse views and perspectives. However, existing works fall short in learning multi-view molecular representations, due to challenges in explicitly incorporating view information and handling molecular knowledge from heterogeneous sources. To address these issues, we present MV-Mol, a molecular representation learning model that harvests multi-view molecular expertise from chemical structures, unstructured knowledge from biomedical texts, and structured knowledge from knowledge graphs. We utilize text prompts to model view information and design a fusion architecture to extract view-based molecular representations. We develop a two-stage pre-training procedure, exploiting heterogeneous data of varying quality and quantity. Through extensive experiments, we show that MV-Mol provides improved representations that substantially benefit molecular property prediction. Additionally, MV-Mol exhibits state-of-the-art performance in multi-modal comprehension of molecular structures and texts. Code and data are available at https://github.com/PharMolix/OpenBioMed. ",Kein DOI-Link verfügbar,2406.09841v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",A Holistic Robust Motion Controller Framework for Autonomous Platooning,1970,"  Safety is the foremost concern for autonomous platooning. The vehicle-to-vehicle (V2V) communication delay and the sudden appearance of obstacles will trigger the safety of the intended functionality (SOTIF) issues for autonomous platooning. This research proposes a holistic robust motion controller framework (MCF) for an intelligent and connected vehicle platoon system. The MCF utilizes a hierarchical structure to resolve the longitudinal string stability and the lateral control problem under the complex driving environment and time-varying communication delay. Firstly, the H-infinity feedback controller is developed to ensure the robustness of the platoon under time-varying communication delay in the upper-level coordination layer (UCL). The output from UCL will be delivered to the lower-level motion-planning layer (LML) as reference signals. Secondly, the model predictive control (MPC) algorithm is implemented in the LML to achieve multi-objective control, which comprehensively considers the reference signals, the artificial potential field, and multiple vehicle dynamics constraints. Furthermore, three critical scenarios are co-simulated for case studies, including platooning under time-varying communication delay, merging, and obstacle avoidance scenarios. The simulation results indicate that, compared with single-structure MPC, the proposed MCF can offer a better suppression on position error propagation, and get improvements on maximum position error in the three scenarios by $19.2\%$, $59.8\%$, and $15.3\%$, respectively. Last, the practicability and effectiveness of the proposed MCF are verified via hardware-in-the-loop experiment. The average conducting time of the proposed method on Speedgoat real-time target machine is 1.1 milliseconds, which meets the real-time requirements. ",Kein DOI-Link verfügbar,2206.04948v1,Yes,potent(1)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Few-Shot Recognition and Classification Framework for Jamming Signal: A   CGAN-Based Fusion CNN Approach,1970,"  Subject to intricate environmental variables, the precise classification of jamming signals holds paramount significance in the effective implementation of anti-jamming strategies within communication systems. In light of this imperative, we propose an innovative fusion algorithm based on conditional generative adversarial network (CGAN) and convolutional neural network (CNN), which aims to deal with the difficulty in applying deep learning (DL) algorithms due to the instantaneous nature of jamming signals in practical communication systems. Compared with previous methods, our algorithm embeds jamming category labels to constrain the range of generated signals in the frequency domain by using the CGAN model, which simultaneously captures potential label information while learning the distribution of signal data thus achieves an 8% improvement in accuracy even when working with a few-sample dataset. Real-world satellite communication scenarios are simulated by adopting hardware platform, and we validate our algorithm by using the resulting time-domain waveform data. The experimental results indicate that our algorithm still performs extremely well, which demonstrates significant potential for practical application in real-world communication scenarios. ",Kein DOI-Link verfügbar,2311.05273v3,Yes,"innovative(1), intricate(1), potent(2)"
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Experimental and numerical studies on kV scattered x-ray imaging for   real-time image guidance in radiation therapy,1970,"  Motion management is a critical component of image guidance radiotherapy for lung cancer. We previously proposed a scheme using kV scattered x-ray photons for marker-less real-time image guidance in lung cancer radiotherapy. This study reports our recently progress using the photon counting detection technique to demonstrate potential feasibility of this method and using Monte Carlo (MC) simulations and ray-tracing calculations to characterize the performance. In our scheme, a thin slice of x-ray beam was directed to the target and we measured the outgoing scattered photons using a photon counting detector with a parallel-hole collimator to establish the correspondence between detector pixels and scatter positions. Image corrections of geometry, beam attenuation and scattering angle were performed to convert the raw image to the actual image of Compton attenuation coefficient. We set up a MC simulation system using an in-house developed GPU-based MC package modeling the image formation process. We also performed ray-tracing calculations to investigate the impacts of imaging system geometry on resulting image resolution. The experiment demonstrated feasibility of using a photon counting detector to measure scattered x-ray photons and generate the proposed scattered x-ray image. After correction, x-ray scattering image intensity and Compton scattering attenuation coefficient were linearly related, with R2=0.91. Contrast to Noise Ratios of different objects were improved and the values in experimental results and MC simulation results agreed with each other. Ray-tracing calculations revealed the dependence of image resolution on imaging geometry. The image resolution increases with reduced source to object distance and increased collimator height. The study demonstrated potential feasibility of using scattered x-ray imaging as a real-time image guidance method in radiation therapy. ",Kein DOI-Link verfügbar,2008.02428v2,Yes,potent(2)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Millimeter Wave Communications for Future Mobile Networks,1970,"  Millimeter wave (mmWave) communications have recently attracted large research interest, since the huge available bandwidth can potentially lead to rates of multiple Gbps (gigabit per second) per user. Though mmWave can be readily used in stationary scenarios such as indoor hotspots or backhaul, it is challenging to use mmWave in mobile networks, where the transmitting/receiving nodes may be moving, channels may have a complicated structure, and the coordination among multiple nodes is difficult. To fully exploit the high potential rates of mmWave in mobile networks, lots of technical problems must be addressed. This paper presents a comprehensive survey of mmWave communications for future mobile networks (5G and beyond). We first summarize the recent channel measurement campaigns and modeling results. Then, we discuss in detail recent progresses in multiple input multiple output (MIMO) transceiver design for mmWave communications. After that, we provide an overview of the solution for multiple access and backhauling, followed by analysis of coverage and connectivity. Finally, the progresses in the standardization and deployment of mmWave for mobile networks are discussed. ",Kein DOI-Link verfügbar,1705.06072v1,Yes,potent(2)
0000-0001-5571-6645,Kai Yang,"Zhejiang University, Zhejiang University of Technology",Tuning the exchange bias on a single atom from 1 mT to 10 T,1970,"  Shrinking spintronic devices to the nanoscale ultimately requires localized control of individual atomic magnetic moments. At these length scales, the exchange interaction plays important roles, such as in the stabilization of spin-quantization axes, the production of spin frustration, and creation of magnetic ordering. Here, we demonstrate the precise control of the exchange bias experienced by a single atom on a surface, covering an energy range of four orders of magnitude. The exchange interaction is continuously tunable from milli-eV to micro-eV by adjusting the separation between a spin-1/2 atom on a surface and the magnetic tip of a scanning tunneling microscope (STM). We seamlessly combine inelastic electron tunneling spectroscopy (IETS) and electron spin resonance (ESR) to map out the different energy scales. This control of exchange bias over a wide span of energies provides versatile control of spin states, with applications ranging from precise tuning of quantum state properties, to strong exchange bias for local spin doping. In addition we show that a time-varying exchange interaction generates a localized AC magnetic field that resonantly drives the surface spin. The static and dynamic control of the exchange interaction at the atomic-scale provides a new tool to tune the quantum states of coupled-spin systems. ",https://doi.org/10.1103/PhysRevLett.122.227203,1906.03213v1,Yes,versatile(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Defending against Whitebox Adversarial Attacks via Randomized   Discretization,1970,"  Adversarial perturbations dramatically decrease the accuracy of state-of-the-art image classifiers. In this paper, we propose and analyze a simple and computationally efficient defense strategy: inject random Gaussian noise, discretize each pixel, and then feed the result into any pre-trained classifier. Theoretically, we show that our randomized discretization strategy reduces the KL divergence between original and adversarial inputs, leading to a lower bound on the classification accuracy of any classifier against any (potentially whitebox) $\ell_\infty$-bounded adversarial attack. Empirically, we evaluate our defense on adversarial examples generated by a strong iterative PGD attack. On ImageNet, our defense is more robust than adversarially-trained networks and the winning defenses of the NIPS 2017 Adversarial Attacks & Defenses competition. ",Kein DOI-Link verfügbar,1903.10586v1,Yes,potent(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Privacy Preservation in Delay-Based Localization Systems: Artificial   Noise or Artificial Multipath?,1970,"  Localization plays an increasingly pivotal role in 5G/6G systems, enabling various applications. This paper focuses on the privacy concerns associated with delay-based localization, where unauthorized base stations attempt to infer the location of the end user. We propose a method to disrupt localization at unauthorized nodes by injecting artificial components into the pilot signal, exploiting model mismatches inherent in these nodes. Specifically, we investigate the effectiveness of two techniques, namely artificial multipath (AM) and artificial noise (AN), in mitigating location leakage. By leveraging the misspecified Cram\'er-Rao bound framework, we evaluate the impact of these techniques on unauthorized localization performance. Our results demonstrate that pilot manipulation significantly degrades the accuracy of unauthorized localization while minimally affecting legitimate localization. Moreover, we find that the superiority of AM over AN varies depending on the specific scenario. ",Kein DOI-Link verfügbar,2408.11290v1,Yes,pivotal(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Tarsier: Recipes for Training and Evaluating Large Video Description   Models,1970,"  Generating fine-grained video descriptions is a fundamental challenge in video understanding. In this work, we introduce Tarsier, a family of large-scale video-language models designed to generate high-quality video descriptions. Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships. Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities than any existing open-source model, showing a $+51.4\%$ advantage in human side-by-side evaluation over the strongest model. Additionally, they are comparable to state-of-the-art proprietary models, with a $+12.3\%$ advantage against GPT-4V and a $-6.7\%$ disadvantage against Gemini 1.5 Pro. Besides video description, Tarsier proves to be a versatile generalist model, achieving new state-of-the-art results across nine public benchmarks, including multi-choice VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is the introduction of a new benchmark for evaluating video description models, consisting of a new challenging dataset featuring videos from diverse sources and varying complexity, along with an automatic method specifically designed to assess the quality of fine-grained video descriptions. We make our models and evaluation benchmark publicly available at \url{https://github.com/bytedance/tarsier}. ",Kein DOI-Link verfügbar,2407.00634v1,Yes,"meticulous(1), versatile(1), meticulously(1)"
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Decomposing the Fundamentals of Creepy Stories,1970,"  Fear is a universal concept; people crave it in urban legends, scary movies, and modern stories. Open questions remain, however, about why these stories are scary and more generally what scares people. In this study, we explore these questions by analyzing tens of thousands of scary stories on forums (known as subreddits) in a social media website, Reddit. We first explore how writing styles have evolved to keep these stories fresh before we analyze the stable core techniques writers use to make stories scary. We find that writers have changed the themes of their stories over years from haunted houses to school-related themes, body horror, and diseases. Yet some features remain stable; words associated with pseudo-human nouns, such as clown or devil are more common in scary stories than baselines. In addition, we collect a range of datasets that annotate sentences containing fear. We use these data to develop a high-accuracy fear detection neural network model, which is used to quantify where people express fear in scary stories. We find that sentences describing fear, and words most often seen in scary stories, spike at particular points in a story, possibly as a way to keep the readers on the edge of their seats until the story's conclusion. These results provide a new understanding of how authors cater to their readers, and how fear may manifest in stories. ",Kein DOI-Link verfügbar,2211.05369v1,Yes,fresh(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Near-Field Wideband Secure Communications: An Analog Beamfocusing   Approach,1970,"  In the rapidly advancing landscape of 6G, characterized by ultra-high-speed wideband transmission in millimeter-wave and terahertz bands, our paper addresses the pivotal task of enhancing physical layer security (PLS) within near-field wideband communications. We introduce true-time delayer (TTD)-incorporated analog beamfocusing techniques designed to address the interplay between near-field propagation and wideband beamsplit, an uncharted domain in existing literature. Our approach to maximizing secrecy rates involves formulating an optimization problem for joint power allocation and analog beamformer design, employing a two-stage process encompassing a semi-digital solution and analog approximation. This problem is efficiently solved through a combination of alternating optimization, fractional programming, and block successive upper-bound minimization techniques. Additionally, we present a low-complexity beamsplit-aware beamfocusing strategy, capitalizing on geometric insights from near-field wideband propagation, which can also serve as a robust initial value for the optimization-based approach. Numerical results substantiate the efficacy of the proposed methods, clearly demonstrating their superiority over TTD-free approaches in fortifying wideband PLS, as well as the advantageous secrecy energy efficiency achieved by leveraging low-cost analog devices. ",https://doi.org/10.1109/TSP.2024.3390177,2311.08738v2,Yes,pivotal(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Heterogeneous Subgraph Transformer for Fake News Detection,1970,"  Fake news is pervasive on social media, inflicting substantial harm on public discourse and societal well-being. We investigate the explicit structural information and textual features of news pieces by constructing a heterogeneous graph concerning the relations among news topics, entities, and content. Through our study, we reveal that fake news can be effectively detected in terms of the atypical heterogeneous subgraphs centered on them, which encapsulate the essential semantics and intricate relations between news elements. However, suffering from the heterogeneity, exploring such heterogeneous subgraphs remains an open problem. To bridge the gap, this work proposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs in our constructed heterogeneous graph. In HeteroSGT, we first employ a pre-trained language model to derive both word-level and sentence-level semantics. Then the random walk with restart (RWR) is applied to extract subgraphs centered on each news, which are further fed to our proposed subgraph Transformer to quantify the authenticity. Extensive experiments on five real-world datasets demonstrate the superior performance of HeteroSGT over five baselines. Further case and ablation studies validate our motivation and demonstrate that performance improvement stems from our specially designed components. ",Kein DOI-Link verfügbar,2404.13192v1,Yes,intricate(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Make Pixels Dance: High-Dynamic Video Generation,1970,"  Creating high-dynamic videos such as motion-rich actions and sophisticated visual effects poses a significant challenge in the field of artificial intelligence. Unfortunately, current state-of-the-art video generation methods, primarily focusing on text-to-video generation, tend to produce video clips with minimal motions despite maintaining high fidelity. We argue that relying solely on text instructions is insufficient and suboptimal for video generation. In this paper, we introduce PixelDance, a novel approach based on diffusion models that incorporates image instructions for both the first and last frames in conjunction with text instructions for video generation. Comprehensive experimental results demonstrate that PixelDance trained with public data exhibits significantly better proficiency in synthesizing videos with complex scenes and intricate motions, setting a new standard for video generation. ",Kein DOI-Link verfügbar,2311.10982v1,Yes,intricate(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Boximator: Generating Rich and Controllable Motions for Video Synthesis,1970,"  Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model. ",Kein DOI-Link verfügbar,2402.01566v1,Yes,pivotal(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Are AI-Generated Text Detectors Robust to Adversarial Perturbations?,1970,"  The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confidence predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at \url{https://github.com/CarlanLark/Robust-AIGC-Detector}. ",Kein DOI-Link verfügbar,2406.01179v2,Yes,potent(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",Navigating Complexity: Toward Lossless Graph Condensation via Expanding   Window Matching,1970,"  Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM. ",Kein DOI-Link verfügbar,2402.05011v3,Yes,potent(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",A Benchmark Dataset for Multimodal Prediction of Enzymatic Function   Coupling DNA Sequences and Natural Language,1970,"  Predicting gene function from its DNA sequence is a fundamental challenge in biology. Many deep learning models have been proposed to embed DNA sequences and predict their enzymatic function, leveraging information in public databases linking DNA sequences to an enzymatic function label. However, much of the scientific community's knowledge of biological function is not represented in these categorical labels, and is instead captured in unstructured text descriptions of mechanisms, reactions, and enzyme behavior. These descriptions are often captured alongside DNA sequences in biological databases, albeit in an unstructured manner. Deep learning of models predicting enzymatic function are likely to benefit from incorporating this multi-modal data encoding scientific knowledge of biological function. There is, however, no dataset designed for machine learning algorithms to leverage this multi-modal information. Here we propose a novel dataset and benchmark suite that enables the exploration and development of large multi-modal neural network models on gene DNA sequences and natural language descriptions of gene function. We present baseline performance on benchmarks for both unsupervised and supervised tasks that demonstrate the difficulty of this modeling objective, while demonstrating the potential benefit of incorporating multi-modal data types in function prediction compared to DNA sequences alone. Our dataset is at: https://hoarfrost-lab.github.io/BioTalk/. ",Kein DOI-Link verfügbar,2407.15888v1,Yes,potent(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",A Chandra Survey of $z\geq4.5$ Quasars,1970,"  X-ray observations provide a unique probe of the accretion disk corona of supermassive black holes (SMBHs). In this paper, we present a uniform \emph{Chandra} X-ray data analysis of a sample of 152 $z\geq4.5$ quasars. We firmly detect 46 quasars of this sample in 0.5-2~keV above 3~$\sigma$ and calculate the upper limits of the X-ray flux of the remaining. We also estimate the power law photon index of the X-ray spectrum of 31 quasars. 24 of our sample quasars are detected in the FIRST or NVSS radio surveys; all of them are radio-loud. We statistically compare the X-ray properties of our $z\geq4.5$ quasars to other X-ray samples of AGN at different redshifts. The relation between the rest-frame X-ray luminosity and other quasar parameters, such as the bolometric luminosity, UV luminosity, or SMBH mass, show large scatters. These large scatters can be attributed to the narrow luminosity range at the highest redshift, the large measurement error based on relatively poor X-ray data, and the inclusion of radio-loud quasars in the sample. The $L_{\rm X}-L_{\rm UV}$ relationship is significantly sub-linear. We do not find a significant redshift evolution of the $L_{\rm X}-L_{\rm UV}$ relation, expressed either in the slope of this relation, or the departure of individual AGNs from the best-fit $\alpha_{\rm OX}-L_{\rm UV}$ relation ($\Delta\alpha_{\rm OX}$). The median value of the X-ray photon index is $\Gamma\approx1.79$, which does not show redshift evolution from $z=0$ to $z\sim7$. The X-ray and UV properties of the most distant quasars could potentially be used as a standard candle to constrain cosmological models. The large scatter of our sample on the Hubble diagram highlights the importance of future large unbiased deep X-ray and radio surveys in using quasars in cosmological studies. ",Kein DOI-Link verfügbar,2105.11619v1,Yes,potent(1)
0000-0002-2993-3467,Yuchen Zhang,"Zhejiang University, Zhejiang University of Technology",ROBBIE: Robust Bias Evaluation of Large Generative Language Models,1970,"  As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold:   (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases.   (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs. ",Kein DOI-Link verfügbar,2311.18140v1,Yes,potent(2)
0000-0001-6978-0007,Chenguang Hu,"Zhejiang University, Zhejiang University of Technology",On Calibration of Speech Classification Models: Insights from   Energy-Based Model Investigations,1970,"  For speech classification tasks, deep learning models often achieve high accuracy but exhibit shortcomings in calibration, manifesting as classifiers exhibiting overconfidence. The significance of calibration lies in its critical role in guaranteeing the reliability of decision-making within deep learning systems. This study explores the effectiveness of Energy-Based Models in calibrating confidence for speech classification tasks by training a joint EBM integrating a discriminative and a generative model, thereby enhancing the classifiers calibration and mitigating overconfidence. Experimental evaluations conducted on three speech classification tasks specifically: age, emotion, and language recognition. Our findings highlight the competitive performance of EBMs in calibrating the speech classification models. This research emphasizes the potential of EBMs in speech classification tasks, demonstrating their ability to enhance calibration without sacrificing accuracy. ",Kein DOI-Link verfügbar,2406.18065v1,Yes,potent(1)
0000-0003-3789-2893,Shuo Ma,"Zhejiang University, Zhejiang University of Technology",Trapped arrays of alkaline earth Rydberg atoms in optical tweezers,1970,"  Neutral atom qubits with Rydberg-mediated interactions are a leading platform for developing large-scale coherent quantum systems. In the majority of experiments to date, the Rydberg states are not trapped by the same potential that confines ground state atoms, resulting in atom loss and constraints on the achievable interaction time. In this work, we demonstrate that the Rydberg states of an alkaline earth atom, ytterbium, can be stably trapped by the same red-detuned optical tweezer that also confines the ground state, by leveraging the polarizability of the Yb$^+$ ion core. Using the previously unobserved \tripletS series, we demonstrate trapped Rydberg atom lifetimes exceeding $100\,\mu$s, and observe no evidence of auto- or photo-ionization from the trap light for these states. We measure a coherence time of $T_2 = 59$ $\mu$s between two Rydberg levels, exceeding the 28 $\mu$s lifetime of untrapped Rydberg atoms under the same conditions. These results are promising for extending the interaction time of Rydberg atom arrays for quantum simulation and computing, and are vital to capitalize on the extended Rydberg lifetimes in circular states or cryogenic environments. ",https://doi.org/10.1103/PhysRevLett.128.033201,1912.08754v2,Yes,potent(1)
0000-0003-3789-2893,Shuo Ma,"Zhejiang University, Zhejiang University of Technology",Guided multi-branch learning systems for sound event detection with   sound separation,1970,"  In this paper, we describe in detail our systems for DCASE 2020 Task 4. The systems are based on the 1st-place system of DCASE 2019 Task 4, which adopts weakly-supervised framework with an attention-based embedding-level pooling module and a semi-supervised learning approach named guided learning. This year, we incorporate multi-branch learning (MBL) into the original system to further improve its performance. MBL uses different branches with different pooling strategies (including instance-level and embedding-level strategies) and different pooling modules (including attention pooling, global max pooling or global average pooling modules), which share the same feature encoder of the model. Therefore, multiple branches pursuing different purposes and focusing on different characteristics of the data can help the feature encoder model the feature space better and avoid over-fitting. To better exploit the strongly-labeled synthetic data, inspired by multi-task learning, we also employ a sound event detection branch. To combine sound separation (SS) with sound event detection (SED), we fuse the results of SED systems with SS-SED systems which are trained using separated sound output by an SS system. The experimental results prove that MBL can improve the model performance and using SS has great potential to improve the performance of SED ensemble system. ",Kein DOI-Link verfügbar,2007.10638v2,Yes,potent(1)
0000-0003-3789-2893,Shuo Ma,"Zhejiang University, Zhejiang University of Technology",Cluster aggregates surrounding Pismis 5 in the Vela Molecular Ridge,1970,"  Context. In the Gaia era, the precision of astrometric data is unprecedented. High-quality data make it easier to find more cluster aggregates and support further confirmation of these open clusters. Aims. We use Gaia DR3 to redetermine the open clusters surrounding Pismis 5 in the Vela Molecular Ridge. We also investigate the basic properties of these clusters. Methods. We apply two clustering algorithms (StarGO and pyUPMASK) to identify the open cluster members in a five-dimensional space with Gaia DR3. Results. We identify eight open clusters surrounding Pismis 5 in the Vela Molecular Ridge. The open cluster QZ 1 is newly discovered. Through investigating the comprehensive properties of the clusters, one open binary cluster candidate (Alessi 43 and Collinder 197) and one triple open cluster candidate (Pismis 5, Pismis 5A, and Pismis 5B) are discussed. Conclusions. Binary and triple open cluster candidates have been identified as potential primordial aggregates based on their similar age, position, and motion. According to kinematic speculations, the two aggregate candidates will gradually separate, and their interiors will slowly disintegrate. ",https://doi.org/10.1051/0004-6361/202244737,2304.00226v3,Yes,potent(1)
0000-0003-3789-2893,Shuo Ma,"Zhejiang University, Zhejiang University of Technology",Spectroscopy and modeling of $^{171}$Yb Rydberg states for high-fidelity   two-qubit gates,1970,"  We present multichannel quantum defect (MQDT) models for highly excited $^{174}$Yb and $^{171}$Yb Rydberg states with $L \leq 2$. The models are developed using a combination of existing literature data and new, high-precision laser and microwave spectroscopy in an atomic beam, and validated by detailed comparison with experimentally measured Stark shifts and magnetic moments. We then use these models to compute interaction potentials between two Yb atoms, and find excellent agreement with direct measurements in an optical tweezer array. From the computed interaction potential, we identify an anomalous F\""orster resonance that likely degraded the fidelity of previous entangling gates in $^{171}$Yb using $F=3/2$ Rydberg states. We then identify a more suitable $F=1/2$ state, and achieve a state-of-the-art controlled-Z gate fidelity of $\mathcal{F}=0.994(1)$, with the remaining error fully explained by known sources. This work establishes a solid foundation for the continued development quantum computing, simulation and entanglement-enhanced metrology with Yb neutral atom arrays. ",Kein DOI-Link verfügbar,2406.01482v1,Yes,potent(2)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Thermodynamic Geometry of the Born-Infeld-anti-de Sitter black holes,1970,"  Thermodynamic geometry is applied to the Born-Infeld-anti-de Sitter black hole (BIAdS) in the four dimensions, which is a nonlinear generalization of the Reissner-Norstr\""Aom-AdS black hole (RNAdS). We compute the Weinhold as well as the Ruppeiner scalar curvature and find that the singular points are not the same with the ones obtained using the heat capacity. Legendre-invariant metric proposed by Quevedo and the metric obtained by using the free energy as the thermodynamic potential are obtained and the corresponding scalar curvatures diverge at the Davies points. ",https://doi.org/10.1142/S0217751X11053742,1104.0546v2,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Stein variational reduced basis Bayesian inversion,1970,"  We propose and analyze a Stein variational reduced basis method (SVRB) to solve large-scale PDE-constrained Bayesian inverse problems. To address the computational challenge of drawing numerous samples requiring expensive PDE solves from the posterior distribution, we integrate an adaptive and goal-oriented model reduction technique with an optimization-based Stein variational gradient descent method (SVGD). The samples are drawn from the prior distribution and iteratively pushed to the posterior by a sequence of transport maps, which are constructed by SVGD, requiring the evaluation of the potential---the negative log of the likelihood function---and its gradient with respect to the random parameters, which depend on the solution of the PDE. To reduce the computational cost, we develop an adaptive and goal-oriented model reduction technique based on reduced basis approximations for the evaluation of the potential and its gradient. We present a detailed analysis for the reduced basis approximation errors of the potential and its gradient, the induced errors of the posterior distribution measured by Kullback--Leibler divergence, as well as the errors of the samples. To demonstrate the computational accuracy and efficiency of SVRB, we report results of numerical experiments on a Bayesian inverse problem governed by a diffusion PDE with random parameters with both uniform and Gaussian prior distributions. Over 100X speedups can be achieved while the accuracy of the approximation of the potential and its gradient is preserved. ",Kein DOI-Link verfügbar,2002.10924v1,Yes,potent(4)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",RIS-ADMM: A RIS and ADMM-Based Passive and Sparse Sensing Method With   Interference Removal,1970,"  Reconfigurable Intelligent Surfaces (RIS) emerge as promising technologies in future radar and wireless communication domains. This letter addresses the passive sensing issue utilizing wireless communication signals and RIS amidst interference from wireless access points (APs). We introduce an atomic norm minimization (ANM) approach to leverage spatial domain target sparsity and estimate the direction of arrival (DOA). However, the conventional semidefinite programming (SDP)-based solutions for the ANM problem are complex and lack efficient realization. Consequently, we propose a RIS-ADMM method, an innovative alternating direction method of multipliers (ADMM)-based iterative approach. This method yields closed-form expressions and effectively suppresses interference signals. Simulation outcomes affirm that our RIS-ADMM method surpasses existing techniques in DOA estimation accuracy while maintaining low computational complexity. The code for the proposed method is available online \url{https://github.com/chenpengseu/RIS-ADMM.git}. ",Kein DOI-Link verfügbar,2206.06172v2,Yes,innovative(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Bochner-Riesz profile of anharmonic oscillator ${\mathcal   L}=-\frac{d^2}{dx^2}+|x|$,1970,"  We investigate spectral multipliers, Bochner-Riesz means and convergence of eigenfunction expansion corresponding to the Schr\""odinger operator with anharmonic potential ${\mathcal L}=-\frac{d^2}{dx^2}+|x|$. We show that the Bochner-Riesz profile of the operator ${\mathcal L}$ completely coincides with such profile of the harmonic oscillator ${\mathcal H}=-\frac{d^2}{dx^2}+x^2$.   It is especially surprising because the Bochner-Riesz profile for the one-dimensional standard Laplace operator is known to be essentially different and the case of operators ${\mathcal H}$ and ${\mathcal L}$ resembles more the profile of multidimensional Laplace operators. Another surprising element of the main obtained result is the fact that the proof is not based on restriction type estimates and instead entirely new perspective have to be developed to obtain the critical exponent for Bochner-Riesz means convergence. ",Kein DOI-Link verfügbar,1408.1259v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Spectral multipliers via resolvent type estimates on non-homogeneous   metric measure spaces,1970,"  We describe a simple but surprisingly effective technique of obtaining spectral multiplier results for abstract operators which satisfy the finite propagation speed property for the corresponding wave equation propagator. We show that, in this setting, spectral multipliers follow from resolvent type estimates. The most notable point of the paper is that our approach is very flexible and can be applied even if the corresponding ambient space does not satisfy the doubling condition or if the semigroup generated by an operator is not uniformly bounded. As a corollary we obtain the $L^p$ spectrum independence for several second order differential operators and recover some known results. Our examples include the Laplace-Belltrami operator on manifolds with ends and Schr\""odinger operators with strongly subcritical potentials. ",Kein DOI-Link verfügbar,1609.01871v1,Yes,"notable(1), potent(1)"
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Power Allocation in Multi-user Cellular Networks With Deep Q Learning   Approach,1970,"  The model-driven power allocation (PA) algorithms in the wireless cellular networks with interfering multiple-access channel (IMAC) have been investigated for decades. Nowadays, the data-driven model-free machine learning-based approaches are rapidly developed in this field, and among them the deep reinforcement learning (DRL) is proved to be of great promising potential. Different from supervised learning, the DRL takes advantages of exploration and exploitation to maximize the objective function under certain constraints. In our paper, we propose a two-step training framework. First, with the off-line learning in simulated environment, a deep Q network (DQN) is trained with deep Q learning (DQL) algorithm, which is well-designed to be in consistent with this PA issue. Second, the DQN will be further fine-tuned with real data in on-line training procedure. The simulation results show that the proposed DQN achieves the highest averaged sum-rate, comparing to the ones with present DQL training. With different user densities, our DQN outperforms benchmark algorithms and thus a good generalization ability is verified. ",Kein DOI-Link verfügbar,1812.02979v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Soft phonon modes driven huge difference on lattice thermal conductivity   between topological semimetal WC and WN,1970,"  Topological semimetals are currently attracting increasing interest due to their potential applications in topological qubits and low-power electronics, which are closely related to their thermal transport properties. In this work, by solving the Boltzmann transport equation based on first-principles calculations, we systematically investigate the phonon transport properties of topological semimetal WC and WN. The predicted room-temperature lattice thermal conductivities of WC (WN) along a and c directions are 1140.64 (7.47) $\mathrm{W m^{-1} K^{-1}}$ and 1214.69 (5.39) $\mathrm{W m^{-1} K^{-1}}$. Considering the similar crystal structure of WC and WN, it is quite interesting to find that the thermal conductivity of WC is more than two orders of magnitude higher than that of WN. It is found that, different from WN, the large acoustic-optical (a-o) gap prohibits the acoustic+acoustic$\rightarrow$optical (aao) scattering, which gives rise to very long phonon lifetimes, leading to ultrahigh lattice thermal conductivity in WC. For WN, the lack of a-o gap is due to soft phonon modes in optical branches, which can provide more scattering channels for aao scattering, producing very short phonon lifetimes. Further deep insight can be attained from their different electronic structures. Distinctly different from that in WC, the density of states (DOS) of WN at the Fermi level becomes very sharp, which leads to destabilization of WN, producing soft phonon modes. It is found that the small shear modulus $G$ and $C_{44}$ limit the stability of WN, compared with WC. Our works provide valuable information for phonon transports in WC and WN, and motivate further experimental works to study their lattice thermal conductivities. ",https://doi.org/10.1063/1.5026644,1712.02599v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",FATNN: Fast and Accurate Ternary Neural Networks,1970,"  Ternary Neural Networks (TNNs) have received much attention due to being potentially orders of magnitude faster in inference, as well as more power efficient, than full-precision counterparts. However, 2 bits are required to encode the ternary representation with only 3 quantization levels leveraged. As a result, conventional TNNs have similar memory consumption and speed compared with the standard 2-bit models, but have worse representational capability. Moreover, there is still a significant gap in accuracy between TNNs and full-precision networks, hampering their deployment to real applications. To tackle these two challenges, in this work, we first show that, under some mild constraints, computational complexity of the ternary inner product can be reduced by a factor of 2. Second, to mitigate the performance gap, we elaborately design an implementation-dependent ternary quantization algorithm. The proposed framework is termed Fast and Accurate Ternary Neural Networks (FATNN). Experiments on image classification demonstrate that our FATNN surpasses the state-of-the-arts by a significant margin in accuracy. More importantly, speedup evaluation compared with various precisions is analyzed on several platforms, which serves as a strong benchmark for further research. ",Kein DOI-Link verfügbar,2008.05101v4,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Tempered Multifidelity Importance Sampling for Gravitational Wave   Parameter Estimation,1970,"  Estimating the parameters of compact binaries which coalesce and produce gravitational waves is a challenging Bayesian inverse problem. Gravitational-wave parameter estimation lies within the class of multifidelity problems, where a variety of models with differing assumptions, levels of fidelity, and computational cost are available for use in inference. In an effort to accelerate the solution of a Bayesian inverse problem, cheaper surrogates for the best models may be used to reduce the cost of likelihood evaluations when sampling the posterior. Importance sampling can then be used to reweight these samples to represent the true target posterior, incurring a reduction in the effective sample size. In cases when the problem is high dimensional, or when the surrogate model produces a poor approximation of the true posterior, this reduction in effective samples can be dramatic and render multifidelity importance sampling ineffective. We propose a novel method of tempered multifidelity importance sampling in order to remedy this issue. With this method the biasing distribution produced by the low-fidelity model is tempered, allowing for potentially better overlap with the target distribution. There is an optimal temperature which maximizes the efficiency in this setting, and we propose a low-cost strategy for approximating this optimal temperature using samples from the untempered distribution. In this paper, we motivate this method by applying it to Gaussian target and biasing distributions. Finally, we apply it to a series of problems in gravitational wave parameter estimation and demonstrate improved efficiencies when applying the method to real gravitational wave detections. ",Kein DOI-Link verfügbar,2405.19407v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Almost everywhere convergence of spectral sums for self-adjoint   operators,1970,"  Let $L$ be a non-negative self-adjoint operator acting on the space $L^2(X)$, where $X$ is a metric measure space. Let ${ L}=\int_0^{\infty} \lambda dE_{ L}({\lambda})$ be the spectral resolution of ${ L}$ and $S_R({ L})f=\int_0^R dE_{ L}(\lambda) f$ denote the spherical partial sums in terms of the resolution of ${ L}$. In this article we give a sufficient condition on $L$ such that $$ \lim_{R\rightarrow \infty} S_R({ L})f(x) =f(x),\ \ {\rm a.e.} $$ for any $f$ such that ${\rm log } (2+L) f\in L^2(X)$.   These results are applicable to large classes of operators including Dirichlet operators on smooth bounded domains, the Hermite operator and Schr\""odinger operators with inverse square potentials. ",Kein DOI-Link verfügbar,2109.01778v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Bring Your Own Character: A Holistic Solution for Automatic Facial   Animation Generation of Customized Characters,1970,"  Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications. ",Kein DOI-Link verfügbar,2402.13724v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology","Restriction estimates, sharp spectral multipliers and endpoint estimates   for Bochner-Riesz means",1970,"  We consider abstract non-negative self-adjoint operators on $L^2(X)$ which satisfy the finite speed propagation property for the corresponding wave equation. For such operators we introduce a restriction type condition which in the case of the standard Laplace operator is equivalent to $(p,2)$ restriction estimate of Stein and Tomas. Next we show that in the considered abstract setting our restriction type condition implies sharp spectral multipliers and endpoint estimates for the Bochner-Riesz summability. We also observe that this restriction estimate holds for operators satisfying dispersive or Strichartz estimates. We obtain new spectral multiplier results for several second order differential operators and recover some known results. Our examples include Schr\""odinger operators with inverse square potentials on $\RR^n$, the harmonic oscillator, elliptic operators on compact manifolds and Schr\""odinger operators on asymptotically conic manifolds. ",Kein DOI-Link verfügbar,1202.4052v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Power Allocation in Multi-User Cellular Networks: Deep Reinforcement   Learning Approaches,1970,"  The model-based power allocation algorithm has been investigated for decades, but it requires the mathematical models to be analytically tractable and it usually has high computational complexity. Recently, the data-driven model-free machine learning enabled approaches are being rapidly developed to obtain near-optimal performance with affordable computational complexity, and deep reinforcement learning (DRL) is regarded as of great potential for future intelligent networks. In this paper, the DRL approaches are considered for power control in multi-user wireless communication cellular networks. Considering the cross-cell cooperation, the off-line/on-line centralized training and the distributed execution, we present a mathematical analysis for the DRL-based top-level design. The concrete DRL design is further developed based on this foundation, and policy-based REINFORCE, value-based deep Q learning (DQL), actor-critic deep deterministic policy gradient (DDPG) algorithms are proposed. Simulation results show that the proposed data-driven approaches outperform the state-of-art model-based methods on sum-rate performance, with good generalization power and faster processing speed. Furthermore, the proposed DDPG outperforms the REINFORCE and DQL in terms of both sum-rate performance and robustness, and can be incorporated into existing resource allocation schemes due to its generality. ",Kein DOI-Link verfügbar,1901.07159v1,Yes,potent(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",A Versatile Software Systolic Execution Model for GPU Memory-Bound   Kernels,1970,"  This paper proposes a versatile high-performance execution model, inspired by systolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs. We formulate a systolic model that shifts partial sums by CUDA warp primitives for the computation. We also employ register files as a cache resource in order to operate the entire model efficiently. We demonstrate the effectiveness and versatility of the proposed model for a wide variety of stencil kernels that appear commonly in HPC, and also convolution kernels (increasingly important in deep learning workloads). Our algorithm outperforms the top reported state-of-the-art stencil implementations, including implementations with sophisticated temporal and spatial blocking techniques, on the two latest Nvidia architectures: Tesla V100 and P100. For 2D convolution of general filter sizes and shapes, our algorithm is on average 2.5x faster than Nvidia's NPP on V100 and P100 GPUs. ",https://doi.org/10.1145/3295500.3356162,1907.06154v2,Yes,versatile(1)
0000-0001-6122-0574,Peng Chen,"Zhejiang University, Zhejiang University of Technology",Data-Locality-Aware Task Assignment and Scheduling for Distributed Job   Executions,1970,"  This paper investigates a data-locality-aware task assignment and scheduling problem aimed at minimizing job completion times for distributed job executions. Without prior knowledge of future job arrivals, we propose an optimal balanced task assignment algorithm (OBTA) that minimizes the completion time of each arriving job. We significantly reduce OBTA's computational overhead by narrowing the search space of potential solutions. Additionally, we extend an approximate algorithm known as water-filling (WF) and nontrivially prove that its approximation factor equals the number of task groups in the job assignment. We also design a novel heuristic, replica-deletion (RD), which outperforms WF. To further reduce the completion time of each job, we expand the problem to include job reordering, where we adjust the order of outstanding jobs following the shortest-estimated-time-first policy. Extensive trace-driven evaluations validate the performance and efficiency of the proposed algorithms. ",Kein DOI-Link verfügbar,2407.08584v3,Yes,potent(1)
0009-0005-6435-6617,Alexander Heemels,Delft University of Technology,Gradient descent-based freeform optics design using algorithmic   differentiable non-sequential ray tracing,1970,"  Algorithmic differentiable ray tracing is a new paradigm that allows one to solve the forward problem of how light propagates through an optical system while obtaining gradients of the simulation results with respect to parameters specifying the optical system. Specifically, the use of algorithmically differentiable non-sequential ray tracing provides an opportunity in the field of illumination design. We demonstrate its potential by designing freeform lenses that project a prescribed irradiance distribution onto a plane. The challenge consists in finding a suitable surface geometry of the lens so that the light emitted by a light source is redistributed into a desired irradiance distribution. We discuss the crucial steps allowing the non-sequential ray tracer to be differentiable. The obtained gradients are used to optimize the geometry of the freeform, and we investigate the effectiveness of adding a multi-layer perceptron neural network to the optimization that outputs parameters defining the freeform lens. Lenses are designed for various sources such as collimated ray bundles or point sources, and finally, a grid of point sources approximating an extended source. The obtained lens designs are finally validated using the commercial non-sequential ray tracer LightTools. ",https://doi.org/10.1007/s11081-023-09841-9,2302.12031v1,Yes,potent(1)
0000-0003-3610-594X,Yuan Chen,Delft University of Technology,Representation Learning for Integrating Multi-domain Outcomes to   Optimize Individualized Treatments,1970,"  For mental disorders, patients' underlying mental states are non-observed latent constructs which have to be inferred from observed multi-domain measurements such as diagnostic symptoms and patient functioning scores. Additionally, substantial heterogeneity in the disease diagnosis between patients needs to be addressed for optimizing individualized treatment policy in order to achieve precision medicine. To address these challenges, we propose an integrated learning framework that can simultaneously learn patients' underlying mental states and recommend optimal treatments for each individual. This learning framework is based on the measurement theory in psychiatry for modeling multiple disease diagnostic measures as arising from the underlying causes (true mental states). It allows incorporation of the multivariate pre- and post-treatment outcomes as well as biological measures while preserving the invariant structure for representing patients' latent mental states. A multi-layer neural network is used to allow complex treatment effect heterogeneity. Optimal treatment policy can be inferred for future patients by comparing their potential mental states under different treatments given the observed multi-domain pre-treatment measurements. Experiments on simulated data and a real-world clinical trial data show that the learned treatment polices compare favorably to alternative methods on heterogeneous treatment effects, and have broad utilities which lead to better patient outcomes on multiple domains. ",Kein DOI-Link verfügbar,2011.00094v1,Yes,potent(1)
0000-0003-1170-9878,Zhenghao Chen,"Kyoto University, kyoto university",MedXChat: A Unified Multimodal Large Language Model Framework towards   CXRs Understanding and Generation,1970,"  Multimodal Large Language Models (MLLMs) have shown success in various general image processing tasks, yet their application in medical imaging is nascent, lacking tailored models. This study investigates the potential of MLLMs in improving the understanding and generation of Chest X-Rays (CXRs). We introduce MedXChat, a unified framework facilitating seamless interactions between medical assistants and users for diverse CXR tasks, including text report generation, visual question-answering (VQA), and Text-to-CXR generation. Our MLLMs using natural language as the input breaks task boundaries, maximally simplifying medical professional training by allowing diverse tasks within a single environment. For CXR understanding, we leverage powerful off-the-shelf visual encoders (e.g., ViT) and LLMs (e.g., mPLUG-Owl) to convert medical imagery into language-like features, and subsequently fine-tune our large pre-trained models for medical applications using a visual adapter network and a delta-tuning approach. For CXR generation, we introduce an innovative synthesis approach that utilizes instruction-following capabilities within the Stable Diffusion (SD) architecture. This technique integrates smoothly with the existing model framework, requiring no extra parameters, thereby maintaining the SD's generative strength while also bestowing upon it the capacity to render fine-grained medical images with high fidelity. Through comprehensive experiments, our model demonstrates exceptional cross-task adaptability, displaying adeptness across all three defined tasks. Our MedXChat model and the instruction dataset utilized in this research will be made publicly available to encourage further exploration in the field. ",Kein DOI-Link verfügbar,2312.02233v2,Yes,"innovative(1), potent(1)"
0000-0003-1170-9878,Zhenghao Chen,"Kyoto University, kyoto university",PoinTramba: A Hybrid Transformer-Mamba Framework for Point Cloud   Analysis,1970,"  Point cloud analysis has seen substantial advancements due to deep learning, although previous Transformer-based methods excel at modeling long-range dependencies on this task, their computational demands are substantial. Conversely, the Mamba offers greater efficiency but shows limited potential compared with Transformer-based methods. In this study, we introduce PoinTramba, a pioneering hybrid framework that synergies the analytical power of Transformer with the remarkable computational efficiency of Mamba for enhanced point cloud analysis. Specifically, our approach first segments point clouds into groups, where the Transformer meticulously captures intricate intra-group dependencies and produces group embeddings, whose inter-group relationships will be simultaneously and adeptly captured by efficient Mamba architecture, ensuring comprehensive analysis. Unlike previous Mamba approaches, we introduce a bi-directional importance-aware ordering (BIO) strategy to tackle the challenges of random ordering effects. This innovative strategy intelligently reorders group embeddings based on their calculated importance scores, significantly enhancing Mamba's performance and optimizing the overall analytical process. Our framework achieves a superior balance between computational efficiency and analytical performance by seamlessly integrating these advanced techniques, marking a substantial leap forward in point cloud analysis. Extensive experiments on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our approach, establishing a new state-of-the-art analysis benchmark on point cloud recognition. For the first time, this paradigm leverages the combined strengths of both Transformer and Mamba architectures, facilitating a new standard in the field. The code is available at https://github.com/xiaoyao3302/PoinTramba. ",Kein DOI-Link verfügbar,2405.15463v2,Yes,"innovative(1), meticulous(1), intricate(1), potent(1), meticulously(1)"
0000-0003-1170-9878,Zhenghao Chen,"Kyoto University, kyoto university",Peer-aided Repairer: Empowering Large Language Models to Repair Advanced   Student Assignments,1970,"  Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively ",Kein DOI-Link verfügbar,2404.01754v1,Yes,"notable(1), potent(1)"
0000-0002-4529-145X,Takeshi Matsumoto,"Kyoto University, Kyoto university",Investigation of meson masses for real and imaginary chemical potential,1970,"  We investigate chemical-potential ($\mu$) and temperature ($T$) dependence of scalar and pseudo-scalar meson masses for both real and imaginary $\mu$, using the Polyakov-loop extended Nambu--Jona-Lasinio (PNJL) model with three-flavor quarks. A three-flavor phase diagram is drawn in $\mu^2$-$T$ plane where positive (negative) $\mu^2$ corresponds to positive (imaginary) $\mu$. A critical surface is plotted as a function of light- and strange-quark current mass and $\mu^2$. We show that $\mu$-dependence of the six-quark Kobayashi-Maskawa-'t Hooft (KMT) determinant interaction originated in $U_\mathrm{A}(1)$ anomaly can be determined from lattice QCD data on $\eta'$ meson mass around $\mu =0$ and $\mu = i \pi T/3$ with $T$ slightly above the critical temperature at $\mu=0$ where the chiral symmetry is restored at $\mu=0$ but broken at $\mu =i \pi T/3$, if it is measured in future. ",https://doi.org/10.1016/j.physletb.2010.09.070,1004.0592v1,Yes,potent(1)
0000-0002-4529-145X,Takeshi Matsumoto,"Kyoto University, Kyoto university",Robust portfolio optimization model for electronic coupon allocation,1970,"  Currently, many e-commerce websites issue online/electronic coupons as an effective tool for promoting sales of various products and services. We focus on the problem of optimally allocating coupons to customers subject to a budget constraint on an e-commerce website. We apply a robust portfolio optimization model based on customer segmentation to the coupon allocation problem. We also validate the efficacy of our method through numerical experiments using actual data from randomly distributed coupons. Main contributions of our research are twofold. First, we handle six types of coupons, thereby making it extremely difficult to accurately estimate the difference in the effects of various coupons. Second, we demonstrate from detailed numerical results that the robust optimization model achieved larger uplifts of sales than did the commonly-used multiple-choice knapsack model and the conventional mean-variance optimization model. Our results open up great potential for robust portfolio optimization as an effective tool for practical coupon allocation. ",Kein DOI-Link verfügbar,2405.12865v1,Yes,potent(1)
0000-0003-0745-2125,Yuya Ishizawa,Kyoto University,N-body Simulations of Ring Formation Process around the Dwarf Planet   Haumea,1970,"  Haumea is the only known trans-Neptunian object with a ring. The ring is in a position that causes a 3:1 spin-orbit resonance with the rotational period of Haumea, which has a triaxial shape. The non-axisymmetric gravitational field around Haumea is thought to affect the dynamics of the ring; however, the process of ring formation has not been elucidated. In this study, we analyze in some detail a potential ring formation scenario for Haumea. We first calculated the gravitational field around the triaxial ellipsoid and estimated the distance at which an object revolving around Haumea can exist stably using simulation that incorporated the time-varying gravitational field. The results of this simulation showed that the trajectory of the object became unstable just inside its current ring position. Next, we analytically derived the Roche radius for a rigid body revolving around a triaxial ellipsoid and showed that the Roche radius could be near the current ring position. Furthermore, as a parameter study, we performed N-body simulations using the coefficient of rubble pile restitution as a variable. Results demonstrated that, according to numerous parameters, the position of the Roche radius was near the current position of Haumea's ring. Based on these findings, we can assume that there is a high possibility that the ring formed in the region between the boundary of the unstable region of the orbit and the Roche radius. The scenario presented in this study could help explain the process by which Haumea's ring formed. ",https://doi.org/10.3847/1538-4357/ab93bb,2005.06744v2,Yes,potent(1)
0000-0002-6139-3472,Matheus de Mello,Kyoto University,Ultrasoft classical systems at zero temperature,1970,"  At low temperatures ultrasoft particle systems develop interesting phases via the self-assembly of particle clusters. In this study we develop a general zero-temperature analysis fully characterizing the ground state of such models in two and three dimensions, considering the classical system with the restriction of a constant integer number of particles per cluster. We show that this methodology allows for an exact prediction of the actual density values at which the different phases emerge, including the zones of uneven cluster occupation, which are studied as coexistence regions of two pure phases. Beyond the method itself, designed to produce exact phase diagrams from general ultrasoft potentials, we reach analytical expressions for the energy and location of the different phases in the large occupancy limit. ",Kein DOI-Link verfügbar,2007.08676v1,Yes,potent(1)
